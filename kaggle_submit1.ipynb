{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n",
    "\n",
    "**Kaggle Assignment: **\n",
    "\n",
    "**Student Name: Jason Walker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## Description\n",
    "This is one of the projects from the course T81-855: Applications of Deep Learning at Washington University in St. Louis. All students must create a Kaggle account and submit a solution. Once you have submitted your solution entry log into Blackboard (at WUSTL) and submit a single file telling me your Kaggle name on the leaderboard (you do not need to register to Kaggle with your real name). This competition will be visible to the public, so there may be non-student submissions as well as student.\n",
    "\n",
    "The data set for this competition consists of a number of input columns that should be used to predict a stores sales. This is a regression problem. The inputs are a mixture of discrete and category values. The data set is from a simulation.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The evaluation pages describes how submissions will be scored and how students should format their submissions. The scores are in RMSE.\n",
    "Submission Format\n",
    "\n",
    "For every store in the dataset, submission files should contain a sales volume.\n",
    "\n",
    "The file should contain a header and have the following format:\n",
    "\n",
    "```\n",
    "100000,1.23\n",
    "100001,1.123\n",
    "100002,3.332\n",
    "100003,1.53\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The data contains data and costs for various office supplies. The data came from a simulation and do not directly correspond to any real-world items. See how well you can predict the cost of an item using the provided data. Feature engineering will likely help you. The *name* column may seem useless at first glance; however, it contains information that you can parse to help your predictions.\n",
    "File descriptions\n",
    "```\n",
    "    id - The identifier/primary key.\n",
    "    name - The name of this item.\n",
    "    manufacturer - The manufacturer.\n",
    "    pack - The number of items in this pack.\n",
    "    weight - The weight of a pack of these items.\n",
    "    height - The height of a pack of these items.\n",
    "    width - The width of a pack of these items.\n",
    "    length - The length of a pack of these items.\n",
    "    cost - The cost for this item pack. This is what you are to predict (the target). \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions\n",
    "\n",
    "You will see these at the top of every module and assignment.  These are simply a set of reusable functions that we will make use of.  Each of them will be explained as the semester progresses.  They are explained in greater detail as the course progresses.  Class 4 contains a complete overview of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n",
    "        \n",
    "# This function submits an assignment.  You can submit an assignment as much as you like, only the final\n",
    "# submission counts.  The paramaters are as follows:\n",
    "# data - Pandas dataframe output.\n",
    "# key - Your student key that was emailed to you.\n",
    "# no - The assignment class number, should be 1 through 1.\n",
    "# source_file - The full path to your Python or IPYNB file.  This must have \"_class1\" as part of its name.  \n",
    "# .             The number must match your assignment number.  For example \"_class2\" for class assignment #2.\n",
    "def submit(data,key,no,source_file=None):\n",
    "    if source_file is None and '__file__' not in globals(): raise Exception('Must specify a filename when a Jupyter notebook.')\n",
    "    if source_file is None: source_file = __file__\n",
    "    suffix = '_class{}'.format(no)\n",
    "    if suffix not in source_file: raise Exception('{} must be part of the filename.'.format(suffix))\n",
    "    with open(source_file, \"rb\") as image_file:\n",
    "        encoded_python = base64.b64encode(image_file.read()).decode('ascii')\n",
    "    ext = os.path.splitext(source_file)[-1].lower()\n",
    "    if ext not in ['.ipynb','.py']: raise Exception(\"Source file is {} must be .py or .ipynb\".format(ext))\n",
    "    r = requests.post(\"https://api.heatonresearch.com/assignment-submit\",\n",
    "        headers={'x-api-key':key}, json={'csv':base64.b64encode(data.to_csv(index=False).encode('ascii')).decode(\"ascii\"),\n",
    "        'assignment': no, 'ext':ext, 'py':encoded_python})\n",
    "    if r.status_code == 200:\n",
    "        print(\"Success: {}\".format(r.text))\n",
    "    else: print(\"Failure: {}\".format(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kaggle Code\n",
    "\n",
    "## Load Data and Encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwalker/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers\n",
    "\n",
    "path = './data'\n",
    "\n",
    "filename_test = os.path.join(path,\"test.csv\")\n",
    "filename_train = os.path.join(path,\"train.csv\")\n",
    "filename_sample = os.path.join(path,\"sample.csv\")\n",
    "filename_submit = os.path.join(path,\"submit.csv\")\n",
    "filename_checkpoint = os.path.join(path,\"checkpoint.hdf5\")\n",
    "\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "\n",
    "np.random.seed(42) # Uncomment this line to get the same shuffle each time\n",
    "df_train = df_train.reindex(np.random.permutation(df_train.index))\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Encode Features\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def extract_and_encode_features(df):\n",
    "    color_regex='(?P<color>red|blue|green|yellow|orange|pink|black|brown|white)'\n",
    "    df['color'] = df.name.str.extract(color_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    quality_regex='(?P<quality>generic|high\\squality)'\n",
    "    df['quality'] = df.name.str.extract(quality_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    size_regex='(?P<size>tiny|small|medium|large)'\n",
    "    df['size'] = df.name.str.extract(size_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    item_regex='(?P<item>paperclips|paperweights|ink\\spens|pencils|stapler|tablets|thumbtacks|post\\sit\\snotes)'\n",
    "    df['item'] = df.name.str.extract(item_regex, flags=re.IGNORECASE, expand=False)\n",
    "    \n",
    "    for column in ['pack','weight','height','width','length']:\n",
    "        missing_median(df,column)\n",
    "    \n",
    "    #df.insert(1,'surface_area',(df['height']*df['width']*df['length']).astype(int))\n",
    "    \n",
    "    ## encode numeric features\n",
    "    #for column in ['pack','weight','height','width','length','surface_area']:\n",
    "    #    encode_numeric_zscore(df,column)\n",
    "\n",
    "    # encode text/categorical features\n",
    "    for column in ['manufacturer','color','quality','size','item']:\n",
    "        encode_text_dummy(df,column)\n",
    "  \n",
    "extract_and_encode_features(df_train)\n",
    "\n",
    "ids_train = df_train['id']\n",
    "df_train.drop('id',1,inplace=True)\n",
    "\n",
    "names_train = df_train['name']\n",
    "df_train.drop('name',1,inplace=True)\n",
    "\n",
    "x,y = to_xy(df_train,'cost')\n",
    "\n",
    "# Used before KFold\n",
    "#x_train, x_test, y_train, y_test = train_test_split(    \n",
    "#    x, y, test_size=0.25, random_state=45)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression (Coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simple function to evaluate the coefficients of a regression\n",
    "%matplotlib inline    \n",
    "from IPython.display import display, HTML    \n",
    "\n",
    "def report_coef(names,coef,intercept):\n",
    "    r = pd.DataFrame( { 'coef': coef, 'positive': coef>=0  }, index = names )\n",
    "    r = r.sort_values(by=['coef'])\n",
    "    display(r)\n",
    "    print(\"Intercept: {}\".format(intercept))\n",
    "    r['coef'].plot(kind='barh', color=r['positive'].map({True: 'b', False: 'r'}))\n",
    "    \n",
    "# Create linear regression\n",
    "regressor = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# Fit/train linear regression\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "print(names)\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_[0,:],\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization (Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Create linear regression\n",
    "regressor = Lasso(random_state=0,alpha=0.01)\n",
    "\n",
    "# Fit/train LASSO\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_,\n",
    "  regressor.intercept_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization (Plot LassoCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lasso = Lasso(random_state=42)\n",
    "alphas = np.logspace(-8, 8, 10)\n",
    "\n",
    "scores = list()\n",
    "scores_std = list()\n",
    "\n",
    "n_folds = 3\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso.alpha = alpha\n",
    "    this_scores = cross_val_score(lasso, x, y, cv=n_folds, n_jobs=1)\n",
    "    scores.append(np.mean(this_scores))\n",
    "    scores_std.append(np.std(this_scores))\n",
    "\n",
    "scores, scores_std = np.array(scores), np.array(scores_std)\n",
    "\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "plt.semilogx(alphas, scores)\n",
    "\n",
    "# plot error lines showing +/- std. errors of the scores\n",
    "std_error = scores_std / np.sqrt(n_folds)\n",
    "\n",
    "plt.semilogx(alphas, scores + std_error, 'b--')\n",
    "plt.semilogx(alphas, scores - std_error, 'b--')\n",
    "\n",
    "# alpha=0.2 controls the translucency of the fill color\n",
    "plt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2)\n",
    "\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(scores), linestyle='--', color='.5')\n",
    "plt.xlim([alphas[0], alphas[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization (Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Create linear regression\n",
    "regressor = Ridge(alpha=1)\n",
    "\n",
    "# Fit/train Ridge\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_[0,:],\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Create linear regression\n",
    "regressor = ElasticNet(alpha=0.01, l1_ratio=0.1)\n",
    "\n",
    "# Fit/train LASSO\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_,\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 9890.1824 - val_loss: 5246.3600\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 5819.8733 - val_loss: 4713.2834\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 5355.4098 - val_loss: 4503.2949\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 4955.7231 - val_loss: 4227.3757\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 4686.3512 - val_loss: 3867.3678\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 4450.1424 - val_loss: 3778.2384\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 4349.5404 - val_loss: 3743.2444\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 4184.5648 - val_loss: 5050.9867\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 4141.9329 - val_loss: 3715.1846\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 3976.0876 - val_loss: 3370.1771\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 3938.1572 - val_loss: 3601.4291\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 3518.1698 - val_loss: 2781.3131\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 3683.6903 - val_loss: 2562.2446\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 3109.7832 - val_loss: 2352.0865\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 2794.6423 - val_loss: 2026.7348\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 2655.9586 - val_loss: 1793.3472\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 2163.1287 - val_loss: 2245.6119\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1948.0693 - val_loss: 1657.4879\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 1481.2980 - val_loss: 1200.7145\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1443.4332 - val_loss: 989.1956\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 1296.4820 - val_loss: 2043.3798\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 1156.7733 - val_loss: 656.4101\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 1017.1018 - val_loss: 1125.4670\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 1098.4434 - val_loss: 638.7101\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 833.3415 - val_loss: 543.3383\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 864.4766 - val_loss: 3044.7442\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 973.1481 - val_loss: 1987.3101\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 790.1015 - val_loss: 566.5931\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 808.3220 - val_loss: 514.8109\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 713.5795 - val_loss: 639.8134\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 849.2965 - val_loss: 551.3250\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 761.5211 - val_loss: 637.9977\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 626.8054 - val_loss: 701.0900\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 632.6853 - val_loss: 491.6377\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 761.7609 - val_loss: 391.0374\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 580.7803 - val_loss: 661.3169\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 710.3852 - val_loss: 388.2298\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 558.8131 - val_loss: 550.4985\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 682.9104 - val_loss: 420.3501\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 587.7096 - val_loss: 1111.4162\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 590.7215 - val_loss: 375.3879\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 561.4961 - val_loss: 388.2131\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 559.3199 - val_loss: 499.6170\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 542.4834 - val_loss: 452.9505\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 580.5165 - val_loss: 537.8653\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 494.9321 - val_loss: 343.6822\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 549.5047 - val_loss: 535.6973\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 534.1490 - val_loss: 330.6444\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 507.9782 - val_loss: 319.4071\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 588.1253 - val_loss: 2875.9947\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 617.1647 - val_loss: 415.3193\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 487.6798 - val_loss: 337.6219\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 466.4376 - val_loss: 327.2343\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 427.9616 - val_loss: 318.0918\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 468.1560 - val_loss: 292.7418\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 561.6406 - val_loss: 436.6787\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 436.0446 - val_loss: 456.4529\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 379.9153 - val_loss: 423.8557\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 425.5668 - val_loss: 312.9135\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 418.2784 - val_loss: 326.8423\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 472.3176 - val_loss: 926.9639\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 410.2043 - val_loss: 290.6905\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 440.0326 - val_loss: 322.4395\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 412.6469 - val_loss: 387.9948\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 407.9860 - val_loss: 298.1319\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 576.7880 - val_loss: 984.6596\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 405.8932 - val_loss: 264.6033\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 388.3324 - val_loss: 495.5255\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 392.8642 - val_loss: 420.6520\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 432.8826 - val_loss: 256.9794\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 386.1703 - val_loss: 429.1710\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 489.2334 - val_loss: 296.3956\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 436.8613 - val_loss: 291.8451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 370.0371 - val_loss: 258.3468\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 324.4351 - val_loss: 435.5742\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 382.2652 - val_loss: 380.6266\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 492.0190 - val_loss: 338.2193\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 454.8989 - val_loss: 754.7912\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 337.9105 - val_loss: 335.8611\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 333.0839 - val_loss: 350.6711\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 375.0729 - val_loss: 472.4104\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 382.3224 - val_loss: 416.9804\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 360.4064 - val_loss: 282.8989\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 344.5908 - val_loss: 253.3803\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 378.3597 - val_loss: 326.9122\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 410.7650 - val_loss: 438.0114\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 408.4067 - val_loss: 531.2910\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 353.6910 - val_loss: 270.8831\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 334.4580 - val_loss: 280.0066\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 356.7878 - val_loss: 231.3992\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 455.3636 - val_loss: 250.4102\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 406.2974 - val_loss: 504.9325\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 348.6374 - val_loss: 355.1922\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 287.7125 - val_loss: 252.7092\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 531.5452 - val_loss: 258.1276\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 351.8374 - val_loss: 275.4434\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 299.9612 - val_loss: 283.4174\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 320.5791 - val_loss: 225.8113\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 368.5297 - val_loss: 328.3105\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 348.0036 - val_loss: 366.8817\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 315.4870 - val_loss: 333.7041\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 312.0333 - val_loss: 476.6915\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 355.7880 - val_loss: 520.8056\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 376.5760 - val_loss: 264.5524\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 302.6209 - val_loss: 240.0850\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 367.7390 - val_loss: 536.2385\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 483.8072 - val_loss: 253.0663\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 354.0328 - val_loss: 344.1510\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 375.5180 - val_loss: 337.7656\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 324.7516 - val_loss: 232.2163\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 326.7572 - val_loss: 466.2662\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 315.4203 - val_loss: 461.6996\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 323.7427 - val_loss: 443.5123\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 327.7879 - val_loss: 326.3051\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 309.6693 - val_loss: 249.5551\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 368.5917 - val_loss: 224.8048\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 304.5603 - val_loss: 308.5165\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 257.7308 - val_loss: 267.5883\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 330.2126 - val_loss: 255.5202\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 337.2722 - val_loss: 321.0050\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 340.2253 - val_loss: 256.1446\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 262.5532 - val_loss: 540.3459\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 374.4149 - val_loss: 348.2387\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 368.8664 - val_loss: 249.4609\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 296.2558 - val_loss: 265.3876\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 363.0566 - val_loss: 226.0007\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 327.2341 - val_loss: 203.7807\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 309.7442 - val_loss: 288.0197\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 287.0217 - val_loss: 216.3148\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 290.1092 - val_loss: 238.8454\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 277.0648 - val_loss: 223.6112\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 281.9229 - val_loss: 212.6131\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 290.0988 - val_loss: 219.9619\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 329.4148 - val_loss: 389.2638\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 346.1971 - val_loss: 305.8032\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 263.1561 - val_loss: 241.0218\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 297.4873 - val_loss: 295.0844\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 327.8945 - val_loss: 304.1406\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 329.6840 - val_loss: 214.8688\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 278.7762 - val_loss: 200.3379\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 333.9570 - val_loss: 203.7970\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 250.4358 - val_loss: 226.7768\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 266.0421 - val_loss: 279.4514\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 239.9659 - val_loss: 201.8225\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 271.5288 - val_loss: 229.2962\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 284.6743 - val_loss: 290.9664\n",
      "Epoch 147/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 258.9568 - val_loss: 308.6782\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 293.5676 - val_loss: 274.8468\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 261.3405 - val_loss: 212.4020\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 266.0192 - val_loss: 284.1592\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 265.1651 - val_loss: 202.2034\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 270.6080 - val_loss: 266.0550\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 284.2711 - val_loss: 261.3678\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 320.9084 - val_loss: 228.8696\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 253.3033 - val_loss: 201.9169\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 295.0887 - val_loss: 260.5014\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 262.5391 - val_loss: 204.8560\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 229.8890 - val_loss: 319.8265\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 247.8551 - val_loss: 275.0362\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 258.1540 - val_loss: 215.0570\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 343.1533 - val_loss: 203.0873\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 290.5905 - val_loss: 203.4680\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 309.4521 - val_loss: 204.1870\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 245.6897 - val_loss: 199.3194\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 264.4785 - val_loss: 186.4778\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 259.2665 - val_loss: 200.8085\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 435.7543 - val_loss: 218.8138\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 255.7687 - val_loss: 304.9771\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 258.9411 - val_loss: 198.7681\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 319.4418 - val_loss: 491.5225\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 245.3241 - val_loss: 258.4930\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 229.7347 - val_loss: 227.5711\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 247.4047 - val_loss: 314.8335\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 277.1331 - val_loss: 200.1984\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 259.1228 - val_loss: 189.4746\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 232.7358 - val_loss: 297.1777\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 318.8278 - val_loss: 273.8667\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 235.2465 - val_loss: 219.1235\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 332.2999 - val_loss: 203.1735\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 288.9498 - val_loss: 265.5566\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 244.6156 - val_loss: 198.4491\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 260.8123 - val_loss: 259.4784\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 272.4666 - val_loss: 188.7635\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 262.2445 - val_loss: 219.4561\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 246.6369 - val_loss: 201.6702\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 255.7972 - val_loss: 253.9762\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 340.3527 - val_loss: 230.8380\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 240.5956 - val_loss: 197.2272\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 218.4151 - val_loss: 682.7306\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 264.1159 - val_loss: 212.4569\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 273.0604 - val_loss: 182.8363\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 240.0309 - val_loss: 246.2890\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 257.5262 - val_loss: 229.4031\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 220.2325 - val_loss: 435.1478\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 261.9540 - val_loss: 196.7517\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 220.4127 - val_loss: 210.3329\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 225.6228 - val_loss: 210.5130\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 236.2695 - val_loss: 303.6633\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 234.0643 - val_loss: 195.3356\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 446.8050 - val_loss: 548.9450\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 354.2656 - val_loss: 238.8627\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 324.8150 - val_loss: 210.7223\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 261.3967 - val_loss: 202.3374\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 301.7519 - val_loss: 910.6937\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 263.0415 - val_loss: 197.3149\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 270.1957 - val_loss: 205.5205\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 248.2040 - val_loss: 192.9684\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 283.1362 - val_loss: 215.4271\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 317.8476 - val_loss: 204.1312\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 218.3991 - val_loss: 276.9508\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 257.2203 - val_loss: 598.3986\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 292.2653 - val_loss: 234.3028\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 209.2436 - val_loss: 206.4633\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 237.0139 - val_loss: 199.1966\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 227.3937 - val_loss: 317.8471\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 277.4775 - val_loss: 178.7394\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 235.4841 - val_loss: 223.8206\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 254.3467 - val_loss: 226.8525\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 243.9521 - val_loss: 185.0570\n",
      "Epoch 220/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 390.9184 - val_loss: 271.7801\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 330.6633 - val_loss: 226.0698\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 305.7132 - val_loss: 203.3361\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 247.2867 - val_loss: 232.3582\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 312.1656 - val_loss: 198.8888\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 241.5766 - val_loss: 237.4470\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 234.7928 - val_loss: 296.8653\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 245.7456 - val_loss: 250.4706\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 306.6969 - val_loss: 226.2948\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 242.6711 - val_loss: 227.3404\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 262.2199 - val_loss: 272.1632\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 251.6891 - val_loss: 225.0051\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 275.1056 - val_loss: 253.4406\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 270.6739 - val_loss: 237.0639\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 230.3231 - val_loss: 185.5899\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 255.2373 - val_loss: 179.2798\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 226.8524 - val_loss: 201.5374\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 249.7971 - val_loss: 241.0519\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 228.8768 - val_loss: 196.6140\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 232.1907 - val_loss: 649.9231\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 244.7889 - val_loss: 194.3974\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 292.4282 - val_loss: 243.4139\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 206.7244 - val_loss: 288.3661\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 229.9241 - val_loss: 398.9839\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 233.7981 - val_loss: 222.1587\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 251.4580 - val_loss: 320.5090\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 229.7947 - val_loss: 312.6251\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 219.3489 - val_loss: 284.3649\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 268.9810 - val_loss: 183.9823\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 215.1095 - val_loss: 302.7639\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 228.6755 - val_loss: 323.0263\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 220.5183 - val_loss: 221.7048\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 254.9694 - val_loss: 218.4825\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 238.0828 - val_loss: 198.6423\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 367.0175 - val_loss: 362.6358\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 277.3519 - val_loss: 471.3426\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 259.4794 - val_loss: 182.5730\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 214.3304 - val_loss: 178.8802\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 232.6110 - val_loss: 183.0143\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 208.9506 - val_loss: 175.3188\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 246.6731 - val_loss: 445.4349\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 247.3585 - val_loss: 210.6361\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 206.1823 - val_loss: 194.4473\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 212.9914 - val_loss: 208.4052\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 261.3785 - val_loss: 227.3390\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 231.0248 - val_loss: 190.7825\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 247.0935 - val_loss: 185.1549\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 233.6107 - val_loss: 187.1192\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 280.0658 - val_loss: 189.2639\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 217.6239 - val_loss: 270.7850\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 244.8193 - val_loss: 177.9732\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 200.3210 - val_loss: 225.4104\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 285.5656 - val_loss: 195.3981\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 219.8723 - val_loss: 345.5913\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 221.3152 - val_loss: 188.4467\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 200.5954 - val_loss: 191.7876\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 217.2729 - val_loss: 182.2777\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 239.4144 - val_loss: 460.1640\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 213.8453 - val_loss: 196.3086\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 225.728 - 0s 44us/step - loss: 224.5954 - val_loss: 241.5679\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 206.9757 - val_loss: 190.9840\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 296.5635 - val_loss: 208.9918\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 236.0927 - val_loss: 247.9114\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 237.0557 - val_loss: 234.0017\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 225.9113 - val_loss: 197.9672\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 214.6714 - val_loss: 221.1932\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 279.9843 - val_loss: 460.8893\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 209.6972 - val_loss: 185.7736\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 232.3863 - val_loss: 170.1831\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 246.2126 - val_loss: 255.9112\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 204.3525 - val_loss: 203.6091\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 318.7021 - val_loss: 202.4577\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 278.5787 - val_loss: 169.2143\n",
      "Epoch 293/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 189.8544 - val_loss: 173.2056\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 255.9976 - val_loss: 189.5207\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 205.6169 - val_loss: 231.5832\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 234.4024 - val_loss: 232.4367\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 244.9484 - val_loss: 248.5802\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 212.7049 - val_loss: 246.0919\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 343.3253 - val_loss: 215.4778\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 214.8449 - val_loss: 182.8334\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 214.0561 - val_loss: 178.9881\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 196.5455 - val_loss: 287.8382\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 201.4914 - val_loss: 203.6299\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 220.6853 - val_loss: 193.1325\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 309.4110 - val_loss: 238.5352\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 287.3367 - val_loss: 214.8564\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 246.9012 - val_loss: 172.0399\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 255.1229 - val_loss: 179.3810\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 209.3023 - val_loss: 175.1797\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 245.9086 - val_loss: 275.5810\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 227.3156 - val_loss: 259.7525\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 200.1853 - val_loss: 333.2445\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 239.0220 - val_loss: 234.0237\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 285.1653 - val_loss: 182.3689\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 248.8417 - val_loss: 176.0635\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 234.3472 - val_loss: 274.0623\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 271.9857 - val_loss: 394.9008\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 219.8439 - val_loss: 176.2041\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 214.7637 - val_loss: 250.6704\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 218.4807 - val_loss: 165.8097\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 240.9740 - val_loss: 283.5801\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 255.8156 - val_loss: 243.7426\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 318.0250 - val_loss: 173.1840\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 217.9816 - val_loss: 188.0839\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 211.0782 - val_loss: 190.4075\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 205.4248 - val_loss: 165.4924\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 208.8992 - val_loss: 304.4679\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 236.4455 - val_loss: 258.1436\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 192.4469 - val_loss: 197.8291\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 235.7515 - val_loss: 176.4866\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 218.6731 - val_loss: 189.3510\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 215.0756 - val_loss: 227.6712\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 236.0268 - val_loss: 308.0175\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 231.2367 - val_loss: 292.2665\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 237.6927 - val_loss: 169.0337\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 199.7774 - val_loss: 169.3652\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 728.4349 - val_loss: 444.8853\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 457.3283 - val_loss: 361.2143\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 373.1418 - val_loss: 290.8869\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 359.7542 - val_loss: 230.0848\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 355.5965 - val_loss: 275.1255\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 278.3578 - val_loss: 230.3246\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 273.5063 - val_loss: 296.5454\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 238.0672 - val_loss: 222.8158\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 257.3592 - val_loss: 225.1560\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 234.1976 - val_loss: 279.3899\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 244.3940 - val_loss: 181.4812\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 262.2766 - val_loss: 262.8206\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 225.0272 - val_loss: 324.8909\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 289.8818 - val_loss: 195.3139\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 221.2529 - val_loss: 202.0393\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 241.0847 - val_loss: 232.6824\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 317.6322 - val_loss: 217.9610\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 208.5104 - val_loss: 182.8100\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 216.9418 - val_loss: 171.5326\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 238.3674 - val_loss: 189.6029\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 233.7422 - val_loss: 191.1122\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 238.7227 - val_loss: 194.4497\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 284.1324 - val_loss: 174.2132\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 211.8615 - val_loss: 174.9739\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 192.5329 - val_loss: 168.4465\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 210.5826 - val_loss: 231.0718\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 200.4143 - val_loss: 164.6931\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 429.8921 - val_loss: 198.0410\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 225.3544 - val_loss: 166.2193\n",
      "Epoch 366/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 44us/step - loss: 204.7434 - val_loss: 217.0384\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 204.9649 - val_loss: 245.3913\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 205.5522 - val_loss: 188.6115\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 206.9873 - val_loss: 163.3753\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 218.4083 - val_loss: 228.7337\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 216.9494 - val_loss: 264.3529\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 195.4150 - val_loss: 216.3882\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 297.8994 - val_loss: 437.1089\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 224.9773 - val_loss: 188.6566\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 219.3205 - val_loss: 206.2088\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 191.9717 - val_loss: 160.1520\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 224.4304 - val_loss: 169.4589\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 203.7132 - val_loss: 161.0203\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 253.8439 - val_loss: 351.7482\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 189.7776 - val_loss: 164.4000\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 236.1787 - val_loss: 179.3147\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 225.3433 - val_loss: 1034.4080\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 307.8596 - val_loss: 159.8521\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 188.7856 - val_loss: 168.8418\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 311.6708 - val_loss: 171.2263\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 205.7645 - val_loss: 161.1776\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 264.5767 - val_loss: 162.3697\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 178.5437 - val_loss: 208.2394\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 212.3386 - val_loss: 180.5188\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 222.3499 - val_loss: 164.0653\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 240.0115 - val_loss: 159.5186\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 219.9383 - val_loss: 363.2125\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 247.8897 - val_loss: 223.9451\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 198.1282 - val_loss: 194.0491\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 206.2970 - val_loss: 743.8084\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 272.4086 - val_loss: 165.7544\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 221.9775 - val_loss: 214.2081\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 203.7866 - val_loss: 203.3294\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 202.4786 - val_loss: 165.8071\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 195.3321 - val_loss: 161.8026\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 191.1606 - val_loss: 169.9943\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 200.0469 - val_loss: 188.4996\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 181.2056 - val_loss: 281.6236\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 213.4114 - val_loss: 178.2036\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 210.3639 - val_loss: 169.4494\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 188.6674 - val_loss: 163.4567\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 206.3429 - val_loss: 170.3653\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 200.7229 - val_loss: 195.2979\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 232.5847 - val_loss: 155.2097\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 231.1306 - val_loss: 449.4416\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 192.1664 - val_loss: 153.9699\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 193.3584 - val_loss: 166.7268\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 238.1972 - val_loss: 499.5924\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 192.3753 - val_loss: 168.7068\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 303.2420 - val_loss: 310.0116\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 193.8768 - val_loss: 166.3805\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 197.8016 - val_loss: 303.6582\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 187.7841 - val_loss: 211.6078\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 210.0951 - val_loss: 172.2382\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 298.4583 - val_loss: 485.8767\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 183.3044 - val_loss: 160.5533\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 220.9561 - val_loss: 164.7459\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 203.1713 - val_loss: 164.5744\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 213.4150 - val_loss: 216.4216\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 215.8774 - val_loss: 212.0291\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 187.2385 - val_loss: 160.4214\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 165.0113 - val_loss: 156.5439\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 191.6100 - val_loss: 209.1773\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 222.8456 - val_loss: 152.7981\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 176.9188 - val_loss: 185.4935\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 207.2598 - val_loss: 178.3760\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 192.1050 - val_loss: 285.1788\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 204.9959 - val_loss: 222.6833\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 208.0972 - val_loss: 382.7331\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 203.3335 - val_loss: 175.4623\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 166.4630 - val_loss: 177.2047\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 201.8989 - val_loss: 205.3887\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 181.8188 - val_loss: 150.6153\n",
      "Epoch 439/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 208.1991 - val_loss: 165.6935\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 192.2525 - val_loss: 153.8304\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 194.1847 - val_loss: 191.8448\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 252.0974 - val_loss: 183.8621\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 175.6624 - val_loss: 171.7246\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 199.7218 - val_loss: 161.0701\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 281.1569 - val_loss: 971.1938\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 240.9424 - val_loss: 210.8946\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 191.3192 - val_loss: 168.8027\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 186.2249 - val_loss: 177.6453\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 203.6717 - val_loss: 160.3800\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 192.0286 - val_loss: 338.0611\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 183.9120 - val_loss: 193.9885\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 179.3934 - val_loss: 157.1423\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 173.2772 - val_loss: 183.8563\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 198.2406 - val_loss: 185.0791\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 217.3891 - val_loss: 165.5257\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 212.6776 - val_loss: 157.5765\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 180.3412 - val_loss: 154.3657\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 182.2736 - val_loss: 163.6541\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 207.1531 - val_loss: 224.4078\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 188.9900 - val_loss: 231.2163\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 182.3589 - val_loss: 164.6826\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 163.1889 - val_loss: 160.0985\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 198.0179 - val_loss: 218.5020\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 190.6659 - val_loss: 159.7274\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 198.0062 - val_loss: 207.4123\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 160.6473 - val_loss: 162.0692\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 241.7781 - val_loss: 565.6929\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 211.0665 - val_loss: 151.8871\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 260.8146 - val_loss: 188.5152\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 220.7923 - val_loss: 262.5840\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 200.8095 - val_loss: 197.9068\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 166.2043 - val_loss: 159.4367\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 179.3964 - val_loss: 165.6034\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 323.8126 - val_loss: 190.6879\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 211.1945 - val_loss: 209.8941\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 190.6208 - val_loss: 159.9627\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 191.3480 - val_loss: 224.5285\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 206.7700 - val_loss: 151.6611\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 178.0604 - val_loss: 166.7343\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 188.1712 - val_loss: 214.4615\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 201.0480 - val_loss: 169.6716\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 212.0204 - val_loss: 152.7692\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 175.0506 - val_loss: 160.3816\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 191.5066 - val_loss: 163.5332\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 173.0575 - val_loss: 171.3920\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 221.3969 - val_loss: 176.1265\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 184.5608 - val_loss: 250.5599\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 203.8210 - val_loss: 210.7030\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 177.1437 - val_loss: 311.5870\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 234.9499 - val_loss: 165.6366\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 184.3919 - val_loss: 223.2133\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 183.5667 - val_loss: 158.5365\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 194.7486 - val_loss: 161.3466\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 193.2991 - val_loss: 173.6612\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 186.2481 - val_loss: 153.6645\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 223.5822 - val_loss: 341.6408\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 196.1247 - val_loss: 165.4362\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 185.3233 - val_loss: 266.4862\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 160.2201 - val_loss: 162.3264\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 250.0288 - val_loss: 224.0315\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 214.9766 - val_loss: 160.0441\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 188.2904 - val_loss: 184.5941\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 185.0582 - val_loss: 155.2326\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 166.6009 - val_loss: 272.3970\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 169.7171 - val_loss: 243.7956\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 182.5095 - val_loss: 195.1242\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 178.2102 - val_loss: 180.6704\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 179.1627 - val_loss: 197.5035\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 197.6119 - val_loss: 150.8161\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 178.6409 - val_loss: 234.7277\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 226.7931 - val_loss: 207.1900\n",
      "Epoch 512/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 200.7365 - val_loss: 172.1090\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 194.9020 - val_loss: 156.9362\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 206.8699 - val_loss: 189.5843\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 185.1953 - val_loss: 196.7456\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 172.6710 - val_loss: 177.8077\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 173.1879 - val_loss: 150.3064\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 170.8524 - val_loss: 185.3837\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 227.5789 - val_loss: 367.5896\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 187.2745 - val_loss: 171.0437\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 262.9937 - val_loss: 178.7421\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 251.9714 - val_loss: 156.5238\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 168.9285 - val_loss: 163.1670\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 173.7194 - val_loss: 159.7150\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 164.6387 - val_loss: 145.8592\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 157.7390 - val_loss: 169.4926\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 171.3649 - val_loss: 160.2410\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 173.1555 - val_loss: 173.9653\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 189.4237 - val_loss: 232.5245\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 172.2422 - val_loss: 158.8820\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 188.4267 - val_loss: 210.0784\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 322.3349 - val_loss: 266.9200\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 164.6563 - val_loss: 148.1059\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 193.4549 - val_loss: 159.2457\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 170.8342 - val_loss: 159.9139\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 252.7114 - val_loss: 443.7335\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 216.6286 - val_loss: 195.0403\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 179.8580 - val_loss: 157.0402\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 175.2311 - val_loss: 153.2286\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 167.3480 - val_loss: 164.0639\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 208.5230 - val_loss: 297.5221\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 188.3936 - val_loss: 208.2515\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 234.3460 - val_loss: 169.1536\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 157.4676 - val_loss: 186.9190\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 182.9541 - val_loss: 256.6119\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 170.9708 - val_loss: 154.2525\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 270.1614 - val_loss: 158.4182\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 178.5041 - val_loss: 148.2927\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 171.5909 - val_loss: 154.1475\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 165.2086 - val_loss: 158.0542\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 175.9133 - val_loss: 150.3659\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 210.9735 - val_loss: 150.1635\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 166.3527 - val_loss: 147.3329\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 191.0517 - val_loss: 211.6816\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 168.7899 - val_loss: 148.1915\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 162.2743 - val_loss: 146.2712\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 196.7683 - val_loss: 177.9703\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 169.8400 - val_loss: 191.8451\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 195.9249 - val_loss: 199.0418\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 178.5008 - val_loss: 383.3042\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 266.9015 - val_loss: 178.2647\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 194.7849 - val_loss: 200.3021\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 163.6084 - val_loss: 170.1688\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 166.9020 - val_loss: 157.0293\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 197.8911 - val_loss: 155.3318\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 167.8846 - val_loss: 317.2915\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 174.0419 - val_loss: 157.0502\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 177.8431 - val_loss: 371.9218\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 184.2366 - val_loss: 885.0368\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 414.0553 - val_loss: 175.3990\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 201.0986 - val_loss: 174.5922\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 202.9063 - val_loss: 243.5102\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 215.3386 - val_loss: 173.5137\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 176.6318 - val_loss: 172.2883\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 179.4473 - val_loss: 253.0491\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 199.8044 - val_loss: 150.3623\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 179.8287 - val_loss: 149.2176\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 172.1626 - val_loss: 148.9130\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 172.9924 - val_loss: 151.7367\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 241.9807 - val_loss: 157.5237\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 171.8468 - val_loss: 154.8204\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 197.1471 - val_loss: 226.9545\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 191.7319 - val_loss: 151.7926\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 169.0107 - val_loss: 174.5806\n",
      "Epoch 585/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 165.4037 - val_loss: 150.4951\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 214.0526 - val_loss: 184.7980\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 178.4836 - val_loss: 172.5306\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 165.1775 - val_loss: 180.0963\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 200.4977 - val_loss: 276.2246\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 184.7858 - val_loss: 149.2874\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 156.9260 - val_loss: 156.6484\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 162.0516 - val_loss: 151.4366\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 168.8133 - val_loss: 189.2212\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 188.4123 - val_loss: 146.8704\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 181.9309 - val_loss: 157.3623\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 191.3538 - val_loss: 154.6798\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 166.1142 - val_loss: 179.1963\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 168.2875 - val_loss: 161.0337\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 174.8515 - val_loss: 186.0610\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 170.4062 - val_loss: 148.3564\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 205.6421 - val_loss: 180.2257\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 164.0826 - val_loss: 149.7974\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 161.4980 - val_loss: 159.2851\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 169.8869 - val_loss: 177.0978\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 187.0183 - val_loss: 146.3811\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 157.5612 - val_loss: 154.8358\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 160.6987 - val_loss: 223.8455\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 165.5292 - val_loss: 223.8006\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 224.5591 - val_loss: 179.5716\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 178.8102 - val_loss: 151.9791\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 196.2669 - val_loss: 168.4492\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 190.9359 - val_loss: 194.4498\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 186.7009 - val_loss: 184.3162\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 160.7825 - val_loss: 212.4334\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 182.8603 - val_loss: 162.3072\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 162.5145 - val_loss: 146.5983\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 159.8712 - val_loss: 172.9634\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 176.8724 - val_loss: 165.1156\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 240.9172 - val_loss: 154.8100\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 156.0606 - val_loss: 153.8801\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 163.7388 - val_loss: 177.9242\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 193.6759 - val_loss: 223.6911\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 171.8251 - val_loss: 169.3621\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 148.3361 - val_loss: 157.7515\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 270.2464 - val_loss: 208.0415\n",
      "Epoch 00625: early stopping\n",
      "Fold score (RMSE): 14.016919136047363\n",
      "Fold #2\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 34763.3213 - val_loss: 6082.8985\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 5581.2170 - val_loss: 5667.6258\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 5266.8322 - val_loss: 5371.3217\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 5025.7488 - val_loss: 5481.2040\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 4881.6483 - val_loss: 5328.8202\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 4582.8212 - val_loss: 4942.6956\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 4514.6927 - val_loss: 5001.1247\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 4210.0432 - val_loss: 4400.3235\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 4097.3653 - val_loss: 4430.8067\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 4037.3542 - val_loss: 4137.3286\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 3855.5599 - val_loss: 4159.5474\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 3706.5016 - val_loss: 5068.4086\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 3672.9338 - val_loss: 3626.8529\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 3540.8423 - val_loss: 3474.2288\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 3492.2783 - val_loss: 3352.1409\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 3343.9182 - val_loss: 3194.4476\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 3052.9215 - val_loss: 2908.8303\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 3010.7066 - val_loss: 3419.5429\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 2654.3147 - val_loss: 4477.6992\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 2537.2170 - val_loss: 2227.2756\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 2315.0996 - val_loss: 2053.2169\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 1999.2241 - val_loss: 1842.7535\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 1872.9058 - val_loss: 2203.3737\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 1584.1651 - val_loss: 1497.4435\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 1589.4612 - val_loss: 1417.4603\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 1397.9097 - val_loss: 1112.8763\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 1205.5055 - val_loss: 936.9644\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1211.5932 - val_loss: 850.2911\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 1094.7909 - val_loss: 841.3339\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 962.7547 - val_loss: 872.1021\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 891.7779 - val_loss: 724.1109\n",
      "Epoch 32/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 875.2039 - val_loss: 716.1273\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 766.1137 - val_loss: 750.6054\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 799.4131 - val_loss: 567.8421\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 748.9074 - val_loss: 598.0723\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 732.2819 - val_loss: 593.4749\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 827.0340 - val_loss: 1148.0481\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 722.3187 - val_loss: 1230.3106\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 717.5948 - val_loss: 800.3081\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 733.7860 - val_loss: 1365.9587\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 619.8869 - val_loss: 584.6258\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 638.3760 - val_loss: 535.1280\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 781.0965 - val_loss: 652.4376\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 591.0509 - val_loss: 702.7309\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 576.7416 - val_loss: 502.6913\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 578.3807 - val_loss: 543.8409\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 582.0232 - val_loss: 445.9975\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 593.5263 - val_loss: 539.6871\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 571.2154 - val_loss: 501.3894\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 511.2982 - val_loss: 408.7503\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 561.1858 - val_loss: 413.5260\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 515.6015 - val_loss: 415.0231\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 505.4874 - val_loss: 440.4407\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 544.1983 - val_loss: 346.4079\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 510.8431 - val_loss: 607.6831\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 554.4718 - val_loss: 476.4432\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 493.4541 - val_loss: 790.7622\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 482.9114 - val_loss: 666.2048\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 519.0304 - val_loss: 525.6823\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 504.9237 - val_loss: 399.2072\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 541.1937 - val_loss: 367.0477\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 609.6809 - val_loss: 525.3454\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 488.6339 - val_loss: 723.6734\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 441.3715 - val_loss: 342.7758\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 427.7452 - val_loss: 343.4396\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 454.4625 - val_loss: 510.4696\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 482.0145 - val_loss: 391.4921\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 543.4922 - val_loss: 424.0693\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 470.1927 - val_loss: 350.7925\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 414.4995 - val_loss: 430.1112\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 471.9942 - val_loss: 319.9372\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 456.6917 - val_loss: 467.3705\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 458.3532 - val_loss: 338.5709\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 417.6237 - val_loss: 322.0671\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 445.4249 - val_loss: 333.1896\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 451.9897 - val_loss: 802.1866\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 470.2755 - val_loss: 466.7589\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 419.1416 - val_loss: 315.7150\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 459.8767 - val_loss: 389.5171\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 369.2199 - val_loss: 485.2932\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 407.9231 - val_loss: 329.3662\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 407.6876 - val_loss: 598.6334\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 443.8636 - val_loss: 372.3826\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 419.4525 - val_loss: 1105.6898\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 522.7421 - val_loss: 872.4216\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 461.5335 - val_loss: 375.8313\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 394.2830 - val_loss: 339.0690\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 386.9486 - val_loss: 303.8935\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 380.0319 - val_loss: 308.4251\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 469.6856 - val_loss: 542.6299\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 419.5539 - val_loss: 404.9086\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 446.6862 - val_loss: 737.1612\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 370.3412 - val_loss: 667.0754\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 388.7128 - val_loss: 614.5764\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 418.0118 - val_loss: 437.7219\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 348.8936 - val_loss: 348.5478\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 433.5588 - val_loss: 471.2317\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 376.2371 - val_loss: 350.1482\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 350.8717 - val_loss: 265.1889\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 321.5987 - val_loss: 298.3952\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 415.6155 - val_loss: 297.5367\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 332.4449 - val_loss: 290.2142\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 386.3666 - val_loss: 471.5253\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 359.4347 - val_loss: 267.9927\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 427.1482 - val_loss: 269.7733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 337.9633 - val_loss: 269.3334\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 419.8882 - val_loss: 487.9683\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 334.4412 - val_loss: 283.3508\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 393.7322 - val_loss: 301.1648\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 369.3440 - val_loss: 432.3541\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 331.8804 - val_loss: 282.6418\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 318.4591 - val_loss: 257.0139\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 343.3268 - val_loss: 368.4074\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 335.0584 - val_loss: 327.6919\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 350.4655 - val_loss: 366.3907\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 298.0292 - val_loss: 257.5958\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 321.0785 - val_loss: 264.3671\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 295.8445 - val_loss: 846.6873\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 410.5214 - val_loss: 303.1533\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 335.1310 - val_loss: 273.5987\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 361.0403 - val_loss: 495.5090\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 316.2739 - val_loss: 298.1870\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 342.6557 - val_loss: 258.5032\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 320.4041 - val_loss: 278.8475\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 313.3923 - val_loss: 294.9846\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 320.2538 - val_loss: 258.1996\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 313.3449 - val_loss: 1134.9125\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 338.7010 - val_loss: 406.8546\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 304.6678 - val_loss: 252.6759\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 344.3983 - val_loss: 291.5029\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 315.8862 - val_loss: 249.6499\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 295.5821 - val_loss: 287.9258\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 316.3951 - val_loss: 276.6304\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 367.2503 - val_loss: 482.7235\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 367.7655 - val_loss: 244.8065\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 345.3739 - val_loss: 380.8047\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 301.5768 - val_loss: 253.0525\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 323.8351 - val_loss: 220.4286\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 294.7338 - val_loss: 241.5517\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 281.8396 - val_loss: 282.0151\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 307.6829 - val_loss: 410.6269\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 314.5578 - val_loss: 237.1015\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 298.6872 - val_loss: 278.9132\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 288.7291 - val_loss: 314.8344\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 329.1571 - val_loss: 277.4854\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 265.4619 - val_loss: 303.6620\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 259.5720 - val_loss: 446.3579\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 327.7075 - val_loss: 267.1644\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 322.1922 - val_loss: 469.9780\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 333.2937 - val_loss: 452.5114\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 331.4823 - val_loss: 584.1537\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 299.7043 - val_loss: 218.1242\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 307.4648 - val_loss: 345.4916\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 287.9629 - val_loss: 230.1245\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 251.3079 - val_loss: 356.0959\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 266.4532 - val_loss: 227.8670\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 273.0725 - val_loss: 221.4246\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 260.7297 - val_loss: 296.8123\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 272.7804 - val_loss: 223.6865\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 276.0877 - val_loss: 356.0522\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 377.3784 - val_loss: 255.7176\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 295.5606 - val_loss: 258.5058\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 343.7359 - val_loss: 311.1017\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 257.0360 - val_loss: 336.9131\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 278.4468 - val_loss: 254.2451\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 258.2949 - val_loss: 223.6860\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 265.2461 - val_loss: 307.1178\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 270.8688 - val_loss: 377.3973\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 417.2827 - val_loss: 454.3767\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 282.0180 - val_loss: 343.7618\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 436.1189 - val_loss: 227.4554\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 237.8551 - val_loss: 254.9217\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 382.1824 - val_loss: 348.8930\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 242.6220 - val_loss: 281.0280\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 227.9751 - val_loss: 233.0791\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 278.8302 - val_loss: 223.3743\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 255.9710 - val_loss: 460.6972\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 281.0231 - val_loss: 216.6229\n",
      "Epoch 179/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 44us/step - loss: 240.8094 - val_loss: 306.6000\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 220.2883 - val_loss: 222.2734\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 243.5771 - val_loss: 224.5335\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 241.8149 - val_loss: 226.6517\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 235.2497 - val_loss: 480.9431\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 288.9364 - val_loss: 257.0847\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 288.9191 - val_loss: 207.2589\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 486.8410 - val_loss: 498.4297\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 279.2450 - val_loss: 254.2674\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 248.5468 - val_loss: 292.8455\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 261.6933 - val_loss: 221.4177\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 259.8581 - val_loss: 244.3329\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 230.0983 - val_loss: 260.9585\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 280.3651 - val_loss: 854.8674\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 299.8152 - val_loss: 1231.5639\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 258.6250 - val_loss: 319.6210\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 295.5557 - val_loss: 248.3867\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 230.6395 - val_loss: 213.3208\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 292.6524 - val_loss: 270.6855\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 285.3085 - val_loss: 205.9938\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 216.1647 - val_loss: 230.6297\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 269.1234 - val_loss: 204.2081\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 219.7006 - val_loss: 225.8872\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 263.8312 - val_loss: 280.7592\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 236.3696 - val_loss: 242.7799\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 227.7850 - val_loss: 215.3770\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 255.8826 - val_loss: 206.3369\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 220.4699 - val_loss: 213.1617\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 257.8237 - val_loss: 212.1080\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 529.2485 - val_loss: 229.9804\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 246.2379 - val_loss: 264.8796\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 235.8272 - val_loss: 202.2512\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 249.0436 - val_loss: 413.3812\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 285.7392 - val_loss: 940.8819\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 270.9424 - val_loss: 268.8570\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 208.0440 - val_loss: 258.6058\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 300.8228 - val_loss: 435.7416\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 322.6097 - val_loss: 423.4772\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 258.7274 - val_loss: 471.2332\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 254.7045 - val_loss: 197.1940\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 274.0380 - val_loss: 214.8548\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 211.1327 - val_loss: 198.0536\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 268.3940 - val_loss: 196.5400\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 245.4313 - val_loss: 445.0419\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 277.1999 - val_loss: 250.8029\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 229.9357 - val_loss: 198.2719\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 284.7857 - val_loss: 340.7049\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 243.1203 - val_loss: 264.0669\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 204.6951 - val_loss: 256.0585\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 263.2556 - val_loss: 186.1343\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 217.4992 - val_loss: 310.3131\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 241.4206 - val_loss: 284.9351\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 240.4541 - val_loss: 299.3074\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 239.6765 - val_loss: 187.2154\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 273.0316 - val_loss: 256.5754\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 242.1890 - val_loss: 211.1484\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 213.1925 - val_loss: 226.9431\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 268.8517 - val_loss: 208.1321\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 331.3756 - val_loss: 198.5144\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 279.2030 - val_loss: 256.9233\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 259.9893 - val_loss: 324.1619\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 238.8866 - val_loss: 211.4591\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 258.8251 - val_loss: 349.0849\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 263.9557 - val_loss: 197.7737\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 213.9334 - val_loss: 181.2877\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 253.9518 - val_loss: 272.7841\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 257.2801 - val_loss: 364.1804\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 274.2211 - val_loss: 200.9915\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 279.7490 - val_loss: 253.6525\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 228.5618 - val_loss: 184.8934\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 218.7248 - val_loss: 188.2223\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 208.5911 - val_loss: 524.8250\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 271.9067 - val_loss: 218.3076\n",
      "Epoch 252/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 44us/step - loss: 273.7956 - val_loss: 351.0182\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 227.4363 - val_loss: 399.2698\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 204.0473 - val_loss: 193.4522\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 322.4168 - val_loss: 350.9254\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 293.1693 - val_loss: 205.8559\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 281.8573 - val_loss: 241.4594\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 200.5716 - val_loss: 220.8192\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 215.5998 - val_loss: 211.9634\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 196.0383 - val_loss: 191.0942\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 326.5321 - val_loss: 194.0513\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 198.8614 - val_loss: 203.5653\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 205.0707 - val_loss: 183.0411\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 218.5917 - val_loss: 203.1541\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 216.2142 - val_loss: 323.7758\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 235.8481 - val_loss: 257.8394\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 311.2915 - val_loss: 635.7202\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 340.8078 - val_loss: 269.9980\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 196.0897 - val_loss: 198.9173\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 191.1723 - val_loss: 316.1375\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 274.5915 - val_loss: 232.3970\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 189.2089 - val_loss: 197.9293\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 194.6695 - val_loss: 235.3933\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 210.1940 - val_loss: 231.3288\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 201.9819 - val_loss: 245.4445\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 187.2176 - val_loss: 192.9538\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 205.4408 - val_loss: 245.1641\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 224.6676 - val_loss: 311.3582\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 247.8497 - val_loss: 283.5255\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 216.8504 - val_loss: 210.5699\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 238.9417 - val_loss: 225.7196\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 351.3299 - val_loss: 298.5481\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 301.7007 - val_loss: 422.4392\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 197.5930 - val_loss: 226.6183\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 196.5489 - val_loss: 185.9375\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 264.3194 - val_loss: 176.4183\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 218.1711 - val_loss: 210.7951\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 208.8040 - val_loss: 287.3612\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 286.3974 - val_loss: 182.6652\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 201.8978 - val_loss: 205.9184\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 269.2936 - val_loss: 260.2991\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 233.9661 - val_loss: 249.6641\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 254.9521 - val_loss: 375.7458\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 214.1240 - val_loss: 180.5541\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 245.9623 - val_loss: 981.0495\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 230.2051 - val_loss: 266.7278\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 195.5720 - val_loss: 311.4993\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 211.7716 - val_loss: 196.2176\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 236.5486 - val_loss: 205.7221\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 248.4751 - val_loss: 251.5529\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 217.3299 - val_loss: 306.2424\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 273.3935 - val_loss: 204.8570\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 203.8126 - val_loss: 230.9188\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 197.4129 - val_loss: 211.2684\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 204.1374 - val_loss: 258.5621\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 260.5622 - val_loss: 267.2712\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 210.2289 - val_loss: 613.0494\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 384.0636 - val_loss: 259.9057\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 208.6895 - val_loss: 299.8720\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 329.7838 - val_loss: 205.9672\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 197.7681 - val_loss: 201.4033\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 196.8496 - val_loss: 176.0389\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 212.9852 - val_loss: 177.9505\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 237.0475 - val_loss: 406.4798\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 252.4458 - val_loss: 251.2609\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 184.5099 - val_loss: 236.9961\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 271.5173 - val_loss: 184.4654\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 190.2829 - val_loss: 189.7353\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 242.4562 - val_loss: 200.5879\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 221.6732 - val_loss: 212.2694\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 231.5014 - val_loss: 180.9931\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 192.8285 - val_loss: 251.8259\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 343.2664 - val_loss: 199.1085\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 212.0494 - val_loss: 312.5753\n",
      "Epoch 325/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 44us/step - loss: 203.6775 - val_loss: 170.3398\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 197.3530 - val_loss: 180.6181\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 440.6365 - val_loss: 274.0275\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 191.7017 - val_loss: 210.5956\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 195.6722 - val_loss: 175.4994\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 199.0136 - val_loss: 489.4648\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 219.8656 - val_loss: 204.3823\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 243.2409 - val_loss: 188.1462\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 185.4934 - val_loss: 185.5263\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 227.6419 - val_loss: 201.2129\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 185.4108 - val_loss: 226.0436\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 187.7472 - val_loss: 244.6361\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 212.3441 - val_loss: 293.9669\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 190.9431 - val_loss: 287.3737\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 210.3686 - val_loss: 193.0684\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 215.0344 - val_loss: 173.1193\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 190.4651 - val_loss: 167.4087\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 203.2562 - val_loss: 186.6779\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 287.2875 - val_loss: 211.2406\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 226.0084 - val_loss: 249.5950\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 231.4256 - val_loss: 354.5349\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 196.4584 - val_loss: 201.9666\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 213.5584 - val_loss: 195.2613\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 202.6251 - val_loss: 174.0298\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 182.9844 - val_loss: 201.3358\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 192.5579 - val_loss: 226.6479\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 203.1912 - val_loss: 184.6008\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 403.6094 - val_loss: 175.7123\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 191.4387 - val_loss: 182.4222\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 186.4719 - val_loss: 200.0650\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 242.5601 - val_loss: 170.0409\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 198.5050 - val_loss: 207.3891\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 181.9053 - val_loss: 253.5683\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 260.5960 - val_loss: 274.6636\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 192.6260 - val_loss: 180.3515\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 214.8258 - val_loss: 555.2387\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 278.7223 - val_loss: 194.0234\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 180.8623 - val_loss: 169.8163\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 199.6884 - val_loss: 183.2387\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 181.5229 - val_loss: 177.0384\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 198.1370 - val_loss: 209.3122\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 231.8356 - val_loss: 389.0139\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 373.2801 - val_loss: 362.0011\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 186.7551 - val_loss: 199.8460\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 197.6911 - val_loss: 292.8661\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 203.2300 - val_loss: 187.1709\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 214.4250 - val_loss: 354.3626\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 182.7206 - val_loss: 167.3657\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 236.3122 - val_loss: 272.1897\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 193.0239 - val_loss: 175.7657\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 210.8494 - val_loss: 197.4494\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 183.1579 - val_loss: 169.6542\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 188.1527 - val_loss: 192.8629\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 192.7385 - val_loss: 180.2241\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 235.7561 - val_loss: 180.2611\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 194.7046 - val_loss: 186.1811\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 197.3734 - val_loss: 195.2414\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 195.0730 - val_loss: 196.4654\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 334.7769 - val_loss: 269.3298\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 224.9322 - val_loss: 192.3426\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 195.9951 - val_loss: 261.2656\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 225.2194 - val_loss: 258.6771\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 304.0575 - val_loss: 209.9754\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 189.7992 - val_loss: 179.1816\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 179.8573 - val_loss: 256.0251\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 185.2966 - val_loss: 203.6238\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 188.7803 - val_loss: 201.8878\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 185.3855 - val_loss: 196.9649\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 187.2725 - val_loss: 276.2522\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 252.6267 - val_loss: 221.1083\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 215.8346 - val_loss: 204.9574\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 254.4336 - val_loss: 178.4843\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 175.0479 - val_loss: 209.8354\n",
      "Epoch 398/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 179.1928 - val_loss: 255.0202\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 191.317 - 0s 44us/step - loss: 191.0055 - val_loss: 251.1571\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 227.9684 - val_loss: 197.3628\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 176.4139 - val_loss: 389.4033\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 242.4687 - val_loss: 184.2986\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 303.9499 - val_loss: 184.0681\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 182.8080 - val_loss: 176.0713\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 182.4719 - val_loss: 166.8759\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 189.9436 - val_loss: 245.1314\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 189.7460 - val_loss: 184.5306\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 242.4632 - val_loss: 217.3943\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 420.7383 - val_loss: 302.4850\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 269.0454 - val_loss: 194.8720\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 191.4503 - val_loss: 202.0879\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 202.3357 - val_loss: 193.3044\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 189.5868 - val_loss: 179.9695\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 188.9526 - val_loss: 197.2570\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 186.2128 - val_loss: 255.1399\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 195.2416 - val_loss: 171.0646\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 217.3758 - val_loss: 182.1778\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 175.6528 - val_loss: 173.7918\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 192.1783 - val_loss: 175.5742\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 233.2471 - val_loss: 210.4494\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 229.2458 - val_loss: 896.3056\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 204.8103 - val_loss: 189.7549\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 193.2454 - val_loss: 304.4303\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 209.1245 - val_loss: 193.8398\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 194.8403 - val_loss: 565.2538\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 205.2942 - val_loss: 174.4718\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 177.7702 - val_loss: 559.1347\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 178.1564 - val_loss: 170.8342\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 188.2246 - val_loss: 170.8037\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 232.1444 - val_loss: 175.9677\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 212.6808 - val_loss: 175.8762\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 210.6277 - val_loss: 171.7329\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 206.2191 - val_loss: 259.6477\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.1631 - val_loss: 225.1713\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 249.9661 - val_loss: 414.5378\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 184.1823 - val_loss: 168.5207\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 198.8021 - val_loss: 165.7574\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 192.5451 - val_loss: 235.6258\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 196.0498 - val_loss: 203.1251\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 208.7175 - val_loss: 179.9620\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 180.6918 - val_loss: 178.4280\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 189.5881 - val_loss: 230.9422\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 200.0115 - val_loss: 191.6374\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 191.2284 - val_loss: 182.0055\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 192.0261 - val_loss: 173.6494\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 212.1273 - val_loss: 533.8301\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 204.3489 - val_loss: 199.5945\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 194.5314 - val_loss: 172.6917\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 175.3419 - val_loss: 217.6418\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 198.5141 - val_loss: 182.6073\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 203.7091 - val_loss: 179.9130\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 199.1186 - val_loss: 180.1752\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 178.5375 - val_loss: 227.8055\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 206.7340 - val_loss: 200.2052\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 209.0702 - val_loss: 266.9204\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 328.0838 - val_loss: 169.0316\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 191.7270 - val_loss: 228.9072\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 183.5184 - val_loss: 171.2581\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 187.4751 - val_loss: 174.9005\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 233.0709 - val_loss: 215.4692\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 197.9644 - val_loss: 193.4911\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 242.4843 - val_loss: 310.9550\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 184.8850 - val_loss: 177.4232\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 171.4506 - val_loss: 185.5657\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 192.1675 - val_loss: 211.4799\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 182.4519 - val_loss: 201.7823\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 229.3488 - val_loss: 298.3245\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 185.4020 - val_loss: 178.8294\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 216.4721 - val_loss: 176.8932\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 203.5872 - val_loss: 339.4253\n",
      "Epoch 471/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 204.9209 - val_loss: 181.4645\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 182.9557 - val_loss: 177.8731\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 213.4132 - val_loss: 179.6355\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 303.7626 - val_loss: 933.8045\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 380.4305 - val_loss: 246.5895\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 237.8065 - val_loss: 214.8002\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 242.6531 - val_loss: 194.4256\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 210.3859 - val_loss: 196.0364\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 208.6093 - val_loss: 201.7032\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 250.0914 - val_loss: 193.2959\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 208.6776 - val_loss: 186.6233\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 219.4017 - val_loss: 217.1613\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 215.0248 - val_loss: 233.0774\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 209.9721 - val_loss: 209.0706\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 247.2957 - val_loss: 178.3130\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 197.8276 - val_loss: 208.7382\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 208.2324 - val_loss: 230.8450\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 207.3233 - val_loss: 183.1535\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 195.5360 - val_loss: 509.4677\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 210.0527 - val_loss: 208.2763\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 201.6527 - val_loss: 176.4940\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 198.4660 - val_loss: 198.6003\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 197.8056 - val_loss: 186.8358\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 212.7592 - val_loss: 204.7423\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 204.3859 - val_loss: 283.3364\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 208.4142 - val_loss: 203.8389\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 220.7459 - val_loss: 396.1296\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 193.0365 - val_loss: 196.9309\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 207.5951 - val_loss: 197.5360\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 181.8800 - val_loss: 179.2674\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 192.8672 - val_loss: 279.3190\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 246.9731 - val_loss: 192.3658\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 203.8108 - val_loss: 204.9936\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 206.4315 - val_loss: 209.9070\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 175.0070 - val_loss: 191.7101\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 206.7935 - val_loss: 218.1333\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 193.2278 - val_loss: 180.2520\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 204.6413 - val_loss: 172.7604\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 193.2701 - val_loss: 279.7310\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 182.4910 - val_loss: 387.1917\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 185.2447 - val_loss: 201.7375\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 178.3762 - val_loss: 173.0562\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 169.9992 - val_loss: 181.5763\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 204.4710 - val_loss: 245.0270\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 267.5897 - val_loss: 177.2137\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 188.7784 - val_loss: 200.0168\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 226.9238 - val_loss: 282.4244\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 177.4645 - val_loss: 179.3594\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 165.2853 - val_loss: 167.2678\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 179.3061 - val_loss: 167.2671\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 167.5867 - val_loss: 175.8789\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 218.4026 - val_loss: 360.3314\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 194.0890 - val_loss: 163.2543\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 180.6410 - val_loss: 199.0406\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 233.8096 - val_loss: 238.8773\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 183.7380 - val_loss: 164.6178\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 233.2852 - val_loss: 188.0476\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 225.3466 - val_loss: 164.2939\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 192.8415 - val_loss: 219.6216\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 170.6204 - val_loss: 276.9909\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 177.5232 - val_loss: 299.1327\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 187.3720 - val_loss: 156.9885\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 181.2130 - val_loss: 188.8603\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 185.3320 - val_loss: 262.5459\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 204.4822 - val_loss: 235.7767\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 169.7415 - val_loss: 183.1734\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 227.6361 - val_loss: 170.8970\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 188.8957 - val_loss: 178.9799\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 181.5168 - val_loss: 183.3989\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 166.2224 - val_loss: 168.8216\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 179.2333 - val_loss: 162.5246\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 176.5541 - val_loss: 260.3863\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 255.3866 - val_loss: 265.8655\n",
      "Epoch 544/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 178.4577 - val_loss: 168.8204\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 178.6645 - val_loss: 189.8753\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 181.2751 - val_loss: 157.8807\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 179.4266 - val_loss: 381.1705\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 187.8636 - val_loss: 181.1389\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 188.1208 - val_loss: 282.0679\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 166.1933 - val_loss: 189.1584\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 177.9115 - val_loss: 180.2734\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 246.1529 - val_loss: 244.4850\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 200.0404 - val_loss: 406.2718\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 172.7257 - val_loss: 159.4635\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 313.5878 - val_loss: 180.0560\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 165.1133 - val_loss: 206.8781\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 175.7641 - val_loss: 165.8958\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 159.0419 - val_loss: 161.3793\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 223.1149 - val_loss: 164.0076\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 233.1340 - val_loss: 175.7013\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 188.9791 - val_loss: 178.6188\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 180.9844 - val_loss: 212.3575\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 242.2271 - val_loss: 424.3505\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 213.0714 - val_loss: 250.5030\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 165.7204 - val_loss: 162.5358\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 169.1952 - val_loss: 161.8359\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 198.1582 - val_loss: 188.8205\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 186.0866 - val_loss: 224.9813\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 245.1285 - val_loss: 248.9418\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 208.5882 - val_loss: 277.0527\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 188.4046 - val_loss: 206.6403\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 170.7693 - val_loss: 222.0930\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 182.7430 - val_loss: 168.3998\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 168.4209 - val_loss: 176.0191\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 217.0640 - val_loss: 173.5650\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 194.3516 - val_loss: 209.4248\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 182.2591 - val_loss: 345.2170\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 187.7967 - val_loss: 404.7952\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 209.4456 - val_loss: 165.6321\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 179.1946 - val_loss: 460.0101\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 194.9321 - val_loss: 161.5934\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 167.2383 - val_loss: 203.6766\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 188.6785 - val_loss: 164.5555\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 240.0040 - val_loss: 330.3186\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 245.0051 - val_loss: 202.1159\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 159.4027 - val_loss: 164.1278\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 190.1494 - val_loss: 169.8695\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 162.8477 - val_loss: 170.8787\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 173.1509 - val_loss: 160.5594\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 169.7919 - val_loss: 220.9464\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 179.6625 - val_loss: 222.6993\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 185.3033 - val_loss: 168.4873\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 196.2006 - val_loss: 302.3709\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 209.6565 - val_loss: 184.2227\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 264.2850 - val_loss: 296.8710\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 201.8455 - val_loss: 174.3201\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 163.8267 - val_loss: 166.7223\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 233.7561 - val_loss: 179.6656\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 176.0028 - val_loss: 181.3176\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 178.7212 - val_loss: 213.6182\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 175.4630 - val_loss: 169.2158\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 200.7876 - val_loss: 184.1352\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 193.0546 - val_loss: 163.9605\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 194.0049 - val_loss: 219.7376\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 169.9019 - val_loss: 219.6236\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 207.7425 - val_loss: 202.0059\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 167.5345 - val_loss: 160.6467\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 159.5655 - val_loss: 188.5669\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 195.3131 - val_loss: 161.5139\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 200.5120 - val_loss: 188.6854\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 197.1267 - val_loss: 200.9902\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 196.3236 - val_loss: 157.9319\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 174.0787 - val_loss: 156.7532\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 164.4875 - val_loss: 167.4931\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 154.9179 - val_loss: 175.1146\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 200.1296 - val_loss: 177.3691\n",
      "Epoch 617/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 167.5135 - val_loss: 167.6960\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 184.1930 - val_loss: 160.9953\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 158.6712 - val_loss: 154.6788\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 193.6119 - val_loss: 200.5921\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 172.4127 - val_loss: 169.2232\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 217.0919 - val_loss: 242.2067\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 182.7429 - val_loss: 159.1266\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 195.1184 - val_loss: 170.3681\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 158.4265 - val_loss: 157.1811\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 182.0015 - val_loss: 330.2751\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 194.2254 - val_loss: 168.5075\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 158.1123 - val_loss: 316.6173\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.0932 - val_loss: 164.6203\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 179.9955 - val_loss: 199.7195\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 179.1805 - val_loss: 177.5499\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 172.7457 - val_loss: 185.1602\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 284.7209 - val_loss: 165.7256\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 162.162 - 0s 44us/step - loss: 166.8441 - val_loss: 272.7875\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 189.1728 - val_loss: 229.0246\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 188.3880 - val_loss: 221.8907\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 181.1189 - val_loss: 161.3420\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 179.2749 - val_loss: 179.1445\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 176.3421 - val_loss: 160.9579\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 217.8649 - val_loss: 439.4554\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 212.0731 - val_loss: 185.6674\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 158.6252 - val_loss: 182.6825\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 190.8648 - val_loss: 373.4761\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 206.0114 - val_loss: 180.5792\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 198.1608 - val_loss: 172.9896\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 187.6618 - val_loss: 165.8052\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 168.8205 - val_loss: 313.3571\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 180.1050 - val_loss: 323.5912\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 171.5510 - val_loss: 174.6265\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 278.0066 - val_loss: 168.8523\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 194.7125 - val_loss: 159.0661\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 167.6924 - val_loss: 188.3526\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 151.4601 - val_loss: 175.1856\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 164.7319 - val_loss: 161.0438\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 173.1206 - val_loss: 180.0879\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 165.5295 - val_loss: 188.6143\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 196.5127 - val_loss: 162.7285\n",
      "Epoch 658/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 169.8610 - val_loss: 182.3781\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 158.6711 - val_loss: 232.8978\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 221.2193 - val_loss: 170.9519\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 196.0834 - val_loss: 185.3025\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 300.2345 - val_loss: 166.0191\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 216.4146 - val_loss: 172.2439\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 201.4845 - val_loss: 339.8775\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 219.1467 - val_loss: 172.4649\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 163.9078 - val_loss: 171.9925\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 186.9714 - val_loss: 250.3374\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 183.8207 - val_loss: 263.4243\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 192.6384 - val_loss: 159.7818\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 177.7629 - val_loss: 169.6789\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 188.4513 - val_loss: 181.1931\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 238.4288 - val_loss: 231.8791\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 189.7133 - val_loss: 164.2391\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 169.7843 - val_loss: 162.5033\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 173.9606 - val_loss: 171.4412\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 167.9840 - val_loss: 167.1852\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 168.7440 - val_loss: 153.9588\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 180.1845 - val_loss: 156.5798\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 172.8486 - val_loss: 159.1719\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 191.1469 - val_loss: 163.8479\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 197.3543 - val_loss: 207.4020\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 318.2532 - val_loss: 258.1091\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 190.3278 - val_loss: 177.1737\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 214.8213 - val_loss: 173.3168\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 174.2167 - val_loss: 183.3900\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 190.7663 - val_loss: 640.8695\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 193.8155 - val_loss: 178.6802\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 195.0312 - val_loss: 156.5664\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 170.4398 - val_loss: 164.4058\n",
      "Epoch 690/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 193.8053 - val_loss: 295.9091\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 204.2208 - val_loss: 160.0610\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 188.3126 - val_loss: 171.9688\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 199.9278 - val_loss: 212.5959\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 178.8240 - val_loss: 165.1667\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 184.9655 - val_loss: 159.6226\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 179.9236 - val_loss: 179.8847\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 195.6611 - val_loss: 273.6807\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 173.4395 - val_loss: 155.9279\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 176.5904 - val_loss: 172.7339\n",
      "Epoch 700/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 193.6385 - val_loss: 201.6660\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 203.6431 - val_loss: 162.2246\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 252.1543 - val_loss: 179.0946\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 176.3987 - val_loss: 196.8514\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 185.2077 - val_loss: 181.4498\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 167.3537 - val_loss: 161.8632\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 176.0507 - val_loss: 182.0312\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 161.5088 - val_loss: 162.7361\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 193.5244 - val_loss: 186.7046\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 171.1699 - val_loss: 170.0663\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 191.2479 - val_loss: 181.7588\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 172.6348 - val_loss: 187.8744\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 175.7648 - val_loss: 171.8589\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 217.9127 - val_loss: 179.1194\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 190.9882 - val_loss: 166.9894\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 219.5356 - val_loss: 175.5077\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 180.8641 - val_loss: 160.2017\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 274.5604 - val_loss: 198.1954\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 160.6163 - val_loss: 163.5236\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 180.8954 - val_loss: 177.2015\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 171.8033 - val_loss: 273.5853\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 207.9029 - val_loss: 218.8519\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 187.3067 - val_loss: 164.6548\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 160.1946 - val_loss: 177.8479\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 169.3792 - val_loss: 286.4068\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 245.5277 - val_loss: 243.4039\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 173.6026 - val_loss: 202.9707\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 164.7488 - val_loss: 180.1542\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 179.1584 - val_loss: 155.0071\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 175.0155 - val_loss: 231.9134\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 165.9610 - val_loss: 209.0878\n",
      "Epoch 731/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 171.0945 - val_loss: 173.7166\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 181.7439 - val_loss: 160.5278\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 166.1088 - val_loss: 197.8124\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 183.6188 - val_loss: 205.7571\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 227.7513 - val_loss: 259.6106\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 194.5986 - val_loss: 173.3655\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 197.0684 - val_loss: 175.8858\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 156.5865 - val_loss: 192.1162\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 163.5344 - val_loss: 155.3869\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 194.1019 - val_loss: 188.2647\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 176.5861 - val_loss: 158.4666\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 171.1977 - val_loss: 174.1570\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 180.8544 - val_loss: 153.9990\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 201.1959 - val_loss: 153.4460\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 193.8305 - val_loss: 169.0219\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 163.0910 - val_loss: 165.9448\n",
      "Epoch 747/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 228.3082 - val_loss: 195.0413\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 188.6939 - val_loss: 167.6270\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 169.7785 - val_loss: 156.3511\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 178.4676 - val_loss: 170.3609\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 167.8214 - val_loss: 180.1927\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 167.0819 - val_loss: 228.1860\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 170.7258 - val_loss: 159.2514\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 183.5388 - val_loss: 155.1735\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 176.1663 - val_loss: 220.9439\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 191.2419 - val_loss: 159.9746\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 180.3914 - val_loss: 170.1191\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 175.2505 - val_loss: 218.1688\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 162.5793 - val_loss: 176.2646\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 168.6060 - val_loss: 161.8812\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 179.5549 - val_loss: 174.9840\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 162.6476 - val_loss: 209.2161\n",
      "Epoch 763/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.9443 - val_loss: 168.0989\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 159.9528 - val_loss: 160.4562\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 166.0717 - val_loss: 223.8250\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.8356 - val_loss: 164.0470\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 212.7428 - val_loss: 175.2564\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 162.7277 - val_loss: 186.5517\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 173.5961 - val_loss: 189.8190\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 204.9450 - val_loss: 167.8396\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 170.9518 - val_loss: 167.0706\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 182.7862 - val_loss: 161.2495\n",
      "Epoch 773/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 219.4771 - val_loss: 196.4801\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 165.4441 - val_loss: 168.7259\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 190.7811 - val_loss: 160.8359\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 164.3630 - val_loss: 157.2587\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 173.1923 - val_loss: 161.4237\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 182.9454 - val_loss: 153.3552\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 174.8984 - val_loss: 178.8591\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 221.2252 - val_loss: 165.0433\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 156.3636 - val_loss: 178.0607\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 182.4565 - val_loss: 203.4471\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 216.1780 - val_loss: 163.8512\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 166.7991 - val_loss: 196.5948\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 163.9071 - val_loss: 155.6434\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 175.1349 - val_loss: 156.0184\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 163.8115 - val_loss: 171.2860\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 166.0422 - val_loss: 201.6715\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 208.1968 - val_loss: 181.3469\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 178.3128 - val_loss: 168.9841\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 170.7762 - val_loss: 167.8853\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 166.4000 - val_loss: 204.5228\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 162.5063 - val_loss: 187.2770\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 181.8517 - val_loss: 175.8546\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 209.8586 - val_loss: 177.2605\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 171.9984 - val_loss: 163.0661\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.9478 - val_loss: 189.4037\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 161.8798 - val_loss: 155.3241\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 429.6212 - val_loss: 267.5753\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 262.4039 - val_loss: 213.1113\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 200.9595 - val_loss: 228.4901\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 207.8090 - val_loss: 169.3397\n",
      "Epoch 803/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 189.2136 - val_loss: 298.8864\n",
      "Epoch 804/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 179.2566 - val_loss: 163.9399\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 209.8273 - val_loss: 233.9618\n",
      "Epoch 806/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 347.8318 - val_loss: 213.5870\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 216.2471 - val_loss: 197.3874\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 154.8554 - val_loss: 245.6921\n",
      "Epoch 809/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 157.3950 - val_loss: 186.2000\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 172.3146 - val_loss: 176.5184\n",
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 161.1957 - val_loss: 220.1273\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 153.5962 - val_loss: 173.0923\n",
      "Epoch 813/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 156.2634 - val_loss: 162.8805\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 428.0696 - val_loss: 379.2997\n",
      "Epoch 815/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 347.3399 - val_loss: 228.5207\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 243.0815 - val_loss: 494.8430\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 284.9783 - val_loss: 206.1661\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 248.0634 - val_loss: 272.5000\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 239.6740 - val_loss: 227.2811\n",
      "Epoch 820/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 195.6163 - val_loss: 181.5568\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 197.2715 - val_loss: 164.0948\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 188.0156 - val_loss: 163.8110\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 184.5582 - val_loss: 186.5804\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 168.0500 - val_loss: 157.2118\n",
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 181.6339 - val_loss: 202.1012\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 194.1935 - val_loss: 174.1714\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 164.6030 - val_loss: 162.5250\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 176.1671 - val_loss: 159.1967\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 158.9290 - val_loss: 191.2664\n",
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 162.2240 - val_loss: 164.8682\n",
      "Epoch 831/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 167.0713 - val_loss: 156.1642\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 164.7625 - val_loss: 182.1411\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 155.2318 - val_loss: 166.7030\n",
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 154.8887 - val_loss: 152.3804\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 164.2326 - val_loss: 152.4204\n",
      "Epoch 836/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 152.5154 - val_loss: 170.3462\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 174.9129 - val_loss: 168.3091\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 185.5349 - val_loss: 391.7282\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 203.3409 - val_loss: 163.7565\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 200.0213 - val_loss: 164.0354\n",
      "Epoch 841/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.8349 - val_loss: 158.5228\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 200.9569 - val_loss: 171.5820\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 182.8061 - val_loss: 159.2067\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 159.2600 - val_loss: 175.9885\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 174.8425 - val_loss: 171.3872\n",
      "Epoch 846/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 170.1142 - val_loss: 207.6606\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.3189 - val_loss: 178.9123\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 179.8902 - val_loss: 239.3612\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 182.3077 - val_loss: 236.0479\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 178.3201 - val_loss: 226.1098\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 225.8187 - val_loss: 210.0150\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 182.9708 - val_loss: 245.9825\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 169.2219 - val_loss: 191.5874\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 154.3205 - val_loss: 168.0460\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 164.8219 - val_loss: 153.4787\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 165.1568 - val_loss: 164.4279\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 152.8846 - val_loss: 156.8746\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 174.7487 - val_loss: 160.6956\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 167.5081 - val_loss: 182.1875\n",
      "Epoch 860/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 182.7404 - val_loss: 157.0943\n",
      "Epoch 861/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 171.8939 - val_loss: 152.2990\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 162.3532 - val_loss: 243.8915\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 156.9492 - val_loss: 275.4257\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 163.9782 - val_loss: 171.3496\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 166.6223 - val_loss: 183.5990\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 162.2378 - val_loss: 154.6073\n",
      "Epoch 867/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 191.7138 - val_loss: 278.3232\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.5954 - val_loss: 162.9689\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 157.8299 - val_loss: 306.8344\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 193.5025 - val_loss: 176.7846\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 154.9266 - val_loss: 154.0809\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 160.3995 - val_loss: 150.6034\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 195.3693 - val_loss: 160.5616\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 158.4989 - val_loss: 155.9491\n",
      "Epoch 875/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 161.1395 - val_loss: 184.1372\n",
      "Epoch 876/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 162.2495 - val_loss: 174.5080\n",
      "Epoch 877/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 166.4536 - val_loss: 179.8426\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 167.5179 - val_loss: 266.1799\n",
      "Epoch 879/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 161.0369 - val_loss: 168.5301\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 154.8564 - val_loss: 167.6160\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 149.8275 - val_loss: 196.9011\n",
      "Epoch 882/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 156.1799 - val_loss: 150.8415\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 215.4569 - val_loss: 220.7270\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 181.7927 - val_loss: 161.3842\n",
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 168.8400 - val_loss: 157.3685\n",
      "Epoch 886/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 153.6755 - val_loss: 158.0954\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 185.4910 - val_loss: 332.3465\n",
      "Epoch 888/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 172.3855 - val_loss: 165.0843\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 170.4837 - val_loss: 161.3253\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 169.5039 - val_loss: 173.1080\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 164.0295 - val_loss: 162.9629\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 175.5927 - val_loss: 193.0360\n",
      "Epoch 893/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 348.9436 - val_loss: 179.6865\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 181.9725 - val_loss: 163.8531\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 165.5359 - val_loss: 187.7264\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 176.0433 - val_loss: 406.6317\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 154.7285 - val_loss: 154.5483\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 152.1433 - val_loss: 153.3954\n",
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 179.9173 - val_loss: 165.6663\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 171.3064 - val_loss: 235.7585\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 177.0766 - val_loss: 175.9212\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 229.2919 - val_loss: 172.7190\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 152.7834 - val_loss: 150.3782\n",
      "Epoch 904/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 174.4362 - val_loss: 155.4955\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 148.3406 - val_loss: 166.7059\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 158.4166 - val_loss: 274.1617\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 180.2916 - val_loss: 170.9758\n",
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 160.5028 - val_loss: 151.4711\n",
      "Epoch 909/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 185.4585 - val_loss: 181.5120\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 163.2403 - val_loss: 176.5284\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 186.4800 - val_loss: 221.8521\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 156.8298 - val_loss: 174.8696\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 152.5991 - val_loss: 153.0787\n",
      "Epoch 914/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 153.7298 - val_loss: 180.3281\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 184.4100 - val_loss: 187.0556\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 155.1683 - val_loss: 156.3907\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 171.0214 - val_loss: 165.6848\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 152.2466 - val_loss: 185.6792\n",
      "Epoch 919/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 173.8278 - val_loss: 245.3162\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 169.2487 - val_loss: 243.8744\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 159.3310 - val_loss: 154.8704\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 165.2302 - val_loss: 164.9137\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 159.5395 - val_loss: 238.3101\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 295.6395 - val_loss: 162.1618\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 170.2467 - val_loss: 158.6184\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.3662 - val_loss: 156.7061\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 158.9109 - val_loss: 249.0107\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 190.7818 - val_loss: 171.1253\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 169.7749 - val_loss: 177.9344\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 174.7275 - val_loss: 177.2899\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 202.3000 - val_loss: 504.6515\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 168.9699 - val_loss: 157.2145\n",
      "Epoch 933/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 149.1603 - val_loss: 166.1549\n",
      "Epoch 934/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 156.1273 - val_loss: 165.5765\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 162.8490 - val_loss: 213.7054\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 223.6411 - val_loss: 163.3742\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 153.5076 - val_loss: 152.1340\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 164.2194 - val_loss: 170.5183\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 152.8484 - val_loss: 156.5970\n",
      "Epoch 940/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 155.9925 - val_loss: 190.6870\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 159.9376 - val_loss: 184.5283\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 176.2201 - val_loss: 204.5136\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 184.5376 - val_loss: 153.3963\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 164.8042 - val_loss: 157.9302\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 183.5877 - val_loss: 404.1035\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 215.1888 - val_loss: 182.7482\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 148.9333 - val_loss: 168.8410\n",
      "Epoch 948/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 150.6111 - val_loss: 240.3516\n",
      "Epoch 949/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 150.5936 - val_loss: 176.2780\n",
      "Epoch 950/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 239.5805 - val_loss: 241.2132\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 161.4624 - val_loss: 157.2073\n",
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 158.1532 - val_loss: 151.6688\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.7917 - val_loss: 213.6141\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.4469 - val_loss: 151.2418\n",
      "Epoch 955/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 157.6792 - val_loss: 186.6250\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 186.2589 - val_loss: 162.4211\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 194.6143 - val_loss: 171.1805\n",
      "Epoch 958/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 168.4918 - val_loss: 160.0586\n",
      "Epoch 959/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 151.1921 - val_loss: 157.0696\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 160.2894 - val_loss: 171.9869\n",
      "Epoch 961/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 156.9437 - val_loss: 181.0552\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.8419 - val_loss: 180.5015\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 203.571 - 0s 44us/step - loss: 200.7467 - val_loss: 156.6914\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 169.1537 - val_loss: 173.0923\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 160.8268 - val_loss: 166.3288\n",
      "Epoch 966/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 174.5221 - val_loss: 162.4195\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 150.9831 - val_loss: 160.6921\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 175.7253 - val_loss: 159.1118\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 161.7836 - val_loss: 207.4939\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 193.4782 - val_loss: 175.2544\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 160.1607 - val_loss: 218.2388\n",
      "Epoch 972/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 157.4849 - val_loss: 204.9096\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 184.4086 - val_loss: 154.9958\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 199.2310 - val_loss: 263.6678\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 172.7247 - val_loss: 263.4855\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 204.5355 - val_loss: 157.5579\n",
      "Epoch 977/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.8341 - val_loss: 169.3701\n",
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.1301 - val_loss: 222.8049\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 172.4109 - val_loss: 191.4787\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.6550 - val_loss: 312.9286\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 199.2880 - val_loss: 175.7732\n",
      "Epoch 982/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 167.1165 - val_loss: 154.8058\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 190.0707 - val_loss: 608.0856\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 197.4794 - val_loss: 157.6099\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 159.7160 - val_loss: 203.2155\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 169.4750 - val_loss: 159.9664\n",
      "Epoch 987/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 163.4242 - val_loss: 192.8941\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 147.0276 - val_loss: 168.8887\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 178.5998 - val_loss: 161.9566\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 290.9718 - val_loss: 321.3184\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 157.1627 - val_loss: 166.6308\n",
      "Epoch 992/10000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 146.3557 - val_loss: 196.3105\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.0988 - val_loss: 152.9331\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 161.6249 - val_loss: 179.5308\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 224.3619 - val_loss: 165.2454\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 154.9313 - val_loss: 152.6230\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 154.6067 - val_loss: 300.5804\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 173.8181 - val_loss: 168.3683\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 159.8550 - val_loss: 216.5341\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 189.3182 - val_loss: 167.0044\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.3082 - val_loss: 184.9954\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 210.6760 - val_loss: 163.9561\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 147.0789 - val_loss: 157.1631\n",
      "Epoch 01003: early stopping\n",
      "Fold score (RMSE): 11.945008277893066\n",
      "Fold #3\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 6553.5706 - val_loss: 5629.3882\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 5214.5478 - val_loss: 4998.4176\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 4752.9064 - val_loss: 4564.1551\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 4425.6782 - val_loss: 4577.1588\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 4282.6996 - val_loss: 4712.2044\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 4192.0303 - val_loss: 4695.2708\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 4152.0498 - val_loss: 4198.9395\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 4059.6232 - val_loss: 4121.1776\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 3911.8895 - val_loss: 4866.5519\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 3716.0170 - val_loss: 3596.1042\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 3764.2855 - val_loss: 3684.4342\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 3502.6788 - val_loss: 4207.8138\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 3350.7769 - val_loss: 3728.0591\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 3094.7199 - val_loss: 3067.7133\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 2865.7893 - val_loss: 2728.2680\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 2663.3803 - val_loss: 2475.3718\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 2472.7628 - val_loss: 2105.8146\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 2219.1041 - val_loss: 1829.7106\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 2018.0717 - val_loss: 1750.1285\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 1961.3924 - val_loss: 2085.3001\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1617.5947 - val_loss: 1158.4835\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1389.5743 - val_loss: 998.7483\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1391.9252 - val_loss: 1304.4250\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 1203.9395 - val_loss: 865.3738\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1137.7795 - val_loss: 1152.2589\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 992.5759 - val_loss: 758.5362\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 977.9506 - val_loss: 688.2874\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 1401.3791 - val_loss: 846.1177\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 867.6651 - val_loss: 821.2329\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 795.5012 - val_loss: 682.4307\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 797.5058 - val_loss: 614.8724\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 822.2885 - val_loss: 637.5440\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 902.2323 - val_loss: 693.4067\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 681.3924 - val_loss: 535.7678\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 648.2980 - val_loss: 1342.2135\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 682.4467 - val_loss: 622.0409\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 679.4248 - val_loss: 872.5619\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 676.3023 - val_loss: 1063.7469\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 675.6505 - val_loss: 430.2775\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 612.9559 - val_loss: 793.0988\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 549.7253 - val_loss: 802.2961\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 586.6577 - val_loss: 374.5606\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 491.5020 - val_loss: 359.2363\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 516.0685 - val_loss: 385.2655\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 534.3242 - val_loss: 1003.2509\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 582.3427 - val_loss: 341.9969\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 473.6953 - val_loss: 492.6414\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 516.9272 - val_loss: 656.7192\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 426.9022 - val_loss: 334.4451\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 511.2905 - val_loss: 647.6599\n",
      "Epoch 51/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 424.2322 - val_loss: 657.9402\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 429.8788 - val_loss: 533.5743\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 493.2576 - val_loss: 734.2746\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 451.5499 - val_loss: 332.7549\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 420.3962 - val_loss: 309.2323\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 391.7439 - val_loss: 302.7026\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 471.9080 - val_loss: 304.6637\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 406.1864 - val_loss: 452.5621\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 396.3584 - val_loss: 322.7461\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 421.7787 - val_loss: 333.3607\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 514.5513 - val_loss: 403.6772\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 392.9518 - val_loss: 688.7052\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 380.7859 - val_loss: 312.1419\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 429.6388 - val_loss: 384.5012\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 374.4534 - val_loss: 299.9287\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 359.7670 - val_loss: 649.1642\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 371.5258 - val_loss: 290.0767\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 366.8807 - val_loss: 384.4572\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 440.3316 - val_loss: 715.5738\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 516.5050 - val_loss: 283.7722\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 329.6989 - val_loss: 672.5930\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 410.8125 - val_loss: 817.7409\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 325.2274 - val_loss: 342.4535\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 372.7422 - val_loss: 317.3924\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 350.4090 - val_loss: 264.5784\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 336.2457 - val_loss: 352.4659\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 348.6409 - val_loss: 290.8075\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 352.9572 - val_loss: 294.0677\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 323.4681 - val_loss: 260.6920\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 343.5829 - val_loss: 383.9476\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 365.5401 - val_loss: 255.4661\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 377.5709 - val_loss: 317.3017\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 444.9968 - val_loss: 261.5966\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 375.5296 - val_loss: 312.7664\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 344.6516 - val_loss: 510.8572\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 314.3946 - val_loss: 247.8095\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 338.9823 - val_loss: 276.7100\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 341.6975 - val_loss: 757.5809\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 731.8689 - val_loss: 381.4572\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 418.4338 - val_loss: 398.8447\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 334.9170 - val_loss: 327.9490\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 367.1422 - val_loss: 274.5791\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 378.2411 - val_loss: 317.7720\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 338.2161 - val_loss: 251.1051\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 444.8526 - val_loss: 343.7574\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 333.8454 - val_loss: 290.3970\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 323.2145 - val_loss: 289.3511\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 310.9409 - val_loss: 334.4727\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 278.4099 - val_loss: 232.4982\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 374.3418 - val_loss: 242.0104\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 309.6017 - val_loss: 276.3925\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 317.7849 - val_loss: 274.6028\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 353.8487 - val_loss: 247.5991\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 353.8045 - val_loss: 367.5639\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 310.4897 - val_loss: 309.6587\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 371.4419 - val_loss: 438.2782\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 316.1652 - val_loss: 290.0991\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 274.8376 - val_loss: 256.6077\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 375.1347 - val_loss: 336.1675\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 311.9591 - val_loss: 621.8583\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 333.5990 - val_loss: 665.0848\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 307.7395 - val_loss: 334.2314\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 302.9631 - val_loss: 665.2823\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 361.6773 - val_loss: 279.4477\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 277.9528 - val_loss: 226.2791\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 350.0656 - val_loss: 231.7495\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 328.2899 - val_loss: 307.7018\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 407.4739 - val_loss: 220.3227\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 296.0303 - val_loss: 224.2018\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 268.7497 - val_loss: 231.4288\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 406.3331 - val_loss: 255.6663\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 322.5186 - val_loss: 331.2088\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 280.2201 - val_loss: 252.2075\n",
      "Epoch 124/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 302.8123 - val_loss: 259.6350\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 515.3053 - val_loss: 309.3815\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 438.2918 - val_loss: 286.3666\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 342.5023 - val_loss: 305.0932\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 299.0371 - val_loss: 272.5985\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 301.2499 - val_loss: 220.8027\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 308.7304 - val_loss: 355.4551\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 297.7444 - val_loss: 249.6336\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 401.8512 - val_loss: 239.2658\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 330.1346 - val_loss: 256.7855\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 308.8944 - val_loss: 227.0958\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 345.9959 - val_loss: 219.5707\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 278.3719 - val_loss: 266.6405\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 269.0952 - val_loss: 299.8991\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 274.9184 - val_loss: 213.4030\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 254.6383 - val_loss: 204.4268\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 272.5793 - val_loss: 296.7843\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 313.1675 - val_loss: 258.3983\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 288.2908 - val_loss: 384.5178\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 349.7159 - val_loss: 445.2256\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 336.6223 - val_loss: 210.6656\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 266.7221 - val_loss: 386.5803\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 274.7582 - val_loss: 225.2438\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 301.1778 - val_loss: 461.3626\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 344.9921 - val_loss: 194.9203\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 281.4468 - val_loss: 367.7640\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 312.8934 - val_loss: 392.1619\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 283.6899 - val_loss: 232.0272\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 253.5424 - val_loss: 420.0593\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 260.8149 - val_loss: 268.7151\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 402.3353 - val_loss: 220.7401\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 285.1028 - val_loss: 257.6464\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 308.2162 - val_loss: 257.3847\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 292.9072 - val_loss: 217.3359\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 242.5299 - val_loss: 210.4366\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 316.9751 - val_loss: 210.3025\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 263.9449 - val_loss: 350.7219\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 296.6700 - val_loss: 244.1614\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 250.3540 - val_loss: 186.9059\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 236.6863 - val_loss: 279.7508\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 278.5448 - val_loss: 271.6255\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 247.5554 - val_loss: 209.1665\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 292.2020 - val_loss: 207.0985\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 245.3809 - val_loss: 343.0750\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 238.3343 - val_loss: 314.4803\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 241.3749 - val_loss: 205.5316\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 260.9784 - val_loss: 399.1743\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 260.6845 - val_loss: 465.2125\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 289.9186 - val_loss: 544.2356\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 291.2129 - val_loss: 209.4349\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 279.1227 - val_loss: 289.6534\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 231.8037 - val_loss: 194.3293\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 285.7947 - val_loss: 187.0413\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 287.7484 - val_loss: 244.4125\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 229.5298 - val_loss: 525.2878\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 278.1740 - val_loss: 434.6076\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 309.1328 - val_loss: 222.8143\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 223.5059 - val_loss: 175.2591\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 254.8994 - val_loss: 195.8521\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 255.4503 - val_loss: 215.6198\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 268.2052 - val_loss: 203.7030\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 217.6528 - val_loss: 187.6087\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 224.4267 - val_loss: 227.6398\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 235.1766 - val_loss: 263.8123\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 283.1581 - val_loss: 207.6840\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 238.8831 - val_loss: 250.9750\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 224.7392 - val_loss: 191.4198\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 289.8999 - val_loss: 274.5140\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 256.3537 - val_loss: 184.4981\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 292.2081 - val_loss: 225.0664\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 225.6955 - val_loss: 221.7904\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 280.4428 - val_loss: 251.6299\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 216.0983 - val_loss: 264.7887\n",
      "Epoch 197/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 242.9529 - val_loss: 239.0737\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 233.1580 - val_loss: 346.7737\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 258.9568 - val_loss: 190.1469\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 329.2977 - val_loss: 279.2463\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 244.4749 - val_loss: 184.9500\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 246.3914 - val_loss: 199.7628\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 213.9852 - val_loss: 258.1366\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 271.4646 - val_loss: 200.1345\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 303.9813 - val_loss: 200.2294\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 226.9000 - val_loss: 190.7484\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 225.4887 - val_loss: 173.1836\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 218.6078 - val_loss: 294.0053\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 245.8981 - val_loss: 189.7510\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 257.5433 - val_loss: 204.8589\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 280.2395 - val_loss: 265.0083\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 261.3827 - val_loss: 410.2129\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 251.3040 - val_loss: 240.9775\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 272.1008 - val_loss: 235.7216\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 244.1545 - val_loss: 346.4735\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 206.7267 - val_loss: 175.4874\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 223.6934 - val_loss: 210.5748\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 240.6268 - val_loss: 170.3741\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 250.4250 - val_loss: 196.1870\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 208.0012 - val_loss: 213.4124\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 255.5698 - val_loss: 390.9728\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 239.5159 - val_loss: 183.5322\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 220.1094 - val_loss: 166.7458\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 218.0753 - val_loss: 257.6559\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 252.6208 - val_loss: 214.1021\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 263.1298 - val_loss: 642.5990\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 251.0232 - val_loss: 244.6678\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 204.8240 - val_loss: 692.2341\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 228.1195 - val_loss: 302.5054\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 287.2400 - val_loss: 418.5116\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 236.2785 - val_loss: 210.2801\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 208.0529 - val_loss: 191.0467\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 283.7503 - val_loss: 250.2478\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 214.9108 - val_loss: 198.3594\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 237.2742 - val_loss: 196.8589\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 264.3911 - val_loss: 192.8461\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 203.9625 - val_loss: 178.7134\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 232.8496 - val_loss: 236.3216\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 215.2976 - val_loss: 174.8477\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 191.4834 - val_loss: 197.7199\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 234.1567 - val_loss: 204.8899\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 214.8476 - val_loss: 164.4335\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 233.5658 - val_loss: 412.6103\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 276.1312 - val_loss: 170.3404\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 261.6830 - val_loss: 281.4443\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 257.5976 - val_loss: 219.8981\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 257.8916 - val_loss: 333.4271\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 221.6815 - val_loss: 316.3361\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 239.3063 - val_loss: 194.0278\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 183.3115 - val_loss: 201.1959\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 246.2511 - val_loss: 183.3752\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 210.7427 - val_loss: 191.0985\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 255.2143 - val_loss: 204.2708\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 193.9246 - val_loss: 252.8192\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 214.5104 - val_loss: 206.1836\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 209.8727 - val_loss: 168.3617\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 210.4419 - val_loss: 244.6216\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 228.1788 - val_loss: 213.0978\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 197.0638 - val_loss: 211.9684\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 222.8025 - val_loss: 242.5040\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 255.0598 - val_loss: 252.7876\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 288.1016 - val_loss: 188.8959\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 236.6738 - val_loss: 162.4286\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 241.9966 - val_loss: 477.1866\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 557.0492 - val_loss: 197.9461\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 252.8848 - val_loss: 223.3088\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 239.5416 - val_loss: 201.9073\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 232.2073 - val_loss: 390.5991\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 249.5219 - val_loss: 270.1779\n",
      "Epoch 270/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 236.4323 - val_loss: 345.1403\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 283.3673 - val_loss: 183.1953\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 242.9659 - val_loss: 218.0010\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 216.6030 - val_loss: 200.8833\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 213.5408 - val_loss: 210.3251\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 262.6870 - val_loss: 182.7362\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 295.9629 - val_loss: 538.8631\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 250.4857 - val_loss: 230.2066\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 210.6258 - val_loss: 186.5429\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 222.9016 - val_loss: 214.9618\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 219.7602 - val_loss: 439.7474\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 228.6987 - val_loss: 198.0830\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 239.1030 - val_loss: 202.1191\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 201.9961 - val_loss: 178.8845\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 240.9769 - val_loss: 222.9607\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 234.0738 - val_loss: 189.5057\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 252.0368 - val_loss: 203.7815\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 209.5983 - val_loss: 176.9382\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 231.4195 - val_loss: 182.7250\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 242.5840 - val_loss: 230.7004\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 213.8957 - val_loss: 216.4945\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 242.2443 - val_loss: 160.1396\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 240.4635 - val_loss: 180.9477\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 195.5475 - val_loss: 227.2721\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 222.9798 - val_loss: 321.1486\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 222.0712 - val_loss: 193.9144\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 207.2211 - val_loss: 168.5178\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 218.1910 - val_loss: 214.0632\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 222.8567 - val_loss: 181.5732\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 216.5115 - val_loss: 160.8939\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 203.3873 - val_loss: 163.1557\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 259.4750 - val_loss: 289.3188\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 228.1843 - val_loss: 189.8213\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 216.3377 - val_loss: 167.8643\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 210.6012 - val_loss: 255.3101\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 224.1787 - val_loss: 208.4002\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 199.8415 - val_loss: 323.1420\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 218.9300 - val_loss: 184.1486\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 226.7300 - val_loss: 183.6401\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 252.8752 - val_loss: 207.1450\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 206.5833 - val_loss: 169.5961\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 213.4326 - val_loss: 201.3342\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 205.2174 - val_loss: 240.8122\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 216.0536 - val_loss: 180.4457\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 260.3093 - val_loss: 224.1014\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 210.6448 - val_loss: 183.2545\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 194.6290 - val_loss: 185.1241\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 239.0356 - val_loss: 175.0116\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 222.1259 - val_loss: 182.6769\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 193.1556 - val_loss: 186.8738\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 290.2253 - val_loss: 159.0827\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 180.9637 - val_loss: 156.2417\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 188.9621 - val_loss: 165.9426\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 255.0733 - val_loss: 162.0683\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 250.8073 - val_loss: 158.9295\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 223.4421 - val_loss: 172.7477\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 195.1027 - val_loss: 185.3235\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 214.8359 - val_loss: 392.0683\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 234.6930 - val_loss: 160.2274\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 220.9994 - val_loss: 203.9963\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 209.4934 - val_loss: 239.0026\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 242.2675 - val_loss: 195.1801\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 223.3078 - val_loss: 229.8265\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 192.9924 - val_loss: 181.3168\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 213.8539 - val_loss: 163.9699\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 218.9542 - val_loss: 269.5689\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 228.9367 - val_loss: 189.3260\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 261.2030 - val_loss: 188.1096\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 229.2078 - val_loss: 370.4029\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 204.9315 - val_loss: 169.8925\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 211.4825 - val_loss: 166.5301\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 286.8333 - val_loss: 165.7407\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 197.9698 - val_loss: 171.3326\n",
      "Epoch 343/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 212.3417 - val_loss: 162.7629\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 176.8974 - val_loss: 162.4774\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 193.2720 - val_loss: 156.7130\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 212.0600 - val_loss: 225.8869\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 203.6895 - val_loss: 196.6125\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 247.2505 - val_loss: 186.8518\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 196.5688 - val_loss: 159.8912\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 231.9101 - val_loss: 215.3799\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 257.5714 - val_loss: 246.4317\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 240.9126 - val_loss: 174.0637\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 213.5193 - val_loss: 334.3042\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 203.2263 - val_loss: 179.2287\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 235.8688 - val_loss: 249.7501\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 209.8399 - val_loss: 267.6728\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 214.6479 - val_loss: 178.0172\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 175.9522 - val_loss: 153.8419\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 206.0405 - val_loss: 258.8114\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 193.6238 - val_loss: 254.4709\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 235.1948 - val_loss: 170.9258\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 255.7445 - val_loss: 172.2404\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 188.0187 - val_loss: 161.1246\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 201.2362 - val_loss: 185.4031\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 214.9471 - val_loss: 157.0131\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 205.1332 - val_loss: 240.2059\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 178.3912 - val_loss: 224.7860\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 238.7691 - val_loss: 170.4358\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 196.8390 - val_loss: 157.8149\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 217.8605 - val_loss: 332.3799\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 272.3297 - val_loss: 170.1092\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 216.9893 - val_loss: 220.6672\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 224.6352 - val_loss: 149.5540\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 177.8545 - val_loss: 164.9448\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 198.6749 - val_loss: 168.7879\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 197.8175 - val_loss: 164.1539\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 197.9056 - val_loss: 156.0739\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 235.7946 - val_loss: 221.0123\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 279.5222 - val_loss: 166.2371\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 196.4489 - val_loss: 196.8865\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 213.7684 - val_loss: 187.8547\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 194.7502 - val_loss: 159.2927\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 187.8503 - val_loss: 333.9398\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 210.7027 - val_loss: 153.4195\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 198.2470 - val_loss: 168.0718\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 179.0113 - val_loss: 151.3949\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 273.4328 - val_loss: 183.3756\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 266.0929 - val_loss: 157.4695\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 170.2139 - val_loss: 155.6750\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 184.5885 - val_loss: 149.6693\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 172.7960 - val_loss: 147.0353\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 243.2265 - val_loss: 517.9568\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 211.9832 - val_loss: 180.4172\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 232.3681 - val_loss: 185.7599\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 179.1763 - val_loss: 151.0066\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 190.3288 - val_loss: 186.3444\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 207.2538 - val_loss: 242.9289\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 181.0851 - val_loss: 151.6122\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 201.1423 - val_loss: 152.2325\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 291.6925 - val_loss: 568.8365\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 213.9988 - val_loss: 176.3690\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 188.0523 - val_loss: 583.6565\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 279.5439 - val_loss: 199.1387\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 197.7352 - val_loss: 148.8688\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 201.8721 - val_loss: 179.4948\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 203.4397 - val_loss: 211.8989\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 197.5228 - val_loss: 170.7437\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 216.6104 - val_loss: 346.0937\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 222.5710 - val_loss: 145.3697\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 187.4951 - val_loss: 156.4254\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 181.8456 - val_loss: 166.6065\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 206.7724 - val_loss: 227.5903\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 214.6382 - val_loss: 276.3571\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 219.5950 - val_loss: 166.7436\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 190.5663 - val_loss: 150.2891\n",
      "Epoch 416/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 202.5248 - val_loss: 155.7727\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 204.3100 - val_loss: 204.8265\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 209.3543 - val_loss: 202.9153\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 224.5767 - val_loss: 214.9690\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 229.5772 - val_loss: 141.3438\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 174.4932 - val_loss: 221.4972\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 195.4312 - val_loss: 254.7558\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 291.2935 - val_loss: 169.4996\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 168.1370 - val_loss: 404.9510\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 235.8122 - val_loss: 186.3523\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 184.5537 - val_loss: 160.7768\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 208.0732 - val_loss: 389.9023\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 449.7755 - val_loss: 433.8521\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 258.3421 - val_loss: 260.5182\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 221.6339 - val_loss: 168.3649\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 233.5948 - val_loss: 207.4409\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 214.4655 - val_loss: 339.5616\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 238.4519 - val_loss: 176.9656\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 203.2135 - val_loss: 168.8633\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 211.1240 - val_loss: 166.0304\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 187.1120 - val_loss: 185.4547\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 188.2995 - val_loss: 152.6333\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 182.5722 - val_loss: 145.9359\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 205.4962 - val_loss: 168.9617\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 204.2152 - val_loss: 158.7999\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 183.1772 - val_loss: 222.4985\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 183.4367 - val_loss: 335.8559\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 179.7710 - val_loss: 153.5298\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 261.5508 - val_loss: 219.5223\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 180.3638 - val_loss: 155.7078\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 191.7110 - val_loss: 170.8892\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 187.6841 - val_loss: 179.5147\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 483.4061 - val_loss: 249.2852\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 208.8026 - val_loss: 175.6592\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 173.5945 - val_loss: 246.0813\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 183.5935 - val_loss: 181.4064\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 193.0353 - val_loss: 214.7115\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 178.3318 - val_loss: 166.0841\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 165.0518 - val_loss: 147.4598\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 196.2094 - val_loss: 187.9441\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 190.0821 - val_loss: 146.0354\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 175.9889 - val_loss: 163.8861\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 197.3495 - val_loss: 154.3614\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 318.6823 - val_loss: 261.1011\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 201.2308 - val_loss: 207.8904\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 172.6939 - val_loss: 175.7689\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 180.9077 - val_loss: 155.3651\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 196.9784 - val_loss: 269.5847\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 310.2266 - val_loss: 420.1554\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 202.5530 - val_loss: 225.2645\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 183.1272 - val_loss: 180.4705\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 199.8985 - val_loss: 228.5406\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 203.7807 - val_loss: 259.3110\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 196.7369 - val_loss: 156.2906\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 176.8965 - val_loss: 177.2560\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 167.2975 - val_loss: 211.7206\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 312.5234 - val_loss: 507.2987\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 187.0810 - val_loss: 156.8712\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 173.9181 - val_loss: 173.9654\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 188.4712 - val_loss: 253.4763\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 208.7091 - val_loss: 199.6489\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 237.7065 - val_loss: 218.6903\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 178.1840 - val_loss: 185.9968\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 213.0035 - val_loss: 176.6518\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 189.3297 - val_loss: 154.5470\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 175.5607 - val_loss: 179.4099\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 184.1270 - val_loss: 215.5592\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 226.0534 - val_loss: 168.1043\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 180.5140 - val_loss: 161.7278\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 208.6899 - val_loss: 408.0463\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 198.5440 - val_loss: 359.3373\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 180.1721 - val_loss: 177.2088\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 253.8457 - val_loss: 199.5114\n",
      "Epoch 489/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 280.4282 - val_loss: 201.6775\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 180.7047 - val_loss: 175.3330\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 194.5646 - val_loss: 232.5710\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 203.8165 - val_loss: 149.1525\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 203.1437 - val_loss: 159.8828\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 241.5726 - val_loss: 296.8021\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 242.2175 - val_loss: 182.0728\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 185.9007 - val_loss: 179.7406\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 204.0946 - val_loss: 331.0398\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 179.0260 - val_loss: 147.5131\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 167.5636 - val_loss: 179.0777\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 180.2597 - val_loss: 421.1213\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 216.1860 - val_loss: 291.6258\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 182.9913 - val_loss: 741.3428\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 194.8382 - val_loss: 164.7054\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 218.6038 - val_loss: 186.9136\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 181.6917 - val_loss: 144.9948\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 185.2053 - val_loss: 166.2786\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 182.2946 - val_loss: 219.5256\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 170.0122 - val_loss: 200.4345\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 184.4077 - val_loss: 159.5454\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 189.5655 - val_loss: 150.7602\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 231.9838 - val_loss: 178.7678\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 180.2005 - val_loss: 146.6889\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 173.8446 - val_loss: 159.1235\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 174.2544 - val_loss: 149.4138\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 205.0688 - val_loss: 162.8735\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 201.2473 - val_loss: 254.7492\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 196.5127 - val_loss: 147.0494\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 180.2812 - val_loss: 154.1323\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 189.6984 - val_loss: 155.0907\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 202.4228 - val_loss: 141.7064\n",
      "Epoch 00520: early stopping\n",
      "Fold score (RMSE): 11.196551322937012\n",
      "Fold #4\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 19321.7733 - val_loss: 5746.0025\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 5443.5819 - val_loss: 5271.7848\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 5011.9920 - val_loss: 4916.0959\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 4781.6250 - val_loss: 4715.1806\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 4544.3293 - val_loss: 4475.2819\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 4454.9178 - val_loss: 4354.2645\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 4370.7060 - val_loss: 4343.8853\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 4167.7372 - val_loss: 4095.2538\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 4118.7043 - val_loss: 4000.3065\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 4173.0495 - val_loss: 4009.5989\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 3858.6849 - val_loss: 3783.8132\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 3978.8322 - val_loss: 3741.1334\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 3767.1519 - val_loss: 3798.0918\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 3830.7456 - val_loss: 3445.8987\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 3545.7554 - val_loss: 3407.5053\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 3295.0966 - val_loss: 3746.0996\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 3183.1024 - val_loss: 3341.4167\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 3049.1551 - val_loss: 2679.8969\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 2786.5000 - val_loss: 2441.3383\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 2714.4841 - val_loss: 2522.1609\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 2551.1388 - val_loss: 2030.3499\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 2136.5196 - val_loss: 2600.3601\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 2123.3158 - val_loss: 1651.1802\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 2052.2314 - val_loss: 1489.8918\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1600.7669 - val_loss: 1952.9029\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 1558.0013 - val_loss: 1117.2682\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1499.2682 - val_loss: 1146.3098\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1405.6890 - val_loss: 929.4633\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1190.1722 - val_loss: 1055.4970\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1069.9052 - val_loss: 1140.3741\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 980.4607 - val_loss: 1451.7455\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 900.6309 - val_loss: 605.6195\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 948.9999 - val_loss: 539.1837\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 808.4669 - val_loss: 668.6565\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 994.3570 - val_loss: 673.1313\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 959.8102 - val_loss: 592.4668\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 664.8830 - val_loss: 436.2932\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 734.8417 - val_loss: 468.9860\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 621.5424 - val_loss: 410.2540\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 604.3130 - val_loss: 705.4109\n",
      "Epoch 41/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 704.8727 - val_loss: 672.6603\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 614.3412 - val_loss: 410.5497\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 574.5677 - val_loss: 664.6871\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 674.1533 - val_loss: 397.3775\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 720.1849 - val_loss: 777.6183\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 643.2617 - val_loss: 442.6958\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 662.4166 - val_loss: 1524.3115\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 564.1909 - val_loss: 706.9937\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 545.0872 - val_loss: 453.6535\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 500.3082 - val_loss: 366.3489\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 557.2728 - val_loss: 333.0600\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 579.2104 - val_loss: 636.1195\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 569.1962 - val_loss: 404.6545\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 489.4400 - val_loss: 376.3553\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 566.1869 - val_loss: 337.0380\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 498.6625 - val_loss: 522.4407\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 482.0310 - val_loss: 298.6256\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 541.0938 - val_loss: 603.1585\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 448.5993 - val_loss: 293.8446\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 509.3544 - val_loss: 289.0689\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 658.6637 - val_loss: 835.6038\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 483.2358 - val_loss: 286.7319\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 399.0727 - val_loss: 317.7154\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 489.1125 - val_loss: 504.3520\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 557.5684 - val_loss: 353.5546\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 459.6982 - val_loss: 337.4031\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 438.5174 - val_loss: 380.2096\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 436.2490 - val_loss: 309.3982\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 474.7819 - val_loss: 324.0122\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 431.0507 - val_loss: 285.7557\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 458.0117 - val_loss: 682.8363\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 485.3612 - val_loss: 298.4178\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 474.7164 - val_loss: 295.2581\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 410.9146 - val_loss: 342.3164\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 412.9771 - val_loss: 410.3642\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 492.6577 - val_loss: 313.8275\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 377.1924 - val_loss: 390.0106\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 385.2323 - val_loss: 559.5371\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 478.6337 - val_loss: 327.3597\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 378.1652 - val_loss: 265.5271\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 413.4955 - val_loss: 286.8628\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 458.9711 - val_loss: 331.7901\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 404.7571 - val_loss: 287.0467\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 374.8575 - val_loss: 269.8671\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 401.0430 - val_loss: 285.8813\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 432.7444 - val_loss: 371.7427\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 463.8394 - val_loss: 253.8753\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 351.2901 - val_loss: 441.0328\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 393.2919 - val_loss: 344.0057\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 404.6953 - val_loss: 869.4818\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 370.7021 - val_loss: 238.9623\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 402.4914 - val_loss: 303.8112\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 374.6576 - val_loss: 649.4683\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 441.9313 - val_loss: 311.6180\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 356.9322 - val_loss: 400.0578\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 349.7125 - val_loss: 624.7239\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 386.1837 - val_loss: 310.9200\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 409.9663 - val_loss: 366.2230\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 344.4167 - val_loss: 262.6636\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 348.7758 - val_loss: 262.4597\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 442.7852 - val_loss: 418.2918\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 350.0234 - val_loss: 307.9705\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 355.4637 - val_loss: 246.3008\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 402.9033 - val_loss: 338.2433\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 308.6009 - val_loss: 307.9915\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 339.9362 - val_loss: 332.4060\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 339.0664 - val_loss: 224.3616\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 422.8908 - val_loss: 255.6608\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 419.4792 - val_loss: 284.9096\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 316.4849 - val_loss: 241.3750\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 307.7562 - val_loss: 255.9061\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 375.9728 - val_loss: 225.0360\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 329.8805 - val_loss: 232.8381\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 346.3085 - val_loss: 211.5961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 385.3452 - val_loss: 227.3964\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 377.9382 - val_loss: 238.7402\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 318.6660 - val_loss: 292.1299\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 366.4546 - val_loss: 436.6080\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 366.6172 - val_loss: 339.5554\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 376.3288 - val_loss: 224.2982\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 366.5488 - val_loss: 413.4670\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 311.9708 - val_loss: 302.2842\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 308.3245 - val_loss: 272.2621\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 339.0382 - val_loss: 243.8857\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 357.6752 - val_loss: 343.9765\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 298.1920 - val_loss: 241.5267\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 362.8065 - val_loss: 209.9214\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 381.8891 - val_loss: 541.6311\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 352.3800 - val_loss: 205.7703\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 349.9263 - val_loss: 295.8823\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 323.2377 - val_loss: 220.2622\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 288.9068 - val_loss: 289.7534\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 325.0429 - val_loss: 214.2050\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 330.2669 - val_loss: 225.8556\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 325.8493 - val_loss: 216.0564\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 321.9677 - val_loss: 260.8090\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 282.8567 - val_loss: 232.7406\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 358.9526 - val_loss: 301.1126\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 295.7791 - val_loss: 193.1162\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 329.1427 - val_loss: 198.6138\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 281.1797 - val_loss: 286.6047\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 287.4178 - val_loss: 259.7566\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 297.9713 - val_loss: 425.9264\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 329.5559 - val_loss: 228.9668\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 282.3725 - val_loss: 224.9559\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 358.1084 - val_loss: 559.1141\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 310.4446 - val_loss: 213.6659\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 284.8710 - val_loss: 270.9747\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 352.2145 - val_loss: 489.1623\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 317.1677 - val_loss: 209.6591\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 267.2486 - val_loss: 292.9280\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 334.3449 - val_loss: 206.8365\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 270.1879 - val_loss: 243.0465\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 341.8607 - val_loss: 224.6417\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 269.5121 - val_loss: 495.1399\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 308.1786 - val_loss: 195.2927\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 277.2108 - val_loss: 253.1002\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 249.7691 - val_loss: 288.5770\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 292.3022 - val_loss: 260.6459\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 292.9254 - val_loss: 220.2231\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 350.0845 - val_loss: 193.0261\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 312.6784 - val_loss: 188.6596\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 343.3684 - val_loss: 464.0829\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 302.2582 - val_loss: 210.5591\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 309.2621 - val_loss: 207.1008\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 303.0958 - val_loss: 338.8112\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 271.2147 - val_loss: 200.9048\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 284.2013 - val_loss: 213.7881\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 272.6404 - val_loss: 249.2002\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 273.8439 - val_loss: 202.1503\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 336.5607 - val_loss: 711.6403\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 298.7259 - val_loss: 190.6313\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 320.7156 - val_loss: 202.3458\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 259.4915 - val_loss: 205.4183\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 325.0562 - val_loss: 202.8895\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 279.0330 - val_loss: 242.6584\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 273.5047 - val_loss: 315.0239\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 297.4636 - val_loss: 734.5273\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 311.7070 - val_loss: 215.7395\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 301.2449 - val_loss: 247.2288\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 252.6977 - val_loss: 196.2778\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 288.6333 - val_loss: 196.2967\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 267.6865 - val_loss: 685.8625\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 308.7715 - val_loss: 212.7227\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 268.6738 - val_loss: 263.5250\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 263.4081 - val_loss: 203.0780\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 295.3408 - val_loss: 181.0576\n",
      "Epoch 188/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 289.6198 - val_loss: 181.5071\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 256.4857 - val_loss: 194.2631\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 294.0486 - val_loss: 184.6871\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 264.9067 - val_loss: 188.5050\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 288.6306 - val_loss: 247.7800\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 307.1418 - val_loss: 197.7338\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 247.2998 - val_loss: 193.0743\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 275.9313 - val_loss: 203.8288\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 277.7886 - val_loss: 209.2514\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 253.5577 - val_loss: 214.1269\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 293.3766 - val_loss: 227.9410\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 239.9130 - val_loss: 230.4062\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 247.5238 - val_loss: 196.4753\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 293.8440 - val_loss: 174.5565\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 253.0437 - val_loss: 180.7924\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 244.6218 - val_loss: 191.1031\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 281.1894 - val_loss: 181.9985\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 258.7559 - val_loss: 244.5982\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 277.2790 - val_loss: 203.0817\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 259.8112 - val_loss: 608.3535\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 282.8975 - val_loss: 184.6313\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 304.2762 - val_loss: 189.8683\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 250.5308 - val_loss: 168.4830\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 232.7860 - val_loss: 174.1780\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 261.0030 - val_loss: 170.5413\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 251.3122 - val_loss: 241.9058\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 288.2919 - val_loss: 219.3290\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 262.8501 - val_loss: 207.1003\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 261.0136 - val_loss: 300.2113\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 265.0616 - val_loss: 224.7202\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 297.8675 - val_loss: 181.1150\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 243.0805 - val_loss: 201.1504\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 241.8176 - val_loss: 173.6502\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 266.7229 - val_loss: 179.6086\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 264.7041 - val_loss: 192.8758\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 270.3407 - val_loss: 231.6250\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 501.5039 - val_loss: 218.7118\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 242.3594 - val_loss: 174.5590\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 221.1974 - val_loss: 182.6033\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 220.2297 - val_loss: 171.9699\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 245.7218 - val_loss: 333.1476\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 246.1007 - val_loss: 315.4081\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 282.1284 - val_loss: 172.2315\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 238.3667 - val_loss: 164.0621\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 258.7621 - val_loss: 216.1829\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 243.6332 - val_loss: 177.5836\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 241.7927 - val_loss: 163.7146\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 288.4617 - val_loss: 195.7658\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 236.1990 - val_loss: 164.6778\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 221.8887 - val_loss: 167.1594\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 261.4727 - val_loss: 330.9522\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 284.2507 - val_loss: 172.2463\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 264.3178 - val_loss: 170.7430\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 234.3835 - val_loss: 206.2515\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 252.0331 - val_loss: 206.5097\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 217.4093 - val_loss: 175.5197\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 253.8527 - val_loss: 267.4881\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 299.5455 - val_loss: 199.5458\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 266.8981 - val_loss: 186.0180\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 229.7885 - val_loss: 283.6134\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 308.5778 - val_loss: 195.2953\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 238.2719 - val_loss: 202.9728\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 267.7398 - val_loss: 159.3673\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 225.9777 - val_loss: 203.6585\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 263.6347 - val_loss: 246.1339\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 311.7586 - val_loss: 178.0201\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 266.1806 - val_loss: 195.2680\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 231.8341 - val_loss: 161.7250\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 273.1158 - val_loss: 223.9083\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 223.2671 - val_loss: 169.5545\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 302.7631 - val_loss: 184.9610\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 235.9503 - val_loss: 266.0885\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 225.8736 - val_loss: 272.9587\n",
      "Epoch 261/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 240.2162 - val_loss: 195.9271\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 218.7050 - val_loss: 172.5327\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 284.9573 - val_loss: 169.7134\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 216.2140 - val_loss: 167.9711\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 241.8483 - val_loss: 248.8201\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 240.2647 - val_loss: 154.6736\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 305.5329 - val_loss: 327.4372\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 264.9361 - val_loss: 170.5839\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 379.8233 - val_loss: 303.7083\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 259.5763 - val_loss: 429.3757\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 291.1487 - val_loss: 169.1686\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 247.6742 - val_loss: 228.1445\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 274.4661 - val_loss: 216.0727\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 219.0503 - val_loss: 182.3468\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 221.0043 - val_loss: 194.6400\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 255.5633 - val_loss: 184.7061\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 229.7184 - val_loss: 161.9462\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 298.6641 - val_loss: 243.8739\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 218.9452 - val_loss: 191.4573\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 216.6228 - val_loss: 152.5485\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 239.9517 - val_loss: 172.1816\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 224.0319 - val_loss: 194.0507\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 226.9832 - val_loss: 187.9280\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 218.8431 - val_loss: 198.6926\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 249.5758 - val_loss: 492.4909\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 324.8760 - val_loss: 175.2673\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 203.3241 - val_loss: 160.0668\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 217.5745 - val_loss: 240.0309\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 223.6999 - val_loss: 247.0482\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 276.2070 - val_loss: 174.7131\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 235.1724 - val_loss: 153.5905\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 221.9171 - val_loss: 408.4926\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 288.8241 - val_loss: 156.2859\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 280.4786 - val_loss: 186.9056\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 200.0816 - val_loss: 179.1854\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 229.9068 - val_loss: 171.4969\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 223.2339 - val_loss: 177.1414\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 196.3180 - val_loss: 206.1128\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 237.9916 - val_loss: 248.3995\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 262.9949 - val_loss: 225.2957\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 258.6065 - val_loss: 211.3391\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 240.5097 - val_loss: 183.2424\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 222.5225 - val_loss: 297.9379\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 233.5481 - val_loss: 170.1407\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 216.6273 - val_loss: 192.1170\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 226.2040 - val_loss: 157.2136\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 229.0198 - val_loss: 169.7524\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 251.4776 - val_loss: 281.8837\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 224.1719 - val_loss: 167.4724\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 249.9815 - val_loss: 233.1573\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 341.6864 - val_loss: 194.8224\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 195.4997 - val_loss: 182.5811\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 226.9552 - val_loss: 157.2652\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 220.5427 - val_loss: 174.8439\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 289.3457 - val_loss: 307.6790\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 225.9346 - val_loss: 155.8282\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 200.4666 - val_loss: 180.3920\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 219.7367 - val_loss: 160.5140\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 218.8349 - val_loss: 154.7377\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 218.6209 - val_loss: 148.5067\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 235.8161 - val_loss: 216.0626\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 221.8199 - val_loss: 187.2261\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 204.4467 - val_loss: 178.1998\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 218.9250 - val_loss: 159.5795\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 253.6740 - val_loss: 248.9540\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 238.0608 - val_loss: 163.5482\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 222.7345 - val_loss: 168.2425\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 222.8157 - val_loss: 146.9207\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 226.0972 - val_loss: 193.8398\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 216.6160 - val_loss: 175.1341\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 210.3973 - val_loss: 173.3541\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 270.2022 - val_loss: 173.4874\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 225.7567 - val_loss: 163.3684\n",
      "Epoch 334/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 224.6315 - val_loss: 236.3749\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 242.5696 - val_loss: 260.6282\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 204.1150 - val_loss: 146.2108\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 211.3605 - val_loss: 157.2696\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 235.0444 - val_loss: 160.5519\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 219.9601 - val_loss: 144.7117\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 243.6432 - val_loss: 149.7460\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 208.4418 - val_loss: 270.6498\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 292.8466 - val_loss: 146.9606\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 212.8212 - val_loss: 184.5973\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 203.4352 - val_loss: 143.1303\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 211.9722 - val_loss: 197.9953\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 211.4620 - val_loss: 194.5055\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 283.0615 - val_loss: 152.6778\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 265.2062 - val_loss: 155.2115\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 213.6905 - val_loss: 167.4428\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 217.7841 - val_loss: 150.8725\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 214.4097 - val_loss: 178.1843\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 202.5278 - val_loss: 156.7034\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 206.1071 - val_loss: 159.5549\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 213.0945 - val_loss: 164.1699\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 264.8235 - val_loss: 199.8245\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 222.8079 - val_loss: 342.8188\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 206.5491 - val_loss: 171.0866\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 205.8977 - val_loss: 179.1635\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 211.0317 - val_loss: 171.9463\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 192.6511 - val_loss: 276.9394\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 552.0323 - val_loss: 276.7820\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 271.6710 - val_loss: 218.3748\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 231.8544 - val_loss: 175.1422\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 252.4769 - val_loss: 164.8526\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 254.1567 - val_loss: 245.0827\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 257.2802 - val_loss: 278.9252\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 224.6779 - val_loss: 150.7953\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 253.7763 - val_loss: 180.8677\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 240.6112 - val_loss: 307.0132\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 226.1724 - val_loss: 157.8073\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 221.9931 - val_loss: 199.7547\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 208.7529 - val_loss: 171.7717\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 248.4758 - val_loss: 155.4460\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 234.7098 - val_loss: 170.6890\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 207.1203 - val_loss: 184.8512\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 247.1817 - val_loss: 322.6559\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 278.6669 - val_loss: 143.3872\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 207.7670 - val_loss: 195.5466\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 221.8676 - val_loss: 167.3686\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 197.8671 - val_loss: 216.2101\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 217.1247 - val_loss: 269.0588\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 262.2744 - val_loss: 143.2962\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 241.9963 - val_loss: 153.9579\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 214.0334 - val_loss: 246.1004\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 210.6653 - val_loss: 139.7834\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 200.2976 - val_loss: 149.7081\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 194.5283 - val_loss: 145.4519\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 224.3900 - val_loss: 319.1410\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 223.9863 - val_loss: 167.5223\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 257.8737 - val_loss: 174.6773\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 217.1364 - val_loss: 167.0981\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 245.6075 - val_loss: 153.8712\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 239.4136 - val_loss: 177.3904\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 196.1971 - val_loss: 148.1243\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 216.0596 - val_loss: 158.2781\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 208.7840 - val_loss: 143.6460\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 218.7337 - val_loss: 186.9867\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 233.6488 - val_loss: 287.5987\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 229.0351 - val_loss: 278.1162\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 212.7166 - val_loss: 150.7777\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 215.7391 - val_loss: 156.7309\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 237.6286 - val_loss: 146.0608\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 237.4062 - val_loss: 331.9595\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 268.1402 - val_loss: 159.8135\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 210.8764 - val_loss: 153.2609\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 273.7742 - val_loss: 207.6605\n",
      "Epoch 407/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 212.6667 - val_loss: 154.7757\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 218.0838 - val_loss: 167.9072\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 196.3211 - val_loss: 310.0718\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 201.5477 - val_loss: 242.1588\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 224.8167 - val_loss: 138.2151\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 349.0620 - val_loss: 243.9903\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 223.0108 - val_loss: 152.0171\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 199.6089 - val_loss: 138.6381\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 186.8641 - val_loss: 158.8021\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 194.3569 - val_loss: 166.7424\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 207.7559 - val_loss: 283.6925\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 225.8459 - val_loss: 175.2031\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 201.4946 - val_loss: 246.5289\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 224.8982 - val_loss: 139.9653\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 193.8274 - val_loss: 168.0040\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 197.5721 - val_loss: 237.2004\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 201.1344 - val_loss: 149.3328\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 240.0016 - val_loss: 175.8560\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 216.7129 - val_loss: 145.3755\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 202.8791 - val_loss: 148.6997\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 225.4501 - val_loss: 209.4509\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 212.1545 - val_loss: 189.2889\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 218.4774 - val_loss: 146.9438\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 189.1886 - val_loss: 142.3137\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 206.8237 - val_loss: 292.5062\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 210.0303 - val_loss: 178.2840\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 270.4926 - val_loss: 235.9832\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 190.7988 - val_loss: 150.5045\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 196.5405 - val_loss: 140.5823\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 194.3098 - val_loss: 148.9847\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 198.4724 - val_loss: 169.0914\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 241.0441 - val_loss: 144.0687\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 205.4191 - val_loss: 196.8446\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 220.5545 - val_loss: 145.9293\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 213.0425 - val_loss: 164.0149\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 213.8127 - val_loss: 153.0208\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 217.9418 - val_loss: 161.5103\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 212.1352 - val_loss: 530.5951\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 249.7662 - val_loss: 226.4846\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 211.6770 - val_loss: 143.8966\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 217.7085 - val_loss: 156.8219\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 214.3051 - val_loss: 185.3941\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 206.6175 - val_loss: 147.9123\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 187.3993 - val_loss: 157.2895\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 202.6138 - val_loss: 307.3731\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 227.5358 - val_loss: 141.5524\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 229.7229 - val_loss: 152.0352\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 233.3018 - val_loss: 152.9006\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 207.0134 - val_loss: 254.7593\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 205.4717 - val_loss: 177.8982\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 202.2865 - val_loss: 140.7702\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 237.4919 - val_loss: 192.3255\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 219.1111 - val_loss: 180.6291\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 207.1019 - val_loss: 155.7257\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 183.5965 - val_loss: 156.3486\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 235.8064 - val_loss: 173.6836\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 229.0479 - val_loss: 181.1206\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 254.8496 - val_loss: 322.4155\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 191.3719 - val_loss: 147.2306\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 189.3896 - val_loss: 146.4577\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 207.2432 - val_loss: 213.5000\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 211.3891 - val_loss: 154.1669\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 238.3446 - val_loss: 142.4173\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 186.9505 - val_loss: 149.1330\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 282.0164 - val_loss: 163.0879\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 199.0138 - val_loss: 142.4684\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 194.3714 - val_loss: 141.9203\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 182.1829 - val_loss: 143.7944\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 193.2202 - val_loss: 211.2460\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 199.2273 - val_loss: 139.8331\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 195.6489 - val_loss: 170.7771\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 211.9563 - val_loss: 149.4785\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 206.3856 - val_loss: 143.2616\n",
      "Epoch 480/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 230.8081 - val_loss: 185.0412\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 198.5894 - val_loss: 137.5696\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 186.7415 - val_loss: 185.7250\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 209.0696 - val_loss: 156.2596\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 201.0714 - val_loss: 151.7582\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 195.0239 - val_loss: 253.5822\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 205.3818 - val_loss: 156.7307\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 225.3914 - val_loss: 158.8690\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 187.5392 - val_loss: 167.1382\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 191.8138 - val_loss: 140.6890\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 188.0098 - val_loss: 180.5300\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 228.4285 - val_loss: 187.4319\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 220.4518 - val_loss: 149.0454\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 206.0590 - val_loss: 254.8457\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 182.5784 - val_loss: 160.3189\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 186.1507 - val_loss: 153.7494\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 175.0520 - val_loss: 144.1705\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 201.5973 - val_loss: 153.4487\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 231.8472 - val_loss: 412.2130\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 241.0929 - val_loss: 254.2478\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 199.4389 - val_loss: 225.8931\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 175.5072 - val_loss: 132.8251\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 192.4027 - val_loss: 142.8089\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 188.2919 - val_loss: 241.5645\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 242.6178 - val_loss: 157.7776\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 184.8188 - val_loss: 153.8702\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 193.4864 - val_loss: 138.4503\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 192.9513 - val_loss: 141.2604\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 190.5773 - val_loss: 186.7269\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 205.3837 - val_loss: 147.6585\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 245.1730 - val_loss: 438.6241\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 216.8069 - val_loss: 156.4920\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 197.6116 - val_loss: 157.2205\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 178.4268 - val_loss: 133.9498\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 200.1488 - val_loss: 231.2500\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 202.0919 - val_loss: 132.2421\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 188.3072 - val_loss: 135.9300\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 173.8990 - val_loss: 163.8178\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 189.0394 - val_loss: 178.3132\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 188.5065 - val_loss: 142.8796\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 198.6267 - val_loss: 149.6658\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 229.0924 - val_loss: 162.4090\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 191.6240 - val_loss: 146.4684\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 187.2269 - val_loss: 273.1616\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 201.5357 - val_loss: 150.7614\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 196.0170 - val_loss: 164.1812\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 184.3280 - val_loss: 132.8525\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 217.4832 - val_loss: 166.5396\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 222.7575 - val_loss: 184.3228\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 187.2443 - val_loss: 137.6146\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 190.4639 - val_loss: 136.5015\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 216.3740 - val_loss: 166.9770\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 252.5281 - val_loss: 146.9173\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 193.1771 - val_loss: 137.7091\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 190.3655 - val_loss: 150.3561\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 189.2524 - val_loss: 168.4118\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 196.0785 - val_loss: 159.0059\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 200.4803 - val_loss: 161.1023\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 185.0695 - val_loss: 137.2574\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 196.1634 - val_loss: 136.8159\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 208.7997 - val_loss: 137.0071\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 265.2183 - val_loss: 156.1089\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 178.4757 - val_loss: 130.2337\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 186.0363 - val_loss: 155.2374\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 184.4671 - val_loss: 128.6271\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 210.2940 - val_loss: 157.1531\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 172.9088 - val_loss: 171.2276\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 239.2809 - val_loss: 170.6187\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.3054 - val_loss: 155.6441\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 182.3442 - val_loss: 196.5938\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 187.2326 - val_loss: 131.3013\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 194.1650 - val_loss: 132.2960\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 212.8005 - val_loss: 158.0153\n",
      "Epoch 553/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 195.1423 - val_loss: 141.9394\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 177.2539 - val_loss: 255.2931\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 179.0879 - val_loss: 151.1701\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 189.8871 - val_loss: 132.2346\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 182.0463 - val_loss: 147.0254\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 194.0989 - val_loss: 142.2842\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 189.2440 - val_loss: 239.5637\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 184.9237 - val_loss: 132.9390\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 190.1072 - val_loss: 144.5515\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 204.4662 - val_loss: 137.1986\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 179.4248 - val_loss: 153.7582\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 210.7556 - val_loss: 206.0197\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 229.5565 - val_loss: 154.7826\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 250.6772 - val_loss: 248.3721\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 214.3642 - val_loss: 149.1834\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 202.6471 - val_loss: 169.3055\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 171.9598 - val_loss: 141.1054\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 206.2805 - val_loss: 194.2525\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 204.3310 - val_loss: 131.2009\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 204.2984 - val_loss: 137.3786\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 200.6630 - val_loss: 144.9464\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 175.1723 - val_loss: 238.7976\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 194.1280 - val_loss: 136.2961\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 206.0321 - val_loss: 139.7076\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 179.1515 - val_loss: 187.4680\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 224.5244 - val_loss: 380.0790\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 208.3677 - val_loss: 140.1925\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 191.6873 - val_loss: 166.0361\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 181.1893 - val_loss: 140.2396\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 184.4030 - val_loss: 139.6557\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 176.4566 - val_loss: 152.7825\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 178.7943 - val_loss: 263.7931\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 185.9760 - val_loss: 129.1141\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 186.2923 - val_loss: 284.2391\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 275.6051 - val_loss: 135.7461\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 210.8726 - val_loss: 160.2114\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 222.6740 - val_loss: 175.5756\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 195.7267 - val_loss: 131.4958\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 185.7978 - val_loss: 144.1405\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 183.4342 - val_loss: 168.1550\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 189.1097 - val_loss: 133.6468\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 184.6835 - val_loss: 132.2671\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 184.7625 - val_loss: 157.2712\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 191.7988 - val_loss: 132.1306\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 193.0776 - val_loss: 136.0266\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 199.6289 - val_loss: 133.5895\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 183.4784 - val_loss: 193.6750\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 186.1848 - val_loss: 182.2716\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 172.5032 - val_loss: 180.6732\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 244.8624 - val_loss: 129.5544\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 194.6873 - val_loss: 214.7876\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 170.9347 - val_loss: 147.3561\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 202.5634 - val_loss: 221.0681\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 501.8230 - val_loss: 707.4744\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 343.8341 - val_loss: 207.5048\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 248.2295 - val_loss: 154.6601\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 236.8829 - val_loss: 154.1372\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 219.6115 - val_loss: 141.9583\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 210.2436 - val_loss: 257.1428\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 224.4553 - val_loss: 161.0992\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 239.0635 - val_loss: 158.3743\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 222.8953 - val_loss: 139.1437\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 247.8054 - val_loss: 201.1236\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 212.0460 - val_loss: 149.2423\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 218.3288 - val_loss: 142.6018\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 228.4088 - val_loss: 168.3773\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 239.5667 - val_loss: 249.2214\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 302.9551 - val_loss: 206.7211\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 228.3109 - val_loss: 141.0626\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 206.5724 - val_loss: 162.2966\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 219.8962 - val_loss: 149.3316\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 241.3154 - val_loss: 150.7458\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 227.0993 - val_loss: 143.1889\n",
      "Epoch 626/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 57us/step - loss: 233.6845 - val_loss: 170.8400\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 216.8609 - val_loss: 141.2566\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 286.2729 - val_loss: 190.8508\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 215.9577 - val_loss: 184.1059\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 224.0556 - val_loss: 439.3639\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 220.2732 - val_loss: 156.8889\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 245.6347 - val_loss: 258.7013\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 229.6220 - val_loss: 187.9897\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 253.8726 - val_loss: 166.8025\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 288.9491 - val_loss: 161.0049\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 213.6813 - val_loss: 154.5688\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 263.2415 - val_loss: 187.2108\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 213.0130 - val_loss: 227.9537\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 223.5801 - val_loss: 188.8029\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 206.4763 - val_loss: 247.3773\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 228.4433 - val_loss: 161.7788\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 209.6230 - val_loss: 204.6521\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 275.3677 - val_loss: 163.2707\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 214.6774 - val_loss: 225.0482\n",
      "Epoch 00644: early stopping\n",
      "Fold score (RMSE): 14.3052978515625\n",
      "Fold #5\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 36946.9644 - val_loss: 5573.7615\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 5758.9342 - val_loss: 4795.2076\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 5346.2677 - val_loss: 4689.6574\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 5106.4707 - val_loss: 4217.9545\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 4849.8815 - val_loss: 4779.0027\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 4628.0736 - val_loss: 3787.0958\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 4370.4543 - val_loss: 3744.8564\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 4316.4959 - val_loss: 3591.1813\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 4185.7408 - val_loss: 3623.9455\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 4201.4955 - val_loss: 3557.4668\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 3939.4104 - val_loss: 3324.0461\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 3826.1248 - val_loss: 3420.2341\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 3611.3456 - val_loss: 2831.8084\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 3369.7710 - val_loss: 2791.6303\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 3323.9814 - val_loss: 2598.2026\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 3116.7157 - val_loss: 2327.3152\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 2802.7607 - val_loss: 2252.3843\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 2867.3154 - val_loss: 3221.4691\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 2628.8650 - val_loss: 2572.2617\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 2225.4942 - val_loss: 2650.4914\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 1889.6993 - val_loss: 1320.0290\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 1853.9867 - val_loss: 1820.3767\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 1821.6656 - val_loss: 910.5289\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 1535.2510 - val_loss: 1139.1600\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 1281.3529 - val_loss: 866.3747\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 1108.8902 - val_loss: 698.8392\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 1287.5980 - val_loss: 667.9341\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 1078.0990 - val_loss: 593.8683\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 1107.3695 - val_loss: 838.1173\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 953.0827 - val_loss: 568.4745\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 872.1309 - val_loss: 643.5612\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 904.9739 - val_loss: 906.1504\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 858.9330 - val_loss: 534.6621\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 837.1343 - val_loss: 467.3446\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 875.5718 - val_loss: 473.2156\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 711.0931 - val_loss: 467.7466\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 798.2554 - val_loss: 607.4599\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 713.2307 - val_loss: 440.6403\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 721.1108 - val_loss: 476.1869\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 696.4554 - val_loss: 963.0528\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 793.3032 - val_loss: 498.5933\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 821.4090 - val_loss: 1439.6836\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 678.1790 - val_loss: 437.4276\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 640.8116 - val_loss: 561.7738\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 687.3813 - val_loss: 632.1083\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 708.9283 - val_loss: 589.3076\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 688.4453 - val_loss: 360.3511\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 591.4015 - val_loss: 364.7869\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 571.0266 - val_loss: 1265.6153\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 598.2973 - val_loss: 487.4902\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 589.0993 - val_loss: 387.4350\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 601.1868 - val_loss: 324.5671\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 652.2472 - val_loss: 731.0885\n",
      "Epoch 54/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 559.9938 - val_loss: 350.6512\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 509.8995 - val_loss: 318.3373\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 525.8610 - val_loss: 332.9494\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 471.1072 - val_loss: 556.2466\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 472.1635 - val_loss: 313.8845\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 458.5682 - val_loss: 323.8237\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 618.4131 - val_loss: 317.8259\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 437.8913 - val_loss: 297.1428\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 461.9775 - val_loss: 319.3524\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 468.5755 - val_loss: 304.1137\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 522.2933 - val_loss: 325.5729\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 503.2013 - val_loss: 271.2161\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 494.5320 - val_loss: 496.0512\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 413.8122 - val_loss: 266.9616\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 512.4570 - val_loss: 322.0577\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 459.0311 - val_loss: 374.3700\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 401.4168 - val_loss: 286.9831\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 500.7911 - val_loss: 358.4707\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 394.8033 - val_loss: 547.4881\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 403.6350 - val_loss: 373.2634\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 398.7768 - val_loss: 1000.8197\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 444.7218 - val_loss: 263.4641\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 389.2019 - val_loss: 364.6969\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 378.9339 - val_loss: 275.0022\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 370.7840 - val_loss: 243.9686\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 415.8921 - val_loss: 266.5735\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 412.3824 - val_loss: 450.9569\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 413.6770 - val_loss: 283.0327\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 361.5795 - val_loss: 400.2004\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 408.0633 - val_loss: 505.5084\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 420.3202 - val_loss: 261.3357\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 380.6576 - val_loss: 282.5317\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 420.1962 - val_loss: 629.8652\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 372.2987 - val_loss: 232.1756\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 453.3082 - val_loss: 275.4452\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 358.7030 - val_loss: 264.1465\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 334.2108 - val_loss: 573.3690\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 443.8613 - val_loss: 296.2325\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 328.5452 - val_loss: 244.9990\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 382.5563 - val_loss: 351.5814\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 323.4359 - val_loss: 247.6068\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 381.1267 - val_loss: 248.8212\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 375.2576 - val_loss: 233.3501\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 332.6442 - val_loss: 219.8608\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 404.8048 - val_loss: 260.9143\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 358.6495 - val_loss: 247.4751\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 388.9558 - val_loss: 237.5443\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 310.0544 - val_loss: 233.5621\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 333.4777 - val_loss: 306.6409\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 322.894 - 1s 63us/step - loss: 318.2294 - val_loss: 307.2080\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 342.7632 - val_loss: 220.7451\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 336.4288 - val_loss: 271.0239\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 316.2738 - val_loss: 268.8096\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 393.4149 - val_loss: 262.7491\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 406.8349 - val_loss: 286.8951\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 305.5979 - val_loss: 220.8381\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 339.7895 - val_loss: 360.3793\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 332.6980 - val_loss: 213.6750\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 332.5145 - val_loss: 255.9912\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 306.9672 - val_loss: 220.3571\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 321.5540 - val_loss: 224.7935\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 303.6192 - val_loss: 245.3265\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 306.4157 - val_loss: 239.8746\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 291.6387 - val_loss: 223.3254\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 397.1493 - val_loss: 238.1198\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 287.7203 - val_loss: 209.9798\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 339.4177 - val_loss: 282.9570\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 292.9280 - val_loss: 196.1705\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 286.5157 - val_loss: 221.6368\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 352.3752 - val_loss: 208.8770\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 332.8052 - val_loss: 254.7962\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 309.0954 - val_loss: 566.9726\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 314.8743 - val_loss: 202.1914\n",
      "Epoch 127/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 64us/step - loss: 301.8171 - val_loss: 315.8511\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 289.4313 - val_loss: 237.9318\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 326.5881 - val_loss: 218.8567\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 307.5869 - val_loss: 220.3381\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 307.7087 - val_loss: 201.6165\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 340.3919 - val_loss: 539.8440\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 317.8746 - val_loss: 316.8313\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 298.4024 - val_loss: 217.8677\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 308.2807 - val_loss: 215.9392\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 286.2389 - val_loss: 260.7972\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 280.5651 - val_loss: 199.0324\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 290.1958 - val_loss: 246.4737\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 326.9793 - val_loss: 212.0985\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 294.8296 - val_loss: 189.7870\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 339.6365 - val_loss: 205.9982\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 295.0347 - val_loss: 265.4261\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 269.5303 - val_loss: 198.4069\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 263.4366 - val_loss: 205.0350\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 269.6198 - val_loss: 223.5626\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 273.2006 - val_loss: 295.2067\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 301.1039 - val_loss: 303.5934\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 406.5718 - val_loss: 224.3309\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 301.5740 - val_loss: 200.8909\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 281.4587 - val_loss: 180.0389\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 262.6246 - val_loss: 184.5742\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 316.3093 - val_loss: 478.1750\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 285.2453 - val_loss: 247.1195\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 364.4715 - val_loss: 181.4846\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 250.8389 - val_loss: 278.1936\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 377.8197 - val_loss: 201.8958\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 270.9442 - val_loss: 331.0338\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 279.7357 - val_loss: 266.2426\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 258.1813 - val_loss: 250.0021\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 266.9077 - val_loss: 184.3152\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 292.8974 - val_loss: 282.3236\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 269.7209 - val_loss: 394.9277\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 263.7547 - val_loss: 215.1381\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 261.1807 - val_loss: 173.3034\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 270.3069 - val_loss: 187.2952\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 305.6098 - val_loss: 213.0000\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 312.5774 - val_loss: 216.1761\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 263.7281 - val_loss: 199.1615\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 316.1007 - val_loss: 178.2893\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 302.4376 - val_loss: 299.9373\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 269.6218 - val_loss: 196.4582\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 240.7023 - val_loss: 194.0275\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 230.0903 - val_loss: 174.0185\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 302.0286 - val_loss: 238.2847\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 249.0693 - val_loss: 211.1531\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 293.3064 - val_loss: 181.8698\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 233.0696 - val_loss: 183.3642\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 264.2053 - val_loss: 255.6549\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 253.1278 - val_loss: 266.8326\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 284.9350 - val_loss: 460.4076\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 293.2979 - val_loss: 196.9033\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 250.0062 - val_loss: 166.9543\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 240.5964 - val_loss: 175.5503\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 272.2096 - val_loss: 343.8238\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 253.3094 - val_loss: 179.0744\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 266.0958 - val_loss: 178.9017\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 271.6043 - val_loss: 338.9185\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 277.7750 - val_loss: 167.6832\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 256.2234 - val_loss: 187.9751\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 249.4952 - val_loss: 387.5781\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 242.3395 - val_loss: 163.8142\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 263.0891 - val_loss: 175.3537\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 277.6559 - val_loss: 323.2869\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 346.1141 - val_loss: 202.2469\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 220.2043 - val_loss: 215.2767\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 219.8700 - val_loss: 156.8380\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 235.0001 - val_loss: 155.1514\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 290.1312 - val_loss: 210.0208\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 249.2961 - val_loss: 166.9311\n",
      "Epoch 200/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 241.0725 - val_loss: 294.9919\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 233.9744 - val_loss: 487.5063\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 340.2430 - val_loss: 237.7918\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 280.6730 - val_loss: 319.2711\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 224.2318 - val_loss: 180.6209\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 241.5718 - val_loss: 225.2382\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 300.2623 - val_loss: 187.9003\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 223.9888 - val_loss: 453.0546\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 252.0432 - val_loss: 166.5224\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 266.1587 - val_loss: 238.4835\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 248.7319 - val_loss: 243.2812\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 263.0586 - val_loss: 159.0507\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 247.1880 - val_loss: 157.4148\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 244.8017 - val_loss: 188.1076\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 270.1378 - val_loss: 157.6919\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 241.6893 - val_loss: 173.6388\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 241.7164 - val_loss: 210.3711\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 234.4059 - val_loss: 166.3715\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 234.5249 - val_loss: 226.3710\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 276.6843 - val_loss: 180.9529\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 311.3345 - val_loss: 263.3174\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 227.3563 - val_loss: 163.4418\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 284.2219 - val_loss: 157.2016\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 202.1493 - val_loss: 193.7771\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 229.9030 - val_loss: 181.9122\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 270.4813 - val_loss: 321.0067\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 255.5911 - val_loss: 213.7035\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 238.9779 - val_loss: 158.1353\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 243.4449 - val_loss: 169.0749\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 227.3152 - val_loss: 181.5549\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 231.0757 - val_loss: 204.4593\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 247.3420 - val_loss: 183.8732\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 265.3816 - val_loss: 224.2975\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 245.8057 - val_loss: 165.1040\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 232.5204 - val_loss: 196.6858\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 229.6392 - val_loss: 164.9433\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 223.1050 - val_loss: 242.0900\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 219.0044 - val_loss: 164.0053\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 273.5919 - val_loss: 157.8489\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 228.6693 - val_loss: 194.6559\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 255.7770 - val_loss: 244.6556\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 231.0861 - val_loss: 167.6241\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 252.5931 - val_loss: 153.7793\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 335.3279 - val_loss: 165.2630\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 260.1895 - val_loss: 203.1741\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 283.2544 - val_loss: 151.5711\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 254.3877 - val_loss: 157.7843\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 283.5064 - val_loss: 166.1444\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 285.9302 - val_loss: 171.9887\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 214.4198 - val_loss: 160.6137\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 235.5180 - val_loss: 229.2647\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 232.0529 - val_loss: 206.4733\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 218.7543 - val_loss: 212.7827\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 218.1665 - val_loss: 173.8388\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 250.2844 - val_loss: 165.1280\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 204.6365 - val_loss: 156.0834\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 216.7062 - val_loss: 204.0356\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 267.9290 - val_loss: 185.1143\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 218.4428 - val_loss: 151.4587\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 282.1394 - val_loss: 288.1430\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 225.7655 - val_loss: 444.0577\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 251.1000 - val_loss: 311.6691\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 272.7511 - val_loss: 600.3492\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 243.6877 - val_loss: 160.9376\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 222.5366 - val_loss: 162.0755\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 242.8929 - val_loss: 145.7609\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 224.6236 - val_loss: 150.1966\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 255.2127 - val_loss: 227.2852\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 246.3593 - val_loss: 153.5319\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 216.4349 - val_loss: 347.9668\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 251.3597 - val_loss: 171.3203\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 228.0084 - val_loss: 211.3459\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 240.7190 - val_loss: 164.2031\n",
      "Epoch 273/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 237.7582 - val_loss: 201.6375\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 217.2045 - val_loss: 173.7644\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 202.8258 - val_loss: 199.6527\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 242.0918 - val_loss: 170.9395\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 242.2625 - val_loss: 159.6735\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 232.2660 - val_loss: 181.9343\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 299.1081 - val_loss: 158.8462\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 269.2650 - val_loss: 147.5264\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 333.8270 - val_loss: 162.4041\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 251.4569 - val_loss: 143.8060\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 219.9462 - val_loss: 173.9786\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 227.6924 - val_loss: 178.2705\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 239.0169 - val_loss: 453.2822\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 228.5548 - val_loss: 279.3618\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 217.4852 - val_loss: 183.0637\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 211.1085 - val_loss: 203.5508\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 240.7963 - val_loss: 238.6587\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 218.5244 - val_loss: 147.2603\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 265.2981 - val_loss: 684.2535\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 238.7412 - val_loss: 159.6471\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 194.7466 - val_loss: 152.7463\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 234.3611 - val_loss: 157.8081\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 260.0843 - val_loss: 203.3428\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 258.2515 - val_loss: 314.6977\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 224.3463 - val_loss: 168.3538\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 202.8401 - val_loss: 200.7788\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 229.7926 - val_loss: 183.7362\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 215.3578 - val_loss: 275.9684\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 211.2094 - val_loss: 171.9672\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 212.4911 - val_loss: 185.3963\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 249.3886 - val_loss: 229.1037\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 205.0850 - val_loss: 367.2941\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 222.1809 - val_loss: 156.5833\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 296.9050 - val_loss: 182.0574\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 202.2693 - val_loss: 145.2073\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 207.0544 - val_loss: 150.2901\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 217.3640 - val_loss: 194.9155\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 225.2425 - val_loss: 166.6984\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 307.9752 - val_loss: 708.4885\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 223.8676 - val_loss: 156.9160\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 219.5024 - val_loss: 154.9484\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 220.7153 - val_loss: 160.1534\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 224.5186 - val_loss: 192.3053\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 236.7214 - val_loss: 169.1261\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 219.7138 - val_loss: 160.8905\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 197.7396 - val_loss: 160.4127\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 230.4404 - val_loss: 144.9795\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 239.5532 - val_loss: 170.2693\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 228.1612 - val_loss: 234.0088\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 290.5483 - val_loss: 230.3369\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 221.9274 - val_loss: 186.0199\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 222.4978 - val_loss: 142.1108\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 238.2110 - val_loss: 147.7663\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 204.3251 - val_loss: 185.2415\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 204.5898 - val_loss: 141.9758\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 195.1380 - val_loss: 360.4242\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 232.2482 - val_loss: 231.5310\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 195.7933 - val_loss: 139.9500\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 245.7002 - val_loss: 227.5132\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 202.1333 - val_loss: 157.3505\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 198.2318 - val_loss: 146.4326\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 314.9626 - val_loss: 164.9879\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 194.4325 - val_loss: 186.9083\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 183.5898 - val_loss: 141.6391\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 238.3832 - val_loss: 172.4895\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 220.0849 - val_loss: 157.4739\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 222.4412 - val_loss: 232.2863\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 229.1924 - val_loss: 187.5440\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 206.6064 - val_loss: 279.3703\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 223.1037 - val_loss: 189.7316\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 199.0594 - val_loss: 194.3500\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 208.7172 - val_loss: 207.6444\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 277.3300 - val_loss: 261.1593\n",
      "Epoch 346/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 240.8096 - val_loss: 151.3730\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 248.8822 - val_loss: 245.5599\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 193.5892 - val_loss: 143.3253\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 206.5140 - val_loss: 151.2720\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 210.2462 - val_loss: 147.7201\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 213.0258 - val_loss: 186.7118\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 195.1841 - val_loss: 151.5850\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 219.8472 - val_loss: 138.9260\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 179.6433 - val_loss: 147.5795\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 333.8710 - val_loss: 178.3878\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 263.1852 - val_loss: 171.5662\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 224.8342 - val_loss: 141.9617\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 180.4463 - val_loss: 162.0930\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 202.7356 - val_loss: 178.5201\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 193.1551 - val_loss: 138.8118\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 202.5724 - val_loss: 179.2648\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 200.3008 - val_loss: 532.1752\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 295.9299 - val_loss: 148.0667\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 248.6969 - val_loss: 269.3979\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 205.3242 - val_loss: 173.0405\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 227.5729 - val_loss: 308.9811\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 270.4019 - val_loss: 146.9196\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 197.7634 - val_loss: 196.4126\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 220.9822 - val_loss: 159.4659\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 235.1922 - val_loss: 148.4756\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 226.122 - 0s 51us/step - loss: 223.8912 - val_loss: 134.5157\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 188.4252 - val_loss: 205.6683\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 312.7116 - val_loss: 726.2112\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 385.3032 - val_loss: 212.0516\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 245.9907 - val_loss: 147.3431\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 267.7850 - val_loss: 259.0789\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 224.8400 - val_loss: 179.2404\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 195.3377 - val_loss: 173.2793\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 228.8044 - val_loss: 161.4944\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 224.7020 - val_loss: 162.2015\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 224.3398 - val_loss: 169.1540\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 231.8164 - val_loss: 199.2428\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 187.4581 - val_loss: 168.3648\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 236.3121 - val_loss: 169.7483\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 210.0419 - val_loss: 181.9709\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 238.3640 - val_loss: 183.6980\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 223.4285 - val_loss: 154.0421\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 212.2910 - val_loss: 160.7951\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 196.5207 - val_loss: 136.7607\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 184.1064 - val_loss: 186.1965\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 211.2778 - val_loss: 140.7718\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 229.1056 - val_loss: 139.7376\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 232.5408 - val_loss: 340.2666\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 215.9670 - val_loss: 133.0108\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 220.9822 - val_loss: 202.5899\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 198.7555 - val_loss: 204.5239\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 231.2765 - val_loss: 166.2452\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 199.8921 - val_loss: 141.6839\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 239.8179 - val_loss: 134.2258\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 196.3488 - val_loss: 140.5662\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 217.5052 - val_loss: 185.8578\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 201.9162 - val_loss: 159.5550\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 185.8866 - val_loss: 200.2593\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 198.3318 - val_loss: 158.3007\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 236.1452 - val_loss: 134.8190\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 196.7360 - val_loss: 148.5467\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 199.3019 - val_loss: 156.7456\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 198.7701 - val_loss: 174.2055\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 212.9444 - val_loss: 381.2898\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 219.0312 - val_loss: 168.2149\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 260.2554 - val_loss: 181.0182\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 247.0656 - val_loss: 151.3597\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 235.8590 - val_loss: 153.1783\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 235.2920 - val_loss: 148.3806\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 200.2845 - val_loss: 170.1989\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 193.5408 - val_loss: 190.7159\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 183.0563 - val_loss: 150.4539\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 194.3367 - val_loss: 137.4896\n",
      "Epoch 419/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 69us/step - loss: 225.6963 - val_loss: 168.9105\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 193.2495 - val_loss: 207.1532\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 200.9445 - val_loss: 135.6189\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 218.7209 - val_loss: 144.9700\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 199.8270 - val_loss: 243.7440\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 187.4639 - val_loss: 155.6383\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 226.7575 - val_loss: 340.0275\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 199.7939 - val_loss: 172.4631\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 198.3277 - val_loss: 147.2791\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 212.4366 - val_loss: 270.1891\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 179.9325 - val_loss: 140.5955\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 187.2308 - val_loss: 142.4887\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 211.8282 - val_loss: 134.6735\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 209.0811 - val_loss: 151.6402\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 178.7412 - val_loss: 135.9770\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 237.1099 - val_loss: 283.2510\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 218.6972 - val_loss: 304.7623\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 195.2522 - val_loss: 204.0299\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 189.5046 - val_loss: 146.5249\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 204.9969 - val_loss: 149.8814\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 302.5742 - val_loss: 329.9615\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 180.7328 - val_loss: 136.4880\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 190.2561 - val_loss: 236.7462\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 302.3531 - val_loss: 152.5831\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 205.8896 - val_loss: 145.0140\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 217.3453 - val_loss: 173.8006\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 207.3442 - val_loss: 189.1813\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 229.9545 - val_loss: 270.6289\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 189.7223 - val_loss: 145.6176\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 206.7743 - val_loss: 175.6729\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 192.0441 - val_loss: 257.6590\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 193.3749 - val_loss: 373.8716\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 266.5886 - val_loss: 211.4843\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 207.1963 - val_loss: 131.8265\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 202.0731 - val_loss: 128.2332\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 181.3745 - val_loss: 135.1048\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 197.6013 - val_loss: 131.8796\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 175.4082 - val_loss: 264.8400\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 202.1083 - val_loss: 229.7858\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 197.7347 - val_loss: 137.4428\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 192.1703 - val_loss: 193.0577\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 216.6675 - val_loss: 147.0628\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 234.1892 - val_loss: 151.0195\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 219.7393 - val_loss: 467.2683\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 227.4143 - val_loss: 175.0883\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 191.8539 - val_loss: 275.8313\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 215.8849 - val_loss: 145.1101\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 183.0515 - val_loss: 220.8103\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 232.1821 - val_loss: 155.8555\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 196.9150 - val_loss: 159.6574\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 181.7781 - val_loss: 137.1613\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 213.7435 - val_loss: 161.8083\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 188.1967 - val_loss: 200.5566\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 270.6462 - val_loss: 170.7146\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 197.1417 - val_loss: 169.0775\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 198.1354 - val_loss: 135.1078\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 197.2055 - val_loss: 188.1662\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 394.5165 - val_loss: 241.4675\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 193.9940 - val_loss: 131.8081\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 189.0189 - val_loss: 132.0120\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 248.4541 - val_loss: 162.7145\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 175.3632 - val_loss: 138.0150\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 187.3705 - val_loss: 144.7538\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 220.1372 - val_loss: 133.0962\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 1s 127us/step - loss: 197.9115 - val_loss: 127.4697\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 203.3428 - val_loss: 132.4827\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 200.4140 - val_loss: 153.5655\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 194.0010 - val_loss: 226.7141\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 201.7105 - val_loss: 131.3281\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 192.3905 - val_loss: 129.1204\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 184.3266 - val_loss: 159.9118\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 213.100 - 0s 51us/step - loss: 214.5397 - val_loss: 165.3060\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 217.0452 - val_loss: 425.5349\n",
      "Epoch 492/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 219.8327 - val_loss: 140.1226\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 187.7571 - val_loss: 136.2064\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 194.1597 - val_loss: 173.7092\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 202.1550 - val_loss: 141.6891\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 187.2245 - val_loss: 129.2815\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 191.9860 - val_loss: 130.0535\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 196.6177 - val_loss: 141.6924\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 241.1085 - val_loss: 137.7501\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 188.5755 - val_loss: 144.1121\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 194.0275 - val_loss: 287.1281\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 181.9009 - val_loss: 134.9491\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 219.5741 - val_loss: 138.5381\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 287.8018 - val_loss: 141.4918\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 179.8602 - val_loss: 137.2075\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 173.6474 - val_loss: 162.4354\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 166.9693 - val_loss: 135.1101\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 187.5019 - val_loss: 222.1679\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 203.4857 - val_loss: 145.6687\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 255.2612 - val_loss: 130.8219\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.1792 - val_loss: 138.7320\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 195.4993 - val_loss: 159.5272\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 165.5379 - val_loss: 129.6521\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 167.7047 - val_loss: 125.3940\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 188.5256 - val_loss: 131.6057\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 282.5115 - val_loss: 172.6421\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 254.4049 - val_loss: 216.7617\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 227.1814 - val_loss: 165.8163\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 235.0348 - val_loss: 129.5196\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 217.3395 - val_loss: 140.3661\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 200.6155 - val_loss: 179.4750\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 185.5345 - val_loss: 254.9154\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 176.2070 - val_loss: 130.8727\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 210.5593 - val_loss: 172.2193\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 234.8023 - val_loss: 147.5715\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 220.5791 - val_loss: 178.7407\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.7977 - val_loss: 333.7137\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 224.0301 - val_loss: 134.2555\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 186.5866 - val_loss: 749.8078\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 206.3949 - val_loss: 141.7874\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 185.0631 - val_loss: 142.1528\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 188.5962 - val_loss: 139.7894\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 187.4851 - val_loss: 199.0455\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 211.3176 - val_loss: 138.9103\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 196.9665 - val_loss: 140.9958\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 199.3558 - val_loss: 134.4305\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 184.6731 - val_loss: 135.1139\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 181.9073 - val_loss: 191.7700\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 170.5858 - val_loss: 155.4150\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 206.9750 - val_loss: 243.2595\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 232.1506 - val_loss: 177.5165\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 182.6693 - val_loss: 128.0683\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 325.5960 - val_loss: 178.1545\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 235.1895 - val_loss: 162.9878\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 325.0725 - val_loss: 214.2573\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 281.5324 - val_loss: 141.1698\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 185.3701 - val_loss: 127.2335\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 162.6654 - val_loss: 166.9547\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 196.7407 - val_loss: 135.6662\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 191.3518 - val_loss: 128.1820\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.7690 - val_loss: 144.8279\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 230.0701 - val_loss: 209.7815\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 206.0195 - val_loss: 192.2503\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 165.2109 - val_loss: 138.1855\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 173.2474 - val_loss: 122.5602\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 215.9084 - val_loss: 142.0496\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 175.5114 - val_loss: 201.8116\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 224.8365 - val_loss: 168.3647\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 203.1272 - val_loss: 170.7944\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 180.6225 - val_loss: 130.1952\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.6932 - val_loss: 191.5750\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 364.4497 - val_loss: 210.7859\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 275.8178 - val_loss: 194.3081\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 211.5958 - val_loss: 159.9286\n",
      "Epoch 565/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 211.6176 - val_loss: 160.5619\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 250.8884 - val_loss: 168.0935\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 212.3380 - val_loss: 135.0296\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 202.2939 - val_loss: 127.2839\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 189.0346 - val_loss: 145.2117\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 175.2529 - val_loss: 124.5822\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 180.5656 - val_loss: 150.9253\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 177.0951 - val_loss: 137.9971\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 183.6173 - val_loss: 192.2125\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 174.4716 - val_loss: 123.5432\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 488.3307 - val_loss: 230.7878\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 265.4626 - val_loss: 423.3156\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 402.3240 - val_loss: 494.4966\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 272.2670 - val_loss: 137.1083\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 204.1018 - val_loss: 124.3827\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 172.3672 - val_loss: 126.8243\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 192.4196 - val_loss: 158.9380\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 208.7356 - val_loss: 145.9755\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 177.0213 - val_loss: 127.9956\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 191.1043 - val_loss: 190.4179\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 202.6852 - val_loss: 177.9327\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 215.1155 - val_loss: 178.9209\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 240.8060 - val_loss: 132.1485\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 172.5118 - val_loss: 160.8501\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 179.5497 - val_loss: 141.4346\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 233.1690 - val_loss: 201.8412\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 193.5205 - val_loss: 142.8887\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 200.7318 - val_loss: 2755.3591\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 566.9831 - val_loss: 200.5974\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 266.6172 - val_loss: 469.2006\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 246.0943 - val_loss: 178.3669\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 224.2824 - val_loss: 145.0090\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 208.6029 - val_loss: 171.8375\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 197.2572 - val_loss: 322.6611\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 220.2108 - val_loss: 142.1695\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 207.2109 - val_loss: 238.9268\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 230.5105 - val_loss: 186.8887\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 196.3358 - val_loss: 138.1381\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 195.7128 - val_loss: 131.5932\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 188.6005 - val_loss: 186.4364\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 189.6391 - val_loss: 155.2661\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 177.9050 - val_loss: 301.4778\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 187.4324 - val_loss: 131.5804\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 250.2685 - val_loss: 154.5361\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 214.1751 - val_loss: 176.4745\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 178.9667 - val_loss: 152.1747\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 215.7287 - val_loss: 219.8786\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 193.5141 - val_loss: 129.4061\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 172.6181 - val_loss: 132.5995\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 177.5826 - val_loss: 146.1710\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 217.6245 - val_loss: 147.1193\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 203.7230 - val_loss: 190.8515\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 241.3601 - val_loss: 134.8667\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 176.0324 - val_loss: 151.5187\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 181.5383 - val_loss: 133.5836\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 187.5491 - val_loss: 190.7852\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 191.3096 - val_loss: 130.3820\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 180.4715 - val_loss: 124.9527\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 176.1867 - val_loss: 134.8564\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 209.8267 - val_loss: 143.5415\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 200.1960 - val_loss: 138.5740\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 183.0956 - val_loss: 142.9282\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 175.1302 - val_loss: 128.6913\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 218.2813 - val_loss: 162.4907\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 175.6970 - val_loss: 131.5974\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 179.2195 - val_loss: 125.3919\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 181.3000 - val_loss: 128.5715\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 182.4640 - val_loss: 134.5255\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 214.5797 - val_loss: 143.5639\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 224.2874 - val_loss: 140.6571\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 187.8220 - val_loss: 146.0459\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 201.9550 - val_loss: 150.3236\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 193.4911 - val_loss: 214.5398\n",
      "Epoch 638/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 186.2918 - val_loss: 151.8640\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 184.1792 - val_loss: 159.6387\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 181.3420 - val_loss: 199.2456\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 183.8585 - val_loss: 133.4702- ETA: 0s - loss: 184.\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 170.9659 - val_loss: 217.6070\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 200.8481 - val_loss: 138.0464\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 247.8519 - val_loss: 137.5231\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 220.8420 - val_loss: 132.2742\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 204.1068 - val_loss: 189.4061\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 317.4896 - val_loss: 138.2643\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 205.3948 - val_loss: 159.2781\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 188.0958 - val_loss: 138.1071\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 172.3543 - val_loss: 126.7509\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 183.5394 - val_loss: 127.2394\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 226.8576 - val_loss: 131.3488\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 218.0449 - val_loss: 126.7059\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 172.4438 - val_loss: 134.3739\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 198.9860 - val_loss: 136.6042\n",
      "Epoch 00655: early stopping\n",
      "Fold score (RMSE): 11.221752166748047\n"
     ]
    }
   ],
   "source": [
    "# Cross-Validate\n",
    "kf = KFold(5)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=filename_checkpoint, verbose=0, save_best_only=True)\n",
    "\n",
    "# Turn off KFold\n",
    "#if (0):\n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "    \n",
    "for train, test in kf.split(x):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "    model.add(Dropout(0.01)) # Dropout Layer 1\n",
    "    model.add(Dense(50, activation='relu')) # Hidden 2\n",
    "    #model.add(Dropout(0.01)) # Dropout Layer 2\n",
    "    model.add(Dense(25, \n",
    "                    kernel_regularizer=regularizers.l2(0.01), #L2 regularization\n",
    "                    activity_regularizer=regularizers.l1(0.01), #L1 Lasso regularization\n",
    "                    activation='relu')) # Hidden 3 \n",
    "    model.add(Dense(10, activation='relu')) # Hidden 4\n",
    "    model.add(Dense(1)) # Output\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=1000, verbose=1, mode='auto')\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpoint],verbose=1,epochs=10000)\n",
    "    \n",
    "    pred = model.predict(x_test)\n",
    "    \n",
    "    oos_y.append(y_test)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure this fold's RMSE\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "    print(\"Fold score (RMSE): {}\".format(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-sample RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final, out of sample score (RMSE): 12.610228538513184\n"
     ]
    }
   ],
   "source": [
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))\n",
    "print(\"Final, out of sample score (RMSE): {}\".format(score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#pred = model.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "#score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "#print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "#chart_regression(pred.flatten(),y_test)\n",
    "#chart_regression(pred.flatten(),y_test,sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Best Weights and Predict on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(filename_checkpoint)\n",
    "\n",
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "extract_and_encode_features(df_test)\n",
    "\n",
    "ids_test = df_test['id']\n",
    "df_test.drop('id',1,inplace=True)\n",
    "\n",
    "names_test = df_test['name']\n",
    "df_test.drop('name',1,inplace=True)\n",
    "\n",
    "x_submit = df_test.as_matrix().astype(np.float32)\n",
    "pred_submit = model.predict(x_submit)\n",
    "\n",
    "# Handles negative cost values. Use inverse\n",
    "cost = [n if n > 0 else n * -1 for n in pred_submit[:,0]]\n",
    "\n",
    "df_submit = pd.DataFrame({'id': ids_test,'cost': cost})\n",
    "df_submit = df_submit[['id', 'cost']]\n",
    "df_submit.to_csv(filename_submit, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
