{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n",
    "\n",
    "**Kaggle Assignment: **\n",
    "\n",
    "**Student Name: Jason Walker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## Description\n",
    "This is one of the projects from the course T81-855: Applications of Deep Learning at Washington University in St. Louis. All students must create a Kaggle account and submit a solution. Once you have submitted your solution entry log into Blackboard (at WUSTL) and submit a single file telling me your Kaggle name on the leaderboard (you do not need to register to Kaggle with your real name). This competition will be visible to the public, so there may be non-student submissions as well as student.\n",
    "\n",
    "The data set for this competition consists of a number of input columns that should be used to predict a stores sales. This is a regression problem. The inputs are a mixture of discrete and category values. The data set is from a simulation.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The evaluation pages describes how submissions will be scored and how students should format their submissions. The scores are in RMSE.\n",
    "Submission Format\n",
    "\n",
    "For every store in the dataset, submission files should contain a sales volume.\n",
    "\n",
    "The file should contain a header and have the following format:\n",
    "\n",
    "```\n",
    "100000,1.23\n",
    "100001,1.123\n",
    "100002,3.332\n",
    "100003,1.53\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The data contains data and costs for various office supplies. The data came from a simulation and do not directly correspond to any real-world items. See how well you can predict the cost of an item using the provided data. Feature engineering will likely help you. The *name* column may seem useless at first glance; however, it contains information that you can parse to help your predictions.\n",
    "File descriptions\n",
    "```\n",
    "    id - The identifier/primary key.\n",
    "    name - The name of this item.\n",
    "    manufacturer - The manufacturer.\n",
    "    pack - The number of items in this pack.\n",
    "    weight - The weight of a pack of these items.\n",
    "    height - The height of a pack of these items.\n",
    "    width - The width of a pack of these items.\n",
    "    length - The length of a pack of these items.\n",
    "    cost - The cost for this item pack. This is what you are to predict (the target). \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions\n",
    "\n",
    "You will see these at the top of every module and assignment.  These are simply a set of reusable functions that we will make use of.  Each of them will be explained as the semester progresses.  They are explained in greater detail as the course progresses.  Class 4 contains a complete overview of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n",
    "        \n",
    "# This function submits an assignment.  You can submit an assignment as much as you like, only the final\n",
    "# submission counts.  The paramaters are as follows:\n",
    "# data - Pandas dataframe output.\n",
    "# key - Your student key that was emailed to you.\n",
    "# no - The assignment class number, should be 1 through 1.\n",
    "# source_file - The full path to your Python or IPYNB file.  This must have \"_class1\" as part of its name.  \n",
    "# .             The number must match your assignment number.  For example \"_class2\" for class assignment #2.\n",
    "def submit(data,key,no,source_file=None):\n",
    "    if source_file is None and '__file__' not in globals(): raise Exception('Must specify a filename when a Jupyter notebook.')\n",
    "    if source_file is None: source_file = __file__\n",
    "    suffix = '_class{}'.format(no)\n",
    "    if suffix not in source_file: raise Exception('{} must be part of the filename.'.format(suffix))\n",
    "    with open(source_file, \"rb\") as image_file:\n",
    "        encoded_python = base64.b64encode(image_file.read()).decode('ascii')\n",
    "    ext = os.path.splitext(source_file)[-1].lower()\n",
    "    if ext not in ['.ipynb','.py']: raise Exception(\"Source file is {} must be .py or .ipynb\".format(ext))\n",
    "    r = requests.post(\"https://api.heatonresearch.com/assignment-submit\",\n",
    "        headers={'x-api-key':key}, json={'csv':base64.b64encode(data.to_csv(index=False).encode('ascii')).decode(\"ascii\"),\n",
    "        'assignment': no, 'ext':ext, 'py':encoded_python})\n",
    "    if r.status_code == 200:\n",
    "        print(\"Success: {}\".format(r.text))\n",
    "    else: print(\"Failure: {}\".format(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kaggle Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers\n",
    "\n",
    "path = './data'\n",
    "\n",
    "filename_test = os.path.join(path,\"test.csv\")\n",
    "filename_train = os.path.join(path,\"train.csv\")\n",
    "filename_sample = os.path.join(path,\"sample.csv\")\n",
    "filename_submit = os.path.join(path,\"submit.csv\")\n",
    "filename_checkpoint = os.path.join(path,\"checkpoint.hdf5\")\n",
    "\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "\n",
    "np.random.seed(42) # Uncomment this line to get the same shuffle each time\n",
    "df_train = df_train.reindex(np.random.permutation(df_train.index))\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Encode Features\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def extract_and_encode_features(df):\n",
    "    color_regex='(?P<color>red|blue|green|yellow|orange|pink|black|brown|white)'\n",
    "    df['color'] = df.name.str.extract(color_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    quality_regex='(?P<quality>generic|high\\squality)'\n",
    "    df['quality'] = df.name.str.extract(quality_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    size_regex='(?P<size>tiny|small|medium|large)'\n",
    "    df['size'] = df.name.str.extract(size_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    item_regex='(?P<item>paperclips|paperweights|ink\\spens|pencils|stapler|tablets|thumbtacks|post\\sit\\snotes)'\n",
    "    df['item'] = df.name.str.extract(item_regex, flags=re.IGNORECASE, expand=False)\n",
    "    \n",
    "    for column in ['pack','weight','height','width','length']:\n",
    "        missing_median(df,column)\n",
    "    \n",
    "    #df.insert(1,'surface_area',(df['height']*df['width']*df['length']).astype(int))\n",
    "    \n",
    "    ## encode numeric features\n",
    "    #for column in ['pack','weight','height','width','length','surface_area']:\n",
    "    #    encode_numeric_zscore(df,column)\n",
    "\n",
    "    # encode text/categorical features\n",
    "    for column in ['manufacturer','color','quality','size','item']:\n",
    "        encode_text_dummy(df,column)\n",
    "  \n",
    "extract_and_encode_features(df_train)\n",
    "\n",
    "ids_train = df_train['id']\n",
    "df_train.drop('id',1,inplace=True)\n",
    "\n",
    "names_train = df_train['name']\n",
    "df_train.drop('name',1,inplace=True)\n",
    "\n",
    "x,y = to_xy(df_train,'cost')\n",
    "\n",
    "# Cross-Validate\n",
    "kf = KFold(5)\n",
    "\n",
    "# Used before KFold\n",
    "#x_train, x_test, y_train, y_test = train_test_split(    \n",
    "#    x, y, test_size=0.25, random_state=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 32.5279541015625\n",
      "['pack', 'weight', 'height', 'width', 'length', 'manufacturer-6% Solution', 'manufacturer-Deep Office Supplies', 'manufacturer-Duck Lake', 'manufacturer-Offices-R-Us', 'manufacturer-WizBang', 'color-Black', 'color-Blue', 'color-Brown', 'color-Green', 'color-Pink', 'color-Red', 'color-White', 'quality-Generic', 'quality-High Quality', 'size-Large', 'size-Medium', 'size-Small', 'size-Tiny', 'item-Ink Pens', 'item-Paperclips', 'item-Paperweights', 'item-Pencils', 'item-Post It Notes', 'item-Stapler', 'item-Tablets', 'item-Thumbtacks']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item-Post It Notes</th>\n",
       "      <td>-60.369431</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Thumbtacks</th>\n",
       "      <td>-57.847595</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Pencils</th>\n",
       "      <td>-50.215904</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperclips</th>\n",
       "      <td>-43.970024</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Red</th>\n",
       "      <td>-39.540787</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Green</th>\n",
       "      <td>-27.516720</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Blue</th>\n",
       "      <td>-13.250540</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>-10.317136</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>-8.247688</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>width</th>\n",
       "      <td>-6.184750</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Ink Pens</th>\n",
       "      <td>-3.713280</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Generic</th>\n",
       "      <td>-3.706334</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Tiny</th>\n",
       "      <td>-2.535614</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Offices-R-Us</th>\n",
       "      <td>-1.422012</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Brown</th>\n",
       "      <td>-1.184677</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Small</th>\n",
       "      <td>-0.868019</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Deep Office Supplies</th>\n",
       "      <td>-0.601757</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack</th>\n",
       "      <td>0.019117</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>0.034992</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-6% Solution</th>\n",
       "      <td>0.206511</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Duck Lake</th>\n",
       "      <td>0.396286</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-WizBang</th>\n",
       "      <td>1.421118</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Medium</th>\n",
       "      <td>2.418503</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Large</th>\n",
       "      <td>5.069157</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperweights</th>\n",
       "      <td>7.354851</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-High Quality</th>\n",
       "      <td>8.346462</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Black</th>\n",
       "      <td>12.828798</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-White</th>\n",
       "      <td>27.035027</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Pink</th>\n",
       "      <td>37.166756</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Stapler</th>\n",
       "      <td>48.908066</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Tablets</th>\n",
       "      <td>159.851807</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         coef  positive\n",
       "item-Post It Notes                 -60.369431     False\n",
       "item-Thumbtacks                    -57.847595     False\n",
       "item-Pencils                       -50.215904     False\n",
       "item-Paperclips                    -43.970024     False\n",
       "color-Red                          -39.540787     False\n",
       "color-Green                        -27.516720     False\n",
       "color-Blue                         -13.250540     False\n",
       "height                             -10.317136     False\n",
       "length                              -8.247688     False\n",
       "width                               -6.184750     False\n",
       "item-Ink Pens                       -3.713280     False\n",
       "quality-Generic                     -3.706334     False\n",
       "size-Tiny                           -2.535614     False\n",
       "manufacturer-Offices-R-Us           -1.422012     False\n",
       "color-Brown                         -1.184677     False\n",
       "size-Small                          -0.868019     False\n",
       "manufacturer-Deep Office Supplies   -0.601757     False\n",
       "pack                                 0.019117      True\n",
       "weight                               0.034992      True\n",
       "manufacturer-6% Solution             0.206511      True\n",
       "manufacturer-Duck Lake               0.396286      True\n",
       "manufacturer-WizBang                 1.421118      True\n",
       "size-Medium                          2.418503      True\n",
       "size-Large                           5.069157      True\n",
       "item-Paperweights                    7.354851      True\n",
       "quality-High Quality                 8.346462      True\n",
       "color-Black                         12.828798      True\n",
       "color-White                         27.035027      True\n",
       "color-Pink                          37.166756      True\n",
       "item-Stapler                        48.908066      True\n",
       "item-Tablets                       159.851807      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [ 84.28936005]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAD8CAYAAADExYYgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIwR7rwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2f15fb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple function to evaluate the coefficients of a regression\n",
    "%matplotlib inline    \n",
    "from IPython.display import display, HTML    \n",
    "\n",
    "def report_coef(names,coef,intercept):\n",
    "    r = pd.DataFrame( { 'coef': coef, 'positive': coef>=0  }, index = names )\n",
    "    r = r.sort_values(by=['coef'])\n",
    "    display(r)\n",
    "    print(\"Intercept: {}\".format(intercept))\n",
    "    r['coef'].plot(kind='barh', color=r['positive'].map({True: 'b', False: 'r'}))\n",
    "    \n",
    "# Create linear regression\n",
    "regressor = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# Fit/train linear regression\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "print(names)\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_[0,:],\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 3s 317us/step - loss: 5624.3019 - val_loss: 4982.2033\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 5068.3938 - val_loss: 3891.3121\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 4688.6452 - val_loss: 3762.7149\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 4459.6021 - val_loss: 3707.6230\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 4422.3044 - val_loss: 3635.7628\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 4254.1534 - val_loss: 3585.1237\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 4248.8254 - val_loss: 3482.9459\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 4125.7861 - val_loss: 3490.6342\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 4167.9337 - val_loss: 3379.9448\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 3949.0243 - val_loss: 3477.1045\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 3908.7172 - val_loss: 4350.0599\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 3787.0852 - val_loss: 3253.3505\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 3486.7471 - val_loss: 2688.0965\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 3273.0546 - val_loss: 3471.3681\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 2937.7539 - val_loss: 1966.8903\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 2516.2970 - val_loss: 1653.9447\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 2134.7474 - val_loss: 1587.9957\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 1880.1207 - val_loss: 1260.2921\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 1397.4983 - val_loss: 840.1960\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 1143.9425 - val_loss: 729.8945\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 980.7806 - val_loss: 617.5401\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 1060.4991 - val_loss: 2219.6034\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 992.3560 - val_loss: 903.1955\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 728.0620 - val_loss: 544.4245\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 719.7324 - val_loss: 639.2225\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 859.3998 - val_loss: 720.6135\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 634.8608 - val_loss: 405.6296\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 634.0879 - val_loss: 395.2502\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 637.1631 - val_loss: 652.3098\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 667.2859 - val_loss: 558.7488\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 649.5768 - val_loss: 698.0188\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 605.3821 - val_loss: 447.6536\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 500.9776 - val_loss: 428.1725\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 665.5693 - val_loss: 513.5792\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 522.9787 - val_loss: 452.6145\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 538.4831 - val_loss: 767.5220\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 584.0479 - val_loss: 345.7885\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 555.0465 - val_loss: 327.3341\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 575.7523 - val_loss: 407.3612\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 502.7564 - val_loss: 396.9693\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 515.5638 - val_loss: 326.9135\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 512.9163 - val_loss: 335.6835\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 458.5628 - val_loss: 459.5759\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 477.0854 - val_loss: 418.4476\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 502.7304 - val_loss: 350.7602\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 428.0419 - val_loss: 461.2359\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 442.8806 - val_loss: 737.0550\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 452.7669 - val_loss: 312.5606\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 570.1187 - val_loss: 360.0007\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 448.1030 - val_loss: 295.1386\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 446.5313 - val_loss: 282.4428\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 374.9795 - val_loss: 505.5954\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 459.0762 - val_loss: 337.5811\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 376.5788 - val_loss: 698.9862\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 444.6474 - val_loss: 281.8791\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 390.6143 - val_loss: 268.0792\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 378.8475 - val_loss: 360.4248\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 482.3842 - val_loss: 678.6701\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 413.4457 - val_loss: 272.6357\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 380.7041 - val_loss: 907.8881\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 468.5462 - val_loss: 272.6864\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 384.6701 - val_loss: 311.5289\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 348.0226 - val_loss: 285.2620\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 378.5211 - val_loss: 316.4328\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 393.8154 - val_loss: 354.0144\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 424.4494 - val_loss: 277.6020\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 359.0469 - val_loss: 328.1143\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 349.5004 - val_loss: 232.5626\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 327.9535 - val_loss: 353.4723\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 498.7391 - val_loss: 308.6538\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 329.6072 - val_loss: 320.8643\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 349.1890 - val_loss: 251.9155\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 331.3123 - val_loss: 435.2090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 371.0766 - val_loss: 953.3586\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 485.6512 - val_loss: 287.1195\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 323.0671 - val_loss: 301.3118\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 328.6502 - val_loss: 244.8098\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 294.5819 - val_loss: 358.8018\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 283.8425 - val_loss: 249.2717\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 310.9377 - val_loss: 244.4925\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 359.8256 - val_loss: 231.2172\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 395.9669 - val_loss: 477.0307\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 295.2019 - val_loss: 239.6518\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 290.6420 - val_loss: 281.5659\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 325.2355 - val_loss: 296.7584\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 340.7731 - val_loss: 237.4514\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 327.7077 - val_loss: 895.8609\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 304.5016 - val_loss: 244.0357\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 356.5341 - val_loss: 1165.7105\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 291.1275 - val_loss: 208.1291\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 331.3614 - val_loss: 307.5503\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 339.7873 - val_loss: 259.7881\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 289.2506 - val_loss: 389.3948\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 333.5235 - val_loss: 486.4902\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 283.0310 - val_loss: 236.4982\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 349.9899 - val_loss: 234.9699\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 285.4544 - val_loss: 410.8477\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 336.0361 - val_loss: 256.9022\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 290.8071 - val_loss: 212.4524\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 475.5787 - val_loss: 896.6752\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 444.1068 - val_loss: 572.5103\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 301.6787 - val_loss: 227.9107\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 269.6640 - val_loss: 219.9504\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 279.7015 - val_loss: 212.0549\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 269.7662 - val_loss: 212.5060\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 478.9538 - val_loss: 440.3073\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 325.1020 - val_loss: 255.1999\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 349.9494 - val_loss: 229.2492\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 259.0175 - val_loss: 227.6210\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 272.7638 - val_loss: 259.4457\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 251.7151 - val_loss: 200.6680\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 333.5816 - val_loss: 265.1721\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 338.8519 - val_loss: 241.2333\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 262.2642 - val_loss: 266.2617\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 260.9058 - val_loss: 201.2062\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 296.9099 - val_loss: 222.4637\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 365.7970 - val_loss: 227.3335\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 357.2289 - val_loss: 202.8501\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 270.2099 - val_loss: 218.3548\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 251.5503 - val_loss: 342.9821\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 309.3318 - val_loss: 336.1127\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 324.9853 - val_loss: 220.4296\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 259.1269 - val_loss: 218.7139\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 282.1730 - val_loss: 336.4959\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 298.2392 - val_loss: 462.5091\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 311.2795 - val_loss: 214.0225\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 242.9018 - val_loss: 187.6596\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 298.8646 - val_loss: 270.5740\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 268.9132 - val_loss: 294.5002\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 259.5764 - val_loss: 191.9636\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 283.4567 - val_loss: 266.6291\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 251.0703 - val_loss: 304.7211\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 257.4255 - val_loss: 380.7572\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 331.9371 - val_loss: 237.1789\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 297.6381 - val_loss: 278.3349\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 263.5909 - val_loss: 275.5421\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 270.5561 - val_loss: 408.5577\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 303.5120 - val_loss: 225.2398\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 251.2183 - val_loss: 224.0238\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 246.7392 - val_loss: 200.3690\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 299.8632 - val_loss: 401.8777\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 283.1720 - val_loss: 244.3496\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 265.0580 - val_loss: 240.3993\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 215.3121 - val_loss: 296.7447\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 245.3966 - val_loss: 210.9902\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 232.4448 - val_loss: 399.7348\n",
      "Epoch 147/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 66us/step - loss: 287.7566 - val_loss: 209.1656\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 270.7238 - val_loss: 361.9252\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 232.3643 - val_loss: 341.4110\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 336.3872 - val_loss: 238.8333\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 228.6927 - val_loss: 191.2031\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 238.5040 - val_loss: 182.8452\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 228.1476 - val_loss: 175.0697\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 242.3212 - val_loss: 1199.6603\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 413.0803 - val_loss: 199.8866\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 229.2499 - val_loss: 197.1927\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 274.0170 - val_loss: 362.6804\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 218.6333 - val_loss: 177.3308\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 293.6555 - val_loss: 239.4489\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 239.7922 - val_loss: 280.8630\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 249.5202 - val_loss: 199.4959\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 282.4585 - val_loss: 232.9699\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 283.6887 - val_loss: 468.1193\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 246.6489 - val_loss: 227.5922\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 235.2653 - val_loss: 224.8242\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 262.1301 - val_loss: 202.5122\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 235.1205 - val_loss: 243.2324\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 244.3727 - val_loss: 475.4330\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 366.8506 - val_loss: 693.0326\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 276.7752 - val_loss: 203.8342\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 222.0247 - val_loss: 278.9981\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 253.6110 - val_loss: 185.5644\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 278.7376 - val_loss: 261.1102\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 259.9606 - val_loss: 258.3342\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 235.8342 - val_loss: 185.9240\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 363.7369 - val_loss: 190.2194\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 200.2088 - val_loss: 195.5148\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 223.9622 - val_loss: 183.4113\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 215.9478 - val_loss: 208.2825\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 251.7196 - val_loss: 620.9364\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 354.1243 - val_loss: 217.9979\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 237.9382 - val_loss: 176.1013\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 241.5322 - val_loss: 223.3097\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 260.3477 - val_loss: 219.8376\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 215.3560 - val_loss: 341.8716\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 340.2761 - val_loss: 429.1601\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 289.7523 - val_loss: 195.2931\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 218.3034 - val_loss: 366.0261\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 257.8669 - val_loss: 206.7446\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 215.8712 - val_loss: 185.2017\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 235.3236 - val_loss: 216.8582\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 232.5871 - val_loss: 184.2898\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 220.0347 - val_loss: 231.6995\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 245.9501 - val_loss: 517.2318\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 401.6412 - val_loss: 189.8816\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 234.8177 - val_loss: 169.6116\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 263.5523 - val_loss: 322.4859\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 251.7582 - val_loss: 202.2134\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 227.0541 - val_loss: 172.5108\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 241.3833 - val_loss: 268.7323\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 233.5607 - val_loss: 235.9685\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 309.0695 - val_loss: 237.7934\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 217.1714 - val_loss: 173.1823\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 220.1490 - val_loss: 201.4576\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 222.1418 - val_loss: 171.2593\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 219.3002 - val_loss: 171.1633\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 212.5588 - val_loss: 256.6567\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 327.0551 - val_loss: 309.4573\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 320.6879 - val_loss: 239.0631\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 210.4862 - val_loss: 239.0412\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 215.7192 - val_loss: 174.9897\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 202.7371 - val_loss: 207.8806\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 213.4293 - val_loss: 191.3114\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 313.4764 - val_loss: 223.3956\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 408.9243 - val_loss: 182.9392\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 202.1594 - val_loss: 282.2585\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 271.3993 - val_loss: 320.5228\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 316.3781 - val_loss: 181.1254\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 217.1280 - val_loss: 172.5153\n",
      "Epoch 220/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 63us/step - loss: 215.7802 - val_loss: 485.3178\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 221.0094 - val_loss: 371.7806\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 263.7156 - val_loss: 169.7739\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 248.1027 - val_loss: 208.7873\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 247.3086 - val_loss: 165.4756\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 263.0621 - val_loss: 192.6232\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 210.6889 - val_loss: 172.2014\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 330.2348 - val_loss: 192.2240\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 223.8258 - val_loss: 188.0233\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 195.3741 - val_loss: 240.1258\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 474.7355 - val_loss: 256.0272\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 276.5052 - val_loss: 235.7056\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 264.5877 - val_loss: 198.3085\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 240.5083 - val_loss: 174.0780\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 208.1286 - val_loss: 164.7862\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 227.3940 - val_loss: 276.1418\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 237.1473 - val_loss: 167.2486\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 215.8086 - val_loss: 251.9590\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 223.7742 - val_loss: 166.5458\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 420.5483 - val_loss: 289.6740\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 275.2844 - val_loss: 263.1328\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 233.3144 - val_loss: 205.1477\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 198.5562 - val_loss: 187.7681\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 226.7084 - val_loss: 173.7030\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 188.8015 - val_loss: 166.6433\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 251.6592 - val_loss: 221.2493\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 200.7642 - val_loss: 278.6955\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 211.6364 - val_loss: 186.7552\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 216.6236 - val_loss: 193.1659\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 193.4798 - val_loss: 184.3090\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 374.8155 - val_loss: 177.0969\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 190.0235 - val_loss: 166.1374\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 301.3992 - val_loss: 163.6460\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 221.2074 - val_loss: 238.0832\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 248.6581 - val_loss: 496.1708\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 199.1877 - val_loss: 175.4023\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 215.0406 - val_loss: 188.5694\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 249.2786 - val_loss: 265.1194\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 208.0715 - val_loss: 165.0285\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 220.8881 - val_loss: 182.4243\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 208.5157 - val_loss: 215.3211\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 202.5383 - val_loss: 174.6445\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 328.8243 - val_loss: 311.5036\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 241.4021 - val_loss: 195.1741\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 185.1999 - val_loss: 304.1905\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 226.8159 - val_loss: 180.3498\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 265.5925 - val_loss: 172.9176\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 187.1166 - val_loss: 237.5878\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 369.9759 - val_loss: 166.8891\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 201.7817 - val_loss: 191.6748\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 202.5382 - val_loss: 167.8959\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 206.1238 - val_loss: 157.1224\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 211.8905 - val_loss: 172.4982\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 224.2198 - val_loss: 296.9612\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 186.6660 - val_loss: 216.1309\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 175.8497 - val_loss: 170.3085\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 192.2824 - val_loss: 197.5998\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 457.3665 - val_loss: 294.4971\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 302.5053 - val_loss: 217.2170\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 251.0539 - val_loss: 173.0936\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 310.4761 - val_loss: 172.8668\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 242.7655 - val_loss: 166.6400\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 341.1217 - val_loss: 270.3174\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 234.3338 - val_loss: 187.2159\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 220.8400 - val_loss: 290.8998\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 255.5171 - val_loss: 165.5943\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 252.4710 - val_loss: 174.7109\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 244.4493 - val_loss: 273.8683\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 277.4819 - val_loss: 234.2647\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 278.4861 - val_loss: 183.5585\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 279.7209 - val_loss: 239.3051\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 228.5756 - val_loss: 194.3078\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 213.6017 - val_loss: 170.1781\n",
      "Epoch 293/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 232.6794 - val_loss: 283.2064\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 223.3971 - val_loss: 202.2654\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 360.7760 - val_loss: 380.2499\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 237.7064 - val_loss: 198.5851\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 199.0697 - val_loss: 232.3643\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 307.6969 - val_loss: 305.4369\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 205.1341 - val_loss: 188.3225\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 229.4939 - val_loss: 176.6202\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 221.2225 - val_loss: 182.5453\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 231.5449 - val_loss: 180.2771\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 215.5016 - val_loss: 188.1542\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 203.2808 - val_loss: 176.5851\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 223.4629 - val_loss: 164.0984\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 265.0463 - val_loss: 180.2528\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 213.2205 - val_loss: 158.3603\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 207.2913 - val_loss: 159.3947\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 261.1405 - val_loss: 183.8823\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 190.4783 - val_loss: 314.7510\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 277.7743 - val_loss: 276.8542\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 248.8017 - val_loss: 205.6515\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 193.8030 - val_loss: 441.2630\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 238.2962 - val_loss: 214.6334\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 282.1573 - val_loss: 206.7799\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 182.7301 - val_loss: 197.8156\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 207.4778 - val_loss: 169.1779\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 238.2567 - val_loss: 325.5062\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 228.2074 - val_loss: 174.6260\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 227.1808 - val_loss: 228.9286\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 280.4146 - val_loss: 160.5643\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 231.0232 - val_loss: 373.8941\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.6404 - val_loss: 237.4940\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 186.8400 - val_loss: 206.7607\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 237.8976 - val_loss: 328.8152\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 384.6895 - val_loss: 199.5952\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 211.6557 - val_loss: 174.1641\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 201.4602 - val_loss: 223.4369\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 200.1534 - val_loss: 163.5406\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 224.7151 - val_loss: 180.9889\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 273.3621 - val_loss: 184.8361\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 334.5191 - val_loss: 1350.6001\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 256.2681 - val_loss: 163.1126\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 234.1417 - val_loss: 175.2493\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 280.8422 - val_loss: 446.8205\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 190.5567 - val_loss: 152.9021\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 287.8079 - val_loss: 179.1594\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 188.7109 - val_loss: 167.8302\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 206.8623 - val_loss: 159.7588\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 224.5694 - val_loss: 177.4732\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 239.9151 - val_loss: 336.4940\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 206.9006 - val_loss: 202.9261\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 197.7964 - val_loss: 149.6229\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 254.6157 - val_loss: 160.1694\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 238.3261 - val_loss: 197.3784\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 202.1934 - val_loss: 212.5075\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 193.1677 - val_loss: 306.5035\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 303.9176 - val_loss: 157.7761\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 316.2005 - val_loss: 453.7286\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 240.4923 - val_loss: 152.1150\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 197.2759 - val_loss: 166.3291\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 202.7911 - val_loss: 195.8801\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 219.8401 - val_loss: 161.8063\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 203.5723 - val_loss: 265.3939\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 175.6280 - val_loss: 157.0289\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 908.2050 - val_loss: 274.8574\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 260.6339 - val_loss: 180.9593\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 213.1519 - val_loss: 242.1196\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 246.2998 - val_loss: 180.7596\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 260.7302 - val_loss: 166.9354\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.4772 - val_loss: 188.3401\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 211.9499 - val_loss: 351.2932\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 477.2009 - val_loss: 194.5168\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 260.3848 - val_loss: 182.0439\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 249.1760 - val_loss: 190.8719\n",
      "Epoch 366/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 243.5909 - val_loss: 165.3372\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 293.3701 - val_loss: 324.5879\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 224.1930 - val_loss: 224.2980\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 214.4459 - val_loss: 199.0175\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 228.2316 - val_loss: 178.0408\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 200.5987 - val_loss: 275.4594\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 219.1679 - val_loss: 220.3596\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 211.8766 - val_loss: 164.7564\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 253.1624 - val_loss: 385.1811\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 267.5198 - val_loss: 168.8430\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 225.5116 - val_loss: 282.6298\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 215.9854 - val_loss: 195.3111\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 232.0895 - val_loss: 208.9132\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.4523 - val_loss: 176.6167\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 212.8244 - val_loss: 239.1417\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 227.8647 - val_loss: 180.1118\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 326.2809 - val_loss: 163.9926\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 207.9423 - val_loss: 196.5053\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 197.1017 - val_loss: 270.6732\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 221.0522 - val_loss: 385.0431\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 236.2669 - val_loss: 176.6704\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 198.1292 - val_loss: 162.8964\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 329.0785 - val_loss: 155.4125\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 212.9914 - val_loss: 165.0282\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 212.6434 - val_loss: 188.6980\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 202.0423 - val_loss: 180.6901\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 197.1059 - val_loss: 170.7615\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 260.8247 - val_loss: 169.9087\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.3535 - val_loss: 166.1158\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 199.6948 - val_loss: 164.1743\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 307.2124 - val_loss: 174.0549\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 220.1491 - val_loss: 295.2005\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 210.7582 - val_loss: 165.4175\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 212.3518 - val_loss: 155.7909\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 263.2543 - val_loss: 167.3023\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 218.5730 - val_loss: 223.0022\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 276.5967 - val_loss: 163.7958\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 197.0274 - val_loss: 199.3714\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 264.7620 - val_loss: 170.0312\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 247.2385 - val_loss: 310.0849\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.8319 - val_loss: 216.6747\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 194.4066 - val_loss: 253.5151\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 237.1382 - val_loss: 210.0423\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 211.0932 - val_loss: 210.3472\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 191.1259 - val_loss: 153.0776\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 251.3307 - val_loss: 331.4622\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 191.6509 - val_loss: 194.1756\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 279.4436 - val_loss: 165.1957\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 190.0484 - val_loss: 192.7680\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 207.8743 - val_loss: 155.9546\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 247.0291 - val_loss: 152.7157\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.2874 - val_loss: 152.7443\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 226.6104 - val_loss: 255.1787\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 229.9184 - val_loss: 155.4437\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 203.1534 - val_loss: 166.5747\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 212.7549 - val_loss: 288.4482\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 210.1204 - val_loss: 155.6932\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 285.3650 - val_loss: 170.9475\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 176.9062 - val_loss: 153.2641\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 210.7180 - val_loss: 171.6607\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.4948 - val_loss: 216.0519\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 196.3119 - val_loss: 159.0714\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 216.0029 - val_loss: 184.2663\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 188.6932 - val_loss: 171.1400\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 573.4532 - val_loss: 219.4185\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 217.9206 - val_loss: 214.6693\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 164.5830 - val_loss: 145.0736\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 185.0029 - val_loss: 162.7913\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 334.9940 - val_loss: 159.0426\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 190.3228 - val_loss: 219.9781\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 194.1148 - val_loss: 152.7151\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 185.7457 - val_loss: 188.9483\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 191.8814 - val_loss: 172.6489\n",
      "Epoch 439/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 187.7005 - val_loss: 163.7499\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.6217 - val_loss: 155.5521\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 269.8212 - val_loss: 696.3603\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 231.1107 - val_loss: 232.4163\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 201.9253 - val_loss: 363.4618\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 278.2773 - val_loss: 195.0331\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 185.5816 - val_loss: 167.3198\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 184.5327 - val_loss: 170.9548\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.6772 - val_loss: 195.3663\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 184.7261 - val_loss: 154.3655\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 183.9443 - val_loss: 189.7564\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 222.2650 - val_loss: 159.5589\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 214.3468 - val_loss: 150.0037\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 194.0509 - val_loss: 177.0098\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 189.1816 - val_loss: 157.5591\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 402.6918 - val_loss: 157.7689\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 203.8657 - val_loss: 158.4739\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 194.6185 - val_loss: 196.4146\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 180.0393 - val_loss: 156.5135\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 208.7082 - val_loss: 313.4159\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 182.9013 - val_loss: 163.9885\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 217.6401 - val_loss: 173.5476\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 222.4933 - val_loss: 166.9999\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 184.0063 - val_loss: 156.2074\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 191.2769 - val_loss: 195.8713\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 221.8947 - val_loss: 177.5393\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 222.6403 - val_loss: 158.5327\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 180.1911 - val_loss: 158.4060\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 211.0781 - val_loss: 158.1679\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 208.4318 - val_loss: 185.0344\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 178.0154 - val_loss: 166.7945\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 200.8556 - val_loss: 177.8716\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 188.1764 - val_loss: 239.8521\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 237.8452 - val_loss: 268.5227\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 194.7126 - val_loss: 161.5230\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 232.8992 - val_loss: 246.8785\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 193.6261 - val_loss: 243.9505\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 214.3556 - val_loss: 196.6453\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 207.1223 - val_loss: 179.4257\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.6636 - val_loss: 153.4424\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.4454 - val_loss: 152.7332\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 181.9384 - val_loss: 152.7860\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 203.1625 - val_loss: 171.4114\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 191.8692 - val_loss: 177.3506\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 199.5145 - val_loss: 147.9380\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 198.3895 - val_loss: 183.7068\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 177.4661 - val_loss: 156.8528\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 210.4962 - val_loss: 171.9367\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 189.2097 - val_loss: 154.7931\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 210.6494 - val_loss: 167.2403\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.8747 - val_loss: 222.5933\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 290.5921 - val_loss: 154.1003\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 189.8458 - val_loss: 146.9141\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.4897 - val_loss: 214.4702\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 192.9749 - val_loss: 187.0458\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.4557 - val_loss: 148.9431\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 196.1304 - val_loss: 191.1718\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 241.3618 - val_loss: 160.5860\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 256.0989 - val_loss: 199.1336\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.7690 - val_loss: 257.3379\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.1006 - val_loss: 154.1763\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.6690 - val_loss: 150.0734\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 279.0164 - val_loss: 499.3901\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 212.7427 - val_loss: 183.2510\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.9232 - val_loss: 153.1730\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 192.7137 - val_loss: 257.7221\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 220.1124 - val_loss: 180.0395\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 211.3528 - val_loss: 171.5573\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 219.9495 - val_loss: 245.7990\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 193.0919 - val_loss: 180.9290\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 197.0313 - val_loss: 155.8223\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 229.6164 - val_loss: 152.8199\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 199.4918 - val_loss: 203.0607\n",
      "Epoch 512/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 201.0607 - val_loss: 152.3924\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 196.2989 - val_loss: 162.8822\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 189.2500 - val_loss: 148.4142\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 211.1780 - val_loss: 357.8523\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 243.7025 - val_loss: 147.9555\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 180.5890 - val_loss: 151.2866\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.5671 - val_loss: 145.5552\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 171.1836 - val_loss: 169.1728\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.4264 - val_loss: 149.1209\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 197.5243 - val_loss: 172.8011\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 192.0379 - val_loss: 181.0496\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 238.1038 - val_loss: 185.2369\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 187.4592 - val_loss: 249.7987\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 208.9842 - val_loss: 156.2579\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 168.2241 - val_loss: 310.5088\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 250.4297 - val_loss: 329.3627\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 189.9107 - val_loss: 149.4498\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.4461 - val_loss: 153.8531\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 175.5323 - val_loss: 146.6241\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 200.9827 - val_loss: 190.1232\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 190.4449 - val_loss: 151.9021\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 193.9193 - val_loss: 164.4716\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 206.8133 - val_loss: 240.9926\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 194.4493 - val_loss: 259.8632\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 196.0616 - val_loss: 201.0693\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 201.1820 - val_loss: 163.0077\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 231.8576 - val_loss: 160.5276\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 162.2945 - val_loss: 175.0765\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.8518 - val_loss: 151.9279\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 190.0532 - val_loss: 149.1192\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.3956 - val_loss: 146.3068\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 261.3691 - val_loss: 173.2599\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 179.8750 - val_loss: 162.5004\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 197.9177 - val_loss: 227.8562\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 259.2465 - val_loss: 507.1858\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 237.3894 - val_loss: 154.4643\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 188.0084 - val_loss: 162.8032\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 178.6469 - val_loss: 197.8301\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 181.1283 - val_loss: 172.4990\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 169.4066 - val_loss: 236.7757\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.5488 - val_loss: 154.1851\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.9657 - val_loss: 168.6024\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.2245 - val_loss: 151.2317\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 182.2560 - val_loss: 183.7720\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 242.7564 - val_loss: 154.0902\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 238.8582 - val_loss: 147.0655\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.1904 - val_loss: 161.3613\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.3597 - val_loss: 171.3774\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 215.9766 - val_loss: 160.1878\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.8002 - val_loss: 229.2346\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.1147 - val_loss: 150.5495\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.4783 - val_loss: 163.6442\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.9839 - val_loss: 161.8973\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.8244 - val_loss: 147.5044\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 187.7722 - val_loss: 146.9735\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 185.3342 - val_loss: 712.2155\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 212.6998 - val_loss: 146.5945\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.1860 - val_loss: 183.7886\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 192.5634 - val_loss: 148.0583\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 189.0845 - val_loss: 450.8317\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 204.7268 - val_loss: 157.0804\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 184.3051 - val_loss: 262.2609\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 228.9498 - val_loss: 156.5611\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.6795 - val_loss: 167.6733\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 183.1474 - val_loss: 186.2616\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 228.2938 - val_loss: 189.7847\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 185.6949 - val_loss: 210.5560\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.6060 - val_loss: 157.1655\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.0255 - val_loss: 184.4353\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.4317 - val_loss: 153.9240\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.2983 - val_loss: 152.5315\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.9907 - val_loss: 149.7354\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 187.7694 - val_loss: 146.6161\n",
      "Epoch 585/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 227.1508 - val_loss: 247.0917\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.0170 - val_loss: 147.2380\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 205.9321 - val_loss: 160.3853\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 219.2597 - val_loss: 157.1139\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 190.8135 - val_loss: 178.7156\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 177.3799 - val_loss: 176.4029\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 189.0750 - val_loss: 150.5969\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.9128 - val_loss: 188.9616\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 218.7443 - val_loss: 169.4307\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.8203 - val_loss: 209.9935\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 189.3706 - val_loss: 176.4937\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 168.3218 - val_loss: 184.4004\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 162.9875 - val_loss: 323.8343\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 201.0175 - val_loss: 152.8780\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 189.0135 - val_loss: 185.4360\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.2450 - val_loss: 293.7737\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 178.3385 - val_loss: 173.1197\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 244.7069 - val_loss: 153.6901\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.3476 - val_loss: 154.4801\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 167.1852 - val_loss: 163.9832\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 192.2473 - val_loss: 151.9308\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 183.8026 - val_loss: 144.9294\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 279.2677 - val_loss: 150.6628\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 242.5638 - val_loss: 178.6538\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.7453 - val_loss: 149.2803\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 163.4122 - val_loss: 229.3210\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 198.4123 - val_loss: 166.5312\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.8384 - val_loss: 164.3738\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.7841 - val_loss: 146.0134\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.6071 - val_loss: 151.4870\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 206.6989 - val_loss: 153.2159\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.3116 - val_loss: 145.8751\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 188.9880 - val_loss: 240.7443\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 202.0826 - val_loss: 161.1983\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.0958 - val_loss: 152.7927\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.5174 - val_loss: 163.5312\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.3830 - val_loss: 202.8279\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 175.1365 - val_loss: 221.7086\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 185.9561 - val_loss: 151.0089\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.2228 - val_loss: 147.3405\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.0290 - val_loss: 336.7938\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.2186 - val_loss: 155.1255\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.0285 - val_loss: 232.4608\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 183.2189 - val_loss: 216.8045\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 180.9918 - val_loss: 168.0640\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 169.0306 - val_loss: 307.4787\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.7168 - val_loss: 148.7875\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.9632 - val_loss: 153.4121\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 188.9535 - val_loss: 163.5684\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.2051 - val_loss: 195.7562\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 173.1946 - val_loss: 142.9261\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 188.2511 - val_loss: 212.9151\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.5038 - val_loss: 299.4884\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 183.7770 - val_loss: 177.7211\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 174.4722 - val_loss: 154.5784\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.9391 - val_loss: 172.9653\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 168.9280 - val_loss: 156.1183\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.9281 - val_loss: 156.2220\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.0346 - val_loss: 183.7512\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 220.0514 - val_loss: 185.1757\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 176.2640 - val_loss: 184.7207\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.5036 - val_loss: 188.0466\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.6988 - val_loss: 200.5484\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.0284 - val_loss: 164.7126\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 175.5363 - val_loss: 150.9368\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 196.3033 - val_loss: 224.7729\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.9166 - val_loss: 240.9252\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 165.5733 - val_loss: 168.9515\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 177.6345 - val_loss: 150.8220\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 177.5519 - val_loss: 185.7244\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.3035 - val_loss: 162.5605\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 177.0799 - val_loss: 151.0968\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 202.1063 - val_loss: 204.2039\n",
      "Epoch 658/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 72us/step - loss: 161.3746 - val_loss: 174.6527\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 545.9673 - val_loss: 221.2997\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 199.4549 - val_loss: 166.4489\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 158.2227 - val_loss: 142.2650\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 177.7230 - val_loss: 150.1362\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 280.6576 - val_loss: 234.2186\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.2062 - val_loss: 183.9594\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.2015 - val_loss: 215.2163\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.1798 - val_loss: 170.9650\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.9064 - val_loss: 184.9163\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 158.2053 - val_loss: 160.9624\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.2075 - val_loss: 158.1028\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.4923 - val_loss: 216.6418\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.6295 - val_loss: 149.5148\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.4298 - val_loss: 771.1351\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 242.1289 - val_loss: 152.3080\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 187.9046 - val_loss: 162.4013\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 191.4142 - val_loss: 152.7326\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.5925 - val_loss: 164.8492\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.5839 - val_loss: 168.7635\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.9086 - val_loss: 171.4987\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.9706 - val_loss: 151.4695\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.6351 - val_loss: 162.9415\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 175.9061 - val_loss: 166.4168\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.7040 - val_loss: 148.1642\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.8792 - val_loss: 156.7044\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 184.0950 - val_loss: 220.0244\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 203.6317 - val_loss: 186.2064\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.7334 - val_loss: 148.3170\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 169.9185 - val_loss: 151.4397\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 203.3474 - val_loss: 163.7012\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 190.1055 - val_loss: 158.6587\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.9269 - val_loss: 224.4912\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 187.5623 - val_loss: 228.0026\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 190.1989 - val_loss: 162.8218\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.6737 - val_loss: 168.2292\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.6752 - val_loss: 148.4472\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 190.0067 - val_loss: 160.1294\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 161.1413 - val_loss: 146.9251\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.0381 - val_loss: 192.1871\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 271.1105 - val_loss: 216.3126\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.2065 - val_loss: 165.0867\n",
      "Epoch 700/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.1363 - val_loss: 373.4445\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 201.1572 - val_loss: 172.9627\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 191.5122 - val_loss: 151.7919\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.9563 - val_loss: 208.8634\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 175.3489 - val_loss: 175.7086\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.1643 - val_loss: 155.9219\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 220.6749 - val_loss: 234.1766\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.1779 - val_loss: 155.1718\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.7023 - val_loss: 148.5226\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.8266 - val_loss: 157.0900\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 217.2384 - val_loss: 146.9749\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 335.4290 - val_loss: 462.0194\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 368.2553 - val_loss: 286.0060\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 216.8743 - val_loss: 212.8996\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.5291 - val_loss: 145.3853\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 207.8838 - val_loss: 191.7956\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 203.6227 - val_loss: 168.4954\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 194.3707 - val_loss: 274.1810\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 190.3547 - val_loss: 144.3662\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.4308 - val_loss: 148.4435\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 215.7281 - val_loss: 270.0181\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 228.2393 - val_loss: 148.3301\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.2936 - val_loss: 160.2519\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.9194 - val_loss: 150.7454\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.5841 - val_loss: 161.7423\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.2435 - val_loss: 153.5814\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 159.6794 - val_loss: 161.6869\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 161.7027 - val_loss: 165.0753\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 173.2167 - val_loss: 152.3189\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 162.2826 - val_loss: 168.8973\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.8200 - val_loss: 152.0858\n",
      "Epoch 731/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 66us/step - loss: 271.1828 - val_loss: 242.7306\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 177.2478 - val_loss: 181.9000\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 160.1946 - val_loss: 201.8385\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.4494 - val_loss: 151.3395\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.6140 - val_loss: 162.1801\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.7678 - val_loss: 175.4274\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 247.6237 - val_loss: 295.3348\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 181.2419 - val_loss: 168.9422\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.0106 - val_loss: 210.0810\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 269.9369 - val_loss: 144.2774\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.7489 - val_loss: 160.5271\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 297.6659 - val_loss: 230.6559\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 261.9224 - val_loss: 165.5613\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 248.9654 - val_loss: 165.0048\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 194.2089 - val_loss: 211.8153\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 207.4463 - val_loss: 238.4769\n",
      "Epoch 747/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 229.5166 - val_loss: 173.9726\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 216.9997 - val_loss: 169.2037\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 188.9366 - val_loss: 165.8750\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 189.0002 - val_loss: 158.8672\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.8349 - val_loss: 233.6346\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 190.6293 - val_loss: 219.7561\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 198.9127 - val_loss: 360.6294\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 194.3475 - val_loss: 167.8210\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 206.1654 - val_loss: 207.0016\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 177.1138 - val_loss: 147.3511\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 194.9717 - val_loss: 195.5329\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 198.3030 - val_loss: 197.6535\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 239.0240 - val_loss: 153.8757\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.0380 - val_loss: 242.6931\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 179.1816 - val_loss: 152.3339\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 207.3299 - val_loss: 150.9935\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.0962 - val_loss: 213.7418\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 193.8481 - val_loss: 157.1454\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 200.2840 - val_loss: 172.9036\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.8293 - val_loss: 159.9031\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.5661 - val_loss: 149.3282\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.3512 - val_loss: 153.6095\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 212.2319 - val_loss: 140.8183\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.7634 - val_loss: 143.2035\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 180.4134 - val_loss: 206.1518\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.2601 - val_loss: 205.4126\n",
      "Epoch 773/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.5425 - val_loss: 157.6847\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 193.5898 - val_loss: 151.5736\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.4111 - val_loss: 182.8118\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.5826 - val_loss: 160.1481\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 158.3838 - val_loss: 192.8220\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 213.3945 - val_loss: 201.0217\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 206.4039 - val_loss: 159.4187\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 167.6712 - val_loss: 167.7389\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 212.1729 - val_loss: 157.4490\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.2116 - val_loss: 177.0325\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 235.8036 - val_loss: 231.7008\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 222.8398 - val_loss: 145.3150\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 167.1935 - val_loss: 161.5161\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 169.2752 - val_loss: 146.9714\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 192.3761 - val_loss: 157.4817\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 321.5415 - val_loss: 191.1925\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 207.5666 - val_loss: 177.3650\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 217.9712 - val_loss: 245.3778\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 190.7675 - val_loss: 180.7115\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.2177 - val_loss: 186.9734\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 222.9823 - val_loss: 147.7445\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 206.1211 - val_loss: 165.3748\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.3002 - val_loss: 168.8813\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 214.1522 - val_loss: 555.5624\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 247.1512 - val_loss: 156.0965\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 165.5542 - val_loss: 158.6179\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.3407 - val_loss: 152.6392\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 268.8258 - val_loss: 166.9399\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 205.2372 - val_loss: 167.6996\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 182.2702 - val_loss: 148.7999\n",
      "Epoch 803/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.3234 - val_loss: 150.3947\n",
      "Epoch 804/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 208.4023 - val_loss: 158.6036\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 207.0609 - val_loss: 246.2863\n",
      "Epoch 806/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 180.0662 - val_loss: 158.0904\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 188.4233 - val_loss: 179.1957\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 201.0714 - val_loss: 168.6310\n",
      "Epoch 809/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 210.9704 - val_loss: 172.6833\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 206.9296 - val_loss: 405.8064\n",
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 199.1632 - val_loss: 189.9887\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 194.2682 - val_loss: 146.8185\n",
      "Epoch 813/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 171.6275 - val_loss: 143.7390\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 192.7631 - val_loss: 277.4308\n",
      "Epoch 815/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.0770 - val_loss: 145.8616\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.0655 - val_loss: 400.9225\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 192.1503 - val_loss: 227.6236\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 181.9866 - val_loss: 225.2180\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 214.6200 - val_loss: 185.7514\n",
      "Epoch 820/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 182.0315 - val_loss: 162.3104\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 203.2872 - val_loss: 283.6192\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 184.1969 - val_loss: 161.2160\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 180.3663 - val_loss: 225.4855\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.4459 - val_loss: 154.3491\n",
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 195.2034 - val_loss: 197.4418\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.6464 - val_loss: 164.2983\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 186.4984 - val_loss: 146.7164\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 178.4099 - val_loss: 176.0601\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 180.0028 - val_loss: 156.3279\n",
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.6255 - val_loss: 179.5945\n",
      "Epoch 831/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.7426 - val_loss: 167.9606\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 171.5898 - val_loss: 237.9761\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 178.8376 - val_loss: 149.8844\n",
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 180.7301 - val_loss: 146.1081\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.6627 - val_loss: 164.3876\n",
      "Epoch 836/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.4786 - val_loss: 191.1903\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 193.0654 - val_loss: 342.1857\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 187.7703 - val_loss: 147.8736\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 155.2344 - val_loss: 146.3813\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 174.8147 - val_loss: 166.7721\n",
      "Epoch 841/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 203.7061 - val_loss: 150.7422\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 179.4569 - val_loss: 157.0031\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.4149 - val_loss: 188.3886\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.7767 - val_loss: 216.3352\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 188.7390 - val_loss: 394.9062\n",
      "Epoch 846/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 185.1136 - val_loss: 166.9560\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.9503 - val_loss: 155.4488\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 220.4387 - val_loss: 190.7504\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.9455 - val_loss: 171.8382\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 180.4381 - val_loss: 184.5241\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.3755 - val_loss: 188.9590\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 184.9960 - val_loss: 432.4603\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 169.4275 - val_loss: 145.0789\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 198.3319 - val_loss: 519.4859\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 198.6068 - val_loss: 175.9086\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.3897 - val_loss: 271.4892\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 232.0456 - val_loss: 427.9295\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.6139 - val_loss: 165.2405\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 176.1838 - val_loss: 169.3070\n",
      "Epoch 860/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 199.9386 - val_loss: 295.9596\n",
      "Epoch 861/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.5893 - val_loss: 175.4418\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 187.4456 - val_loss: 153.9989\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.2616 - val_loss: 150.9818\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.9446 - val_loss: 172.6764\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 241.2051 - val_loss: 148.0378\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.0373 - val_loss: 196.1094\n",
      "Epoch 867/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.4461 - val_loss: 170.6799\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.9274 - val_loss: 157.0803\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 184.3155 - val_loss: 156.8052\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 199.8072 - val_loss: 148.3524\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 183.8797 - val_loss: 186.9453\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 315.4236 - val_loss: 142.4731\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.2519 - val_loss: 161.1070\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.0358 - val_loss: 150.2516\n",
      "Epoch 875/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.7257 - val_loss: 142.4519\n",
      "Epoch 876/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.7421 - val_loss: 182.3756\n",
      "Epoch 877/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 205.6283 - val_loss: 159.2562\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 172.8233 - val_loss: 174.0923\n",
      "Epoch 879/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 199.7009 - val_loss: 151.9098\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 253.7354 - val_loss: 190.3630\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 180.0039 - val_loss: 147.6964\n",
      "Epoch 882/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 224.6456 - val_loss: 171.2053\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.9610 - val_loss: 196.9269\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.9629 - val_loss: 167.7736\n",
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.4507 - val_loss: 155.9926\n",
      "Epoch 886/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 181.4525 - val_loss: 210.3513\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.8203 - val_loss: 153.1128\n",
      "Epoch 888/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.7081 - val_loss: 143.6324\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 225.4827 - val_loss: 166.4709\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.8124 - val_loss: 143.8290\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.8199 - val_loss: 148.6684\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.9849 - val_loss: 161.0497\n",
      "Epoch 893/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 184.5890 - val_loss: 163.5331\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.6868 - val_loss: 142.5412\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 184.7610 - val_loss: 199.1993\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.2834 - val_loss: 154.3194\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.2515 - val_loss: 141.5886\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.2267 - val_loss: 176.6773\n",
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 197.3568 - val_loss: 185.1530\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 286.3087 - val_loss: 222.9167\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 172.7751 - val_loss: 144.7120\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 166.4739 - val_loss: 186.9319\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.1193 - val_loss: 145.9657\n",
      "Epoch 904/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.5632 - val_loss: 161.2857\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.0121 - val_loss: 147.3316\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.9646 - val_loss: 196.5396\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 171.2217 - val_loss: 152.0677\n",
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.9870 - val_loss: 162.0191\n",
      "Epoch 909/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.9557 - val_loss: 392.3384\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 169.8680 - val_loss: 161.6357\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.8313 - val_loss: 147.4982\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 217.6434 - val_loss: 177.5617\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.1128 - val_loss: 156.4073\n",
      "Epoch 914/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 169.5384 - val_loss: 152.3063\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.6419 - val_loss: 151.7708\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.0035 - val_loss: 151.5147\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.1873 - val_loss: 172.5422\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 221.6681 - val_loss: 278.1136\n",
      "Epoch 919/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.8500 - val_loss: 145.2537\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 164.3387 - val_loss: 249.7431\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 206.1413 - val_loss: 209.1335\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.3279 - val_loss: 144.6686\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 190.9966 - val_loss: 332.3953\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.7473 - val_loss: 171.7202\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.4932 - val_loss: 167.3361\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 189.8737 - val_loss: 174.6900\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.0888 - val_loss: 185.0189\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.4909 - val_loss: 263.6057\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 184.3470 - val_loss: 235.2753\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 205.3962 - val_loss: 147.2146\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.9431 - val_loss: 161.5356\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 197.5123 - val_loss: 186.6949\n",
      "Epoch 933/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 155.7686 - val_loss: 140.7159\n",
      "Epoch 934/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 201.8259 - val_loss: 394.7574\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 167.9079 - val_loss: 138.6835\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.8864 - val_loss: 156.7939\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.2782 - val_loss: 158.4944\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 172.5806 - val_loss: 144.1453\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.8287 - val_loss: 211.1426\n",
      "Epoch 940/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.2242 - val_loss: 143.7969\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.9640 - val_loss: 157.9764\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.5199 - val_loss: 151.7677\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 213.3442 - val_loss: 180.5208\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.8081 - val_loss: 152.1903\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.4169 - val_loss: 164.4050\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 176.3696 - val_loss: 169.1219\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.2461 - val_loss: 143.3096\n",
      "Epoch 948/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.9026 - val_loss: 143.1207\n",
      "Epoch 949/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 277.9524 - val_loss: 249.4684\n",
      "Epoch 950/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.4779 - val_loss: 237.0188\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.5646 - val_loss: 155.1285\n",
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.2310 - val_loss: 160.4563\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.2782 - val_loss: 153.2310\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.1963 - val_loss: 140.1901\n",
      "Epoch 955/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.4265 - val_loss: 190.1810\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.0929 - val_loss: 158.6265\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 181.3245 - val_loss: 172.4371\n",
      "Epoch 958/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 178.6523 - val_loss: 208.9709\n",
      "Epoch 959/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 182.2839 - val_loss: 220.3057\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 169.6661 - val_loss: 139.5741\n",
      "Epoch 961/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 148.8388 - val_loss: 167.1459\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 157.3850 - val_loss: 177.6773\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 157.5244 - val_loss: 182.2903\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.7069 - val_loss: 143.0575\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.4888 - val_loss: 189.1733\n",
      "Epoch 966/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 201.0276 - val_loss: 268.6922\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 206.5583 - val_loss: 156.8353\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 172.5820 - val_loss: 169.1189\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 169.7140 - val_loss: 160.3581\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.6165 - val_loss: 168.9049\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 225.5322 - val_loss: 147.4412\n",
      "Epoch 972/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.8412 - val_loss: 199.7398\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.2011 - val_loss: 142.3775\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.8893 - val_loss: 190.7588\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 177.3957 - val_loss: 209.9236\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 175.2648 - val_loss: 210.6347\n",
      "Epoch 977/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.5053 - val_loss: 144.3579\n",
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.8962 - val_loss: 148.9345\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 196.1639 - val_loss: 200.8941\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 188.0329 - val_loss: 150.2536\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.9695 - val_loss: 153.5481\n",
      "Epoch 982/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.6191 - val_loss: 201.0816\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 207.5385 - val_loss: 145.3637\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 193.1705 - val_loss: 150.8640\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.3195 - val_loss: 175.7345\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.6818 - val_loss: 200.3471\n",
      "Epoch 987/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 194.0656 - val_loss: 142.7690\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.6925 - val_loss: 152.2140\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 177.9509 - val_loss: 139.0232\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 148.9841 - val_loss: 155.4240\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.3067 - val_loss: 200.4590\n",
      "Epoch 992/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.6817 - val_loss: 217.7617\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 168.0703 - val_loss: 148.3378\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.9139 - val_loss: 178.4269\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.0735 - val_loss: 167.2624\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 174.3817 - val_loss: 161.1572\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.5712 - val_loss: 149.3462\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.9950 - val_loss: 147.0147\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 277.7270 - val_loss: 155.0017\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.4472 - val_loss: 187.7108\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.1624 - val_loss: 186.0624\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.0040 - val_loss: 406.2182\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 180.6490 - val_loss: 143.0318\n",
      "Epoch 1004/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.2263 - val_loss: 141.5584\n",
      "Epoch 1005/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.3030 - val_loss: 150.6788\n",
      "Epoch 1006/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 177.9848 - val_loss: 154.3278\n",
      "Epoch 1007/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 187.8898 - val_loss: 179.3082\n",
      "Epoch 1008/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.5496 - val_loss: 151.4983\n",
      "Epoch 1009/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.2388 - val_loss: 167.4345\n",
      "Epoch 1010/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 183.4116 - val_loss: 146.7152\n",
      "Epoch 1011/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.1943 - val_loss: 257.9984\n",
      "Epoch 1012/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.7969 - val_loss: 289.2178\n",
      "Epoch 1013/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.4457 - val_loss: 221.6272\n",
      "Epoch 1014/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.3918 - val_loss: 164.0899\n",
      "Epoch 1015/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.8231 - val_loss: 142.8802\n",
      "Epoch 1016/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.8224 - val_loss: 198.4793\n",
      "Epoch 1017/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.6208 - val_loss: 159.2226\n",
      "Epoch 1018/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.1208 - val_loss: 142.3681\n",
      "Epoch 1019/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.9214 - val_loss: 140.6172\n",
      "Epoch 1020/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 190.3190 - val_loss: 157.6538\n",
      "Epoch 1021/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.1847 - val_loss: 159.3470\n",
      "Epoch 1022/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 192.3078 - val_loss: 153.2317\n",
      "Epoch 1023/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 74us/step - loss: 192.3422 - val_loss: 142.3976\n",
      "Epoch 1024/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 158.3634 - val_loss: 141.9293\n",
      "Epoch 1025/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 151.9735 - val_loss: 140.2522\n",
      "Epoch 1026/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 173.2566 - val_loss: 163.8193\n",
      "Epoch 1027/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.2444 - val_loss: 151.7893\n",
      "Epoch 1028/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 171.4490 - val_loss: 158.8351\n",
      "Epoch 1029/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 174.3796 - val_loss: 219.7320\n",
      "Epoch 1030/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.3561 - val_loss: 183.9259\n",
      "Epoch 1031/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.3247 - val_loss: 148.5114\n",
      "Epoch 1032/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.8176 - val_loss: 146.5106\n",
      "Epoch 1033/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.3767 - val_loss: 138.3294\n",
      "Epoch 1034/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.5464 - val_loss: 144.8306\n",
      "Epoch 1035/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.7354 - val_loss: 150.4368\n",
      "Epoch 1036/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.0821 - val_loss: 148.0495\n",
      "Epoch 1037/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.6380 - val_loss: 150.1884\n",
      "Epoch 1038/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 193.0976 - val_loss: 154.8226\n",
      "Epoch 1039/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.2479 - val_loss: 144.5567\n",
      "Epoch 1040/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.4864 - val_loss: 165.9076\n",
      "Epoch 1041/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.2102 - val_loss: 149.9674\n",
      "Epoch 1042/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.5033 - val_loss: 150.2071\n",
      "Epoch 1043/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.6556 - val_loss: 147.9910\n",
      "Epoch 1044/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.1040 - val_loss: 185.0562\n",
      "Epoch 1045/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.0956 - val_loss: 178.9905\n",
      "Epoch 1046/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.2043 - val_loss: 203.5052\n",
      "Epoch 1047/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.8172 - val_loss: 147.7059\n",
      "Epoch 1048/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.2858 - val_loss: 154.3291\n",
      "Epoch 1049/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 187.9701 - val_loss: 160.0707\n",
      "Epoch 1050/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.5764 - val_loss: 156.4058\n",
      "Epoch 1051/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.3620 - val_loss: 228.9787\n",
      "Epoch 1052/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.1169 - val_loss: 176.3093\n",
      "Epoch 1053/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.9332 - val_loss: 157.3790\n",
      "Epoch 1054/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.0650 - val_loss: 154.5256\n",
      "Epoch 1055/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.0890 - val_loss: 153.3767\n",
      "Epoch 1056/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 186.1801 - val_loss: 218.7958\n",
      "Epoch 1057/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 186.9944 - val_loss: 148.3783\n",
      "Epoch 1058/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.0705 - val_loss: 208.8794\n",
      "Epoch 1059/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.7880 - val_loss: 152.8477\n",
      "Epoch 1060/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.1331 - val_loss: 153.9143\n",
      "Epoch 1061/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 143.6677 - val_loss: 167.1325\n",
      "Epoch 1062/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.6441 - val_loss: 211.5442\n",
      "Epoch 1063/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.6850 - val_loss: 164.0890\n",
      "Epoch 1064/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.0297 - val_loss: 147.8916\n",
      "Epoch 1065/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.6530 - val_loss: 151.4126\n",
      "Epoch 1066/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 235.9700 - val_loss: 149.7758\n",
      "Epoch 1067/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.0377 - val_loss: 229.5901\n",
      "Epoch 1068/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.1263 - val_loss: 144.2651\n",
      "Epoch 1069/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.0602 - val_loss: 141.5567\n",
      "Epoch 1070/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.3246 - val_loss: 208.8183\n",
      "Epoch 1071/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.4291 - val_loss: 143.7084\n",
      "Epoch 1072/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 182.8566 - val_loss: 239.3875\n",
      "Epoch 1073/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.1278 - val_loss: 139.5102\n",
      "Epoch 1074/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.8415 - val_loss: 195.7446\n",
      "Epoch 1075/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 173.8166 - val_loss: 153.8248\n",
      "Epoch 1076/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 164.8457 - val_loss: 146.3533\n",
      "Epoch 1077/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.4825 - val_loss: 160.0165\n",
      "Epoch 1078/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 179.6567 - val_loss: 367.9370\n",
      "Epoch 1079/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 175.9040 - val_loss: 154.0851\n",
      "Epoch 1080/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.5409 - val_loss: 157.9654\n",
      "Epoch 1081/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.9239 - val_loss: 191.0044\n",
      "Epoch 1082/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 150.7947 - val_loss: 208.2257\n",
      "Epoch 1083/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 155.8743 - val_loss: 274.0343\n",
      "Epoch 1084/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 169.3515 - val_loss: 220.9585\n",
      "Epoch 1085/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 164.5541 - val_loss: 145.1707\n",
      "Epoch 1086/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 182.6379 - val_loss: 190.3928\n",
      "Epoch 1087/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.2103 - val_loss: 158.0321\n",
      "Epoch 1088/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.5667 - val_loss: 163.4732\n",
      "Epoch 1089/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.8950 - val_loss: 174.6471\n",
      "Epoch 1090/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.2916 - val_loss: 152.4132\n",
      "Epoch 1091/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 189.6789 - val_loss: 143.0080\n",
      "Epoch 1092/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 178.8822 - val_loss: 310.0995\n",
      "Epoch 1093/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.6154 - val_loss: 156.7286\n",
      "Epoch 1094/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.4510 - val_loss: 139.7854\n",
      "Epoch 1095/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.1572 - val_loss: 158.9781\n",
      "Epoch 1096/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.9496 - val_loss: 155.4967\n",
      "Epoch 1097/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.1255 - val_loss: 282.6894\n",
      "Epoch 1098/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 169.2981 - val_loss: 144.9399\n",
      "Epoch 1099/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 164.1344 - val_loss: 240.2325\n",
      "Epoch 1100/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 178.7128 - val_loss: 186.7308\n",
      "Epoch 1101/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 183.0336 - val_loss: 227.2011\n",
      "Epoch 1102/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 184.8263 - val_loss: 181.6613\n",
      "Epoch 1103/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 158.0542 - val_loss: 174.0749\n",
      "Epoch 1104/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 141.5230 - val_loss: 166.8475\n",
      "Epoch 1105/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 140.0103 - val_loss: 142.4418\n",
      "Epoch 1106/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 156.7280 - val_loss: 168.1736\n",
      "Epoch 1107/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 165.0372 - val_loss: 156.5472\n",
      "Epoch 1108/10000\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 171.6390 - val_loss: 144.3329\n",
      "Epoch 1109/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 175.5649 - val_loss: 140.1097\n",
      "Epoch 1110/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 148.9579 - val_loss: 150.7288\n",
      "Epoch 1111/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 169.6083 - val_loss: 139.1023\n",
      "Epoch 1112/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 139.7275 - val_loss: 161.9715\n",
      "Epoch 1113/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 169.2638 - val_loss: 141.9813\n",
      "Epoch 1114/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 142.9310 - val_loss: 267.9651\n",
      "Epoch 1115/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 151.0310 - val_loss: 139.1872\n",
      "Epoch 1116/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 167.2471 - val_loss: 153.5268\n",
      "Epoch 1117/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 212.2956 - val_loss: 184.4559\n",
      "Epoch 1118/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 162.7150 - val_loss: 144.9910\n",
      "Epoch 1119/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 141.3321 - val_loss: 154.2176\n",
      "Epoch 1120/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 150.7747 - val_loss: 349.8137\n",
      "Epoch 1121/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 148.8558 - val_loss: 152.9534\n",
      "Epoch 1122/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 177.4507 - val_loss: 177.1679\n",
      "Epoch 1123/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 154.7599 - val_loss: 172.8116\n",
      "Epoch 1124/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 148.2726 - val_loss: 150.3285\n",
      "Epoch 1125/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 155.5738 - val_loss: 181.5706\n",
      "Epoch 1126/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 157.1141 - val_loss: 146.0940\n",
      "Epoch 1127/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 156.9857 - val_loss: 232.6392\n",
      "Epoch 1128/10000\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 146.7364 - val_loss: 153.8201\n",
      "Epoch 1129/10000\n",
      "8000/8000 [==============================] - 1s 129us/step - loss: 171.2636 - val_loss: 157.6416\n",
      "Epoch 1130/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 165.2077 - val_loss: 182.6781\n",
      "Epoch 1131/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 204.5712 - val_loss: 168.4280\n",
      "Epoch 1132/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 172.0081 - val_loss: 165.2182\n",
      "Epoch 1133/10000\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 166.1332 - val_loss: 145.1782\n",
      "Epoch 1134/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 141.5023 - val_loss: 161.2239\n",
      "Epoch 1135/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 163.9701 - val_loss: 149.1296\n",
      "Epoch 1136/10000\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 143.6516 - val_loss: 154.0490\n",
      "Epoch 1137/10000\n",
      "8000/8000 [==============================] - 1s 124us/step - loss: 177.1413 - val_loss: 188.3140\n",
      "Epoch 1138/10000\n",
      "8000/8000 [==============================] - 1s 141us/step - loss: 159.4023 - val_loss: 180.7505\n",
      "Epoch 1139/10000\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 164.1463 - val_loss: 149.6928\n",
      "Epoch 1140/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 151.0993 - val_loss: 138.0518\n",
      "Epoch 1141/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 142.6801 - val_loss: 168.9603\n",
      "Epoch 1142/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 136.784 - 1s 109us/step - loss: 139.5126 - val_loss: 219.6149\n",
      "Epoch 1143/10000\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 159.0920 - val_loss: 209.0248\n",
      "Epoch 1144/10000\n",
      "8000/8000 [==============================] - 1s 152us/step - loss: 156.1183 - val_loss: 152.1148\n",
      "Epoch 1145/10000\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 141.2633 - val_loss: 149.2555\n",
      "Epoch 1146/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 173.1256 - val_loss: 143.7170\n",
      "Epoch 1147/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 147.0419 - val_loss: 190.2640\n",
      "Epoch 1148/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 155.9860 - val_loss: 239.8401\n",
      "Epoch 1149/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.5576 - val_loss: 139.1668\n",
      "Epoch 1150/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 194.1024 - val_loss: 153.7032\n",
      "Epoch 1151/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 235.6567 - val_loss: 159.4848\n",
      "Epoch 1152/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.1982 - val_loss: 211.5724\n",
      "Epoch 1153/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.8688 - val_loss: 178.8501\n",
      "Epoch 1154/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 163.9274 - val_loss: 173.3259\n",
      "Epoch 1155/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.0275 - val_loss: 144.2614\n",
      "Epoch 1156/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.7217 - val_loss: 177.8453\n",
      "Epoch 1157/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.8417 - val_loss: 141.9201\n",
      "Epoch 1158/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 180.1107 - val_loss: 158.0640\n",
      "Epoch 1159/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 183.4472 - val_loss: 153.6898\n",
      "Epoch 1160/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 167.2696 - val_loss: 148.0270\n",
      "Epoch 1161/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 140.3994 - val_loss: 163.2233\n",
      "Epoch 1162/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.9734 - val_loss: 204.3692\n",
      "Epoch 1163/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.4430 - val_loss: 154.2576\n",
      "Epoch 1164/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 146.0211 - val_loss: 137.0910\n",
      "Epoch 1165/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 225.8073 - val_loss: 199.6378\n",
      "Epoch 1166/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.7631 - val_loss: 158.1770\n",
      "Epoch 1167/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 64us/step - loss: 166.4574 - val_loss: 153.7531\n",
      "Epoch 1168/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.7425 - val_loss: 147.7794\n",
      "Epoch 1169/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.9076 - val_loss: 146.8309\n",
      "Epoch 1170/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.8352 - val_loss: 218.1869\n",
      "Epoch 1171/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.4197 - val_loss: 156.5442\n",
      "Epoch 1172/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 174.5061 - val_loss: 142.7630\n",
      "Epoch 1173/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 186.5857 - val_loss: 161.3810\n",
      "Epoch 1174/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 158.6539 - val_loss: 150.8102\n",
      "Epoch 1175/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.6007 - val_loss: 303.6585\n",
      "Epoch 1176/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.6741 - val_loss: 137.8644\n",
      "Epoch 1177/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.0506 - val_loss: 189.2083\n",
      "Epoch 1178/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.3787 - val_loss: 138.5789\n",
      "Epoch 1179/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.3265 - val_loss: 149.5285\n",
      "Epoch 1180/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.4145 - val_loss: 145.6841\n",
      "Epoch 1181/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.5321 - val_loss: 139.7789\n",
      "Epoch 1182/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.3752 - val_loss: 138.7838\n",
      "Epoch 1183/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.2709 - val_loss: 151.6916\n",
      "Epoch 1184/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.2418 - val_loss: 157.2414\n",
      "Epoch 1185/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.4268 - val_loss: 140.7263\n",
      "Epoch 1186/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.9891 - val_loss: 142.7089\n",
      "Epoch 1187/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.8973 - val_loss: 165.0963\n",
      "Epoch 1188/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.4759 - val_loss: 177.4194\n",
      "Epoch 1189/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 193.9315 - val_loss: 153.6072\n",
      "Epoch 1190/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.8431 - val_loss: 138.8874\n",
      "Epoch 1191/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.7869 - val_loss: 155.1195\n",
      "Epoch 1192/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.4499 - val_loss: 178.8542\n",
      "Epoch 1193/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.3209 - val_loss: 141.4937\n",
      "Epoch 1194/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.9169 - val_loss: 275.1251\n",
      "Epoch 1195/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 185.8434 - val_loss: 225.3251\n",
      "Epoch 1196/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.3308 - val_loss: 176.2625\n",
      "Epoch 1197/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.5169 - val_loss: 177.7573\n",
      "Epoch 1198/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.3405 - val_loss: 166.3118\n",
      "Epoch 1199/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 214.8607 - val_loss: 168.2109\n",
      "Epoch 1200/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.4646 - val_loss: 144.5032\n",
      "Epoch 1201/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.3182 - val_loss: 135.7104\n",
      "Epoch 1202/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.3619 - val_loss: 143.7000\n",
      "Epoch 1203/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.5846 - val_loss: 201.7033\n",
      "Epoch 1204/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 150.7235 - val_loss: 134.3692\n",
      "Epoch 1205/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.9369 - val_loss: 196.0384\n",
      "Epoch 1206/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 184.5599 - val_loss: 154.9567\n",
      "Epoch 1207/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 151.2757 - val_loss: 138.4679\n",
      "Epoch 1208/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 172.5560 - val_loss: 139.9894\n",
      "Epoch 1209/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.5782 - val_loss: 138.3004\n",
      "Epoch 1210/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.1279 - val_loss: 169.2823\n",
      "Epoch 1211/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.1543 - val_loss: 223.6733\n",
      "Epoch 1212/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 171.6825 - val_loss: 174.8016\n",
      "Epoch 1213/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.4088 - val_loss: 175.4379\n",
      "Epoch 1214/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.0277 - val_loss: 136.0832\n",
      "Epoch 1215/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.3816 - val_loss: 230.6828\n",
      "Epoch 1216/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.8214 - val_loss: 154.9574\n",
      "Epoch 1217/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.5241 - val_loss: 183.7352\n",
      "Epoch 1218/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.0955 - val_loss: 145.3653\n",
      "Epoch 1219/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.0909 - val_loss: 148.0131\n",
      "Epoch 1220/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 138.9787 - val_loss: 139.8941\n",
      "Epoch 1221/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 142.4583 - val_loss: 141.0817\n",
      "Epoch 1222/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.4220 - val_loss: 210.8939\n",
      "Epoch 1223/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.6037 - val_loss: 173.1611\n",
      "Epoch 1224/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.3301 - val_loss: 141.1493\n",
      "Epoch 1225/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 135.9773 - val_loss: 143.7030\n",
      "Epoch 1226/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.1375 - val_loss: 214.6237\n",
      "Epoch 1227/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.8883 - val_loss: 137.7105\n",
      "Epoch 1228/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.3118 - val_loss: 213.4739\n",
      "Epoch 1229/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.2107 - val_loss: 194.1895\n",
      "Epoch 1230/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.9250 - val_loss: 196.3425\n",
      "Epoch 1231/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.8261 - val_loss: 224.0213\n",
      "Epoch 1232/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.4031 - val_loss: 156.7357\n",
      "Epoch 1233/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 200.9321 - val_loss: 158.2748\n",
      "Epoch 1234/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 141.2823 - val_loss: 138.4385\n",
      "Epoch 1235/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 205.5695 - val_loss: 145.1697\n",
      "Epoch 1236/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.1856 - val_loss: 261.9763\n",
      "Epoch 1237/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.3362 - val_loss: 143.4329\n",
      "Epoch 1238/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.7940 - val_loss: 166.7484\n",
      "Epoch 1239/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.7035 - val_loss: 143.2790\n",
      "Epoch 1240/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.9184 - val_loss: 165.8089\n",
      "Epoch 1241/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.5526 - val_loss: 137.7563\n",
      "Epoch 1242/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.9666 - val_loss: 143.1695\n",
      "Epoch 1243/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.1032 - val_loss: 178.4571\n",
      "Epoch 1244/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.3366 - val_loss: 160.5072\n",
      "Epoch 1245/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.4715 - val_loss: 164.8155\n",
      "Epoch 1246/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.8007 - val_loss: 144.2186\n",
      "Epoch 1247/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.7499 - val_loss: 161.3091\n",
      "Epoch 1248/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.0835 - val_loss: 166.8653\n",
      "Epoch 1249/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.4866 - val_loss: 145.4137\n",
      "Epoch 1250/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.2838 - val_loss: 152.8415\n",
      "Epoch 1251/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.9799 - val_loss: 140.5659\n",
      "Epoch 1252/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.8932 - val_loss: 137.1417\n",
      "Epoch 1253/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.5281 - val_loss: 144.5590\n",
      "Epoch 1254/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.0132 - val_loss: 145.0875\n",
      "Epoch 1255/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.3964 - val_loss: 140.0405\n",
      "Epoch 1256/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.3992 - val_loss: 239.4145\n",
      "Epoch 1257/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.8184 - val_loss: 153.2310\n",
      "Epoch 1258/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.9023 - val_loss: 162.6579\n",
      "Epoch 1259/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.3799 - val_loss: 159.4147\n",
      "Epoch 1260/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.8121 - val_loss: 182.7752\n",
      "Epoch 1261/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.2217 - val_loss: 227.0399\n",
      "Epoch 1262/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 183.1892 - val_loss: 153.0371\n",
      "Epoch 1263/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.7122 - val_loss: 150.7490\n",
      "Epoch 1264/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 186.6723 - val_loss: 566.4386\n",
      "Epoch 1265/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.3836 - val_loss: 164.1220\n",
      "Epoch 1266/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.9578 - val_loss: 162.0410\n",
      "Epoch 1267/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.0746 - val_loss: 150.6124\n",
      "Epoch 1268/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.5878 - val_loss: 184.1318\n",
      "Epoch 1269/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.5461 - val_loss: 135.4298\n",
      "Epoch 1270/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.8400 - val_loss: 141.9415\n",
      "Epoch 1271/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.8214 - val_loss: 144.4034\n",
      "Epoch 1272/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.4984 - val_loss: 159.3074\n",
      "Epoch 1273/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.5776 - val_loss: 139.6331\n",
      "Epoch 1274/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.7347 - val_loss: 146.9137\n",
      "Epoch 1275/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 156.1630 - val_loss: 150.8436\n",
      "Epoch 1276/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.3975 - val_loss: 134.5020\n",
      "Epoch 1277/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.7994 - val_loss: 156.7414\n",
      "Epoch 1278/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.9063 - val_loss: 190.0670\n",
      "Epoch 1279/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 237.2723 - val_loss: 206.7610\n",
      "Epoch 1280/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.6852 - val_loss: 148.7525\n",
      "Epoch 1281/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.4014 - val_loss: 142.1085\n",
      "Epoch 1282/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.4427 - val_loss: 166.9784\n",
      "Epoch 1283/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.4484 - val_loss: 166.8005\n",
      "Epoch 1284/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.0758 - val_loss: 146.0372\n",
      "Epoch 1285/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.0518 - val_loss: 142.0084\n",
      "Epoch 1286/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.9726 - val_loss: 157.9016\n",
      "Epoch 1287/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 184.0803 - val_loss: 140.1193\n",
      "Epoch 1288/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.8166 - val_loss: 177.9651\n",
      "Epoch 1289/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.0047 - val_loss: 138.3423\n",
      "Epoch 1290/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.8176 - val_loss: 143.8504\n",
      "Epoch 1291/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.7338 - val_loss: 143.1587\n",
      "Epoch 1292/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.7685 - val_loss: 141.3085\n",
      "Epoch 1293/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.2728 - val_loss: 158.6511\n",
      "Epoch 1294/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.8191 - val_loss: 216.6579\n",
      "Epoch 1295/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.0714 - val_loss: 142.9988\n",
      "Epoch 1296/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.4580 - val_loss: 166.4332\n",
      "Epoch 1297/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.6516 - val_loss: 138.6599\n",
      "Epoch 1298/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.8329 - val_loss: 180.0503\n",
      "Epoch 1299/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 190.5233 - val_loss: 231.9363\n",
      "Epoch 1300/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.9599 - val_loss: 150.4410\n",
      "Epoch 1301/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.7203 - val_loss: 148.8801\n",
      "Epoch 1302/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.1460 - val_loss: 151.8056\n",
      "Epoch 1303/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 155.7260 - val_loss: 149.7351\n",
      "Epoch 1304/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 145.0161 - val_loss: 141.9307\n",
      "Epoch 1305/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 144.4811 - val_loss: 139.7261\n",
      "Epoch 1306/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.4609 - val_loss: 168.0208\n",
      "Epoch 1307/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.6450 - val_loss: 153.9284\n",
      "Epoch 1308/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.7140 - val_loss: 692.5451\n",
      "Epoch 1309/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 231.3437 - val_loss: 182.3688\n",
      "Epoch 1310/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 159.9117 - val_loss: 169.2956\n",
      "Epoch 1311/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.8672 - val_loss: 150.4224\n",
      "Epoch 1312/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.3574 - val_loss: 226.7223\n",
      "Epoch 1313/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.4411 - val_loss: 174.5947\n",
      "Epoch 1314/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.0770 - val_loss: 140.6805\n",
      "Epoch 1315/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 184.8489 - val_loss: 229.1853\n",
      "Epoch 1316/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.5516 - val_loss: 140.4828\n",
      "Epoch 1317/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.5181 - val_loss: 165.0748\n",
      "Epoch 1318/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.9202 - val_loss: 155.9792\n",
      "Epoch 1319/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.1729 - val_loss: 145.0121\n",
      "Epoch 1320/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.5880 - val_loss: 172.0340\n",
      "Epoch 1321/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.3659 - val_loss: 138.5479\n",
      "Epoch 1322/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.7957 - val_loss: 192.9056\n",
      "Epoch 1323/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 222.9394 - val_loss: 407.0765\n",
      "Epoch 1324/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.3621 - val_loss: 186.5817\n",
      "Epoch 1325/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.3904 - val_loss: 159.5979\n",
      "Epoch 1326/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.9024 - val_loss: 222.7541\n",
      "Epoch 1327/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.3096 - val_loss: 140.9313\n",
      "Epoch 1328/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.6458 - val_loss: 167.4169\n",
      "Epoch 1329/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.9593 - val_loss: 151.2436\n",
      "Epoch 1330/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.5079 - val_loss: 168.1216\n",
      "Epoch 1331/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 188.2114 - val_loss: 142.7998\n",
      "Epoch 1332/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.8090 - val_loss: 199.0069\n",
      "Epoch 1333/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.2390 - val_loss: 134.9118\n",
      "Epoch 1334/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.6919 - val_loss: 142.6070\n",
      "Epoch 1335/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.1975 - val_loss: 156.7308\n",
      "Epoch 1336/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.0719 - val_loss: 137.0786\n",
      "Epoch 1337/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.7548 - val_loss: 136.9091\n",
      "Epoch 1338/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.9460 - val_loss: 220.1041\n",
      "Epoch 1339/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.2818 - val_loss: 190.6077\n",
      "Epoch 1340/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.8355 - val_loss: 141.2201\n",
      "Epoch 1341/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.0813 - val_loss: 178.1449\n",
      "Epoch 1342/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.2146 - val_loss: 154.8444\n",
      "Epoch 1343/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 140.8189 - val_loss: 157.5604\n",
      "Epoch 1344/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.6754 - val_loss: 144.8324\n",
      "Epoch 1345/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.2556 - val_loss: 142.8560\n",
      "Epoch 1346/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 139.9366 - val_loss: 145.1595\n",
      "Epoch 1347/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.7654 - val_loss: 148.7848\n",
      "Epoch 1348/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.5309 - val_loss: 163.2852\n",
      "Epoch 1349/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.3359 - val_loss: 154.9519\n",
      "Epoch 1350/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.4380 - val_loss: 177.9321\n",
      "Epoch 1351/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.5579 - val_loss: 173.4138\n",
      "Epoch 1352/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.1122 - val_loss: 159.5723\n",
      "Epoch 1353/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.7111 - val_loss: 163.7883\n",
      "Epoch 1354/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.6017 - val_loss: 168.6814\n",
      "Epoch 1355/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 137.8582 - val_loss: 164.9414\n",
      "Epoch 1356/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 173.2511 - val_loss: 190.0241\n",
      "Epoch 1357/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 188.0021 - val_loss: 136.1071\n",
      "Epoch 1358/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.4821 - val_loss: 276.0538\n",
      "Epoch 1359/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 133.4565 - val_loss: 141.3498\n",
      "Epoch 1360/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.7945 - val_loss: 142.2530\n",
      "Epoch 1361/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.4904 - val_loss: 145.7413\n",
      "Epoch 1362/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.1393 - val_loss: 147.7690\n",
      "Epoch 1363/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 209.5857 - val_loss: 198.0486\n",
      "Epoch 1364/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.4312 - val_loss: 195.4008\n",
      "Epoch 1365/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 184.1392 - val_loss: 178.1782\n",
      "Epoch 1366/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.9505 - val_loss: 146.6362\n",
      "Epoch 1367/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.4388 - val_loss: 146.4656\n",
      "Epoch 1368/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.2253 - val_loss: 143.3343\n",
      "Epoch 1369/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.6903 - val_loss: 148.9106\n",
      "Epoch 1370/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.3259 - val_loss: 153.1036\n",
      "Epoch 1371/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.2421 - val_loss: 161.9084\n",
      "Epoch 1372/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.8527 - val_loss: 195.7842\n",
      "Epoch 1373/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.5989 - val_loss: 149.0532\n",
      "Epoch 1374/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.5038 - val_loss: 181.4041\n",
      "Epoch 1375/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.7466 - val_loss: 137.4537\n",
      "Epoch 1376/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.2603 - val_loss: 155.6548\n",
      "Epoch 1377/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.0260 - val_loss: 155.2191\n",
      "Epoch 1378/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.6547 - val_loss: 142.0531\n",
      "Epoch 1379/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.1934 - val_loss: 167.7030\n",
      "Epoch 1380/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.7395 - val_loss: 200.0177\n",
      "Epoch 1381/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.3047 - val_loss: 169.2144\n",
      "Epoch 1382/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.3218 - val_loss: 148.3564\n",
      "Epoch 1383/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 64us/step - loss: 135.6061 - val_loss: 348.8819\n",
      "Epoch 1384/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 196.5150 - val_loss: 156.4727\n",
      "Epoch 1385/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.1568 - val_loss: 161.4917\n",
      "Epoch 1386/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.5578 - val_loss: 188.9980\n",
      "Epoch 1387/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.9929 - val_loss: 177.4564\n",
      "Epoch 1388/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 221.0907 - val_loss: 153.0806\n",
      "Epoch 1389/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.1141 - val_loss: 179.9901\n",
      "Epoch 1390/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.0946 - val_loss: 141.4955\n",
      "Epoch 1391/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.3972 - val_loss: 139.2392\n",
      "Epoch 1392/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.8248 - val_loss: 139.6370\n",
      "Epoch 1393/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.2913 - val_loss: 204.4822\n",
      "Epoch 1394/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.3093 - val_loss: 141.8238\n",
      "Epoch 1395/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.9154 - val_loss: 332.3042\n",
      "Epoch 1396/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.8067 - val_loss: 150.8937\n",
      "Epoch 1397/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.8771 - val_loss: 138.7115\n",
      "Epoch 1398/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.2187 - val_loss: 188.9050\n",
      "Epoch 1399/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.6521 - val_loss: 182.3274\n",
      "Epoch 1400/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.4600 - val_loss: 176.6769\n",
      "Epoch 1401/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.2935 - val_loss: 164.4507\n",
      "Epoch 1402/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 159.0334 - val_loss: 240.6832\n",
      "Epoch 1403/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.9384 - val_loss: 180.2462\n",
      "Epoch 1404/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.1517 - val_loss: 143.6617\n",
      "Epoch 1405/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.4051 - val_loss: 143.7634\n",
      "Epoch 1406/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.9275 - val_loss: 139.4256\n",
      "Epoch 1407/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.2484 - val_loss: 137.8684\n",
      "Epoch 1408/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.7725 - val_loss: 154.4095\n",
      "Epoch 1409/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.0085 - val_loss: 143.1313\n",
      "Epoch 1410/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.3125 - val_loss: 146.0266\n",
      "Epoch 1411/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.6574 - val_loss: 147.2492\n",
      "Epoch 1412/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 201.1420 - val_loss: 161.5641\n",
      "Epoch 1413/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.3564 - val_loss: 139.9705\n",
      "Epoch 1414/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 214.9361 - val_loss: 186.3340\n",
      "Epoch 1415/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.1877 - val_loss: 145.0421\n",
      "Epoch 1416/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 159.2843 - val_loss: 198.7792\n",
      "Epoch 1417/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 142.1335 - val_loss: 141.7167\n",
      "Epoch 1418/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 135.0700 - val_loss: 190.5781\n",
      "Epoch 1419/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.3648 - val_loss: 151.1905\n",
      "Epoch 1420/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.0136 - val_loss: 151.5291\n",
      "Epoch 1421/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.1709 - val_loss: 136.6515\n",
      "Epoch 1422/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.9432 - val_loss: 144.1907\n",
      "Epoch 1423/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.6408 - val_loss: 145.2718\n",
      "Epoch 1424/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.2990 - val_loss: 141.7368\n",
      "Epoch 1425/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.5767 - val_loss: 152.6793\n",
      "Epoch 1426/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.2764 - val_loss: 148.8361\n",
      "Epoch 1427/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.3537 - val_loss: 157.3504\n",
      "Epoch 1428/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 179.5945 - val_loss: 147.6566\n",
      "Epoch 1429/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.0774 - val_loss: 192.6843\n",
      "Epoch 1430/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.9548 - val_loss: 186.8965\n",
      "Epoch 1431/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.3624 - val_loss: 151.1006\n",
      "Epoch 1432/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.7024 - val_loss: 190.5457\n",
      "Epoch 1433/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 199.1835 - val_loss: 136.6908\n",
      "Epoch 1434/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.1619 - val_loss: 137.4153\n",
      "Epoch 1435/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 173.0919 - val_loss: 141.1722\n",
      "Epoch 1436/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.6407 - val_loss: 161.3864\n",
      "Epoch 1437/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.5650 - val_loss: 140.3643\n",
      "Epoch 1438/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.7747 - val_loss: 140.0966\n",
      "Epoch 1439/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 153.5034 - val_loss: 218.5673\n",
      "Epoch 1440/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 150.3436 - val_loss: 147.3436\n",
      "Epoch 1441/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.6998 - val_loss: 148.5985\n",
      "Epoch 1442/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.8938 - val_loss: 155.8839\n",
      "Epoch 1443/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.9992 - val_loss: 141.6572\n",
      "Epoch 1444/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.9467 - val_loss: 152.1645\n",
      "Epoch 1445/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 189.6619 - val_loss: 209.5648\n",
      "Epoch 1446/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.6618 - val_loss: 158.7441\n",
      "Epoch 1447/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.0857 - val_loss: 217.7643\n",
      "Epoch 1448/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.2016 - val_loss: 164.5038\n",
      "Epoch 1449/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.7507 - val_loss: 144.4198\n",
      "Epoch 1450/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.1200 - val_loss: 159.8075\n",
      "Epoch 1451/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.3483 - val_loss: 139.7357\n",
      "Epoch 1452/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.4568 - val_loss: 139.3965\n",
      "Epoch 1453/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.7706 - val_loss: 175.1432\n",
      "Epoch 1454/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.9237 - val_loss: 144.0266\n",
      "Epoch 1455/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.3183 - val_loss: 148.9192\n",
      "Epoch 1456/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.8769 - val_loss: 149.3657\n",
      "Epoch 1457/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 204.2859 - val_loss: 171.0997\n",
      "Epoch 1458/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 177.0817 - val_loss: 194.4891\n",
      "Epoch 1459/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.5751 - val_loss: 141.8338\n",
      "Epoch 1460/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.6515 - val_loss: 311.3433\n",
      "Epoch 1461/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.3292 - val_loss: 161.0120\n",
      "Epoch 1462/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.6679 - val_loss: 327.3109\n",
      "Epoch 1463/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 208.5582 - val_loss: 169.5239\n",
      "Epoch 1464/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.7578 - val_loss: 146.8943\n",
      "Epoch 1465/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.8810 - val_loss: 143.3210\n",
      "Epoch 1466/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.9900 - val_loss: 143.5963\n",
      "Epoch 1467/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.1915 - val_loss: 148.1927\n",
      "Epoch 1468/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.1192 - val_loss: 190.6720\n",
      "Epoch 1469/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.9847 - val_loss: 164.4849\n",
      "Epoch 1470/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.7125 - val_loss: 162.7628\n",
      "Epoch 1471/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.0419 - val_loss: 149.8032\n",
      "Epoch 1472/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.7266 - val_loss: 172.6112\n",
      "Epoch 1473/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.1837 - val_loss: 151.5408\n",
      "Epoch 1474/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.9428 - val_loss: 135.2809\n",
      "Epoch 1475/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.6873 - val_loss: 143.7533\n",
      "Epoch 1476/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.5735 - val_loss: 139.2887\n",
      "Epoch 1477/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.8880 - val_loss: 148.5280\n",
      "Epoch 1478/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 142.3130 - val_loss: 141.8157\n",
      "Epoch 1479/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 137.6174 - val_loss: 152.9581\n",
      "Epoch 1480/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 137.3559 - val_loss: 151.1353\n",
      "Epoch 1481/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.6691 - val_loss: 141.3263\n",
      "Epoch 1482/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.9390 - val_loss: 172.7777\n",
      "Epoch 1483/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.1563 - val_loss: 140.4459\n",
      "Epoch 1484/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.2489 - val_loss: 137.9053\n",
      "Epoch 1485/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.4414 - val_loss: 146.7589\n",
      "Epoch 1486/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.8831 - val_loss: 137.3565\n",
      "Epoch 1487/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.8164 - val_loss: 179.2011\n",
      "Epoch 1488/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.5730 - val_loss: 154.3488\n",
      "Epoch 1489/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.1906 - val_loss: 141.3277\n",
      "Epoch 1490/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.0488 - val_loss: 557.4397\n",
      "Epoch 1491/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 169.9435 - val_loss: 157.6429\n",
      "Epoch 1492/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.1222 - val_loss: 137.2811\n",
      "Epoch 1493/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.2996 - val_loss: 161.6909\n",
      "Epoch 1494/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.3053 - val_loss: 139.3050\n",
      "Epoch 1495/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.3073 - val_loss: 143.3289\n",
      "Epoch 1496/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.3960 - val_loss: 142.9731\n",
      "Epoch 1497/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.7047 - val_loss: 155.5709\n",
      "Epoch 1498/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.0674 - val_loss: 150.3905\n",
      "Epoch 1499/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.7906 - val_loss: 149.8430\n",
      "Epoch 1500/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.3385 - val_loss: 158.0143\n",
      "Epoch 1501/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.9051 - val_loss: 163.3893\n",
      "Epoch 1502/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.1263 - val_loss: 169.5746\n",
      "Epoch 1503/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.9635 - val_loss: 151.5786\n",
      "Epoch 1504/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 196.5607 - val_loss: 146.6436\n",
      "Epoch 1505/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.6282 - val_loss: 142.4691\n",
      "Epoch 1506/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.6033 - val_loss: 144.7879\n",
      "Epoch 1507/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.8010 - val_loss: 166.0133\n",
      "Epoch 1508/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.1205 - val_loss: 160.8403\n",
      "Epoch 1509/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.7262 - val_loss: 145.9171\n",
      "Epoch 1510/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.9993 - val_loss: 143.7322\n",
      "Epoch 1511/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.2267 - val_loss: 143.8333\n",
      "Epoch 1512/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.7197 - val_loss: 143.0274\n",
      "Epoch 1513/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 183.6425 - val_loss: 178.6963\n",
      "Epoch 1514/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.4479 - val_loss: 137.2856\n",
      "Epoch 1515/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.4443 - val_loss: 140.4005\n",
      "Epoch 1516/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.2652 - val_loss: 209.4921\n",
      "Epoch 1517/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.3944 - val_loss: 181.4381\n",
      "Epoch 1518/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.5284 - val_loss: 141.6303\n",
      "Epoch 1519/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 129.3928 - val_loss: 140.2734\n",
      "Epoch 1520/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.6132 - val_loss: 230.1090\n",
      "Epoch 1521/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.6482 - val_loss: 144.7377\n",
      "Epoch 1522/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.4690 - val_loss: 153.0984\n",
      "Epoch 1523/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 203.6619 - val_loss: 165.3874\n",
      "Epoch 1524/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.1384 - val_loss: 175.2349\n",
      "Epoch 1525/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.0460 - val_loss: 165.2543\n",
      "Epoch 1526/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.7029 - val_loss: 149.4628\n",
      "Epoch 1527/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 205.2110 - val_loss: 154.4004\n",
      "Epoch 1528/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.4351 - val_loss: 140.6825\n",
      "Epoch 1529/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.2078 - val_loss: 158.4286\n",
      "Epoch 1530/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.0877 - val_loss: 153.0097\n",
      "Epoch 1531/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.6134 - val_loss: 146.1399\n",
      "Epoch 1532/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.4223 - val_loss: 161.4653\n",
      "Epoch 1533/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 164.0228 - val_loss: 254.7262\n",
      "Epoch 1534/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.0968 - val_loss: 148.5324\n",
      "Epoch 1535/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.2055 - val_loss: 144.8971\n",
      "Epoch 1536/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.5918 - val_loss: 157.6917\n",
      "Epoch 1537/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.7410 - val_loss: 199.8128\n",
      "Epoch 1538/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.5758 - val_loss: 165.1843\n",
      "Epoch 1539/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 155.1923 - val_loss: 265.3647\n",
      "Epoch 1540/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 149.8622 - val_loss: 150.5561\n",
      "Epoch 1541/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 131.4961 - val_loss: 237.8653\n",
      "Epoch 1542/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.1495 - val_loss: 196.6473\n",
      "Epoch 1543/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.5862 - val_loss: 140.8529\n",
      "Epoch 1544/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.8006 - val_loss: 166.7666\n",
      "Epoch 1545/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.6345 - val_loss: 152.9005\n",
      "Epoch 1546/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.5819 - val_loss: 155.8969\n",
      "Epoch 1547/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.9117 - val_loss: 148.3095\n",
      "Epoch 1548/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.7374 - val_loss: 169.4194\n",
      "Epoch 1549/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.1191 - val_loss: 143.2806\n",
      "Epoch 1550/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.7474 - val_loss: 153.7567\n",
      "Epoch 1551/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.0672 - val_loss: 153.4918\n",
      "Epoch 1552/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.2081 - val_loss: 192.4459\n",
      "Epoch 1553/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.1775 - val_loss: 144.9905\n",
      "Epoch 1554/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.8410 - val_loss: 152.7461\n",
      "Epoch 1555/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.0346 - val_loss: 139.9521\n",
      "Epoch 1556/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.3402 - val_loss: 166.1779\n",
      "Epoch 1557/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.5778 - val_loss: 138.1024\n",
      "Epoch 1558/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.6208 - val_loss: 139.8059\n",
      "Epoch 1559/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.1294 - val_loss: 136.7619\n",
      "Epoch 1560/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.8536 - val_loss: 200.0420\n",
      "Epoch 1561/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.2629 - val_loss: 140.2576\n",
      "Epoch 1562/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.7989 - val_loss: 177.8916\n",
      "Epoch 1563/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.1890 - val_loss: 146.5812\n",
      "Epoch 1564/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.4796 - val_loss: 143.9564\n",
      "Epoch 1565/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.6214 - val_loss: 141.5025\n",
      "Epoch 1566/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.8161 - val_loss: 148.0486\n",
      "Epoch 1567/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.7432 - val_loss: 147.6275\n",
      "Epoch 1568/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.7704 - val_loss: 169.2032\n",
      "Epoch 1569/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.7117 - val_loss: 195.8154\n",
      "Epoch 1570/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.5681 - val_loss: 368.2449\n",
      "Epoch 1571/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.1999 - val_loss: 162.2411\n",
      "Epoch 1572/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.2272 - val_loss: 153.6171\n",
      "Epoch 1573/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.9690 - val_loss: 142.1227\n",
      "Epoch 1574/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.6539 - val_loss: 146.7782\n",
      "Epoch 1575/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 185.8779 - val_loss: 143.3868\n",
      "Epoch 1576/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 130.1580 - val_loss: 142.8705\n",
      "Epoch 1577/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 206.9395 - val_loss: 233.8581\n",
      "Epoch 1578/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.3026 - val_loss: 138.8355\n",
      "Epoch 1579/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.9538 - val_loss: 138.7360\n",
      "Epoch 1580/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.3354 - val_loss: 148.3783\n",
      "Epoch 1581/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.9269 - val_loss: 185.8905\n",
      "Epoch 1582/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.6078 - val_loss: 140.2186\n",
      "Epoch 1583/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 128.9793 - val_loss: 162.3217\n",
      "Epoch 1584/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 181.0007 - val_loss: 148.7240\n",
      "Epoch 1585/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.8018 - val_loss: 142.8445\n",
      "Epoch 1586/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.6792 - val_loss: 143.0886\n",
      "Epoch 1587/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 135.8501 - val_loss: 144.4942\n",
      "Epoch 1588/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.7408 - val_loss: 334.1375\n",
      "Epoch 1589/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 164.7083 - val_loss: 137.2745\n",
      "Epoch 1590/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.8330 - val_loss: 198.6817\n",
      "Epoch 1591/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 187.0542 - val_loss: 169.9228\n",
      "Epoch 1592/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.4662 - val_loss: 181.2206\n",
      "Epoch 1593/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.4019 - val_loss: 140.5430\n",
      "Epoch 1594/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.4689 - val_loss: 145.1354\n",
      "Epoch 1595/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.1619 - val_loss: 159.4125\n",
      "Epoch 1596/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.0350 - val_loss: 138.5772\n",
      "Epoch 1597/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.5937 - val_loss: 145.8650\n",
      "Epoch 1598/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.5746 - val_loss: 228.6672\n",
      "Epoch 1599/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.1413 - val_loss: 141.0655\n",
      "Epoch 1600/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 179.5975 - val_loss: 180.0751\n",
      "Epoch 1601/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 153.9581 - val_loss: 157.3045\n",
      "Epoch 1602/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 136.0564 - val_loss: 173.6756\n",
      "Epoch 1603/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.1376 - val_loss: 141.4393\n",
      "Epoch 1604/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.7979 - val_loss: 155.5298\n",
      "Epoch 1605/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.4359 - val_loss: 136.1860\n",
      "Epoch 1606/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.6003 - val_loss: 154.9332\n",
      "Epoch 1607/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.9086 - val_loss: 138.2351\n",
      "Epoch 1608/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.5316 - val_loss: 141.1237\n",
      "Epoch 1609/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.5425 - val_loss: 416.7797\n",
      "Epoch 1610/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.6792 - val_loss: 142.0047\n",
      "Epoch 1611/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.0353 - val_loss: 165.1117\n",
      "Epoch 1612/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.3908 - val_loss: 190.0577\n",
      "Epoch 1613/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.1905 - val_loss: 159.1007\n",
      "Epoch 1614/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 127.6473 - val_loss: 170.1752\n",
      "Epoch 1615/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.6317 - val_loss: 152.6143\n",
      "Epoch 1616/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.1984 - val_loss: 175.0930\n",
      "Epoch 1617/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.8925 - val_loss: 181.5396\n",
      "Epoch 1618/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.2218 - val_loss: 177.0376\n",
      "Epoch 1619/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.2177 - val_loss: 159.0909\n",
      "Epoch 1620/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.2648 - val_loss: 145.9881\n",
      "Epoch 1621/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.7544 - val_loss: 145.6420\n",
      "Epoch 1622/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 136.5067 - val_loss: 156.4726\n",
      "Epoch 1623/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.1473 - val_loss: 158.4119\n",
      "Epoch 1624/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.5764 - val_loss: 141.1408\n",
      "Epoch 1625/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.1078 - val_loss: 171.3985\n",
      "Epoch 1626/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.0909 - val_loss: 145.5783\n",
      "Epoch 1627/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.1177 - val_loss: 137.3531\n",
      "Epoch 1628/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.6093 - val_loss: 180.6978\n",
      "Epoch 1629/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 184.1233 - val_loss: 214.8707\n",
      "Epoch 1630/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.1742 - val_loss: 161.1856\n",
      "Epoch 1631/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.5179 - val_loss: 151.3125\n",
      "Epoch 1632/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.6461 - val_loss: 171.4773\n",
      "Epoch 1633/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.1568 - val_loss: 139.9025\n",
      "Epoch 1634/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.5560 - val_loss: 161.7140\n",
      "Epoch 1635/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.2847 - val_loss: 192.0968\n",
      "Epoch 1636/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 180.2492 - val_loss: 157.2133\n",
      "Epoch 1637/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.8767 - val_loss: 164.9290\n",
      "Epoch 1638/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.6104 - val_loss: 207.3709\n",
      "Epoch 1639/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.1865 - val_loss: 141.8168\n",
      "Epoch 1640/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.7705 - val_loss: 185.0789\n",
      "Epoch 1641/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.0078 - val_loss: 159.2806\n",
      "Epoch 1642/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.8356 - val_loss: 164.1920\n",
      "Epoch 1643/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.3871 - val_loss: 147.2521\n",
      "Epoch 1644/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.9811 - val_loss: 187.0903\n",
      "Epoch 1645/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.4586 - val_loss: 138.6228\n",
      "Epoch 1646/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.4613 - val_loss: 147.4992\n",
      "Epoch 1647/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.3457 - val_loss: 273.4475\n",
      "Epoch 1648/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.8515 - val_loss: 156.2956\n",
      "Epoch 1649/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.3543 - val_loss: 148.6008\n",
      "Epoch 1650/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.8702 - val_loss: 283.4636\n",
      "Epoch 1651/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.0241 - val_loss: 143.5199\n",
      "Epoch 1652/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.1839 - val_loss: 497.4843\n",
      "Epoch 1653/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.6222 - val_loss: 165.6368\n",
      "Epoch 1654/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.9137 - val_loss: 165.9043\n",
      "Epoch 1655/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.7859 - val_loss: 158.4241\n",
      "Epoch 1656/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.2075 - val_loss: 149.5475\n",
      "Epoch 1657/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.4288 - val_loss: 141.0019\n",
      "Epoch 1658/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.5000 - val_loss: 140.8439\n",
      "Epoch 1659/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.5742 - val_loss: 230.7793\n",
      "Epoch 1660/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.9562 - val_loss: 145.1271\n",
      "Epoch 1661/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 169.0704 - val_loss: 140.4618\n",
      "Epoch 1662/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 140.9247 - val_loss: 178.3808\n",
      "Epoch 1663/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 138.7557 - val_loss: 175.5500\n",
      "Epoch 1664/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.0199 - val_loss: 189.4250\n",
      "Epoch 1665/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 128.9903 - val_loss: 143.6129\n",
      "Epoch 1666/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.8128 - val_loss: 140.1609\n",
      "Epoch 1667/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.7934 - val_loss: 142.7552\n",
      "Epoch 1668/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.2574 - val_loss: 149.5470\n",
      "Epoch 1669/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.5125 - val_loss: 163.7190\n",
      "Epoch 1670/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.6318 - val_loss: 164.2228\n",
      "Epoch 1671/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.7419 - val_loss: 264.4798\n",
      "Epoch 1672/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.6854 - val_loss: 159.7925\n",
      "Epoch 1673/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 135.5571 - val_loss: 163.0126\n",
      "Epoch 1674/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.4022 - val_loss: 163.0647\n",
      "Epoch 1675/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.3286 - val_loss: 180.4256\n",
      "Epoch 1676/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.5667 - val_loss: 197.5948\n",
      "Epoch 1677/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.2618 - val_loss: 187.3803\n",
      "Epoch 1678/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.1144 - val_loss: 168.5725\n",
      "Epoch 1679/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.6695 - val_loss: 146.7737\n",
      "Epoch 1680/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.4419 - val_loss: 183.1670\n",
      "Epoch 1681/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.6443 - val_loss: 167.2680\n",
      "Epoch 1682/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 185.7571 - val_loss: 149.1393\n",
      "Epoch 1683/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.1809 - val_loss: 139.7257\n",
      "Epoch 1684/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.8209 - val_loss: 293.4476\n",
      "Epoch 1685/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.0170 - val_loss: 150.9921\n",
      "Epoch 1686/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.8212 - val_loss: 139.8176\n",
      "Epoch 1687/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.2376 - val_loss: 150.1784\n",
      "Epoch 1688/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.1703 - val_loss: 153.9694\n",
      "Epoch 1689/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.9596 - val_loss: 141.1150\n",
      "Epoch 1690/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.5236 - val_loss: 138.8894\n",
      "Epoch 1691/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.4452 - val_loss: 144.9503\n",
      "Epoch 1692/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 136.0102 - val_loss: 239.6304\n",
      "Epoch 1693/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.9328 - val_loss: 158.2613\n",
      "Epoch 1694/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.8015 - val_loss: 140.1111\n",
      "Epoch 1695/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.3559 - val_loss: 318.8159\n",
      "Epoch 1696/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.2756 - val_loss: 140.6071\n",
      "Epoch 1697/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.7191 - val_loss: 149.3259\n",
      "Epoch 1698/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.0563 - val_loss: 280.2718\n",
      "Epoch 1699/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 218.2680 - val_loss: 218.8688\n",
      "Epoch 1700/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 169.1870 - val_loss: 153.5000\n",
      "Epoch 1701/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.9011 - val_loss: 154.8683\n",
      "Epoch 1702/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.1411 - val_loss: 155.2226\n",
      "Epoch 1703/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.4963 - val_loss: 253.1204\n",
      "Epoch 1704/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.1238 - val_loss: 141.8979\n",
      "Epoch 1705/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.6312 - val_loss: 141.1464\n",
      "Epoch 1706/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.0008 - val_loss: 262.1215\n",
      "Epoch 1707/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.2235 - val_loss: 142.1113\n",
      "Epoch 1708/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.4956 - val_loss: 160.4878\n",
      "Epoch 1709/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 139.0258 - val_loss: 142.1954\n",
      "Epoch 1710/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.8422 - val_loss: 136.0863\n",
      "Epoch 1711/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.1754 - val_loss: 143.3479\n",
      "Epoch 1712/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.9891 - val_loss: 147.2625\n",
      "Epoch 1713/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.5713 - val_loss: 144.3133\n",
      "Epoch 1714/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 133.2034 - val_loss: 227.5598\n",
      "Epoch 1715/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 131.8240 - val_loss: 145.3303\n",
      "Epoch 1716/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 145.7791 - val_loss: 140.7540\n",
      "Epoch 1717/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.7185 - val_loss: 148.6716\n",
      "Epoch 1718/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.1573 - val_loss: 147.5008\n",
      "Epoch 1719/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.9642 - val_loss: 142.9175\n",
      "Epoch 1720/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 185.7527 - val_loss: 165.6203\n",
      "Epoch 1721/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.5905 - val_loss: 216.3049\n",
      "Epoch 1722/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 131.2792 - val_loss: 144.2754\n",
      "Epoch 1723/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 139.3720 - val_loss: 145.7302\n",
      "Epoch 1724/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 138.2994 - val_loss: 153.2641\n",
      "Epoch 1725/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.4299 - val_loss: 136.5872\n",
      "Epoch 1726/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.6714 - val_loss: 164.6489\n",
      "Epoch 1727/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 139.3361 - val_loss: 166.7802\n",
      "Epoch 1728/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.1213 - val_loss: 210.3071\n",
      "Epoch 1729/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.6980 - val_loss: 144.3966\n",
      "Epoch 1730/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 162.2097 - val_loss: 162.5463\n",
      "Epoch 1731/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.2943 - val_loss: 146.6316\n",
      "Epoch 1732/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.7103 - val_loss: 142.1891\n",
      "Epoch 1733/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.3435 - val_loss: 156.6425\n",
      "Epoch 1734/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.2455 - val_loss: 155.0705\n",
      "Epoch 1735/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 135.4146 - val_loss: 136.8504\n",
      "Epoch 1736/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.8418 - val_loss: 157.1054\n",
      "Epoch 1737/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.3027 - val_loss: 144.2903\n",
      "Epoch 1738/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 129.9575 - val_loss: 247.5157\n",
      "Epoch 1739/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 198.7383 - val_loss: 164.1943\n",
      "Epoch 1740/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.2951 - val_loss: 143.4755\n",
      "Epoch 1741/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.5468 - val_loss: 175.4187\n",
      "Epoch 1742/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 173.2511 - val_loss: 162.3874\n",
      "Epoch 1743/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.8744 - val_loss: 357.5225\n",
      "Epoch 1744/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.4873 - val_loss: 138.9512\n",
      "Epoch 1745/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.3787 - val_loss: 147.7488\n",
      "Epoch 1746/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.4266 - val_loss: 169.2451\n",
      "Epoch 1747/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.1333 - val_loss: 325.1717\n",
      "Epoch 1748/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.0720 - val_loss: 219.1198\n",
      "Epoch 1749/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.2739 - val_loss: 162.9460\n",
      "Epoch 1750/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.7115 - val_loss: 149.2000\n",
      "Epoch 1751/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.5858 - val_loss: 145.2079\n",
      "Epoch 1752/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.7693 - val_loss: 251.7650\n",
      "Epoch 1753/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.8052 - val_loss: 179.0097\n",
      "Epoch 1754/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.4821 - val_loss: 151.9435\n",
      "Epoch 1755/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.5658 - val_loss: 153.7058\n",
      "Epoch 1756/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 142.0138 - val_loss: 210.7573\n",
      "Epoch 1757/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 135.4270 - val_loss: 144.1182\n",
      "Epoch 1758/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 199.5032 - val_loss: 172.2881\n",
      "Epoch 1759/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.1796 - val_loss: 140.7724\n",
      "Epoch 1760/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.5853 - val_loss: 144.3594\n",
      "Epoch 1761/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.9066 - val_loss: 165.5123\n",
      "Epoch 1762/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 190.3707 - val_loss: 147.0728\n",
      "Epoch 1763/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.7502 - val_loss: 143.9635\n",
      "Epoch 1764/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.5118 - val_loss: 209.4421\n",
      "Epoch 1765/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.1569 - val_loss: 183.6697\n",
      "Epoch 1766/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.4455 - val_loss: 136.6497\n",
      "Epoch 1767/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.0601 - val_loss: 141.0096\n",
      "Epoch 1768/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.8219 - val_loss: 151.4328\n",
      "Epoch 1769/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.0017 - val_loss: 188.6947\n",
      "Epoch 1770/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.3579 - val_loss: 189.5021\n",
      "Epoch 1771/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.8006 - val_loss: 427.7566\n",
      "Epoch 1772/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.9440 - val_loss: 186.7816\n",
      "Epoch 1773/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.9396 - val_loss: 151.3241\n",
      "Epoch 1774/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.8583 - val_loss: 148.4413\n",
      "Epoch 1775/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.7643 - val_loss: 187.8210\n",
      "Epoch 1776/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.5564 - val_loss: 140.0783\n",
      "Epoch 1777/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 127.0665 - val_loss: 216.2038\n",
      "Epoch 1778/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.7909 - val_loss: 225.0001\n",
      "Epoch 1779/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 205.7018 - val_loss: 146.0138\n",
      "Epoch 1780/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.1674 - val_loss: 140.8168\n",
      "Epoch 1781/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.5027 - val_loss: 219.8666\n",
      "Epoch 1782/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 136.0960 - val_loss: 146.0910\n",
      "Epoch 1783/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 151.6224 - val_loss: 165.9291\n",
      "Epoch 1784/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 149.7023 - val_loss: 165.2913\n",
      "Epoch 1785/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.9268 - val_loss: 151.8703\n",
      "Epoch 1786/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.3630 - val_loss: 140.6803\n",
      "Epoch 1787/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 178.1244 - val_loss: 146.6271\n",
      "Epoch 1788/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.3553 - val_loss: 143.2270\n",
      "Epoch 1789/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.0806 - val_loss: 149.1912\n",
      "Epoch 1790/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 169.6307 - val_loss: 138.4339\n",
      "Epoch 1791/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.4606 - val_loss: 146.9008\n",
      "Epoch 1792/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.6872 - val_loss: 173.6584\n",
      "Epoch 1793/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.1656 - val_loss: 147.1042\n",
      "Epoch 1794/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 185.3113 - val_loss: 160.6015\n",
      "Epoch 1795/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 139.9355 - val_loss: 151.2068\n",
      "Epoch 1796/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.3131 - val_loss: 184.7567\n",
      "Epoch 1797/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.6753 - val_loss: 181.0013\n",
      "Epoch 1798/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.2227 - val_loss: 143.0547\n",
      "Epoch 1799/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.6196 - val_loss: 173.7250\n",
      "Epoch 1800/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.3151 - val_loss: 399.8689\n",
      "Epoch 1801/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.6057 - val_loss: 139.6482\n",
      "Epoch 1802/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.0408 - val_loss: 144.9465\n",
      "Epoch 1803/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.4398 - val_loss: 144.1131\n",
      "Epoch 1804/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.0751 - val_loss: 140.0529\n",
      "Epoch 1805/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.6521 - val_loss: 151.3399\n",
      "Epoch 1806/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.4282 - val_loss: 139.8485\n",
      "Epoch 1807/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 133.8653 - val_loss: 155.0245\n",
      "Epoch 1808/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 156.0553 - val_loss: 161.7064\n",
      "Epoch 1809/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.0254 - val_loss: 139.5906\n",
      "Epoch 1810/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.0922 - val_loss: 144.1781\n",
      "Epoch 1811/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.9892 - val_loss: 168.9652\n",
      "Epoch 1812/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.9403 - val_loss: 136.4435\n",
      "Epoch 1813/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.7493 - val_loss: 146.5965\n",
      "Epoch 1814/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.0471 - val_loss: 170.3688\n",
      "Epoch 1815/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.5124 - val_loss: 178.9016\n",
      "Epoch 1816/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.4856 - val_loss: 157.5683\n",
      "Epoch 1817/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.2121 - val_loss: 146.0123\n",
      "Epoch 1818/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.1593 - val_loss: 143.2394\n",
      "Epoch 1819/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.9619 - val_loss: 184.2472\n",
      "Epoch 1820/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.1875 - val_loss: 177.2857\n",
      "Epoch 1821/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.3837 - val_loss: 205.5654\n",
      "Epoch 1822/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.1168 - val_loss: 225.6775\n",
      "Epoch 1823/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.7905 - val_loss: 173.9291\n",
      "Epoch 1824/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.6582 - val_loss: 143.4915\n",
      "Epoch 1825/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.2160 - val_loss: 149.6138\n",
      "Epoch 1826/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.5998 - val_loss: 145.3252\n",
      "Epoch 1827/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 175.0413 - val_loss: 158.4134\n",
      "Epoch 1828/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.2976 - val_loss: 212.9772\n",
      "Epoch 1829/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.6604 - val_loss: 190.6097\n",
      "Epoch 1830/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 169.0807 - val_loss: 146.6038\n",
      "Epoch 1831/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.7859 - val_loss: 145.9911\n",
      "Epoch 1832/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.3452 - val_loss: 138.6578\n",
      "Epoch 1833/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.6473 - val_loss: 172.3614\n",
      "Epoch 1834/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.1728 - val_loss: 215.2881\n",
      "Epoch 1835/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.7180 - val_loss: 143.8639\n",
      "Epoch 1836/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 202.1502 - val_loss: 137.6949\n",
      "Epoch 1837/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.9518 - val_loss: 144.0472\n",
      "Epoch 1838/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.0159 - val_loss: 182.4355\n",
      "Epoch 1839/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.1768 - val_loss: 228.4577\n",
      "Epoch 1840/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.2086 - val_loss: 138.6478\n",
      "Epoch 1841/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.7144 - val_loss: 144.3024\n",
      "Epoch 1842/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.9350 - val_loss: 138.5341\n",
      "Epoch 1843/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 129.1615 - val_loss: 159.9232\n",
      "Epoch 1844/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 152.4179 - val_loss: 143.5205\n",
      "Epoch 1845/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 141.3660 - val_loss: 139.9162\n",
      "Epoch 1846/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 135.7965 - val_loss: 141.4346\n",
      "Epoch 1847/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.1890 - val_loss: 181.9490\n",
      "Epoch 1848/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.7756 - val_loss: 146.7622\n",
      "Epoch 1849/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.1137 - val_loss: 139.4662\n",
      "Epoch 1850/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.7280 - val_loss: 157.1720\n",
      "Epoch 1851/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 179.1473 - val_loss: 162.4268\n",
      "Epoch 1852/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 235.6175 - val_loss: 161.1347\n",
      "Epoch 1853/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 165.9335 - val_loss: 153.3805\n",
      "Epoch 1854/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.1795 - val_loss: 163.3506\n",
      "Epoch 1855/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.3152 - val_loss: 152.6255\n",
      "Epoch 1856/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.4152 - val_loss: 150.9714\n",
      "Epoch 1857/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.8080 - val_loss: 171.8859\n",
      "Epoch 1858/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.5226 - val_loss: 153.5758\n",
      "Epoch 1859/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.1048 - val_loss: 208.2091\n",
      "Epoch 1860/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.2572 - val_loss: 139.6879\n",
      "Epoch 1861/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.2307 - val_loss: 290.7662\n",
      "Epoch 1862/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.8741 - val_loss: 148.9028\n",
      "Epoch 1863/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.3421 - val_loss: 199.5864\n",
      "Epoch 1864/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.4308 - val_loss: 143.6456\n",
      "Epoch 1865/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.2336 - val_loss: 154.7464\n",
      "Epoch 1866/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.8738 - val_loss: 346.1324\n",
      "Epoch 1867/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.5505 - val_loss: 143.7676\n",
      "Epoch 1868/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.9426 - val_loss: 149.9499\n",
      "Epoch 1869/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.2950 - val_loss: 153.6540\n",
      "Epoch 1870/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.1984 - val_loss: 173.0491\n",
      "Epoch 1871/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.0911 - val_loss: 153.0389\n",
      "Epoch 1872/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.1361 - val_loss: 143.0198\n",
      "Epoch 1873/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.3628 - val_loss: 151.5388\n",
      "Epoch 1874/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.5806 - val_loss: 154.3248\n",
      "Epoch 1875/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 172.4493 - val_loss: 138.8928\n",
      "Epoch 1876/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 162.4404 - val_loss: 139.8122\n",
      "Epoch 1877/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 129.0589 - val_loss: 190.3244\n",
      "Epoch 1878/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.8664 - val_loss: 144.0746\n",
      "Epoch 1879/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.2617 - val_loss: 141.7258\n",
      "Epoch 1880/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.6240 - val_loss: 144.6192\n",
      "Epoch 1881/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.5710 - val_loss: 166.0688\n",
      "Epoch 1882/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.3041 - val_loss: 142.6846\n",
      "Epoch 1883/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.7018 - val_loss: 143.0503\n",
      "Epoch 1884/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.4119 - val_loss: 175.2077\n",
      "Epoch 1885/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 172.2927 - val_loss: 137.0810\n",
      "Epoch 1886/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.9151 - val_loss: 139.4794\n",
      "Epoch 1887/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.4080 - val_loss: 140.3984\n",
      "Epoch 1888/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.6399 - val_loss: 146.6204\n",
      "Epoch 1889/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.1550 - val_loss: 142.9866\n",
      "Epoch 1890/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.5760 - val_loss: 169.5398\n",
      "Epoch 1891/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.1900 - val_loss: 140.7125\n",
      "Epoch 1892/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.9925 - val_loss: 162.3987\n",
      "Epoch 1893/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 196.5888 - val_loss: 145.8224\n",
      "Epoch 1894/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.0613 - val_loss: 159.8240\n",
      "Epoch 1895/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 241.0466 - val_loss: 139.4746\n",
      "Epoch 1896/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.3064 - val_loss: 216.1193\n",
      "Epoch 1897/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.6227 - val_loss: 195.7491\n",
      "Epoch 1898/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.5950 - val_loss: 140.1780\n",
      "Epoch 1899/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.5252 - val_loss: 182.4050\n",
      "Epoch 1900/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.2332 - val_loss: 149.1366\n",
      "Epoch 1901/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.9823 - val_loss: 268.0147\n",
      "Epoch 1902/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.8101 - val_loss: 228.2959\n",
      "Epoch 1903/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.5120 - val_loss: 153.4153\n",
      "Epoch 1904/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 164.7475 - val_loss: 188.0713\n",
      "Epoch 1905/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.1644 - val_loss: 165.0910\n",
      "Epoch 1906/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.4265 - val_loss: 146.7058\n",
      "Epoch 1907/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.0596 - val_loss: 137.3311\n",
      "Epoch 1908/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.9324 - val_loss: 242.9571\n",
      "Epoch 1909/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.2695 - val_loss: 138.7416\n",
      "Epoch 1910/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.8790 - val_loss: 157.0032\n",
      "Epoch 1911/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 163.1030 - val_loss: 189.4221\n",
      "Epoch 1912/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 140.9019 - val_loss: 141.5634\n",
      "Epoch 1913/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 152.5283 - val_loss: 143.8802\n",
      "Epoch 1914/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 139.5494 - val_loss: 152.2486\n",
      "Epoch 1915/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 133.8215 - val_loss: 175.0271\n",
      "Epoch 1916/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.7439 - val_loss: 140.9791\n",
      "Epoch 1917/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.2371 - val_loss: 206.3389\n",
      "Epoch 1918/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.3663 - val_loss: 172.5160\n",
      "Epoch 1919/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.2390 - val_loss: 153.9790\n",
      "Epoch 1920/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 150.0179 - val_loss: 163.7142\n",
      "Epoch 1921/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 198.7285 - val_loss: 145.1200\n",
      "Epoch 1922/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 129.5235 - val_loss: 142.5422\n",
      "Epoch 1923/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.1387 - val_loss: 145.8243\n",
      "Epoch 1924/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.7449 - val_loss: 160.7010\n",
      "Epoch 1925/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.1617 - val_loss: 143.9179\n",
      "Epoch 1926/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 136.8401 - val_loss: 164.2319\n",
      "Epoch 1927/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.3189 - val_loss: 143.9054\n",
      "Epoch 1928/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.3443 - val_loss: 199.8732\n",
      "Epoch 1929/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.0209 - val_loss: 140.7706\n",
      "Epoch 1930/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 130.6568 - val_loss: 147.3627\n",
      "Epoch 1931/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.8854 - val_loss: 146.4669\n",
      "Epoch 1932/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 135.7877 - val_loss: 136.2962\n",
      "Epoch 1933/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 137.1893 - val_loss: 150.3713\n",
      "Epoch 1934/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 134.0519 - val_loss: 145.5819\n",
      "Epoch 1935/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 129.5224 - val_loss: 143.3507\n",
      "Epoch 1936/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 153.6768 - val_loss: 172.8683\n",
      "Epoch 1937/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 139.1537 - val_loss: 160.6865\n",
      "Epoch 1938/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 128.3249 - val_loss: 189.2863\n",
      "Epoch 1939/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 208.5431 - val_loss: 157.9829\n",
      "Epoch 1940/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 152.0941 - val_loss: 155.8868\n",
      "Epoch 1941/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 146.4942 - val_loss: 173.9689\n",
      "Epoch 1942/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 132.5667 - val_loss: 144.4367\n",
      "Epoch 1943/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 151.3876 - val_loss: 354.2222\n",
      "Epoch 1944/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 199.6274 - val_loss: 144.7160\n",
      "Epoch 1945/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 138.0004 - val_loss: 146.3055\n",
      "Epoch 1946/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 138.4695 - val_loss: 137.8873\n",
      "Epoch 1947/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 146.3254 - val_loss: 146.7117\n",
      "Epoch 1948/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 152.7547 - val_loss: 197.4785\n",
      "Epoch 1949/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 139.7509 - val_loss: 139.9883\n",
      "Epoch 1950/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 133.5378 - val_loss: 168.8355\n",
      "Epoch 1951/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 153.2646 - val_loss: 143.6794\n",
      "Epoch 1952/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 156.2062 - val_loss: 268.1901\n",
      "Epoch 1953/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 305.2376 - val_loss: 145.0624\n",
      "Epoch 1954/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 154.6998 - val_loss: 172.6022\n",
      "Epoch 1955/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 137.5376 - val_loss: 168.4979\n",
      "Epoch 1956/10000\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 139.7753 - val_loss: 190.7750\n",
      "Epoch 1957/10000\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 139.8651 - val_loss: 249.6846\n",
      "Epoch 1958/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 133.5353 - val_loss: 152.9433\n",
      "Epoch 1959/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 85us/step - loss: 154.4046 - val_loss: 153.3812\n",
      "Epoch 1960/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 140.1499 - val_loss: 138.9849\n",
      "Epoch 1961/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 168.9474 - val_loss: 150.7682\n",
      "Epoch 1962/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 141.8299 - val_loss: 140.1804\n",
      "Epoch 1963/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 142.0583 - val_loss: 151.9377\n",
      "Epoch 1964/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 162.2048 - val_loss: 159.9645\n",
      "Epoch 1965/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 158.3871 - val_loss: 144.1274\n",
      "Epoch 1966/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 133.5131 - val_loss: 164.1083\n",
      "Epoch 1967/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 132.6635 - val_loss: 142.4172\n",
      "Epoch 1968/10000\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 142.5375 - val_loss: 156.6705\n",
      "Epoch 1969/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 164.1840 - val_loss: 245.6275\n",
      "Epoch 1970/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 162.4930 - val_loss: 148.4099\n",
      "Epoch 1971/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 137.9774 - val_loss: 140.7409\n",
      "Epoch 1972/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 154.8402 - val_loss: 154.5797\n",
      "Epoch 1973/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 141.0977 - val_loss: 180.2040\n",
      "Epoch 1974/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 137.9528 - val_loss: 148.5217\n",
      "Epoch 1975/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 132.9663 - val_loss: 141.7143\n",
      "Epoch 1976/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 128.7954 - val_loss: 196.6128\n",
      "Epoch 1977/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 154.2454 - val_loss: 144.9695\n",
      "Epoch 1978/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 156.9444 - val_loss: 251.0493\n",
      "Epoch 1979/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 139.2464 - val_loss: 192.0183\n",
      "Epoch 1980/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 129.6821 - val_loss: 142.1274\n",
      "Epoch 1981/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 138.6749 - val_loss: 212.8182\n",
      "Epoch 1982/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 247.2233 - val_loss: 227.6697\n",
      "Epoch 1983/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 147.8680 - val_loss: 155.2574\n",
      "Epoch 1984/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 135.9137 - val_loss: 188.1123\n",
      "Epoch 1985/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 130.1548 - val_loss: 145.7121\n",
      "Epoch 1986/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 156.9399 - val_loss: 157.7389\n",
      "Epoch 1987/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 131.3829 - val_loss: 155.6301\n",
      "Epoch 1988/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 132.5320 - val_loss: 142.5958\n",
      "Epoch 1989/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 135.4222 - val_loss: 177.1562\n",
      "Epoch 1990/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 148.4174 - val_loss: 162.0205\n",
      "Epoch 1991/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 147.1271 - val_loss: 145.0344\n",
      "Epoch 1992/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 129.2943 - val_loss: 211.8215\n",
      "Epoch 1993/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 139.5288 - val_loss: 169.1488\n",
      "Epoch 1994/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 133.0519 - val_loss: 177.8615\n",
      "Epoch 1995/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 132.0911 - val_loss: 169.4070\n",
      "Epoch 1996/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 141.2947 - val_loss: 177.7648\n",
      "Epoch 1997/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 150.3934 - val_loss: 143.4196\n",
      "Epoch 1998/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 169.3740 - val_loss: 139.6558\n",
      "Epoch 1999/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 139.6883 - val_loss: 329.0275\n",
      "Epoch 2000/10000\n",
      "8000/8000 [==============================] - 1s 151us/step - loss: 168.0132 - val_loss: 193.0856\n",
      "Epoch 2001/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 137.9396 - val_loss: 168.5896\n",
      "Epoch 2002/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.0292 - val_loss: 140.9323\n",
      "Epoch 2003/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 134.0429 - val_loss: 146.0411\n",
      "Epoch 2004/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 146.0483 - val_loss: 153.0862\n",
      "Epoch 2005/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 134.5855 - val_loss: 158.3473\n",
      "Epoch 2006/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 141.6180 - val_loss: 139.5145\n",
      "Epoch 2007/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 131.7850 - val_loss: 153.2727\n",
      "Epoch 2008/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.5274 - val_loss: 162.4933\n",
      "Epoch 2009/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.7118 - val_loss: 231.7452\n",
      "Epoch 2010/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.8325 - val_loss: 179.9829\n",
      "Epoch 2011/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.2720 - val_loss: 244.1609\n",
      "Epoch 2012/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.1070 - val_loss: 146.2093\n",
      "Epoch 2013/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.2723 - val_loss: 139.6417\n",
      "Epoch 2014/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.3528 - val_loss: 152.0882\n",
      "Epoch 2015/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.4551 - val_loss: 174.3741\n",
      "Epoch 2016/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 127.4221 - val_loss: 145.8715\n",
      "Epoch 2017/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.5650 - val_loss: 145.0475\n",
      "Epoch 2018/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.9655 - val_loss: 142.5090\n",
      "Epoch 2019/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.0950 - val_loss: 143.9165\n",
      "Epoch 2020/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.2337 - val_loss: 134.8298\n",
      "Epoch 2021/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.5491 - val_loss: 142.5329\n",
      "Epoch 2022/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.9798 - val_loss: 145.6512\n",
      "Epoch 2023/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.8008 - val_loss: 158.7636\n",
      "Epoch 2024/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.5122 - val_loss: 147.2438\n",
      "Epoch 2025/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 203.7647 - val_loss: 137.7098\n",
      "Epoch 2026/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 167.2130 - val_loss: 150.8074\n",
      "Epoch 2027/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.9529 - val_loss: 154.2123\n",
      "Epoch 2028/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.9776 - val_loss: 176.6947\n",
      "Epoch 2029/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.2793 - val_loss: 152.4639\n",
      "Epoch 2030/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 182.8917 - val_loss: 137.9462\n",
      "Epoch 2031/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.6634 - val_loss: 147.9910\n",
      "Epoch 2032/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.1742 - val_loss: 158.5973\n",
      "Epoch 2033/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.5504 - val_loss: 301.1750\n",
      "Epoch 2034/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.2221 - val_loss: 157.7699\n",
      "Epoch 2035/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 134.137 - 1s 64us/step - loss: 131.9746 - val_loss: 145.1196\n",
      "Epoch 2036/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.7610 - val_loss: 142.3126\n",
      "Epoch 2037/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.1060 - val_loss: 173.0970\n",
      "Epoch 2038/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.1165 - val_loss: 141.9521\n",
      "Epoch 2039/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 162.4977 - val_loss: 219.2526\n",
      "Epoch 2040/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 218.9344 - val_loss: 156.5035\n",
      "Epoch 2041/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.3993 - val_loss: 159.9817\n",
      "Epoch 2042/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.4593 - val_loss: 141.7870\n",
      "Epoch 2043/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.2978 - val_loss: 145.3234\n",
      "Epoch 2044/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.0797 - val_loss: 164.1948\n",
      "Epoch 2045/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.0729 - val_loss: 147.3675\n",
      "Epoch 2046/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 170.6729 - val_loss: 149.6012\n",
      "Epoch 2047/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.3514 - val_loss: 154.1727\n",
      "Epoch 2048/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.2159 - val_loss: 160.0291\n",
      "Epoch 2049/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.6147 - val_loss: 152.1321\n",
      "Epoch 2050/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.4908 - val_loss: 142.5795\n",
      "Epoch 2051/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.7996 - val_loss: 142.3610\n",
      "Epoch 2052/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.3304 - val_loss: 149.0570\n",
      "Epoch 2053/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.5428 - val_loss: 141.0851\n",
      "Epoch 2054/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.3919 - val_loss: 152.2231\n",
      "Epoch 2055/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 162.2991 - val_loss: 184.8334\n",
      "Epoch 2056/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 130.0816 - val_loss: 135.2247\n",
      "Epoch 2057/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 135.7042 - val_loss: 139.8998\n",
      "Epoch 2058/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.5235 - val_loss: 138.3043\n",
      "Epoch 2059/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.3283 - val_loss: 148.0508\n",
      "Epoch 2060/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.9700 - val_loss: 154.5783\n",
      "Epoch 2061/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 129.2010 - val_loss: 158.7780\n",
      "Epoch 2062/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.7427 - val_loss: 141.9689\n",
      "Epoch 2063/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 191.7754 - val_loss: 139.1056\n",
      "Epoch 2064/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.0482 - val_loss: 139.2392\n",
      "Epoch 2065/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.3605 - val_loss: 157.3560\n",
      "Epoch 2066/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.6614 - val_loss: 169.6597\n",
      "Epoch 2067/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.3974 - val_loss: 155.0708\n",
      "Epoch 2068/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.6667 - val_loss: 154.3311\n",
      "Epoch 2069/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.5767 - val_loss: 181.1303\n",
      "Epoch 2070/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.5073 - val_loss: 140.7311\n",
      "Epoch 2071/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.7134 - val_loss: 144.8722\n",
      "Epoch 2072/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.5952 - val_loss: 144.7958\n",
      "Epoch 2073/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.0792 - val_loss: 139.3537\n",
      "Epoch 2074/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.2729 - val_loss: 189.5265\n",
      "Epoch 2075/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 135.7561 - val_loss: 164.6757\n",
      "Epoch 2076/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.0953 - val_loss: 138.4633\n",
      "Epoch 2077/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.0844 - val_loss: 171.6120\n",
      "Epoch 2078/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.8286 - val_loss: 173.7670\n",
      "Epoch 2079/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 188.4869 - val_loss: 197.5727\n",
      "Epoch 2080/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.5562 - val_loss: 138.5629\n",
      "Epoch 2081/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.1342 - val_loss: 236.6394\n",
      "Epoch 2082/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.4389 - val_loss: 255.8401\n",
      "Epoch 2083/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.2614 - val_loss: 177.8131\n",
      "Epoch 2084/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.9996 - val_loss: 148.0702\n",
      "Epoch 2085/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.1841 - val_loss: 142.6178\n",
      "Epoch 2086/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 131.8639 - val_loss: 160.5320\n",
      "Epoch 2087/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.3830 - val_loss: 170.9706\n",
      "Epoch 2088/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.5488 - val_loss: 156.1473\n",
      "Epoch 2089/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.0076 - val_loss: 191.1475\n",
      "Epoch 2090/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.5984 - val_loss: 196.8691\n",
      "Epoch 2091/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.2118 - val_loss: 141.8850\n",
      "Epoch 2092/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 204.0237 - val_loss: 161.1222\n",
      "Epoch 2093/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.9527 - val_loss: 141.6500\n",
      "Epoch 2094/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.4458 - val_loss: 205.2371\n",
      "Epoch 2095/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.3590 - val_loss: 139.5726\n",
      "Epoch 2096/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 131.5580 - val_loss: 170.6038\n",
      "Epoch 2097/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.6484 - val_loss: 142.6874\n",
      "Epoch 2098/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.9573 - val_loss: 140.5273\n",
      "Epoch 2099/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 123.8360 - val_loss: 137.2787\n",
      "Epoch 2100/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.8549 - val_loss: 145.2423\n",
      "Epoch 2101/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.0578 - val_loss: 135.0438\n",
      "Epoch 2102/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 130.6590 - val_loss: 148.3109\n",
      "Epoch 2103/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.3097 - val_loss: 146.7192\n",
      "Epoch 2104/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 127.8852 - val_loss: 138.9091\n",
      "Epoch 2105/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.6577 - val_loss: 147.8148\n",
      "Epoch 2106/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 186.2183 - val_loss: 159.0029\n",
      "Epoch 2107/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.5101 - val_loss: 165.6715\n",
      "Epoch 2108/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.0168 - val_loss: 139.4671\n",
      "Epoch 2109/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 200.8199 - val_loss: 142.7050\n",
      "Epoch 2110/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.5016 - val_loss: 160.9378\n",
      "Epoch 2111/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 186.2274 - val_loss: 198.1833\n",
      "Epoch 2112/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.1215 - val_loss: 150.9299\n",
      "Epoch 2113/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.6753 - val_loss: 146.3290\n",
      "Epoch 2114/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.0212 - val_loss: 158.8218\n",
      "Epoch 2115/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.9808 - val_loss: 140.9692\n",
      "Epoch 2116/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 127.0514 - val_loss: 152.8910\n",
      "Epoch 2117/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 131.6193 - val_loss: 142.9286\n",
      "Epoch 2118/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 135.1922 - val_loss: 164.3008\n",
      "Epoch 2119/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.2178 - val_loss: 147.3461\n",
      "Epoch 2120/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.2284 - val_loss: 232.0444\n",
      "Epoch 2121/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.6538 - val_loss: 192.2164\n",
      "Epoch 2122/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.3174 - val_loss: 188.7449\n",
      "Epoch 2123/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.1810 - val_loss: 140.9741\n",
      "Epoch 2124/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.8450 - val_loss: 137.2690\n",
      "Epoch 2125/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.7053 - val_loss: 160.7735\n",
      "Epoch 2126/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.8858 - val_loss: 236.2648\n",
      "Epoch 2127/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.8694 - val_loss: 144.3180\n",
      "Epoch 2128/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.4666 - val_loss: 274.4068\n",
      "Epoch 2129/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 129.9619 - val_loss: 142.6319\n",
      "Epoch 2130/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 151.7426 - val_loss: 228.2063\n",
      "Epoch 2131/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 148.2148 - val_loss: 151.1802\n",
      "Epoch 2132/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 136.9649 - val_loss: 147.9911\n",
      "Epoch 2133/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.0222 - val_loss: 169.6604\n",
      "Epoch 2134/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.6446 - val_loss: 136.3946\n",
      "Epoch 2135/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.4021 - val_loss: 141.2763\n",
      "Epoch 2136/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.5345 - val_loss: 177.3562\n",
      "Epoch 2137/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.8900 - val_loss: 141.7308\n",
      "Epoch 2138/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.9189 - val_loss: 140.4915\n",
      "Epoch 2139/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 174.6040 - val_loss: 273.8934\n",
      "Epoch 2140/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.9859 - val_loss: 140.2587\n",
      "Epoch 2141/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 202.6471 - val_loss: 159.1242\n",
      "Epoch 2142/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.9422 - val_loss: 150.5886\n",
      "Epoch 2143/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 126.8731 - val_loss: 150.5360\n",
      "Epoch 2144/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.1054 - val_loss: 162.0055\n",
      "Epoch 2145/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.2870 - val_loss: 146.4063\n",
      "Epoch 2146/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 187.8715 - val_loss: 164.3280\n",
      "Epoch 2147/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.1926 - val_loss: 144.1418\n",
      "Epoch 2148/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.7093 - val_loss: 212.3399\n",
      "Epoch 2149/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.4819 - val_loss: 142.3301\n",
      "Epoch 2150/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.6679 - val_loss: 138.4476\n",
      "Epoch 2151/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.8671 - val_loss: 163.6477\n",
      "Epoch 2152/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.4741 - val_loss: 190.2420\n",
      "Epoch 2153/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.2100 - val_loss: 139.8320\n",
      "Epoch 2154/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.7817 - val_loss: 164.3384\n",
      "Epoch 2155/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.6098 - val_loss: 253.7641\n",
      "Epoch 2156/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 239.8156 - val_loss: 179.4801\n",
      "Epoch 2157/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.1559 - val_loss: 142.3282\n",
      "Epoch 2158/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.6896 - val_loss: 146.4560\n",
      "Epoch 2159/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.8392 - val_loss: 160.0388\n",
      "Epoch 2160/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.3745 - val_loss: 152.4720\n",
      "Epoch 2161/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.1495 - val_loss: 144.2368\n",
      "Epoch 2162/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.4968 - val_loss: 136.3744\n",
      "Epoch 2163/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.3943 - val_loss: 138.9351\n",
      "Epoch 2164/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.1609 - val_loss: 151.5122\n",
      "Epoch 2165/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.5778 - val_loss: 162.5172\n",
      "Epoch 2166/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.1916 - val_loss: 156.0969\n",
      "Epoch 2167/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.5469 - val_loss: 148.6969\n",
      "Epoch 2168/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.9781 - val_loss: 287.7704\n",
      "Epoch 2169/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 137.5866 - val_loss: 165.2153\n",
      "Epoch 2170/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 146.0881 - val_loss: 158.6456\n",
      "Epoch 2171/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 129.6120 - val_loss: 152.2347\n",
      "Epoch 2172/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.1394 - val_loss: 138.1968\n",
      "Epoch 2173/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.0460 - val_loss: 152.1167\n",
      "Epoch 2174/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.5624 - val_loss: 143.3592\n",
      "Epoch 2175/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.5841 - val_loss: 150.5981\n",
      "Epoch 2176/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.9525 - val_loss: 241.7329\n",
      "Epoch 2177/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.5367 - val_loss: 150.9331\n",
      "Epoch 2178/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 154.5571 - val_loss: 149.6088\n",
      "Epoch 2179/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 149.9053 - val_loss: 155.4254\n",
      "Epoch 2180/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.2306 - val_loss: 143.7600\n",
      "Epoch 2181/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.9511 - val_loss: 157.1258\n",
      "Epoch 2182/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.7682 - val_loss: 167.7956\n",
      "Epoch 2183/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 135.1330 - val_loss: 203.6430\n",
      "Epoch 2184/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.9165 - val_loss: 137.6887\n",
      "Epoch 2185/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.5313 - val_loss: 150.0771\n",
      "Epoch 2186/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.0295 - val_loss: 155.6037\n",
      "Epoch 2187/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.1898 - val_loss: 657.7174\n",
      "Epoch 2188/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.9140 - val_loss: 145.0802\n",
      "Epoch 2189/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.6341 - val_loss: 173.1470\n",
      "Epoch 2190/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.0509 - val_loss: 145.4303\n",
      "Epoch 2191/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.2818 - val_loss: 142.7181\n",
      "Epoch 2192/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.5878 - val_loss: 139.7708\n",
      "Epoch 2193/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.4549 - val_loss: 185.0685\n",
      "Epoch 2194/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 187.4391 - val_loss: 145.6316\n",
      "Epoch 2195/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.4856 - val_loss: 154.3893\n",
      "Epoch 2196/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.8196 - val_loss: 138.4425\n",
      "Epoch 2197/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.6292 - val_loss: 145.1838\n",
      "Epoch 2198/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.0092 - val_loss: 140.9679\n",
      "Epoch 2199/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.5942 - val_loss: 172.2980\n",
      "Epoch 2200/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 190.0022 - val_loss: 140.2373\n",
      "Epoch 2201/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.3827 - val_loss: 151.0363\n",
      "Epoch 2202/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 163.0191 - val_loss: 154.6884\n",
      "Epoch 2203/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.0843 - val_loss: 144.2427\n",
      "Epoch 2204/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.3283 - val_loss: 146.5890\n",
      "Epoch 02204: early stopping\n",
      "Fold score (RMSE): 11.841163635253906\n",
      "Fold #2\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 5406.8015 - val_loss: 5592.5401\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 4854.4468 - val_loss: 5425.8417\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 4359.0438 - val_loss: 4697.1236\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 4337.0022 - val_loss: 4793.7614\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 4316.8449 - val_loss: 4551.1005\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 4116.9778 - val_loss: 5470.7582\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 4072.5767 - val_loss: 4451.1767\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 4069.2553 - val_loss: 4822.5165\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 4001.3319 - val_loss: 4452.8040\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 3884.2476 - val_loss: 5465.1905\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 3880.6811 - val_loss: 4009.4759\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 3697.1305 - val_loss: 4799.4786\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 3442.2281 - val_loss: 3511.6933\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 3383.6592 - val_loss: 4733.5328\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 3410.9441 - val_loss: 3550.0115\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 3154.1545 - val_loss: 2811.2450\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 2636.0417 - val_loss: 2266.8967\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 2345.2979 - val_loss: 2134.4383\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 2073.1966 - val_loss: 1631.0288\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 1654.5297 - val_loss: 3076.8569\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 1505.5125 - val_loss: 2506.1407\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 1365.3957 - val_loss: 1033.6353\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 938.0960 - val_loss: 963.6274\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 1016.6916 - val_loss: 1543.0866\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 794.1323 - val_loss: 1377.0282\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 761.5070 - val_loss: 537.7551\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 853.2054 - val_loss: 613.2582\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 662.9524 - val_loss: 460.2469\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 660.2948 - val_loss: 691.6198\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 602.3044 - val_loss: 496.6754\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 723.8521 - val_loss: 418.5628\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 599.0844 - val_loss: 676.3285\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 665.0938 - val_loss: 495.1445\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 570.9933 - val_loss: 523.9135\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 520.9222 - val_loss: 729.6938\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 529.2308 - val_loss: 1142.5431\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 497.9523 - val_loss: 597.3419\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 561.1791 - val_loss: 1126.4598\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 489.7847 - val_loss: 625.9933\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 540.8727 - val_loss: 681.2959\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 484.7475 - val_loss: 407.3382\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 661.1474 - val_loss: 383.1597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 431.8905 - val_loss: 332.5125\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 411.1207 - val_loss: 354.3033\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 510.0522 - val_loss: 333.7220\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 541.6038 - val_loss: 318.7486\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 376.1393 - val_loss: 330.4816\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 542.4082 - val_loss: 480.7974\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 522.5445 - val_loss: 620.4436\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 448.2815 - val_loss: 437.0618\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 353.6873 - val_loss: 365.9012\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 404.1518 - val_loss: 386.3057\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 468.4902 - val_loss: 480.3295\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 394.9047 - val_loss: 518.3068\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 397.2278 - val_loss: 404.0589\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 459.6854 - val_loss: 553.7946\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 428.6787 - val_loss: 333.9804\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 506.9513 - val_loss: 493.3873\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 435.3602 - val_loss: 641.5259\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 419.3536 - val_loss: 314.2990\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 333.4492 - val_loss: 272.0004\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 446.6679 - val_loss: 1266.5226\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 384.3830 - val_loss: 641.4325\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 389.8337 - val_loss: 446.1982\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 383.4573 - val_loss: 274.9856\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 488.3152 - val_loss: 434.5714\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 474.3848 - val_loss: 357.6034\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 365.7344 - val_loss: 316.2677\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 337.1148 - val_loss: 472.2548\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 362.4603 - val_loss: 347.1586\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 469.5483 - val_loss: 514.4386\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 339.0589 - val_loss: 528.0576\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 377.4207 - val_loss: 349.8794\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 351.3573 - val_loss: 248.0951\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 313.5776 - val_loss: 721.4733\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 390.5650 - val_loss: 278.3225\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 344.4448 - val_loss: 307.3026\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 349.5047 - val_loss: 277.1530\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 274.1834 - val_loss: 397.2012\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 377.7836 - val_loss: 691.5800\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 292.8273 - val_loss: 306.6907\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 302.6758 - val_loss: 317.1344\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 375.3136 - val_loss: 299.4872\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 416.6997 - val_loss: 297.6392\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 317.8141 - val_loss: 310.9652\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 365.1258 - val_loss: 409.7969\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 322.6593 - val_loss: 297.9445\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 326.7304 - val_loss: 358.3950\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 313.9651 - val_loss: 444.2471\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 345.6963 - val_loss: 518.4723\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 294.0252 - val_loss: 374.4488\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 284.3174 - val_loss: 237.2213\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 288.6793 - val_loss: 251.4395\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 341.1068 - val_loss: 241.4964\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 360.8896 - val_loss: 786.6542\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 280.0458 - val_loss: 225.0594\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 412.6507 - val_loss: 1300.8920\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 296.3057 - val_loss: 242.6884\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 355.4305 - val_loss: 276.7092\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 278.0746 - val_loss: 245.7585\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 353.1989 - val_loss: 285.1277\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 310.6071 - val_loss: 281.7174\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 284.2432 - val_loss: 233.5174\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 296.5800 - val_loss: 275.3296\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 262.9124 - val_loss: 317.0818\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 290.0867 - val_loss: 210.0857\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 264.7900 - val_loss: 215.9787\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 266.2476 - val_loss: 260.2701\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 294.3278 - val_loss: 332.0380\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 350.9524 - val_loss: 290.9091\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 267.5429 - val_loss: 212.6114\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 233.8575 - val_loss: 210.9946\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 257.7837 - val_loss: 220.8472\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 260.4163 - val_loss: 245.4563\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 329.3494 - val_loss: 811.0305\n",
      "Epoch 116/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 66us/step - loss: 306.8919 - val_loss: 265.5790\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 232.1479 - val_loss: 225.5472\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 229.5031 - val_loss: 271.1985\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 433.1919 - val_loss: 713.2037\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 327.7784 - val_loss: 223.3533\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 261.1842 - val_loss: 530.7807\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 309.4428 - val_loss: 400.4542\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 298.4071 - val_loss: 285.6188\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 270.2992 - val_loss: 210.3999\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 269.5011 - val_loss: 213.6987\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 257.2028 - val_loss: 267.5300\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 247.6660 - val_loss: 236.4353\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 304.3997 - val_loss: 248.9812\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 321.0021 - val_loss: 271.0217\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 299.2844 - val_loss: 374.3745\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 259.7663 - val_loss: 798.6568\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 252.3018 - val_loss: 213.3368\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 304.9714 - val_loss: 215.7501\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 264.5815 - val_loss: 293.7054\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 311.2867 - val_loss: 223.7951\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 284.9994 - val_loss: 231.7528\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 276.0366 - val_loss: 247.6188\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 338.5405 - val_loss: 218.6375\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 244.3755 - val_loss: 218.3425\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 222.2231 - val_loss: 220.8092\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 387.0482 - val_loss: 323.3489\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 256.8340 - val_loss: 252.8563\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 214.6521 - val_loss: 195.8896\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 280.6610 - val_loss: 349.5519\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 297.9361 - val_loss: 198.6511\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 221.9986 - val_loss: 181.5527\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 260.5822 - val_loss: 347.2686\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 253.9589 - val_loss: 229.9646\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 315.9776 - val_loss: 189.4964\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 227.4301 - val_loss: 332.4818\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 349.9289 - val_loss: 228.6748\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 287.3624 - val_loss: 528.5789\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 241.1606 - val_loss: 199.4923\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 261.5488 - val_loss: 219.3234\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 293.8964 - val_loss: 209.8283\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 267.9351 - val_loss: 282.6857\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 256.5801 - val_loss: 195.9370\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 265.5691 - val_loss: 222.9645\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 233.3917 - val_loss: 195.2031\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 240.0572 - val_loss: 571.8783\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 236.9786 - val_loss: 195.7323\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 293.2604 - val_loss: 492.0970\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 278.4066 - val_loss: 347.5133\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 289.7995 - val_loss: 183.4118\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 222.1575 - val_loss: 192.3371\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 204.6337 - val_loss: 206.7639\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 222.4981 - val_loss: 210.2767\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 266.1672 - val_loss: 299.1989\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 236.5660 - val_loss: 221.1932\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 223.5940 - val_loss: 192.7363\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 213.3434 - val_loss: 200.2128\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 213.8532 - val_loss: 195.9110\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 217.7449 - val_loss: 190.8501\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 315.2168 - val_loss: 355.8873\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 259.3910 - val_loss: 295.4222\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 269.6174 - val_loss: 245.8126\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 270.7204 - val_loss: 180.8169\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 228.3779 - val_loss: 194.8824\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 222.5042 - val_loss: 330.1153\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 240.9691 - val_loss: 190.2941\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 218.4028 - val_loss: 180.4846\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 202.3504 - val_loss: 178.6092\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 257.5342 - val_loss: 185.7080\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 253.5628 - val_loss: 174.2370\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 222.1298 - val_loss: 225.4246\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 252.9979 - val_loss: 331.1839\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 231.2163 - val_loss: 178.3709\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 191.2793 - val_loss: 213.9024\n",
      "Epoch 189/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 65us/step - loss: 253.1827 - val_loss: 211.0480\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 236.9135 - val_loss: 190.2235\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 238.9546 - val_loss: 191.1422\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 193.2932 - val_loss: 286.0960\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 275.9816 - val_loss: 174.9612\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 220.7813 - val_loss: 228.1754\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 224.9754 - val_loss: 174.3351\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 275.3518 - val_loss: 224.6040\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 261.3161 - val_loss: 291.2067\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 321.6060 - val_loss: 206.2828\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 198.9268 - val_loss: 196.2144\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 205.3730 - val_loss: 207.8061\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 198.0867 - val_loss: 169.7139\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 219.6372 - val_loss: 327.3047\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 335.6292 - val_loss: 250.0740\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 231.6986 - val_loss: 484.8630\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 217.6336 - val_loss: 244.5428\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 220.0117 - val_loss: 238.4694\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 212.2675 - val_loss: 237.0273\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 259.7395 - val_loss: 367.7919\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 300.4376 - val_loss: 185.8034\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 216.1172 - val_loss: 217.8077\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 256.4798 - val_loss: 201.9864\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 225.3148 - val_loss: 225.5924\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 227.1874 - val_loss: 190.4667\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 247.9800 - val_loss: 173.2822\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 284.7938 - val_loss: 186.3201\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 228.6407 - val_loss: 168.9491\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 240.1804 - val_loss: 241.8216\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 247.3970 - val_loss: 265.2017\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 210.0246 - val_loss: 220.8444\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 219.1434 - val_loss: 178.6789\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 240.4710 - val_loss: 410.4461\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 246.5353 - val_loss: 199.1483\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 223.6265 - val_loss: 222.9915\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 222.7999 - val_loss: 218.9009\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 241.5115 - val_loss: 218.9360\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 223.8610 - val_loss: 446.6028\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 234.1170 - val_loss: 173.3535\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 202.7512 - val_loss: 424.3844\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 213.4421 - val_loss: 246.2102\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 207.5990 - val_loss: 197.0873\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 197.9263 - val_loss: 175.8764\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 192.7202 - val_loss: 217.5091\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 348.4296 - val_loss: 671.8062\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 318.3236 - val_loss: 336.5110\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 202.7522 - val_loss: 315.4485\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 191.5122 - val_loss: 199.1389\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 234.2959 - val_loss: 196.0318\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 204.9783 - val_loss: 294.9675\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 217.0106 - val_loss: 232.5135\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 189.6781 - val_loss: 229.4086\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 206.6045 - val_loss: 163.4365\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 320.6547 - val_loss: 434.4352\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 293.1961 - val_loss: 212.8367\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 214.5570 - val_loss: 175.4441\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 188.7657 - val_loss: 245.3801\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 232.0283 - val_loss: 231.5282\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 214.6064 - val_loss: 163.3979\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 228.5300 - val_loss: 204.4522\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 230.6059 - val_loss: 182.8190\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 306.4425 - val_loss: 180.2065\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.7415 - val_loss: 248.3113\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 241.8821 - val_loss: 271.9510\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 199.0780 - val_loss: 216.0398\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 202.3379 - val_loss: 411.5549\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 280.8223 - val_loss: 175.5281\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 193.9859 - val_loss: 233.9658\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 193.3448 - val_loss: 172.0941\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 220.5882 - val_loss: 270.4687\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 202.4305 - val_loss: 488.0829\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 292.9665 - val_loss: 185.7251\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 273.0009 - val_loss: 186.0634\n",
      "Epoch 262/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 179.8989 - val_loss: 207.0261\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.3551 - val_loss: 174.0523\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 185.9360 - val_loss: 167.3944\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.3212 - val_loss: 199.9549\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 241.6483 - val_loss: 199.6023\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 197.1470 - val_loss: 182.1787\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 190.2301 - val_loss: 225.6863\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 219.2633 - val_loss: 192.5486\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 207.2739 - val_loss: 204.9616\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 223.1339 - val_loss: 257.0475\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 340.3110 - val_loss: 322.7103\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 243.6747 - val_loss: 240.8379\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 206.0790 - val_loss: 624.7243\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 202.9216 - val_loss: 177.4465\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 345.9335 - val_loss: 187.1208\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 189.5718 - val_loss: 184.4610\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 204.8384 - val_loss: 174.5872\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 253.8792 - val_loss: 172.4578\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 188.4059 - val_loss: 165.5925\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 175.2899 - val_loss: 160.6412\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 219.0139 - val_loss: 184.8581\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 178.2669 - val_loss: 171.6218\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 201.6919 - val_loss: 162.5447\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 193.3485 - val_loss: 185.7486\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 178.6548 - val_loss: 157.4269\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 202.5097 - val_loss: 236.1373\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 260.5904 - val_loss: 222.4632\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 250.7864 - val_loss: 166.6622\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 187.1075 - val_loss: 523.6375\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 272.0542 - val_loss: 177.6693\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 215.7510 - val_loss: 186.0958\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 212.2247 - val_loss: 241.8064\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 214.8309 - val_loss: 179.6742\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 248.9001 - val_loss: 175.8102\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 186.5435 - val_loss: 156.0860\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.1567 - val_loss: 160.8495\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 184.0565 - val_loss: 188.9594\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 231.7846 - val_loss: 217.4703\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 196.0935 - val_loss: 251.4879\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.0718 - val_loss: 305.9671\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 234.2905 - val_loss: 230.4038\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 187.3062 - val_loss: 156.7119\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.9531 - val_loss: 161.9196\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 244.9231 - val_loss: 165.3830\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 241.2933 - val_loss: 248.8761\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 253.5960 - val_loss: 630.8060\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 285.8810 - val_loss: 157.5140\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.1068 - val_loss: 201.5309\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.1626 - val_loss: 286.1123\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.0447 - val_loss: 246.5298\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 193.5358 - val_loss: 187.5370\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 224.0156 - val_loss: 158.2220\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 184.1498 - val_loss: 179.0443\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 188.6238 - val_loss: 161.1733\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.9900 - val_loss: 208.9448\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 196.4700 - val_loss: 205.3433\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 187.8292 - val_loss: 470.7715\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 272.0860 - val_loss: 167.2635\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 178.8899 - val_loss: 164.7671\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 199.2601 - val_loss: 171.3068\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.5813 - val_loss: 172.6336\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 272.8834 - val_loss: 212.7873\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 168.1867 - val_loss: 169.9612\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.2466 - val_loss: 182.3667\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 290.6180 - val_loss: 971.0077\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 317.0230 - val_loss: 172.3932\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.3059 - val_loss: 182.5659\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 171.6709 - val_loss: 163.4899\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 192.7080 - val_loss: 168.1955\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 176.1995 - val_loss: 174.6406\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 202.3660 - val_loss: 230.2806\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 240.2719 - val_loss: 184.4955\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 188.0914 - val_loss: 185.1119\n",
      "Epoch 335/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 180.1408 - val_loss: 560.0340\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 220.4551 - val_loss: 167.5697\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.7581 - val_loss: 198.5262\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 212.9901 - val_loss: 342.4395\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 216.0012 - val_loss: 172.1989\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.1124 - val_loss: 178.7735\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 218.2282 - val_loss: 205.3250\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 220.4225 - val_loss: 154.4115\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 190.9052 - val_loss: 161.8643\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.0795 - val_loss: 292.7874\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 246.5349 - val_loss: 165.9746\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.6199 - val_loss: 202.2206\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 227.3677 - val_loss: 273.7146\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 192.7663 - val_loss: 243.8622\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.5522 - val_loss: 161.0647\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 174.1058 - val_loss: 180.6988\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 238.4979 - val_loss: 337.8486\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 355.4121 - val_loss: 252.1335\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 216.2886 - val_loss: 644.8827\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 266.0441 - val_loss: 217.1785\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.5057 - val_loss: 637.3795\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.0484 - val_loss: 188.3574\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 284.6908 - val_loss: 201.5660\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 201.3916 - val_loss: 163.5341\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.6054 - val_loss: 265.4206\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 182.1370 - val_loss: 165.0409\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.4160 - val_loss: 175.3552\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 201.4176 - val_loss: 307.6256\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 239.7905 - val_loss: 170.4266\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 171.9075 - val_loss: 178.1872\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 185.9237 - val_loss: 217.9044\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 233.6529 - val_loss: 196.2155\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 354.5320 - val_loss: 833.9084\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 298.0663 - val_loss: 260.2635\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.2799 - val_loss: 163.7731\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 182.6712 - val_loss: 157.3149\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 189.1813 - val_loss: 159.4740\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 180.8744 - val_loss: 327.4921\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 219.8317 - val_loss: 215.7406\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 260.2054 - val_loss: 275.7491\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 208.8128 - val_loss: 159.6904\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 215.6836 - val_loss: 201.9854\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 189.5910 - val_loss: 166.0370\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.9318 - val_loss: 270.2604\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 205.7314 - val_loss: 182.4992\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 181.8207 - val_loss: 256.5354\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.8206 - val_loss: 256.5922\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 253.4743 - val_loss: 167.5566\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.5085 - val_loss: 178.3239\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 184.2816 - val_loss: 176.1152\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 282.1216 - val_loss: 175.7944\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 187.7074 - val_loss: 198.6481\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 224.5214 - val_loss: 175.5889\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.8732 - val_loss: 157.7469\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 182.9104 - val_loss: 167.4724\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 225.0343 - val_loss: 321.0986\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 227.2900 - val_loss: 454.9726\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 185.5782 - val_loss: 392.5733\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 189.8185 - val_loss: 210.4846\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 174.6200 - val_loss: 185.1801\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.7343 - val_loss: 234.6120\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 176.9538 - val_loss: 204.8968\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 198.6971 - val_loss: 268.3952\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 330.5379 - val_loss: 210.2890\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.4239 - val_loss: 246.1990\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 180.4225 - val_loss: 181.0139\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 195.1838 - val_loss: 157.7517\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 179.3107 - val_loss: 187.2538\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 192.7874 - val_loss: 199.7652\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 340.6020 - val_loss: 190.7438\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 188.9437 - val_loss: 161.6449\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 189.0751 - val_loss: 278.6336\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.4646 - val_loss: 160.2402\n",
      "Epoch 408/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 195.6907 - val_loss: 182.0221\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 187.3004 - val_loss: 462.0090\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 196.8857 - val_loss: 267.1675\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 326.4774 - val_loss: 169.0738\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 199.2101 - val_loss: 178.8564\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 205.5991 - val_loss: 152.2163\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 163.6423 - val_loss: 150.4156\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.8251 - val_loss: 225.7258\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 204.6465 - val_loss: 215.6003\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 185.9815 - val_loss: 296.8591\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 187.1173 - val_loss: 253.5352\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 181.0339 - val_loss: 181.7361\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 186.3662 - val_loss: 153.4264\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 228.1453 - val_loss: 312.3111\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 212.1513 - val_loss: 163.5885\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 169.8278 - val_loss: 217.0462\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 187.2018 - val_loss: 158.0456\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.0657 - val_loss: 239.2144\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 398.1169 - val_loss: 163.9608\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 196.4884 - val_loss: 155.5811\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 210.4739 - val_loss: 187.6067\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 195.3007 - val_loss: 388.4867\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 276.6024 - val_loss: 206.3824\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 193.7745 - val_loss: 159.9540\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 236.3351 - val_loss: 214.3263\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 192.6558 - val_loss: 161.9094\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 183.5832 - val_loss: 183.2363\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 217.1454 - val_loss: 155.4017\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.1874 - val_loss: 176.5259\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 210.8579 - val_loss: 259.4254\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 236.5111 - val_loss: 160.7331\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 185.2064 - val_loss: 175.9472\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 234.9525 - val_loss: 199.6525\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.1443 - val_loss: 157.8531\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 178.6128 - val_loss: 207.9621\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.1333 - val_loss: 259.8545\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 189.2287 - val_loss: 165.3141\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 367.6860 - val_loss: 181.5987\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 213.6952 - val_loss: 164.5711\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 301.1510 - val_loss: 327.7758\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 194.4975 - val_loss: 165.5533\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.1739 - val_loss: 209.1552\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 292.3643 - val_loss: 222.0011\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.4540 - val_loss: 166.1926\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 201.5232 - val_loss: 204.7082\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 170.4135 - val_loss: 257.4632\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 154.1574 - val_loss: 161.7503\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.0841 - val_loss: 170.7761\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.4170 - val_loss: 242.0123\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 237.0763 - val_loss: 154.8779\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.2682 - val_loss: 285.6977\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 371.8802 - val_loss: 208.7833\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 184.2184 - val_loss: 152.8497\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 157.5267 - val_loss: 161.3194\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 177.0226 - val_loss: 180.5996\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.6268 - val_loss: 191.5797\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 191.5926 - val_loss: 227.8297\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 226.4858 - val_loss: 198.3003\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.2126 - val_loss: 180.3609\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 193.4748 - val_loss: 166.2234\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 252.9957 - val_loss: 176.5316\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 206.3016 - val_loss: 166.3485\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.0040 - val_loss: 199.6353\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.3747 - val_loss: 207.6518\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.4689 - val_loss: 167.1973\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 441.5855 - val_loss: 305.7890\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 257.2811 - val_loss: 225.6709\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 252.2507 - val_loss: 179.3697\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 211.5870 - val_loss: 202.7563\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 185.8711 - val_loss: 189.0004\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 185.0667 - val_loss: 264.3859\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 236.3360 - val_loss: 235.4037\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 209.7248 - val_loss: 173.6518\n",
      "Epoch 481/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 180.0218 - val_loss: 167.6059\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 199.1268 - val_loss: 232.3850\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 198.5146 - val_loss: 181.8669\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 195.0622 - val_loss: 153.4417\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.3794 - val_loss: 197.4005\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 198.4535 - val_loss: 159.5812\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 177.0727 - val_loss: 215.0823\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 308.2326 - val_loss: 276.9301\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 216.7269 - val_loss: 315.8116\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 271.9122 - val_loss: 156.3798\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.3413 - val_loss: 432.6150\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 188.9106 - val_loss: 163.6232\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 187.6993 - val_loss: 178.9751\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 255.4093 - val_loss: 154.6912\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.2118 - val_loss: 163.4245\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 377.7376 - val_loss: 471.1599\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 171.9973 - val_loss: 159.2349\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.4804 - val_loss: 209.3778\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.0959 - val_loss: 168.5567\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.9461 - val_loss: 170.4555\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.8588 - val_loss: 308.5026\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.8886 - val_loss: 159.2803\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 168.5599 - val_loss: 413.4952\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 234.4932 - val_loss: 337.4560\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 351.8832 - val_loss: 229.1233\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 201.9735 - val_loss: 223.3206\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 224.7225 - val_loss: 229.3963\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 196.1211 - val_loss: 248.4783\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 219.2203 - val_loss: 312.3448\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 254.2412 - val_loss: 171.8163\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 207.4332 - val_loss: 164.6495\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 181.5304 - val_loss: 217.5533\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 192.4591 - val_loss: 189.2015\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 182.3467 - val_loss: 186.5753\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 189.9998 - val_loss: 152.7017\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 202.7967 - val_loss: 356.9380\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 215.1135 - val_loss: 190.8128\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 300.9372 - val_loss: 162.5749\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 206.8165 - val_loss: 167.4276\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 183.7184 - val_loss: 184.8401\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 201.4622 - val_loss: 734.6916\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 201.9868 - val_loss: 225.7667\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 184.4982 - val_loss: 172.1657\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 229.7598 - val_loss: 149.1368\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 286.9270 - val_loss: 169.8095\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 182.4920 - val_loss: 151.6426\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 184.5539 - val_loss: 185.7150\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 250.2204 - val_loss: 240.5546\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 188.3488 - val_loss: 156.2746\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 194.2789 - val_loss: 193.1443\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 204.7349 - val_loss: 168.8955\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 265.9738 - val_loss: 177.5656\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 211.3444 - val_loss: 159.8240\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 186.8518 - val_loss: 200.1815\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 180.7706 - val_loss: 190.6839\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 228.8783 - val_loss: 171.9873\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 180.4622 - val_loss: 163.8174\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 208.3699 - val_loss: 179.4530\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.9602 - val_loss: 175.6374\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 185.3278 - val_loss: 225.6235\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 211.5883 - val_loss: 296.8111\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 251.8186 - val_loss: 235.6322\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 174.5151 - val_loss: 194.0066\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 169.9540 - val_loss: 164.0810\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 187.9830 - val_loss: 152.9769\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 230.8631 - val_loss: 220.5238\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 199.2883 - val_loss: 183.8702\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.7934 - val_loss: 182.8905\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 329.0642 - val_loss: 915.2008\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 220.9924 - val_loss: 295.2928\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 181.3044 - val_loss: 160.0169\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.8694 - val_loss: 207.3989\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.4246 - val_loss: 178.2797\n",
      "Epoch 554/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.8359 - val_loss: 185.9888\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 199.0633 - val_loss: 282.3595\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.7445 - val_loss: 338.4947\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 291.8019 - val_loss: 154.8408\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.2969 - val_loss: 179.7669\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.7681 - val_loss: 165.9766\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.4227 - val_loss: 151.0892\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.0331 - val_loss: 234.3984\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.8880 - val_loss: 195.0742\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 216.9834 - val_loss: 152.4465\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 303.9895 - val_loss: 326.9209\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 198.3302 - val_loss: 172.9844\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 180.8014 - val_loss: 171.6419\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.6599 - val_loss: 185.8858\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 177.8437 - val_loss: 215.2087\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 278.8212 - val_loss: 186.9433\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 193.5093 - val_loss: 188.2918\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.2913 - val_loss: 162.9180\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.1994 - val_loss: 150.7024\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 209.6700 - val_loss: 217.8198\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 194.1150 - val_loss: 191.3466\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 177.3223 - val_loss: 182.3463\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 222.7072 - val_loss: 150.0999\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 229.4706 - val_loss: 150.5498\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 188.3342 - val_loss: 154.2682\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 156.4687 - val_loss: 154.7983\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.2318 - val_loss: 153.8186\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.8609 - val_loss: 186.4990\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 203.2618 - val_loss: 163.5802\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 162.2007 - val_loss: 166.9277\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 213.6416 - val_loss: 395.4723\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.8405 - val_loss: 171.4353\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 198.4866 - val_loss: 215.3808\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 194.6859 - val_loss: 154.5985\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 288.1967 - val_loss: 453.6150\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 334.1033 - val_loss: 159.7465\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 184.9444 - val_loss: 194.9794\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.1416 - val_loss: 172.4615\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.7651 - val_loss: 155.1298\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.8546 - val_loss: 153.9940\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.0969 - val_loss: 221.4271\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.3066 - val_loss: 158.0573\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 327.5217 - val_loss: 1419.9977\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 254.2350 - val_loss: 168.3717\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 215.2805 - val_loss: 154.6488\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 246.2346 - val_loss: 239.8025\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 194.4777 - val_loss: 156.2224\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.6488 - val_loss: 256.7352\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 246.3927 - val_loss: 211.3813\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 209.6305 - val_loss: 187.3997\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 211.8941 - val_loss: 172.3174\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 239.8299 - val_loss: 219.9235\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 200.7038 - val_loss: 178.7293\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 185.3617 - val_loss: 173.1485\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.3141 - val_loss: 250.9481\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 188.5309 - val_loss: 155.8792\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 189.7626 - val_loss: 169.5810\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 208.6742 - val_loss: 168.7417\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 198.2720 - val_loss: 260.7577\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 248.0681 - val_loss: 189.9665\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 204.2035 - val_loss: 338.8809\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 216.6301 - val_loss: 162.8973\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 197.2506 - val_loss: 218.5711\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 264.7470 - val_loss: 170.6410\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 212.1521 - val_loss: 152.2882\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 191.1236 - val_loss: 173.7708\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 180.5628 - val_loss: 189.8528\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 316.4888 - val_loss: 277.1679\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 295.4056 - val_loss: 168.1699\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 198.9718 - val_loss: 173.0376\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 186.6471 - val_loss: 173.0175\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.3554 - val_loss: 153.6780\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 221.0376 - val_loss: 265.3916\n",
      "Epoch 627/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 63us/step - loss: 203.0686 - val_loss: 493.4188\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 178.0027 - val_loss: 156.8592\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 224.8502 - val_loss: 185.6029\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.5084 - val_loss: 167.8140\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 229.0761 - val_loss: 161.5162\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 183.0972 - val_loss: 237.7291\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 221.1200 - val_loss: 231.9673\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 266.3541 - val_loss: 208.3552\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 215.5043 - val_loss: 183.0349\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 198.9602 - val_loss: 161.8862\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 157.9849 - val_loss: 180.6532\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 187.0591 - val_loss: 186.4989\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 181.3858 - val_loss: 173.2766\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.9827 - val_loss: 167.2260\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.5701 - val_loss: 174.5273\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.1225 - val_loss: 149.2907\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 168.6015 - val_loss: 152.6342\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.3836 - val_loss: 154.4484\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 189.1861 - val_loss: 167.8842\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.7745 - val_loss: 163.1499\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 186.8626 - val_loss: 167.8356\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.9537 - val_loss: 204.1531\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 274.3041 - val_loss: 157.6812\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 187.0575 - val_loss: 162.5924\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.5027 - val_loss: 200.5437\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 188.5124 - val_loss: 147.0569\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 205.3966 - val_loss: 190.1981\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.6146 - val_loss: 173.6747\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.9920 - val_loss: 177.6257\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 171.9799 - val_loss: 304.1994\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.6188 - val_loss: 261.8790\n",
      "Epoch 658/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 183.9219 - val_loss: 171.1290\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 182.9538 - val_loss: 166.5433\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 303.0471 - val_loss: 161.9215\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.2050 - val_loss: 153.8095\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.4581 - val_loss: 150.4066\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.2063 - val_loss: 156.2175\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 180.7118 - val_loss: 261.6853\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.5756 - val_loss: 188.4667\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 523.0540 - val_loss: 220.9779\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 253.9564 - val_loss: 190.0137\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.0738 - val_loss: 216.5785\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 234.4296 - val_loss: 267.5029\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 205.1909 - val_loss: 246.3788\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 202.9404 - val_loss: 173.4664\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 236.3270 - val_loss: 193.3359\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.2047 - val_loss: 163.2419\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 182.9689 - val_loss: 190.4631\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 212.1451 - val_loss: 181.3963\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.6342 - val_loss: 161.7816\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 310.5249 - val_loss: 205.0806\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 169.5588 - val_loss: 202.9419\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 184.0439 - val_loss: 190.4975\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 191.2522 - val_loss: 169.9376\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.0144 - val_loss: 162.5151\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.2814 - val_loss: 202.8887\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 205.5062 - val_loss: 195.6052\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 210.8736 - val_loss: 210.7647\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.0398 - val_loss: 178.0300\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 188.2436 - val_loss: 162.8523\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 194.2499 - val_loss: 170.5905\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 254.0118 - val_loss: 183.2244\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.0164 - val_loss: 174.8821\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 163.1427 - val_loss: 159.7623\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 174.3283 - val_loss: 286.7573\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 197.0307 - val_loss: 170.3698\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.0063 - val_loss: 147.9843\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 189.8808 - val_loss: 178.7310\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 203.3360 - val_loss: 165.9296\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 185.0853 - val_loss: 180.3801\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 295.7445 - val_loss: 245.8953\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 206.3312 - val_loss: 151.6921\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.4038 - val_loss: 155.2693\n",
      "Epoch 700/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 187.3893 - val_loss: 169.4701\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.6057 - val_loss: 251.3241\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.1806 - val_loss: 193.1933\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.7909 - val_loss: 203.8228\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 170.3634 - val_loss: 153.9786\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.8859 - val_loss: 152.7636\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.0817 - val_loss: 157.2199\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 199.6224 - val_loss: 187.7809\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 189.7993 - val_loss: 166.1481\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 212.7035 - val_loss: 149.2441\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.6187 - val_loss: 231.0046\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 312.7558 - val_loss: 566.1127\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.9886 - val_loss: 162.0003\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.5657 - val_loss: 193.0770\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 278.9075 - val_loss: 154.6246\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.4693 - val_loss: 181.7921\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 190.7006 - val_loss: 166.3509\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.7587 - val_loss: 158.4270\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 225.4306 - val_loss: 185.6149\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.5348 - val_loss: 311.5936\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.1278 - val_loss: 227.9250\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.8631 - val_loss: 198.0310\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.1452 - val_loss: 330.9176\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 227.5225 - val_loss: 304.9523\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.1413 - val_loss: 155.5215\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 189.6664 - val_loss: 273.9861\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.2282 - val_loss: 159.3420\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.6147 - val_loss: 193.8077\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 190.8714 - val_loss: 187.0376\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 189.7190 - val_loss: 220.1723\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.7423 - val_loss: 183.3563\n",
      "Epoch 731/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.0936 - val_loss: 153.3570\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 175.6292 - val_loss: 184.0171\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.2691 - val_loss: 158.3269\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 168.3372 - val_loss: 157.2756\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 176.8893 - val_loss: 310.8927\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 207.3449 - val_loss: 184.3697\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.6631 - val_loss: 157.7210\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.2456 - val_loss: 165.6468\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 212.4784 - val_loss: 176.6194\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 184.4920 - val_loss: 292.8272\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 172.2096 - val_loss: 188.5131\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.9886 - val_loss: 161.1177\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.2977 - val_loss: 186.7208\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 185.4178 - val_loss: 164.6915\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.5948 - val_loss: 168.0159\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 349.6267 - val_loss: 590.0363\n",
      "Epoch 747/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 252.9118 - val_loss: 204.5738\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.0775 - val_loss: 187.8359\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 166.0476 - val_loss: 162.8555\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 185.2212 - val_loss: 168.5778\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.7312 - val_loss: 163.6849\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.6009 - val_loss: 182.9833\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.1490 - val_loss: 204.6771\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.3163 - val_loss: 208.2993\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.5252 - val_loss: 226.4331\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 167.8944 - val_loss: 175.2191\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 169.8890 - val_loss: 162.1042\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 165.8157 - val_loss: 169.3485\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 376.2709 - val_loss: 667.1553\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 282.4059 - val_loss: 213.6592\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 249.2241 - val_loss: 191.1999\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 227.1099 - val_loss: 320.3722\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 211.9007 - val_loss: 171.6233\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 201.4961 - val_loss: 197.4220\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 208.6834 - val_loss: 201.3692\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 199.8019 - val_loss: 260.3153\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 192.2171 - val_loss: 415.8867\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 175.1568 - val_loss: 177.4775\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 213.6404 - val_loss: 302.5627\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 197.3923 - val_loss: 353.0940\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 209.6938 - val_loss: 193.8164\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 193.7108 - val_loss: 168.2527\n",
      "Epoch 773/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.2731 - val_loss: 176.0720\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 180.6190 - val_loss: 196.4708\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 188.5698 - val_loss: 202.5644\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 184.2456 - val_loss: 221.3415\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 212.0455 - val_loss: 210.4720\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 229.8720 - val_loss: 181.3834\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 185.1197 - val_loss: 177.7332\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 199.5406 - val_loss: 155.2646\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 164.8368 - val_loss: 178.0884\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 193.1171 - val_loss: 168.7769\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 184.3493 - val_loss: 191.6946\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.1764 - val_loss: 172.7702\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.4339 - val_loss: 151.4352\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.2014 - val_loss: 163.0518\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 209.9288 - val_loss: 253.6028\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.7809 - val_loss: 268.2891\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 198.2185 - val_loss: 148.1814\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 204.4838 - val_loss: 183.0175\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 165.7377 - val_loss: 181.6572\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 217.5890 - val_loss: 185.7658\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 242.2276 - val_loss: 185.9791\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 185.7560 - val_loss: 441.9596\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.7973 - val_loss: 153.8623\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 232.3452 - val_loss: 160.0887\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.7978 - val_loss: 189.5321\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 181.2900 - val_loss: 240.7105\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.2015 - val_loss: 170.2681\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.2391 - val_loss: 236.7561\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 189.2081 - val_loss: 175.0371\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.2775 - val_loss: 151.2650\n",
      "Epoch 803/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.5982 - val_loss: 168.7523\n",
      "Epoch 804/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.4892 - val_loss: 216.4424\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 180.5065 - val_loss: 148.6707\n",
      "Epoch 806/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 197.5910 - val_loss: 156.3343\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.3399 - val_loss: 165.9601\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.8955 - val_loss: 149.8468\n",
      "Epoch 809/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 175.4690 - val_loss: 171.7155\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 376.9329 - val_loss: 477.5390\n",
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 224.5918 - val_loss: 162.0133\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.1082 - val_loss: 153.9268\n",
      "Epoch 813/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.6224 - val_loss: 150.7993\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 214.0509 - val_loss: 159.0798\n",
      "Epoch 815/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.9692 - val_loss: 248.5013\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 201.9219 - val_loss: 189.8005\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 210.1570 - val_loss: 174.1218\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 155.5227 - val_loss: 256.3663\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 215.1312 - val_loss: 345.6033\n",
      "Epoch 820/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 251.5896 - val_loss: 180.4194\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 171.7911 - val_loss: 150.6465\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 174.8504 - val_loss: 199.3266\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.3823 - val_loss: 158.8739\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 376.1950 - val_loss: 674.2027\n",
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 274.3035 - val_loss: 187.2688\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 220.4215 - val_loss: 171.7214\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 195.1491 - val_loss: 175.4718\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 216.4452 - val_loss: 170.8107\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 205.4372 - val_loss: 197.1755\n",
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 188.7099 - val_loss: 170.9631\n",
      "Epoch 831/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 184.3622 - val_loss: 211.2638\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.2568 - val_loss: 173.1722\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.2898 - val_loss: 159.9824\n",
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 185.9495 - val_loss: 228.9279\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 198.1217 - val_loss: 279.5480\n",
      "Epoch 836/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 212.6305 - val_loss: 186.1283\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 202.8212 - val_loss: 161.7889\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 200.3642 - val_loss: 192.7023\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 229.8717 - val_loss: 169.7018\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 196.0239 - val_loss: 161.7185\n",
      "Epoch 841/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 193.9350 - val_loss: 152.1757\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.0032 - val_loss: 160.0311\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 171.0525 - val_loss: 181.4206\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.6606 - val_loss: 169.5676\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.3691 - val_loss: 196.4098\n",
      "Epoch 846/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.9649 - val_loss: 183.5860\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 202.2410 - val_loss: 162.1982\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 198.8601 - val_loss: 261.0711\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 254.1940 - val_loss: 191.2191\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.6236 - val_loss: 195.7468\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.9919 - val_loss: 193.0203\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 172.7002 - val_loss: 195.6297\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 155.3229 - val_loss: 158.9797\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.9685 - val_loss: 169.7929\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 163.4904 - val_loss: 234.9368\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.6905 - val_loss: 240.1222\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.5157 - val_loss: 152.6520\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.7123 - val_loss: 191.5826\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 204.7820 - val_loss: 155.2651\n",
      "Epoch 860/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.1272 - val_loss: 242.6637\n",
      "Epoch 861/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.2169 - val_loss: 180.6936\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 169.5651 - val_loss: 153.0424\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.8745 - val_loss: 155.0832\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 170.4116 - val_loss: 147.1878\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 175.4862 - val_loss: 304.3343\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.9481 - val_loss: 174.2745\n",
      "Epoch 867/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.5993 - val_loss: 203.0215\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.6790 - val_loss: 162.1886\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.8116 - val_loss: 151.8829\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 187.3798 - val_loss: 151.3782\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.0204 - val_loss: 149.4035\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.2728 - val_loss: 166.2680\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 180.1056 - val_loss: 276.8507\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 199.9689 - val_loss: 249.4168\n",
      "Epoch 875/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 189.6422 - val_loss: 163.6640\n",
      "Epoch 876/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.1196 - val_loss: 163.6240\n",
      "Epoch 877/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.0144 - val_loss: 173.2589\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 168.2306 - val_loss: 201.2488\n",
      "Epoch 879/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 233.9014 - val_loss: 150.6033\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 172.3431 - val_loss: 218.8644\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.6163 - val_loss: 245.0923\n",
      "Epoch 882/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.5192 - val_loss: 151.7972\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 177.9804 - val_loss: 195.3507\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 181.2146 - val_loss: 172.7153\n",
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.6068 - val_loss: 157.5035\n",
      "Epoch 886/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 212.2315 - val_loss: 205.9242\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 165.8155 - val_loss: 160.3009\n",
      "Epoch 888/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 207.7268 - val_loss: 234.5944\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 205.7815 - val_loss: 179.6984\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 202.4095 - val_loss: 180.8558\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 306.7133 - val_loss: 286.8305\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.4222 - val_loss: 157.3242\n",
      "Epoch 893/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.7287 - val_loss: 165.0376\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 184.0959 - val_loss: 178.5200\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.9286 - val_loss: 156.2426\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.8793 - val_loss: 155.9516\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 240.0896 - val_loss: 269.2872\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.6531 - val_loss: 147.7165\n",
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.8933 - val_loss: 146.8345\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.8868 - val_loss: 258.5583\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 195.7828 - val_loss: 151.9325\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.2480 - val_loss: 196.2188\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.0095 - val_loss: 152.8810\n",
      "Epoch 904/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.3823 - val_loss: 230.7116\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.3887 - val_loss: 164.6795\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 181.7062 - val_loss: 262.6906\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 198.8265 - val_loss: 154.7975\n",
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.8810 - val_loss: 167.7119\n",
      "Epoch 909/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 203.2124 - val_loss: 210.8343\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 181.4585 - val_loss: 172.3117\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.9281 - val_loss: 164.5887\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.6639 - val_loss: 178.4089\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.3822 - val_loss: 158.9193\n",
      "Epoch 914/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 191.0999 - val_loss: 151.0468\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.2628 - val_loss: 243.3052\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.8498 - val_loss: 170.0330\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.6489 - val_loss: 149.3802\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.4036 - val_loss: 163.6958\n",
      "Epoch 919/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.4139 - val_loss: 166.9283\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 206.0005 - val_loss: 674.2162\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 180.8122 - val_loss: 166.2609\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.7706 - val_loss: 144.0807\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.8158 - val_loss: 176.5052\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.2106 - val_loss: 146.1818\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.6208 - val_loss: 194.5699\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 205.7424 - val_loss: 179.4662\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 259.9156 - val_loss: 153.6405\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.9868 - val_loss: 167.7276\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 160.6274 - val_loss: 165.8757\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.2819 - val_loss: 160.6194\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.3675 - val_loss: 154.3121\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.5748 - val_loss: 163.6020\n",
      "Epoch 933/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 265.5842 - val_loss: 197.0445\n",
      "Epoch 934/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.2315 - val_loss: 147.9628\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.2472 - val_loss: 213.6058\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 187.0888 - val_loss: 168.2236\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 242.4920 - val_loss: 146.9428\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.8713 - val_loss: 157.4593\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 166.5935 - val_loss: 156.4500\n",
      "Epoch 940/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 175.3431 - val_loss: 174.8746\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 172.5676 - val_loss: 181.4229\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.0122 - val_loss: 166.4760\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.4218 - val_loss: 150.9243\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.3815 - val_loss: 153.6625\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 270.3753 - val_loss: 225.3261\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 206.9937 - val_loss: 178.7815\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 181.0678 - val_loss: 145.9433\n",
      "Epoch 948/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 185.5530 - val_loss: 162.1742\n",
      "Epoch 949/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.3535 - val_loss: 174.9426\n",
      "Epoch 950/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.9120 - val_loss: 194.5079\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.7953 - val_loss: 146.9174\n",
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 193.5477 - val_loss: 165.7578\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.6384 - val_loss: 190.9413\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.5207 - val_loss: 257.9353\n",
      "Epoch 955/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.8521 - val_loss: 154.2058\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 174.5725 - val_loss: 162.6327\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.3332 - val_loss: 149.3929\n",
      "Epoch 958/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 240.1063 - val_loss: 150.1406\n",
      "Epoch 959/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.7286 - val_loss: 194.6732\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.6042 - val_loss: 149.4834\n",
      "Epoch 961/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.4993 - val_loss: 170.3930\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 200.0762 - val_loss: 177.8080\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 173.1776 - val_loss: 275.7500\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 168.2400 - val_loss: 144.8265\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.5024 - val_loss: 247.5581\n",
      "Epoch 966/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 184.6765 - val_loss: 154.5700\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.1481 - val_loss: 154.0222\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.2615 - val_loss: 196.1978\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.4805 - val_loss: 175.0627\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 291.3479 - val_loss: 245.8882\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 328.4291 - val_loss: 195.9806\n",
      "Epoch 972/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 216.7506 - val_loss: 197.2616\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 194.3708 - val_loss: 184.9342\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 218.9807 - val_loss: 248.7297\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 235.6144 - val_loss: 299.4181\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 177.5411 - val_loss: 159.2841\n",
      "Epoch 977/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.9713 - val_loss: 180.9466\n",
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.0942 - val_loss: 240.3774\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 181.4132 - val_loss: 160.8601\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 193.0671 - val_loss: 162.0009\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.9584 - val_loss: 186.7222\n",
      "Epoch 982/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 197.0529 - val_loss: 172.9390\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 208.1052 - val_loss: 192.6314\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.7166 - val_loss: 161.0046\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.0724 - val_loss: 202.9832\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.4963 - val_loss: 155.2256\n",
      "Epoch 987/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.9116 - val_loss: 244.2740\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.0651 - val_loss: 205.6849\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.0053 - val_loss: 189.6439\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.0850 - val_loss: 198.0446\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.8318 - val_loss: 155.7827\n",
      "Epoch 992/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.9624 - val_loss: 171.4446\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.4855 - val_loss: 171.2894\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.1780 - val_loss: 177.5196\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.4852 - val_loss: 160.8030\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.8675 - val_loss: 175.5087\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.8848 - val_loss: 283.2175\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 171.2947 - val_loss: 228.6793\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.1371 - val_loss: 199.8584\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 179.1059 - val_loss: 174.2003\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 186.3466 - val_loss: 151.2387\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 162.1994 - val_loss: 179.9584\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.5024 - val_loss: 204.5882\n",
      "Epoch 1004/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 171.8266 - val_loss: 163.3439\n",
      "Epoch 1005/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.8392 - val_loss: 150.7647\n",
      "Epoch 1006/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 183.9664 - val_loss: 188.4448\n",
      "Epoch 1007/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 193.9646 - val_loss: 210.2818\n",
      "Epoch 1008/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.4156 - val_loss: 154.6069\n",
      "Epoch 1009/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 231.7567 - val_loss: 161.2992\n",
      "Epoch 1010/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.9186 - val_loss: 157.7699\n",
      "Epoch 1011/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.2216 - val_loss: 147.3507\n",
      "Epoch 1012/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.9972 - val_loss: 158.1292\n",
      "Epoch 1013/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.1678 - val_loss: 177.7584\n",
      "Epoch 1014/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.3759 - val_loss: 157.5062\n",
      "Epoch 1015/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.1682 - val_loss: 174.2489\n",
      "Epoch 1016/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.7875 - val_loss: 142.5343\n",
      "Epoch 1017/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 186.1059 - val_loss: 154.7264\n",
      "Epoch 1018/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.6600 - val_loss: 176.5422\n",
      "Epoch 1019/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.9049 - val_loss: 147.9785\n",
      "Epoch 1020/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 155.9917 - val_loss: 249.7189\n",
      "Epoch 1021/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 194.5881 - val_loss: 224.3330\n",
      "Epoch 1022/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.5543 - val_loss: 178.4999\n",
      "Epoch 1023/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 240.1099 - val_loss: 161.6782\n",
      "Epoch 1024/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.4797 - val_loss: 145.5272\n",
      "Epoch 1025/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.7586 - val_loss: 229.9381\n",
      "Epoch 1026/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.7852 - val_loss: 154.8198\n",
      "Epoch 1027/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 207.5864 - val_loss: 162.6189\n",
      "Epoch 1028/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.9091 - val_loss: 158.8317\n",
      "Epoch 1029/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.6993 - val_loss: 168.3546\n",
      "Epoch 1030/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 154.6394 - val_loss: 165.9994\n",
      "Epoch 1031/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.9030 - val_loss: 149.6425\n",
      "Epoch 1032/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.2568 - val_loss: 149.7652\n",
      "Epoch 1033/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.2708 - val_loss: 280.2318\n",
      "Epoch 1034/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 164.1661 - val_loss: 183.7540\n",
      "Epoch 1035/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.1615 - val_loss: 172.6413\n",
      "Epoch 1036/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.0723 - val_loss: 160.2844\n",
      "Epoch 1037/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.2257 - val_loss: 249.6449\n",
      "Epoch 1038/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.8384 - val_loss: 256.0707\n",
      "Epoch 1039/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.6513 - val_loss: 146.3504\n",
      "Epoch 1040/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.5243 - val_loss: 197.5613\n",
      "Epoch 1041/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.3246 - val_loss: 159.1420\n",
      "Epoch 1042/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.9403 - val_loss: 172.2893\n",
      "Epoch 1043/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.1822 - val_loss: 195.9743\n",
      "Epoch 1044/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.4422 - val_loss: 173.1884\n",
      "Epoch 1045/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 214.3675 - val_loss: 159.3909\n",
      "Epoch 1046/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.5753 - val_loss: 153.1303\n",
      "Epoch 1047/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 171.1915 - val_loss: 323.5455\n",
      "Epoch 1048/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.7966 - val_loss: 157.0399\n",
      "Epoch 1049/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.8651 - val_loss: 181.5013\n",
      "Epoch 1050/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.5739 - val_loss: 166.0136\n",
      "Epoch 1051/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.8665 - val_loss: 183.9912\n",
      "Epoch 1052/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 192.1367 - val_loss: 440.2958\n",
      "Epoch 1053/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.0165 - val_loss: 162.3696\n",
      "Epoch 1054/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 179.6920 - val_loss: 153.1932\n",
      "Epoch 1055/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.4371 - val_loss: 233.5946\n",
      "Epoch 1056/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.8498 - val_loss: 141.4311\n",
      "Epoch 1057/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.6598 - val_loss: 167.9899\n",
      "Epoch 1058/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 175.9806 - val_loss: 177.2136\n",
      "Epoch 1059/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.0873 - val_loss: 167.5281\n",
      "Epoch 1060/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.0512 - val_loss: 162.3532\n",
      "Epoch 1061/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 199.8574 - val_loss: 147.4447\n",
      "Epoch 1062/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 152.1554 - val_loss: 153.2879\n",
      "Epoch 1063/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 174.8264 - val_loss: 148.2147\n",
      "Epoch 1064/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 142.5752 - val_loss: 213.5616\n",
      "Epoch 1065/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.3878 - val_loss: 261.3740\n",
      "Epoch 1066/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 160.9109 - val_loss: 162.2755\n",
      "Epoch 1067/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 170.4129 - val_loss: 151.7288\n",
      "Epoch 1068/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 198.7809 - val_loss: 196.3778\n",
      "Epoch 1069/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.1196 - val_loss: 210.9434\n",
      "Epoch 1070/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.9855 - val_loss: 163.9397\n",
      "Epoch 1071/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.4842 - val_loss: 157.4132\n",
      "Epoch 1072/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.6500 - val_loss: 150.1927\n",
      "Epoch 1073/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.4483 - val_loss: 151.7784\n",
      "Epoch 1074/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.0964 - val_loss: 179.8418\n",
      "Epoch 1075/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.0761 - val_loss: 161.6257\n",
      "Epoch 1076/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 192.3177 - val_loss: 213.2288\n",
      "Epoch 1077/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.7756 - val_loss: 170.9720\n",
      "Epoch 1078/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.3383 - val_loss: 144.0717\n",
      "Epoch 1079/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.8891 - val_loss: 162.2303\n",
      "Epoch 1080/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.4232 - val_loss: 146.2173\n",
      "Epoch 1081/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 175.8839 - val_loss: 177.2405\n",
      "Epoch 1082/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 164.3032 - val_loss: 175.9285\n",
      "Epoch 1083/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.2494 - val_loss: 155.0037\n",
      "Epoch 1084/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 191.9465 - val_loss: 157.5452\n",
      "Epoch 1085/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.1549 - val_loss: 146.6392\n",
      "Epoch 1086/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.2256 - val_loss: 155.7354\n",
      "Epoch 1087/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 200.8751 - val_loss: 155.9548\n",
      "Epoch 1088/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 164.5865 - val_loss: 153.2318\n",
      "Epoch 1089/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.7364 - val_loss: 151.9991\n",
      "Epoch 1090/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.4479 - val_loss: 160.9893\n",
      "Epoch 1091/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.2470 - val_loss: 175.5828\n",
      "Epoch 1092/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.2018 - val_loss: 148.9551\n",
      "Epoch 1093/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.9955 - val_loss: 183.8977\n",
      "Epoch 1094/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.4702 - val_loss: 240.5281\n",
      "Epoch 1095/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.4022 - val_loss: 141.5483\n",
      "Epoch 1096/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 186.3301 - val_loss: 159.3065\n",
      "Epoch 1097/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.1336 - val_loss: 161.2001\n",
      "Epoch 1098/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.5471 - val_loss: 154.0778\n",
      "Epoch 1099/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.0282 - val_loss: 142.1876\n",
      "Epoch 1100/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.2897 - val_loss: 148.0532\n",
      "Epoch 1101/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 233.0147 - val_loss: 151.2229\n",
      "Epoch 1102/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.9732 - val_loss: 157.3108\n",
      "Epoch 1103/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.5368 - val_loss: 186.8896\n",
      "Epoch 1104/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.9705 - val_loss: 315.1817\n",
      "Epoch 1105/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 182.4073 - val_loss: 243.4907\n",
      "Epoch 1106/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 174.0280 - val_loss: 146.5545\n",
      "Epoch 1107/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.6020 - val_loss: 156.1449\n",
      "Epoch 1108/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.0010 - val_loss: 141.7081\n",
      "Epoch 1109/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.7558 - val_loss: 159.3093\n",
      "Epoch 1110/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 150.4291 - val_loss: 191.0755\n",
      "Epoch 1111/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.0230 - val_loss: 291.8740\n",
      "Epoch 1112/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 192.9239 - val_loss: 182.3185\n",
      "Epoch 1113/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.0450 - val_loss: 164.2674\n",
      "Epoch 1114/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.2379 - val_loss: 145.1609\n",
      "Epoch 1115/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 274.9256 - val_loss: 190.4622\n",
      "Epoch 1116/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.3225 - val_loss: 144.7238\n",
      "Epoch 1117/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.1925 - val_loss: 259.6308\n",
      "Epoch 1118/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.3468 - val_loss: 160.6552\n",
      "Epoch 1119/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.8322 - val_loss: 160.2661\n",
      "Epoch 1120/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.0386 - val_loss: 167.3139\n",
      "Epoch 1121/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.2523 - val_loss: 144.3131\n",
      "Epoch 1122/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 172.7255 - val_loss: 144.9251\n",
      "Epoch 1123/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 182.1787 - val_loss: 198.4179\n",
      "Epoch 1124/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 175.3420 - val_loss: 163.6978\n",
      "Epoch 1125/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.1493 - val_loss: 241.3562\n",
      "Epoch 1126/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 173.2200 - val_loss: 184.4249\n",
      "Epoch 1127/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 155.7546 - val_loss: 154.4926\n",
      "Epoch 1128/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 153.1931 - val_loss: 148.2521\n",
      "Epoch 1129/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.5085 - val_loss: 145.3589\n",
      "Epoch 1130/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.8669 - val_loss: 257.3324\n",
      "Epoch 1131/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.2114 - val_loss: 171.6744\n",
      "Epoch 1132/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.6432 - val_loss: 177.3652\n",
      "Epoch 1133/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.5215 - val_loss: 161.7741\n",
      "Epoch 1134/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.6992 - val_loss: 201.7946\n",
      "Epoch 1135/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.6354 - val_loss: 167.8980\n",
      "Epoch 1136/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.2961 - val_loss: 202.7957\n",
      "Epoch 1137/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 153.8900 - val_loss: 223.9493\n",
      "Epoch 1138/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 183.1467 - val_loss: 229.3356\n",
      "Epoch 1139/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.3209 - val_loss: 146.0219\n",
      "Epoch 1140/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 160.0067 - val_loss: 216.8908\n",
      "Epoch 1141/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.3650 - val_loss: 194.7252\n",
      "Epoch 1142/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.6369 - val_loss: 144.4705\n",
      "Epoch 1143/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.4105 - val_loss: 166.2625\n",
      "Epoch 1144/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 189.9400 - val_loss: 461.2005\n",
      "Epoch 1145/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 184.8360 - val_loss: 245.4760\n",
      "Epoch 1146/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.7527 - val_loss: 186.8348\n",
      "Epoch 1147/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.9894 - val_loss: 175.0834\n",
      "Epoch 1148/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 173.7661 - val_loss: 191.6151\n",
      "Epoch 1149/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.1926 - val_loss: 161.9868\n",
      "Epoch 1150/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.4171 - val_loss: 211.8301\n",
      "Epoch 1151/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 175.1275 - val_loss: 161.3678\n",
      "Epoch 1152/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.5795 - val_loss: 188.2998\n",
      "Epoch 1153/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.5681 - val_loss: 164.7070\n",
      "Epoch 1154/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.8970 - val_loss: 144.5965\n",
      "Epoch 1155/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.0971 - val_loss: 167.5073\n",
      "Epoch 1156/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.0491 - val_loss: 170.4306\n",
      "Epoch 1157/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.5612 - val_loss: 151.2142\n",
      "Epoch 1158/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.2367 - val_loss: 154.9553\n",
      "Epoch 1159/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.5451 - val_loss: 156.5053\n",
      "Epoch 1160/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 160.9016 - val_loss: 201.5804\n",
      "Epoch 1161/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.6976 - val_loss: 206.1246\n",
      "Epoch 1162/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.1117 - val_loss: 154.3682\n",
      "Epoch 1163/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.0786 - val_loss: 152.2229\n",
      "Epoch 1164/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.2518 - val_loss: 152.5380\n",
      "Epoch 1165/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.4176 - val_loss: 179.6193\n",
      "Epoch 1166/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.8607 - val_loss: 142.7941\n",
      "Epoch 1167/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.9733 - val_loss: 152.1813\n",
      "Epoch 1168/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.7691 - val_loss: 150.6494\n",
      "Epoch 1169/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.0974 - val_loss: 166.9818\n",
      "Epoch 1170/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.4221 - val_loss: 153.3387\n",
      "Epoch 1171/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.7903 - val_loss: 148.4056\n",
      "Epoch 1172/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.3131 - val_loss: 146.8688\n",
      "Epoch 1173/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 167.0656 - val_loss: 157.2714\n",
      "Epoch 1174/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.2384 - val_loss: 152.0525\n",
      "Epoch 1175/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.6147 - val_loss: 172.0332\n",
      "Epoch 1176/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 185.6533 - val_loss: 156.2064\n",
      "Epoch 1177/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.7321 - val_loss: 146.1577\n",
      "Epoch 1178/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.5671 - val_loss: 163.3718\n",
      "Epoch 1179/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.6167 - val_loss: 153.6373\n",
      "Epoch 1180/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.0079 - val_loss: 176.4158\n",
      "Epoch 1181/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.4342 - val_loss: 161.9707\n",
      "Epoch 1182/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.9687 - val_loss: 157.1981\n",
      "Epoch 1183/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 138.0428 - val_loss: 150.0775\n",
      "Epoch 1184/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 171.5810 - val_loss: 155.9024\n",
      "Epoch 1185/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 154.4509 - val_loss: 181.7632\n",
      "Epoch 1186/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.3619 - val_loss: 159.7761\n",
      "Epoch 1187/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.8845 - val_loss: 144.4605\n",
      "Epoch 1188/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 168.8257 - val_loss: 151.7783\n",
      "Epoch 1189/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 175.3665 - val_loss: 153.3843\n",
      "Epoch 1190/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.7379 - val_loss: 144.5677\n",
      "Epoch 1191/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.8668 - val_loss: 178.9186\n",
      "Epoch 1192/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 200.0920 - val_loss: 264.1445\n",
      "Epoch 1193/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.6227 - val_loss: 184.4620\n",
      "Epoch 1194/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.7256 - val_loss: 218.8153\n",
      "Epoch 1195/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.5390 - val_loss: 141.5248\n",
      "Epoch 1196/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.2144 - val_loss: 162.4768\n",
      "Epoch 1197/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.5965 - val_loss: 142.3200\n",
      "Epoch 1198/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.1733 - val_loss: 158.0410\n",
      "Epoch 1199/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.7811 - val_loss: 165.4610\n",
      "Epoch 1200/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.0245 - val_loss: 193.4015\n",
      "Epoch 1201/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.6596 - val_loss: 210.5092\n",
      "Epoch 1202/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.2746 - val_loss: 177.1717\n",
      "Epoch 1203/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.0685 - val_loss: 155.0309\n",
      "Epoch 1204/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.5466 - val_loss: 176.7648\n",
      "Epoch 1205/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.4814 - val_loss: 327.6012\n",
      "Epoch 1206/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.5663 - val_loss: 180.4468\n",
      "Epoch 1207/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.4358 - val_loss: 170.4867\n",
      "Epoch 1208/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 217.7394 - val_loss: 148.3264\n",
      "Epoch 1209/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.3641 - val_loss: 148.6999\n",
      "Epoch 1210/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.0042 - val_loss: 145.0826\n",
      "Epoch 1211/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.2682 - val_loss: 158.5351\n",
      "Epoch 1212/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.7376 - val_loss: 166.5405\n",
      "Epoch 1213/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.3123 - val_loss: 142.3613\n",
      "Epoch 1214/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.4425 - val_loss: 162.0168\n",
      "Epoch 1215/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.3451 - val_loss: 205.4289\n",
      "Epoch 1216/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.5193 - val_loss: 164.2311\n",
      "Epoch 1217/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.1257 - val_loss: 155.0037\n",
      "Epoch 1218/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 181.5761 - val_loss: 143.9623\n",
      "Epoch 1219/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.5264 - val_loss: 142.8827\n",
      "Epoch 1220/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.4678 - val_loss: 150.8681\n",
      "Epoch 1221/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.7162 - val_loss: 157.2047\n",
      "Epoch 1222/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.6778 - val_loss: 154.3523\n",
      "Epoch 1223/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.1701 - val_loss: 180.6757\n",
      "Epoch 1224/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 221.2047 - val_loss: 166.1458\n",
      "Epoch 1225/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.8807 - val_loss: 179.3254\n",
      "Epoch 1226/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.0628 - val_loss: 170.5804\n",
      "Epoch 1227/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.3668 - val_loss: 156.5666\n",
      "Epoch 1228/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.4828 - val_loss: 160.7458\n",
      "Epoch 1229/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.8485 - val_loss: 143.9233\n",
      "Epoch 1230/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 150.6529 - val_loss: 168.9516\n",
      "Epoch 1231/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.1659 - val_loss: 168.9108\n",
      "Epoch 1232/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.6531 - val_loss: 142.6172\n",
      "Epoch 1233/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.6980 - val_loss: 171.9663\n",
      "Epoch 1234/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 154.8724 - val_loss: 184.3808\n",
      "Epoch 1235/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 216.2907 - val_loss: 166.7500\n",
      "Epoch 1236/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 181.8553 - val_loss: 147.2864\n",
      "Epoch 1237/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.6939 - val_loss: 162.3062\n",
      "Epoch 1238/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 167.2726 - val_loss: 144.2814\n",
      "Epoch 1239/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.8810 - val_loss: 249.0931\n",
      "Epoch 1240/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.5201 - val_loss: 423.4290\n",
      "Epoch 1241/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 199.2464 - val_loss: 145.9864\n",
      "Epoch 1242/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 151.4718 - val_loss: 143.4203\n",
      "Epoch 1243/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.0160 - val_loss: 248.8100\n",
      "Epoch 1244/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 151.3919 - val_loss: 152.4144\n",
      "Epoch 1245/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 140.6012 - val_loss: 151.8313\n",
      "Epoch 1246/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 162.6969 - val_loss: 166.9130\n",
      "Epoch 1247/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.2859 - val_loss: 185.2730\n",
      "Epoch 1248/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 200.3100 - val_loss: 151.2547\n",
      "Epoch 1249/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.4076 - val_loss: 163.6372\n",
      "Epoch 1250/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.3227 - val_loss: 162.4538\n",
      "Epoch 1251/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 138.5350 - val_loss: 144.9292\n",
      "Epoch 1252/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 181.0450 - val_loss: 153.9284\n",
      "Epoch 1253/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.3149 - val_loss: 239.3468\n",
      "Epoch 1254/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.9449 - val_loss: 158.5518\n",
      "Epoch 1255/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.4956 - val_loss: 189.1438\n",
      "Epoch 1256/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.2655 - val_loss: 149.6003\n",
      "Epoch 1257/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.0162 - val_loss: 152.1135\n",
      "Epoch 1258/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 136.8495 - val_loss: 163.3884\n",
      "Epoch 1259/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.2387 - val_loss: 146.6100\n",
      "Epoch 1260/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.4691 - val_loss: 148.8197\n",
      "Epoch 1261/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 147.2222 - val_loss: 144.9488\n",
      "Epoch 1262/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 160.6366 - val_loss: 151.5993\n",
      "Epoch 1263/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 175.4246 - val_loss: 189.9490\n",
      "Epoch 1264/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 145.8389 - val_loss: 149.5669\n",
      "Epoch 1265/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 155.5618 - val_loss: 144.7639\n",
      "Epoch 1266/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 140.7506 - val_loss: 143.3038\n",
      "Epoch 1267/10000\n",
      "8000/8000 [==============================] - 1s 143us/step - loss: 168.3974 - val_loss: 142.8006\n",
      "Epoch 1268/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 169.5503 - val_loss: 219.5038\n",
      "Epoch 1269/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 182.3916 - val_loss: 182.7324\n",
      "Epoch 1270/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 140.1577 - val_loss: 145.4820\n",
      "Epoch 1271/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 174.483 - 1s 102us/step - loss: 173.4220 - val_loss: 172.9982\n",
      "Epoch 1272/10000\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 152.1373 - val_loss: 176.7374\n",
      "Epoch 1273/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 193.5075 - val_loss: 253.3608\n",
      "Epoch 1274/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 149.6210 - val_loss: 201.1807\n",
      "Epoch 1275/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 182.4751 - val_loss: 460.2613\n",
      "Epoch 1276/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 169.0899 - val_loss: 142.7257\n",
      "Epoch 1277/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 146.7176 - val_loss: 172.5554\n",
      "Epoch 1278/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 172.0306 - val_loss: 322.6549\n",
      "Epoch 1279/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 141.9445 - val_loss: 141.7308\n",
      "Epoch 1280/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 87us/step - loss: 145.4706 - val_loss: 185.6851\n",
      "Epoch 1281/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 143.3082 - val_loss: 145.6681\n",
      "Epoch 1282/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 147.1664 - val_loss: 146.7562\n",
      "Epoch 1283/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 146.1012 - val_loss: 192.3199\n",
      "Epoch 1284/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 159.9368 - val_loss: 150.6758\n",
      "Epoch 1285/10000\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 159.5720 - val_loss: 293.6810\n",
      "Epoch 1286/10000\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 174.8005 - val_loss: 157.8834\n",
      "Epoch 1287/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 167.2257 - val_loss: 165.7349\n",
      "Epoch 1288/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 158.2955 - val_loss: 152.0961\n",
      "Epoch 1289/10000\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 173.1191 - val_loss: 252.8998\n",
      "Epoch 1290/10000\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 164.3146 - val_loss: 145.7683\n",
      "Epoch 1291/10000\n",
      "8000/8000 [==============================] - 1s 124us/step - loss: 159.5836 - val_loss: 155.9192\n",
      "Epoch 1292/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 177.3511 - val_loss: 158.0116\n",
      "Epoch 1293/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 146.2929 - val_loss: 162.1232\n",
      "Epoch 1294/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 159.5481 - val_loss: 415.4159\n",
      "Epoch 1295/10000\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 181.4661 - val_loss: 144.3561\n",
      "Epoch 1296/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 142.0055 - val_loss: 164.9164\n",
      "Epoch 1297/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 146.0338 - val_loss: 141.3455\n",
      "Epoch 1298/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 189.9039 - val_loss: 184.7841\n",
      "Epoch 1299/10000\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 216.1246 - val_loss: 155.4688\n",
      "Epoch 1300/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 156.4169 - val_loss: 143.2116\n",
      "Epoch 1301/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 157.0430 - val_loss: 153.7204\n",
      "Epoch 1302/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 146.3408 - val_loss: 145.6901\n",
      "Epoch 1303/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 139.3510 - val_loss: 163.5990\n",
      "Epoch 1304/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 154.4573 - val_loss: 152.4519\n",
      "Epoch 1305/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 155.0139 - val_loss: 152.5541\n",
      "Epoch 1306/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 136.3664 - val_loss: 163.7604\n",
      "Epoch 1307/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 153.7248 - val_loss: 209.5707\n",
      "Epoch 1308/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 159.9991 - val_loss: 162.6240\n",
      "Epoch 1309/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 146.5382 - val_loss: 161.5858\n",
      "Epoch 1310/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.7267 - val_loss: 183.1529\n",
      "Epoch 1311/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.4557 - val_loss: 146.5262\n",
      "Epoch 1312/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 186.9141 - val_loss: 194.4614\n",
      "Epoch 1313/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 198.6681 - val_loss: 252.1778\n",
      "Epoch 1314/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.7157 - val_loss: 172.7802\n",
      "Epoch 1315/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 160.3185 - val_loss: 152.9692\n",
      "Epoch 1316/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.6745 - val_loss: 258.2514\n",
      "Epoch 1317/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 162.2014 - val_loss: 152.0813\n",
      "Epoch 1318/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 148.2177 - val_loss: 140.4783\n",
      "Epoch 1319/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 160.2625 - val_loss: 147.8235\n",
      "Epoch 1320/10000\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 150.7700 - val_loss: 188.5622\n",
      "Epoch 1321/10000\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 172.0325 - val_loss: 142.1166\n",
      "Epoch 1322/10000\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 157.9740 - val_loss: 141.4192\n",
      "Epoch 1323/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 163.3643 - val_loss: 155.1330\n",
      "Epoch 1324/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 151.2072 - val_loss: 154.1058\n",
      "Epoch 1325/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 170.6779 - val_loss: 178.2367\n",
      "Epoch 1326/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.8108 - val_loss: 139.4171\n",
      "Epoch 1327/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.5759 - val_loss: 187.7368\n",
      "Epoch 1328/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.8370 - val_loss: 149.8897\n",
      "Epoch 1329/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 155.8448 - val_loss: 172.3957\n",
      "Epoch 1330/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 139.8807 - val_loss: 146.4119\n",
      "Epoch 1331/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 167.9691 - val_loss: 255.4632\n",
      "Epoch 1332/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 138.9867 - val_loss: 142.0238\n",
      "Epoch 1333/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 152.9285 - val_loss: 156.2725\n",
      "Epoch 1334/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 181.1927 - val_loss: 150.8631\n",
      "Epoch 1335/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 150.3856 - val_loss: 210.1738\n",
      "Epoch 1336/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 150.9079 - val_loss: 162.1399\n",
      "Epoch 1337/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 148.1865 - val_loss: 189.8821\n",
      "Epoch 1338/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 148.6991 - val_loss: 190.6593\n",
      "Epoch 1339/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.7302 - val_loss: 145.7118\n",
      "Epoch 1340/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.4399 - val_loss: 146.8763\n",
      "Epoch 1341/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.0727 - val_loss: 517.0444\n",
      "Epoch 1342/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 184.7119 - val_loss: 163.5450\n",
      "Epoch 1343/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.8332 - val_loss: 154.9091\n",
      "Epoch 1344/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 189.3972 - val_loss: 149.4326\n",
      "Epoch 1345/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.5611 - val_loss: 150.2951\n",
      "Epoch 1346/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.3047 - val_loss: 141.3493\n",
      "Epoch 1347/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.6538 - val_loss: 140.5504\n",
      "Epoch 1348/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.2660 - val_loss: 152.5630\n",
      "Epoch 1349/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.3801 - val_loss: 147.3879\n",
      "Epoch 1350/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.2690 - val_loss: 176.3795\n",
      "Epoch 1351/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.5578 - val_loss: 168.3827\n",
      "Epoch 1352/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.1713 - val_loss: 145.7188\n",
      "Epoch 1353/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.4310 - val_loss: 183.1925\n",
      "Epoch 1354/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.6547 - val_loss: 155.6730\n",
      "Epoch 1355/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.2112 - val_loss: 149.9183\n",
      "Epoch 1356/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.7314 - val_loss: 161.9180\n",
      "Epoch 1357/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 155.9865 - val_loss: 145.0709\n",
      "Epoch 1358/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 159.8499 - val_loss: 524.0402\n",
      "Epoch 1359/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 181.1976 - val_loss: 154.7209\n",
      "Epoch 1360/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 142.5235 - val_loss: 183.4393\n",
      "Epoch 1361/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.2604 - val_loss: 161.2450\n",
      "Epoch 1362/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.6021 - val_loss: 165.7621\n",
      "Epoch 1363/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 158.9931 - val_loss: 152.7640\n",
      "Epoch 1364/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 177.1360 - val_loss: 139.9789\n",
      "Epoch 1365/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.4735 - val_loss: 141.5605\n",
      "Epoch 1366/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 175.1188 - val_loss: 163.9600\n",
      "Epoch 1367/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.0138 - val_loss: 173.0450\n",
      "Epoch 1368/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.2105 - val_loss: 144.8372\n",
      "Epoch 1369/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.2092 - val_loss: 146.3182\n",
      "Epoch 1370/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.6672 - val_loss: 149.6937\n",
      "Epoch 1371/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.1165 - val_loss: 144.4119\n",
      "Epoch 1372/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 167.6518 - val_loss: 150.7666\n",
      "Epoch 1373/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 144.9848 - val_loss: 162.6925\n",
      "Epoch 1374/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 169.0093 - val_loss: 196.1728\n",
      "Epoch 1375/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 154.4826 - val_loss: 152.9013\n",
      "Epoch 1376/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.3092 - val_loss: 150.3773\n",
      "Epoch 1377/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.1158 - val_loss: 157.1712\n",
      "Epoch 1378/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.8155 - val_loss: 205.5642\n",
      "Epoch 1379/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.0861 - val_loss: 224.7454\n",
      "Epoch 1380/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 217.8586 - val_loss: 175.4138\n",
      "Epoch 1381/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.4516 - val_loss: 171.8157\n",
      "Epoch 1382/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.9504 - val_loss: 164.7679\n",
      "Epoch 1383/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.2457 - val_loss: 156.1120\n",
      "Epoch 1384/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.0409 - val_loss: 148.5928\n",
      "Epoch 1385/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 160.8650 - val_loss: 183.7339\n",
      "Epoch 1386/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.5943 - val_loss: 165.0546\n",
      "Epoch 1387/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 141.6526 - val_loss: 202.3644\n",
      "Epoch 1388/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.9793 - val_loss: 165.5731\n",
      "Epoch 1389/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.5899 - val_loss: 233.2066\n",
      "Epoch 1390/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.6139 - val_loss: 162.3382\n",
      "Epoch 1391/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.1386 - val_loss: 158.2263\n",
      "Epoch 1392/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.6342 - val_loss: 169.7818\n",
      "Epoch 1393/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.3813 - val_loss: 157.0786\n",
      "Epoch 1394/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.3781 - val_loss: 149.7601\n",
      "Epoch 1395/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.9581 - val_loss: 173.6258\n",
      "Epoch 1396/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 171.6428 - val_loss: 182.3255\n",
      "Epoch 1397/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 145.2638 - val_loss: 139.7622\n",
      "Epoch 1398/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 163.5835 - val_loss: 169.5954\n",
      "Epoch 1399/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 153.6070 - val_loss: 165.8183\n",
      "Epoch 1400/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.5667 - val_loss: 150.0612\n",
      "Epoch 1401/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 184.1173 - val_loss: 142.6406\n",
      "Epoch 1402/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.4455 - val_loss: 157.5259\n",
      "Epoch 1403/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.3570 - val_loss: 140.7223\n",
      "Epoch 1404/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.1720 - val_loss: 154.5431\n",
      "Epoch 1405/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.6003 - val_loss: 146.5822\n",
      "Epoch 1406/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.8829 - val_loss: 171.6079\n",
      "Epoch 1407/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 155.8858 - val_loss: 213.6131\n",
      "Epoch 1408/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.0633 - val_loss: 179.3213\n",
      "Epoch 1409/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 156.2984 - val_loss: 148.6888\n",
      "Epoch 1410/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 146.6972 - val_loss: 167.7099\n",
      "Epoch 1411/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.4151 - val_loss: 180.6257\n",
      "Epoch 1412/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.0938 - val_loss: 147.3688\n",
      "Epoch 1413/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.0076 - val_loss: 181.0656\n",
      "Epoch 1414/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 154.3283 - val_loss: 161.2860\n",
      "Epoch 1415/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.8936 - val_loss: 162.5613\n",
      "Epoch 1416/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.0996 - val_loss: 147.4367\n",
      "Epoch 1417/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 180.4332 - val_loss: 258.3589\n",
      "Epoch 1418/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 183.0532 - val_loss: 160.2311\n",
      "Epoch 1419/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.5226 - val_loss: 162.8644\n",
      "Epoch 1420/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.3491 - val_loss: 311.6675\n",
      "Epoch 1421/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.6632 - val_loss: 164.7006\n",
      "Epoch 1422/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.0528 - val_loss: 169.7465\n",
      "Epoch 1423/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.8368 - val_loss: 153.8593\n",
      "Epoch 1424/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 181.6991 - val_loss: 142.8399\n",
      "Epoch 1425/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 173.3040 - val_loss: 144.7737\n",
      "Epoch 1426/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.9764 - val_loss: 161.2512\n",
      "Epoch 1427/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.4218 - val_loss: 144.5785\n",
      "Epoch 1428/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.6497 - val_loss: 184.9979\n",
      "Epoch 1429/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.2180 - val_loss: 148.7137\n",
      "Epoch 1430/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.3466 - val_loss: 140.1153\n",
      "Epoch 1431/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.3899 - val_loss: 170.3306\n",
      "Epoch 1432/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.7332 - val_loss: 155.3772\n",
      "Epoch 1433/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.6479 - val_loss: 156.1072\n",
      "Epoch 1434/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.4266 - val_loss: 261.6753\n",
      "Epoch 1435/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.5273 - val_loss: 147.1206\n",
      "Epoch 1436/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.0890 - val_loss: 169.7741\n",
      "Epoch 1437/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.4002 - val_loss: 201.7236\n",
      "Epoch 1438/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 158.3714 - val_loss: 188.3729\n",
      "Epoch 1439/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.7178 - val_loss: 147.4937\n",
      "Epoch 1440/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 200.2803 - val_loss: 215.5710\n",
      "Epoch 1441/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 174.0804 - val_loss: 142.7555\n",
      "Epoch 1442/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.6719 - val_loss: 146.4492\n",
      "Epoch 1443/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.6553 - val_loss: 237.6283\n",
      "Epoch 1444/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.9671 - val_loss: 182.1234\n",
      "Epoch 1445/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.5564 - val_loss: 143.8309\n",
      "Epoch 1446/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.0215 - val_loss: 145.9097\n",
      "Epoch 1447/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 167.8907 - val_loss: 161.8191\n",
      "Epoch 1448/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 207.7534 - val_loss: 189.7330\n",
      "Epoch 1449/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.9400 - val_loss: 167.0601\n",
      "Epoch 1450/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.7796 - val_loss: 153.4122\n",
      "Epoch 1451/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.9188 - val_loss: 140.9400\n",
      "Epoch 1452/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.1923 - val_loss: 142.2109\n",
      "Epoch 1453/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.8794 - val_loss: 167.5839\n",
      "Epoch 1454/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.9303 - val_loss: 217.6435\n",
      "Epoch 1455/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.3189 - val_loss: 176.4848\n",
      "Epoch 1456/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.1509 - val_loss: 143.6760\n",
      "Epoch 1457/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 160.2293 - val_loss: 213.4505\n",
      "Epoch 1458/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.3930 - val_loss: 155.6810\n",
      "Epoch 1459/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.0096 - val_loss: 166.4783\n",
      "Epoch 1460/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.7512 - val_loss: 154.3190\n",
      "Epoch 1461/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.3547 - val_loss: 170.6295\n",
      "Epoch 1462/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 172.1090 - val_loss: 157.1457\n",
      "Epoch 1463/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.9347 - val_loss: 165.0852\n",
      "Epoch 1464/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.5537 - val_loss: 178.4049\n",
      "Epoch 1465/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 169.7121 - val_loss: 172.7687\n",
      "Epoch 1466/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 144.2845 - val_loss: 232.3578\n",
      "Epoch 1467/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 160.1574 - val_loss: 185.3715\n",
      "Epoch 1468/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.7267 - val_loss: 140.2305\n",
      "Epoch 1469/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.8888 - val_loss: 185.7029\n",
      "Epoch 1470/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.0080 - val_loss: 152.2403\n",
      "Epoch 1471/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.3331 - val_loss: 189.7776\n",
      "Epoch 1472/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.5774 - val_loss: 204.3570\n",
      "Epoch 1473/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.1030 - val_loss: 157.4192\n",
      "Epoch 1474/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.6753 - val_loss: 150.8570\n",
      "Epoch 1475/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.0311 - val_loss: 165.8126\n",
      "Epoch 1476/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.1055 - val_loss: 280.7395\n",
      "Epoch 1477/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.6982 - val_loss: 202.9750\n",
      "Epoch 1478/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.9220 - val_loss: 186.6346\n",
      "Epoch 1479/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.4192 - val_loss: 238.3871\n",
      "Epoch 1480/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 182.2608 - val_loss: 175.1481\n",
      "Epoch 1481/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.3979 - val_loss: 145.0018\n",
      "Epoch 1482/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.9052 - val_loss: 149.8639\n",
      "Epoch 1483/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.8612 - val_loss: 176.3614\n",
      "Epoch 1484/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.1289 - val_loss: 146.5418\n",
      "Epoch 1485/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.6094 - val_loss: 171.1890\n",
      "Epoch 1486/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 173.6360 - val_loss: 166.0137\n",
      "Epoch 1487/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.9232 - val_loss: 177.5094\n",
      "Epoch 1488/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.3085 - val_loss: 146.7329\n",
      "Epoch 1489/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.6068 - val_loss: 267.6082\n",
      "Epoch 1490/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 171.6651 - val_loss: 169.1867\n",
      "Epoch 1491/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.7812 - val_loss: 157.7081\n",
      "Epoch 1492/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.6789 - val_loss: 146.4047\n",
      "Epoch 1493/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.0336 - val_loss: 155.0205\n",
      "Epoch 1494/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.8385 - val_loss: 157.6722\n",
      "Epoch 1495/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 214.3876 - val_loss: 154.7466\n",
      "Epoch 1496/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 167.9276 - val_loss: 148.9621\n",
      "Epoch 1497/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.0804 - val_loss: 184.3372\n",
      "Epoch 1498/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.3752 - val_loss: 149.1342\n",
      "Epoch 1499/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.9193 - val_loss: 217.1779\n",
      "Epoch 1500/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 142.8961 - val_loss: 141.3970\n",
      "Epoch 1501/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.1758 - val_loss: 145.6065\n",
      "Epoch 1502/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.8052 - val_loss: 184.1899\n",
      "Epoch 1503/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.9045 - val_loss: 161.1349\n",
      "Epoch 1504/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.9505 - val_loss: 145.5286\n",
      "Epoch 1505/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.3626 - val_loss: 142.4382\n",
      "Epoch 1506/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 139.6659 - val_loss: 237.4951\n",
      "Epoch 1507/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.5676 - val_loss: 244.9558\n",
      "Epoch 1508/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 148.1493 - val_loss: 147.0064\n",
      "Epoch 1509/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.0811 - val_loss: 142.4843\n",
      "Epoch 1510/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.5388 - val_loss: 152.9664\n",
      "Epoch 1511/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 230.2156 - val_loss: 240.2002\n",
      "Epoch 1512/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.1136 - val_loss: 145.9921\n",
      "Epoch 1513/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.3806 - val_loss: 151.1429\n",
      "Epoch 1514/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.7052 - val_loss: 170.0079\n",
      "Epoch 1515/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.6609 - val_loss: 166.4141\n",
      "Epoch 1516/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.3809 - val_loss: 192.3570\n",
      "Epoch 1517/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.2526 - val_loss: 150.0902\n",
      "Epoch 1518/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 141.6893 - val_loss: 138.9218\n",
      "Epoch 1519/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 145.6008 - val_loss: 159.7841\n",
      "Epoch 1520/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 169.2663 - val_loss: 501.3327\n",
      "Epoch 1521/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 185.9561 - val_loss: 154.0981\n",
      "Epoch 1522/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.2742 - val_loss: 156.0571\n",
      "Epoch 1523/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.5348 - val_loss: 246.5326\n",
      "Epoch 1524/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.5286 - val_loss: 146.1846\n",
      "Epoch 1525/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.6129 - val_loss: 145.2012\n",
      "Epoch 1526/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.5483 - val_loss: 170.3482\n",
      "Epoch 1527/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.6780 - val_loss: 146.0605\n",
      "Epoch 1528/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.1315 - val_loss: 158.7447\n",
      "Epoch 1529/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.5687 - val_loss: 149.1634\n",
      "Epoch 1530/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.5920 - val_loss: 144.6219\n",
      "Epoch 1531/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 143.7043 - val_loss: 158.4701\n",
      "Epoch 1532/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.0435 - val_loss: 171.8932\n",
      "Epoch 1533/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.7996 - val_loss: 162.5796\n",
      "Epoch 1534/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.6805 - val_loss: 200.7435\n",
      "Epoch 1535/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.2510 - val_loss: 148.0347\n",
      "Epoch 1536/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.9679 - val_loss: 156.3369\n",
      "Epoch 1537/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 150.2367 - val_loss: 147.0149\n",
      "Epoch 1538/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.4429 - val_loss: 146.0205\n",
      "Epoch 1539/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.7377 - val_loss: 143.0119\n",
      "Epoch 1540/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.4983 - val_loss: 144.6071\n",
      "Epoch 1541/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.9606 - val_loss: 138.0081\n",
      "Epoch 1542/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.0105 - val_loss: 175.7265\n",
      "Epoch 1543/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 198.9639 - val_loss: 155.9747\n",
      "Epoch 1544/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.1908 - val_loss: 218.4713\n",
      "Epoch 1545/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.2785 - val_loss: 163.3586\n",
      "Epoch 1546/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 197.9236 - val_loss: 171.3821\n",
      "Epoch 1547/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.4342 - val_loss: 140.6769\n",
      "Epoch 1548/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.6139 - val_loss: 191.2057\n",
      "Epoch 1549/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.7981 - val_loss: 294.3055\n",
      "Epoch 1550/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.0834 - val_loss: 157.0286\n",
      "Epoch 1551/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.7050 - val_loss: 150.3655\n",
      "Epoch 1552/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.0557 - val_loss: 168.9540\n",
      "Epoch 1553/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.0322 - val_loss: 155.8089\n",
      "Epoch 1554/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.5610 - val_loss: 145.1784\n",
      "Epoch 1555/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.9846 - val_loss: 165.7236\n",
      "Epoch 1556/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.5308 - val_loss: 145.3560\n",
      "Epoch 1557/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.0289 - val_loss: 144.8557\n",
      "Epoch 1558/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.3664 - val_loss: 149.8826\n",
      "Epoch 1559/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.3275 - val_loss: 143.0635\n",
      "Epoch 1560/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.6044 - val_loss: 175.4315\n",
      "Epoch 1561/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.5744 - val_loss: 180.9394\n",
      "Epoch 1562/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.6060 - val_loss: 201.4873\n",
      "Epoch 1563/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.7350 - val_loss: 152.0668\n",
      "Epoch 1564/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.7842 - val_loss: 139.4414\n",
      "Epoch 1565/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.7408 - val_loss: 141.5232\n",
      "Epoch 1566/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.8701 - val_loss: 338.2878\n",
      "Epoch 1567/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.9303 - val_loss: 164.4685\n",
      "Epoch 1568/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 263.1817 - val_loss: 190.7162\n",
      "Epoch 1569/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 207.4507 - val_loss: 148.8529\n",
      "Epoch 1570/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.1485 - val_loss: 154.4288\n",
      "Epoch 1571/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.0476 - val_loss: 171.9490\n",
      "Epoch 1572/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.7598 - val_loss: 193.8489\n",
      "Epoch 1573/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.1167 - val_loss: 140.8281\n",
      "Epoch 1574/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 155.1382 - val_loss: 150.3508\n",
      "Epoch 1575/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.2044 - val_loss: 180.0492\n",
      "Epoch 1576/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.1511 - val_loss: 178.0301\n",
      "Epoch 1577/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.7420 - val_loss: 162.8450\n",
      "Epoch 1578/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 191.0735 - val_loss: 220.9530\n",
      "Epoch 1579/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.5817 - val_loss: 147.2403\n",
      "Epoch 1580/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 131.9250 - val_loss: 190.8424\n",
      "Epoch 1581/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 176.7655 - val_loss: 170.3036\n",
      "Epoch 1582/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 146.0592 - val_loss: 201.0269\n",
      "Epoch 1583/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.1955 - val_loss: 151.5176\n",
      "Epoch 1584/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.1251 - val_loss: 153.9789\n",
      "Epoch 1585/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.1747 - val_loss: 151.3267\n",
      "Epoch 1586/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 211.8835 - val_loss: 146.5930\n",
      "Epoch 1587/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.2935 - val_loss: 150.2326\n",
      "Epoch 1588/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.0884 - val_loss: 152.6069\n",
      "Epoch 1589/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.9956 - val_loss: 175.9415\n",
      "Epoch 1590/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.9589 - val_loss: 137.1565\n",
      "Epoch 1591/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.1305 - val_loss: 199.7215\n",
      "Epoch 1592/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.6579 - val_loss: 147.5221\n",
      "Epoch 1593/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.0080 - val_loss: 166.3328\n",
      "Epoch 1594/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.2882 - val_loss: 145.0871\n",
      "Epoch 1595/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.4054 - val_loss: 242.4651\n",
      "Epoch 1596/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.5518 - val_loss: 180.3331\n",
      "Epoch 1597/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.2369 - val_loss: 165.3421\n",
      "Epoch 1598/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.8542 - val_loss: 178.5571\n",
      "Epoch 1599/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.6367 - val_loss: 253.9465\n",
      "Epoch 1600/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.0951 - val_loss: 156.3524\n",
      "Epoch 1601/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.9262 - val_loss: 192.8451\n",
      "Epoch 1602/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.0439 - val_loss: 146.4384\n",
      "Epoch 1603/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.3596 - val_loss: 185.1684\n",
      "Epoch 1604/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 155.9749 - val_loss: 179.4753\n",
      "Epoch 1605/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.6967 - val_loss: 175.0950\n",
      "Epoch 1606/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.4632 - val_loss: 186.0181\n",
      "Epoch 1607/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.0861 - val_loss: 140.2510\n",
      "Epoch 1608/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.9849 - val_loss: 170.8619\n",
      "Epoch 1609/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.0796 - val_loss: 165.7847\n",
      "Epoch 1610/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.5729 - val_loss: 206.5121\n",
      "Epoch 1611/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.7609 - val_loss: 153.8841\n",
      "Epoch 1612/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.1497 - val_loss: 157.5120\n",
      "Epoch 1613/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.9465 - val_loss: 157.4532\n",
      "Epoch 1614/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.8210 - val_loss: 156.7658\n",
      "Epoch 1615/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.7613 - val_loss: 143.2617\n",
      "Epoch 1616/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.3835 - val_loss: 161.6893\n",
      "Epoch 1617/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 214.8448 - val_loss: 505.6725\n",
      "Epoch 1618/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.6851 - val_loss: 137.5319\n",
      "Epoch 1619/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.5739 - val_loss: 152.0024\n",
      "Epoch 1620/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.3001 - val_loss: 142.6927\n",
      "Epoch 1621/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.4528 - val_loss: 144.6817\n",
      "Epoch 1622/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.6375 - val_loss: 146.1221\n",
      "Epoch 1623/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.4864 - val_loss: 185.8767\n",
      "Epoch 1624/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.7123 - val_loss: 170.6464\n",
      "Epoch 1625/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.3183 - val_loss: 155.7170\n",
      "Epoch 1626/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 162.1074 - val_loss: 161.7626\n",
      "Epoch 1627/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.9836 - val_loss: 172.0724\n",
      "Epoch 1628/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.1890 - val_loss: 162.0125\n",
      "Epoch 1629/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 163.7160 - val_loss: 168.7103\n",
      "Epoch 1630/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.3077 - val_loss: 142.8101\n",
      "Epoch 1631/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.8078 - val_loss: 160.2218\n",
      "Epoch 1632/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 144.6024 - val_loss: 142.3478\n",
      "Epoch 1633/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.3257 - val_loss: 158.2951\n",
      "Epoch 1634/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.6434 - val_loss: 159.3502\n",
      "Epoch 1635/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.5062 - val_loss: 152.3182\n",
      "Epoch 1636/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.1491 - val_loss: 149.2226\n",
      "Epoch 1637/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.3157 - val_loss: 172.6172\n",
      "Epoch 1638/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.9739 - val_loss: 160.3147\n",
      "Epoch 1639/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.1313 - val_loss: 162.5911\n",
      "Epoch 1640/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.7199 - val_loss: 145.4679\n",
      "Epoch 1641/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.5378 - val_loss: 151.0182\n",
      "Epoch 1642/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 147.9448 - val_loss: 158.7081\n",
      "Epoch 1643/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 137.6388 - val_loss: 240.5054\n",
      "Epoch 1644/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 131.5439 - val_loss: 146.8259\n",
      "Epoch 1645/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.5624 - val_loss: 206.8998\n",
      "Epoch 1646/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 199.3781 - val_loss: 154.7107\n",
      "Epoch 1647/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 185.4653 - val_loss: 183.3711\n",
      "Epoch 1648/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.5297 - val_loss: 221.1868\n",
      "Epoch 1649/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.6233 - val_loss: 156.1596\n",
      "Epoch 1650/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 153.0707 - val_loss: 175.6772\n",
      "Epoch 1651/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.5315 - val_loss: 152.1766\n",
      "Epoch 1652/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.4059 - val_loss: 154.1361\n",
      "Epoch 1653/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.2870 - val_loss: 201.8125\n",
      "Epoch 1654/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.5382 - val_loss: 160.8121\n",
      "Epoch 1655/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.4141 - val_loss: 203.5790\n",
      "Epoch 1656/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 184.3030 - val_loss: 146.8704\n",
      "Epoch 1657/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.3560 - val_loss: 187.4393\n",
      "Epoch 1658/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.2389 - val_loss: 201.3424\n",
      "Epoch 1659/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.4831 - val_loss: 152.9690\n",
      "Epoch 1660/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 150.2770 - val_loss: 212.3421\n",
      "Epoch 1661/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.0368 - val_loss: 158.3057\n",
      "Epoch 1662/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.1346 - val_loss: 153.3535\n",
      "Epoch 1663/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.7267 - val_loss: 143.8324\n",
      "Epoch 1664/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.9139 - val_loss: 306.0421\n",
      "Epoch 1665/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.5851 - val_loss: 160.3055\n",
      "Epoch 1666/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.6053 - val_loss: 167.6452\n",
      "Epoch 1667/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.8337 - val_loss: 141.6709\n",
      "Epoch 1668/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.6997 - val_loss: 170.6686\n",
      "Epoch 1669/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.2348 - val_loss: 242.6389\n",
      "Epoch 1670/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.6950 - val_loss: 142.6002\n",
      "Epoch 1671/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.8438 - val_loss: 145.5126\n",
      "Epoch 1672/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.8890 - val_loss: 150.1221\n",
      "Epoch 1673/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.2707 - val_loss: 159.7350\n",
      "Epoch 1674/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.9391 - val_loss: 206.0396\n",
      "Epoch 1675/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 142.5249 - val_loss: 174.7444\n",
      "Epoch 1676/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.0509 - val_loss: 145.7137\n",
      "Epoch 1677/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.4115 - val_loss: 143.5296\n",
      "Epoch 1678/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.7652 - val_loss: 193.5136\n",
      "Epoch 1679/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 142.7789 - val_loss: 155.0309\n",
      "Epoch 1680/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.8444 - val_loss: 229.7462\n",
      "Epoch 1681/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.5013 - val_loss: 165.7284\n",
      "Epoch 1682/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.0462 - val_loss: 166.6071\n",
      "Epoch 1683/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.4934 - val_loss: 143.7152\n",
      "Epoch 1684/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 148.6965 - val_loss: 171.0544\n",
      "Epoch 1685/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.2153 - val_loss: 171.6933\n",
      "Epoch 1686/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.7553 - val_loss: 203.4817\n",
      "Epoch 1687/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.6685 - val_loss: 221.1274\n",
      "Epoch 1688/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.9243 - val_loss: 153.3914\n",
      "Epoch 1689/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 168.1887 - val_loss: 151.1988\n",
      "Epoch 1690/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.0220 - val_loss: 142.6975\n",
      "Epoch 1691/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.3787 - val_loss: 155.1152\n",
      "Epoch 1692/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.5722 - val_loss: 150.5575\n",
      "Epoch 1693/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.3551 - val_loss: 144.2377\n",
      "Epoch 1694/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.0779 - val_loss: 181.5992\n",
      "Epoch 1695/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.0904 - val_loss: 147.6392\n",
      "Epoch 1696/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.8326 - val_loss: 138.9318\n",
      "Epoch 1697/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.5849 - val_loss: 231.2780\n",
      "Epoch 1698/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.9061 - val_loss: 139.3743\n",
      "Epoch 1699/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.2471 - val_loss: 160.3024\n",
      "Epoch 1700/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 186.0065 - val_loss: 152.7680\n",
      "Epoch 1701/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.3171 - val_loss: 163.9007\n",
      "Epoch 1702/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.8375 - val_loss: 143.4138\n",
      "Epoch 1703/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 127.8866 - val_loss: 142.8554\n",
      "Epoch 1704/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 165.5488 - val_loss: 149.1623\n",
      "Epoch 1705/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 135.3570 - val_loss: 143.0706\n",
      "Epoch 1706/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.9639 - val_loss: 163.9805\n",
      "Epoch 1707/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.0545 - val_loss: 139.2391\n",
      "Epoch 1708/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.2231 - val_loss: 148.3225\n",
      "Epoch 1709/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.6757 - val_loss: 155.1420\n",
      "Epoch 1710/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.5519 - val_loss: 159.6886\n",
      "Epoch 1711/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.7341 - val_loss: 168.7587\n",
      "Epoch 1712/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.7462 - val_loss: 152.9109\n",
      "Epoch 1713/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.9925 - val_loss: 197.5576\n",
      "Epoch 1714/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.3051 - val_loss: 168.7777\n",
      "Epoch 1715/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.1101 - val_loss: 152.7750\n",
      "Epoch 1716/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.1237 - val_loss: 156.9008\n",
      "Epoch 1717/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.5104 - val_loss: 148.8238\n",
      "Epoch 1718/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.9091 - val_loss: 198.4075\n",
      "Epoch 1719/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.8271 - val_loss: 142.3273\n",
      "Epoch 1720/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.5441 - val_loss: 145.4507\n",
      "Epoch 1721/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.2825 - val_loss: 145.4242\n",
      "Epoch 1722/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.9151 - val_loss: 143.5533\n",
      "Epoch 1723/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.8584 - val_loss: 150.9687\n",
      "Epoch 1724/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 175.1950 - val_loss: 185.0708\n",
      "Epoch 1725/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.6005 - val_loss: 186.2660\n",
      "Epoch 1726/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.8533 - val_loss: 161.1215\n",
      "Epoch 1727/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.1806 - val_loss: 139.5303\n",
      "Epoch 1728/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.2470 - val_loss: 145.9602\n",
      "Epoch 1729/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 128.1900 - val_loss: 171.4050\n",
      "Epoch 1730/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.1226 - val_loss: 200.5453\n",
      "Epoch 1731/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.4341 - val_loss: 261.3895\n",
      "Epoch 1732/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.6727 - val_loss: 143.0131\n",
      "Epoch 1733/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.4035 - val_loss: 157.7473\n",
      "Epoch 1734/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.0126 - val_loss: 231.9992\n",
      "Epoch 1735/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.7996 - val_loss: 184.0041\n",
      "Epoch 1736/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.5302 - val_loss: 145.9146\n",
      "Epoch 1737/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.9999 - val_loss: 156.6142\n",
      "Epoch 1738/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.9247 - val_loss: 154.5098\n",
      "Epoch 1739/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.9933 - val_loss: 141.4577\n",
      "Epoch 1740/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.5122 - val_loss: 148.2304\n",
      "Epoch 1741/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.0409 - val_loss: 146.8623\n",
      "Epoch 1742/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.3660 - val_loss: 162.7229\n",
      "Epoch 1743/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.8592 - val_loss: 139.3908\n",
      "Epoch 1744/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.9035 - val_loss: 187.5866\n",
      "Epoch 1745/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.0078 - val_loss: 152.3199\n",
      "Epoch 1746/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.5757 - val_loss: 170.5668\n",
      "Epoch 1747/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.9638 - val_loss: 149.3676\n",
      "Epoch 1748/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 256.0749 - val_loss: 188.4541\n",
      "Epoch 1749/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 190.9378 - val_loss: 159.2282\n",
      "Epoch 1750/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.3999 - val_loss: 148.7059\n",
      "Epoch 1751/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.2044 - val_loss: 154.7138\n",
      "Epoch 1752/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 131.9738 - val_loss: 145.6870\n",
      "Epoch 1753/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.2421 - val_loss: 181.1941\n",
      "Epoch 1754/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.5898 - val_loss: 208.7697\n",
      "Epoch 1755/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.9908 - val_loss: 141.0860\n",
      "Epoch 1756/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.1003 - val_loss: 153.8419\n",
      "Epoch 1757/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.1111 - val_loss: 172.1341\n",
      "Epoch 1758/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.2552 - val_loss: 155.4026\n",
      "Epoch 1759/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.3100 - val_loss: 167.1351\n",
      "Epoch 1760/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.4889 - val_loss: 145.5340\n",
      "Epoch 1761/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.1459 - val_loss: 161.8075\n",
      "Epoch 1762/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.3169 - val_loss: 203.3105\n",
      "Epoch 1763/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.7872 - val_loss: 228.5588\n",
      "Epoch 1764/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.0865 - val_loss: 232.0719\n",
      "Epoch 1765/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 148.0393 - val_loss: 164.8818\n",
      "Epoch 1766/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 149.3031 - val_loss: 195.9864\n",
      "Epoch 1767/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 145.6185 - val_loss: 144.3179\n",
      "Epoch 1768/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.2728 - val_loss: 150.9938\n",
      "Epoch 1769/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.2250 - val_loss: 146.8926\n",
      "Epoch 1770/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.1576 - val_loss: 156.7609\n",
      "Epoch 1771/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 135.7734 - val_loss: 241.4713\n",
      "Epoch 1772/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.7639 - val_loss: 149.1506\n",
      "Epoch 1773/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.6310 - val_loss: 146.3259\n",
      "Epoch 1774/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.1345 - val_loss: 151.1192\n",
      "Epoch 1775/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.0947 - val_loss: 155.6505\n",
      "Epoch 1776/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.3198 - val_loss: 184.7278\n",
      "Epoch 1777/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.5110 - val_loss: 170.5231\n",
      "Epoch 1778/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.5509 - val_loss: 175.5157\n",
      "Epoch 1779/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.5661 - val_loss: 143.5371\n",
      "Epoch 1780/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.8480 - val_loss: 139.9185\n",
      "Epoch 1781/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.2288 - val_loss: 161.0299\n",
      "Epoch 1782/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.2298 - val_loss: 186.1932\n",
      "Epoch 1783/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.8507 - val_loss: 143.9055\n",
      "Epoch 1784/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.1666 - val_loss: 143.9699\n",
      "Epoch 1785/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.9478 - val_loss: 146.3516\n",
      "Epoch 1786/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.8734 - val_loss: 142.6327\n",
      "Epoch 1787/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.3834 - val_loss: 262.1298\n",
      "Epoch 1788/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.3693 - val_loss: 194.5636\n",
      "Epoch 1789/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.3395 - val_loss: 195.0065\n",
      "Epoch 1790/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.2818 - val_loss: 148.3717\n",
      "Epoch 1791/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.1966 - val_loss: 148.9187\n",
      "Epoch 1792/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.2970 - val_loss: 178.1201\n",
      "Epoch 1793/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.1925 - val_loss: 166.3397\n",
      "Epoch 1794/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.7905 - val_loss: 147.0994\n",
      "Epoch 1795/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.4848 - val_loss: 310.3358\n",
      "Epoch 1796/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.4183 - val_loss: 140.1882\n",
      "Epoch 1797/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 150.9985 - val_loss: 157.0945\n",
      "Epoch 1798/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.3357 - val_loss: 144.0351\n",
      "Epoch 1799/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.6617 - val_loss: 141.5792\n",
      "Epoch 1800/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 171.8848 - val_loss: 145.7230\n",
      "Epoch 1801/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.9875 - val_loss: 145.4991\n",
      "Epoch 1802/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.2558 - val_loss: 160.5401\n",
      "Epoch 1803/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.2991 - val_loss: 154.4688\n",
      "Epoch 1804/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.9972 - val_loss: 153.2157\n",
      "Epoch 1805/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.0052 - val_loss: 142.1916\n",
      "Epoch 1806/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.8795 - val_loss: 152.8845\n",
      "Epoch 1807/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.9067 - val_loss: 146.6694\n",
      "Epoch 1808/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.5563 - val_loss: 144.4702\n",
      "Epoch 1809/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.1996 - val_loss: 146.6305\n",
      "Epoch 1810/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.0415 - val_loss: 159.9245\n",
      "Epoch 1811/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.7841 - val_loss: 147.1554\n",
      "Epoch 1812/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.0057 - val_loss: 453.9969\n",
      "Epoch 1813/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.4710 - val_loss: 163.0795\n",
      "Epoch 1814/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.8938 - val_loss: 192.8458\n",
      "Epoch 1815/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.1553 - val_loss: 151.6793\n",
      "Epoch 1816/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 240.4555 - val_loss: 244.6078\n",
      "Epoch 1817/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 187.5539 - val_loss: 197.0903\n",
      "Epoch 1818/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.4198 - val_loss: 174.5988\n",
      "Epoch 1819/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.6513 - val_loss: 234.1682\n",
      "Epoch 1820/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.6665 - val_loss: 209.0257\n",
      "Epoch 1821/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 184.0744 - val_loss: 153.0828\n",
      "Epoch 1822/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 168.9763 - val_loss: 187.2630\n",
      "Epoch 1823/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.6366 - val_loss: 196.9996\n",
      "Epoch 1824/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.2541 - val_loss: 198.4324\n",
      "Epoch 1825/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.9865 - val_loss: 156.5429\n",
      "Epoch 1826/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.8554 - val_loss: 144.2538\n",
      "Epoch 1827/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 145.4380 - val_loss: 200.6198\n",
      "Epoch 1828/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 134.4436 - val_loss: 156.5406\n",
      "Epoch 1829/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 141.3428 - val_loss: 164.8011\n",
      "Epoch 1830/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.9803 - val_loss: 141.5801\n",
      "Epoch 1831/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.4017 - val_loss: 179.5671\n",
      "Epoch 1832/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.5587 - val_loss: 172.0626\n",
      "Epoch 1833/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.7417 - val_loss: 186.2722\n",
      "Epoch 1834/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.9011 - val_loss: 170.8341\n",
      "Epoch 1835/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.5855 - val_loss: 300.9006\n",
      "Epoch 1836/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.8953 - val_loss: 260.3261\n",
      "Epoch 1837/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.3676 - val_loss: 187.1319\n",
      "Epoch 1838/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 293.5155 - val_loss: 211.6889\n",
      "Epoch 1839/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.8541 - val_loss: 172.3229\n",
      "Epoch 1840/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.3715 - val_loss: 174.7585\n",
      "Epoch 1841/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.2541 - val_loss: 148.0470\n",
      "Epoch 1842/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.2497 - val_loss: 151.4979\n",
      "Epoch 1843/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.2762 - val_loss: 201.0488\n",
      "Epoch 1844/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.6605 - val_loss: 145.2851\n",
      "Epoch 1845/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.3957 - val_loss: 157.9043\n",
      "Epoch 1846/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.9089 - val_loss: 167.5437\n",
      "Epoch 1847/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.5702 - val_loss: 144.6034\n",
      "Epoch 1848/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.3714 - val_loss: 143.5628\n",
      "Epoch 1849/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.8916 - val_loss: 186.4638\n",
      "Epoch 1850/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.4749 - val_loss: 213.1928\n",
      "Epoch 1851/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.1382 - val_loss: 138.5138\n",
      "Epoch 1852/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.6946 - val_loss: 201.7740\n",
      "Epoch 1853/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.9137 - val_loss: 171.1162\n",
      "Epoch 1854/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.1700 - val_loss: 169.0453\n",
      "Epoch 1855/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.2254 - val_loss: 253.2715\n",
      "Epoch 1856/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 183.0094 - val_loss: 152.0289\n",
      "Epoch 1857/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 128.9643 - val_loss: 162.2352\n",
      "Epoch 1858/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.7419 - val_loss: 143.5879\n",
      "Epoch 1859/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.2470 - val_loss: 169.8696\n",
      "Epoch 1860/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.2563 - val_loss: 156.6829\n",
      "Epoch 1861/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 135.0281 - val_loss: 157.1065\n",
      "Epoch 1862/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.3573 - val_loss: 212.9667\n",
      "Epoch 1863/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 178.4362 - val_loss: 171.8433\n",
      "Epoch 1864/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.3653 - val_loss: 141.2969\n",
      "Epoch 1865/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.3876 - val_loss: 164.4518\n",
      "Epoch 1866/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.0545 - val_loss: 155.0542\n",
      "Epoch 1867/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.7114 - val_loss: 152.8683\n",
      "Epoch 1868/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.9586 - val_loss: 174.5905\n",
      "Epoch 1869/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.4926 - val_loss: 154.3965\n",
      "Epoch 1870/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.6448 - val_loss: 192.5774\n",
      "Epoch 1871/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 137.5936 - val_loss: 151.7138\n",
      "Epoch 1872/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 131.9213 - val_loss: 182.1361\n",
      "Epoch 1873/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.3412 - val_loss: 186.0548\n",
      "Epoch 1874/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.4992 - val_loss: 145.9694\n",
      "Epoch 1875/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.9372 - val_loss: 143.6094\n",
      "Epoch 1876/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.5793 - val_loss: 163.1368\n",
      "Epoch 1877/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.1461 - val_loss: 145.3080\n",
      "Epoch 1878/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.2285 - val_loss: 155.8102\n",
      "Epoch 1879/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.0440 - val_loss: 152.2573\n",
      "Epoch 1880/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.3315 - val_loss: 152.0811\n",
      "Epoch 1881/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 184.2698 - val_loss: 180.4861\n",
      "Epoch 1882/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.7636 - val_loss: 206.2132\n",
      "Epoch 1883/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.6464 - val_loss: 178.7147\n",
      "Epoch 1884/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.6530 - val_loss: 241.4896\n",
      "Epoch 1885/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.2936 - val_loss: 192.7437\n",
      "Epoch 1886/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.3454 - val_loss: 162.9846\n",
      "Epoch 1887/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.6010 - val_loss: 160.3224\n",
      "Epoch 1888/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.9923 - val_loss: 171.8330\n",
      "Epoch 1889/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 139.8070 - val_loss: 160.9134\n",
      "Epoch 1890/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 143.3553 - val_loss: 165.5396\n",
      "Epoch 1891/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 161.1160 - val_loss: 168.0804\n",
      "Epoch 1892/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.0665 - val_loss: 145.8691\n",
      "Epoch 1893/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 150.0019 - val_loss: 264.6984\n",
      "Epoch 1894/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.2288 - val_loss: 150.0139\n",
      "Epoch 1895/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.7866 - val_loss: 155.2253\n",
      "Epoch 1896/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.4087 - val_loss: 187.0563\n",
      "Epoch 1897/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.4430 - val_loss: 213.1361\n",
      "Epoch 1898/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.1380 - val_loss: 148.0234\n",
      "Epoch 1899/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.0874 - val_loss: 155.3014\n",
      "Epoch 1900/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.5224 - val_loss: 161.8853\n",
      "Epoch 1901/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.3363 - val_loss: 157.9378\n",
      "Epoch 1902/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.8187 - val_loss: 148.2327\n",
      "Epoch 1903/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 149.1150 - val_loss: 145.8074\n",
      "Epoch 1904/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 138.3323 - val_loss: 157.4511\n",
      "Epoch 1905/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.1033 - val_loss: 155.3434\n",
      "Epoch 1906/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.1672 - val_loss: 212.2511\n",
      "Epoch 1907/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.9628 - val_loss: 152.6733\n",
      "Epoch 1908/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.6382 - val_loss: 148.8568\n",
      "Epoch 1909/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.2092 - val_loss: 161.1042\n",
      "Epoch 1910/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.1433 - val_loss: 162.6258\n",
      "Epoch 1911/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.1253 - val_loss: 143.8277\n",
      "Epoch 1912/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.9015 - val_loss: 149.7621\n",
      "Epoch 1913/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.3774 - val_loss: 191.8729\n",
      "Epoch 1914/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 171.1051 - val_loss: 178.7433\n",
      "Epoch 1915/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.0546 - val_loss: 152.1744\n",
      "Epoch 1916/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.7127 - val_loss: 250.5357\n",
      "Epoch 1917/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 218.2766 - val_loss: 160.4009\n",
      "Epoch 1918/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.9852 - val_loss: 194.8740\n",
      "Epoch 1919/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.3304 - val_loss: 161.1828\n",
      "Epoch 1920/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.6212 - val_loss: 187.8435\n",
      "Epoch 1921/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.1378 - val_loss: 183.8440\n",
      "Epoch 1922/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.6028 - val_loss: 153.1154\n",
      "Epoch 1923/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.0253 - val_loss: 162.5141\n",
      "Epoch 1924/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 142.5729 - val_loss: 142.7707\n",
      "Epoch 1925/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.7941 - val_loss: 386.5901\n",
      "Epoch 1926/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.7620 - val_loss: 157.5993\n",
      "Epoch 1927/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.2941 - val_loss: 159.1811\n",
      "Epoch 1928/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.4327 - val_loss: 153.9483\n",
      "Epoch 1929/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.1063 - val_loss: 158.2285\n",
      "Epoch 1930/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.5528 - val_loss: 164.8670\n",
      "Epoch 1931/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.6434 - val_loss: 154.3208\n",
      "Epoch 1932/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.7927 - val_loss: 145.6621\n",
      "Epoch 1933/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.6391 - val_loss: 159.4930\n",
      "Epoch 1934/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.3877 - val_loss: 146.8933\n",
      "Epoch 1935/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.6862 - val_loss: 158.6337\n",
      "Epoch 1936/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.0174 - val_loss: 190.1850\n",
      "Epoch 1937/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.1051 - val_loss: 178.7301\n",
      "Epoch 1938/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.5509 - val_loss: 200.5335\n",
      "Epoch 1939/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 207.5123 - val_loss: 147.2574\n",
      "Epoch 1940/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.2518 - val_loss: 176.9214\n",
      "Epoch 1941/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.1868 - val_loss: 168.6602\n",
      "Epoch 1942/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 164.8203 - val_loss: 164.1314\n",
      "Epoch 1943/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 176.9867 - val_loss: 169.5000\n",
      "Epoch 1944/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.4841 - val_loss: 166.0802\n",
      "Epoch 1945/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.7226 - val_loss: 168.9525\n",
      "Epoch 1946/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.8829 - val_loss: 242.6813\n",
      "Epoch 1947/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 150.8090 - val_loss: 164.6578\n",
      "Epoch 1948/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 129.8699 - val_loss: 197.1596\n",
      "Epoch 1949/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.8841 - val_loss: 192.3841\n",
      "Epoch 1950/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 159.6753 - val_loss: 161.3821\n",
      "Epoch 1951/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 144.6202 - val_loss: 159.2517\n",
      "Epoch 1952/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 137.2839 - val_loss: 142.8320\n",
      "Epoch 1953/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.3825 - val_loss: 170.2318\n",
      "Epoch 1954/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.1468 - val_loss: 198.4923\n",
      "Epoch 1955/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.0805 - val_loss: 403.1312\n",
      "Epoch 1956/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.2608 - val_loss: 150.4635\n",
      "Epoch 1957/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.4233 - val_loss: 144.6340\n",
      "Epoch 1958/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.3290 - val_loss: 149.9873\n",
      "Epoch 1959/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.4900 - val_loss: 169.3529\n",
      "Epoch 1960/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.9538 - val_loss: 158.7634\n",
      "Epoch 1961/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.8220 - val_loss: 165.3925\n",
      "Epoch 1962/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 171.7056 - val_loss: 177.0888\n",
      "Epoch 1963/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.2495 - val_loss: 149.0918\n",
      "Epoch 1964/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.9173 - val_loss: 180.7029\n",
      "Epoch 1965/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.4627 - val_loss: 144.0179\n",
      "Epoch 1966/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.8573 - val_loss: 198.1240\n",
      "Epoch 1967/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.6228 - val_loss: 160.6960\n",
      "Epoch 1968/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.5589 - val_loss: 177.0882\n",
      "Epoch 1969/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.4082 - val_loss: 162.5730\n",
      "Epoch 1970/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.4791 - val_loss: 152.9467\n",
      "Epoch 1971/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.0325 - val_loss: 182.3153\n",
      "Epoch 1972/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.0003 - val_loss: 151.7739\n",
      "Epoch 1973/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.4340 - val_loss: 158.6440\n",
      "Epoch 1974/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.0136 - val_loss: 151.8542\n",
      "Epoch 1975/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.4673 - val_loss: 214.9428\n",
      "Epoch 1976/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.8330 - val_loss: 193.1242\n",
      "Epoch 1977/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.7162 - val_loss: 146.5191\n",
      "Epoch 1978/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.7146 - val_loss: 189.1672\n",
      "Epoch 1979/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.3756 - val_loss: 163.7900\n",
      "Epoch 1980/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.3210 - val_loss: 155.9325\n",
      "Epoch 1981/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.6596 - val_loss: 160.7442\n",
      "Epoch 1982/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.1533 - val_loss: 152.9140\n",
      "Epoch 1983/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.6424 - val_loss: 187.2919\n",
      "Epoch 1984/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.1543 - val_loss: 147.7533\n",
      "Epoch 1985/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.0697 - val_loss: 146.9137\n",
      "Epoch 1986/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.0072 - val_loss: 146.9695\n",
      "Epoch 1987/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.0075 - val_loss: 155.6518\n",
      "Epoch 1988/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.1817 - val_loss: 179.0154\n",
      "Epoch 1989/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.0373 - val_loss: 169.3481\n",
      "Epoch 1990/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.5927 - val_loss: 160.4306\n",
      "Epoch 1991/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.2375 - val_loss: 154.3873\n",
      "Epoch 1992/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.3581 - val_loss: 176.9627\n",
      "Epoch 1993/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.4360 - val_loss: 162.7127\n",
      "Epoch 1994/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.5949 - val_loss: 195.9157\n",
      "Epoch 1995/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.6148 - val_loss: 171.4694\n",
      "Epoch 1996/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.2970 - val_loss: 231.1268\n",
      "Epoch 1997/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 174.5464 - val_loss: 164.9023\n",
      "Epoch 1998/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.5206 - val_loss: 172.6540\n",
      "Epoch 1999/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.6769 - val_loss: 154.7879\n",
      "Epoch 2000/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.7187 - val_loss: 144.3353\n",
      "Epoch 2001/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.9980 - val_loss: 153.7356\n",
      "Epoch 2002/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.5350 - val_loss: 207.0510\n",
      "Epoch 2003/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.1543 - val_loss: 172.5123\n",
      "Epoch 2004/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.5626 - val_loss: 184.2414\n",
      "Epoch 2005/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.5124 - val_loss: 144.7786\n",
      "Epoch 2006/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.6810 - val_loss: 201.6726\n",
      "Epoch 2007/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.7410 - val_loss: 159.7296\n",
      "Epoch 2008/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.5944 - val_loss: 303.3723\n",
      "Epoch 2009/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.7964 - val_loss: 146.2032\n",
      "Epoch 2010/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.1227 - val_loss: 147.5024\n",
      "Epoch 2011/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.5049 - val_loss: 227.9309\n",
      "Epoch 2012/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 147.8626 - val_loss: 190.0171\n",
      "Epoch 2013/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 146.2966 - val_loss: 169.1274\n",
      "Epoch 2014/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 137.9145 - val_loss: 166.2091\n",
      "Epoch 2015/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.6418 - val_loss: 146.6909\n",
      "Epoch 2016/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.6230 - val_loss: 181.1163\n",
      "Epoch 2017/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.1958 - val_loss: 153.3088\n",
      "Epoch 2018/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.7873 - val_loss: 173.7226\n",
      "Epoch 2019/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.4343 - val_loss: 154.5498\n",
      "Epoch 2020/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.3411 - val_loss: 146.9102\n",
      "Epoch 2021/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.0576 - val_loss: 147.6568\n",
      "Epoch 2022/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.6987 - val_loss: 161.1761\n",
      "Epoch 2023/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.1353 - val_loss: 147.4565\n",
      "Epoch 2024/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.9775 - val_loss: 145.1947\n",
      "Epoch 2025/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.4407 - val_loss: 161.9147\n",
      "Epoch 2026/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 173.9309 - val_loss: 149.9204\n",
      "Epoch 2027/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.0380 - val_loss: 168.6790\n",
      "Epoch 2028/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.4444 - val_loss: 166.1838\n",
      "Epoch 2029/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.8791 - val_loss: 172.3140\n",
      "Epoch 2030/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.6522 - val_loss: 172.1008\n",
      "Epoch 2031/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.4902 - val_loss: 154.4594\n",
      "Epoch 2032/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.0972 - val_loss: 153.1286\n",
      "Epoch 2033/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.4299 - val_loss: 153.0012\n",
      "Epoch 2034/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.3207 - val_loss: 161.9299\n",
      "Epoch 2035/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 188.9821 - val_loss: 194.8097\n",
      "Epoch 2036/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.3437 - val_loss: 141.1624\n",
      "Epoch 2037/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 133.2446 - val_loss: 153.6199\n",
      "Epoch 2038/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.3479 - val_loss: 168.0314\n",
      "Epoch 2039/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.2227 - val_loss: 198.0074\n",
      "Epoch 2040/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.3053 - val_loss: 154.9811\n",
      "Epoch 2041/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.8055 - val_loss: 152.5044\n",
      "Epoch 2042/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.3001 - val_loss: 142.6005\n",
      "Epoch 2043/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.1918 - val_loss: 188.8524\n",
      "Epoch 2044/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.0105 - val_loss: 147.3505\n",
      "Epoch 2045/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.3983 - val_loss: 267.1915\n",
      "Epoch 2046/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.4372 - val_loss: 154.4826\n",
      "Epoch 2047/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 171.8772 - val_loss: 147.4530\n",
      "Epoch 2048/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.4154 - val_loss: 158.5074\n",
      "Epoch 2049/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 134.4055 - val_loss: 273.9136\n",
      "Epoch 2050/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.4852 - val_loss: 191.4104\n",
      "Epoch 2051/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 132.0810 - val_loss: 147.4193\n",
      "Epoch 2052/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.8212 - val_loss: 148.6528\n",
      "Epoch 2053/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.3497 - val_loss: 228.3310\n",
      "Epoch 2054/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 132.4747 - val_loss: 188.5078\n",
      "Epoch 2055/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 153.9123 - val_loss: 142.6880\n",
      "Epoch 2056/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 140.7546 - val_loss: 168.7802\n",
      "Epoch 2057/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.0987 - val_loss: 172.5689\n",
      "Epoch 2058/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.3602 - val_loss: 157.0399\n",
      "Epoch 2059/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.1314 - val_loss: 144.7877\n",
      "Epoch 2060/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 187.3519 - val_loss: 169.1936\n",
      "Epoch 2061/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.7288 - val_loss: 157.0326\n",
      "Epoch 2062/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.8598 - val_loss: 149.6608\n",
      "Epoch 2063/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.5447 - val_loss: 155.9371\n",
      "Epoch 2064/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.1041 - val_loss: 149.6607\n",
      "Epoch 2065/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.1104 - val_loss: 184.5675\n",
      "Epoch 2066/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.1987 - val_loss: 149.8150\n",
      "Epoch 2067/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 169.7655 - val_loss: 216.7199\n",
      "Epoch 2068/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.7639 - val_loss: 163.8979\n",
      "Epoch 2069/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.2050 - val_loss: 193.1951\n",
      "Epoch 2070/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.8362 - val_loss: 160.4825\n",
      "Epoch 2071/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.4053 - val_loss: 146.3403\n",
      "Epoch 2072/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.3210 - val_loss: 228.7306\n",
      "Epoch 2073/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.9285 - val_loss: 150.1070\n",
      "Epoch 2074/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 131.3622 - val_loss: 149.1160\n",
      "Epoch 2075/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.6483 - val_loss: 155.4041\n",
      "Epoch 2076/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.8313 - val_loss: 246.9469\n",
      "Epoch 2077/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.4065 - val_loss: 293.0370\n",
      "Epoch 2078/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.0792 - val_loss: 163.3820\n",
      "Epoch 2079/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.9000 - val_loss: 286.4545\n",
      "Epoch 2080/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.3199 - val_loss: 168.8054\n",
      "Epoch 2081/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 135.0858 - val_loss: 157.0179\n",
      "Epoch 2082/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 146.0667 - val_loss: 260.0181\n",
      "Epoch 2083/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 142.4191 - val_loss: 151.2754\n",
      "Epoch 2084/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 151.6935 - val_loss: 175.3308\n",
      "Epoch 2085/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.0100 - val_loss: 156.8187\n",
      "Epoch 2086/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.4905 - val_loss: 240.2742\n",
      "Epoch 2087/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.7552 - val_loss: 175.9824\n",
      "Epoch 2088/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 127.6330 - val_loss: 165.0378\n",
      "Epoch 2089/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.8281 - val_loss: 149.9612\n",
      "Epoch 2090/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 149.7290 - val_loss: 156.9631\n",
      "Epoch 2091/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 174.1331 - val_loss: 187.2724\n",
      "Epoch 2092/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 145.3442 - val_loss: 151.2628\n",
      "Epoch 2093/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.6840 - val_loss: 145.7407\n",
      "Epoch 2094/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.8356 - val_loss: 148.4134\n",
      "Epoch 2095/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.3259 - val_loss: 187.6040\n",
      "Epoch 2096/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.7163 - val_loss: 173.2279\n",
      "Epoch 2097/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 144.3663 - val_loss: 158.2879\n",
      "Epoch 2098/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 135.6342 - val_loss: 183.2684\n",
      "Epoch 2099/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 137.6603 - val_loss: 152.1734\n",
      "Epoch 2100/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 140.1402 - val_loss: 173.4585\n",
      "Epoch 2101/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 126.7645 - val_loss: 183.2913\n",
      "Epoch 2102/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 133.7463 - val_loss: 142.6747\n",
      "Epoch 2103/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 134.5216 - val_loss: 151.8606\n",
      "Epoch 2104/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 145.4670 - val_loss: 160.0075\n",
      "Epoch 2105/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 134.9127 - val_loss: 173.2961\n",
      "Epoch 2106/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.3940 - val_loss: 151.6326\n",
      "Epoch 2107/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 137.7405 - val_loss: 152.5827\n",
      "Epoch 2108/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 135.6740 - val_loss: 176.4846\n",
      "Epoch 2109/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 136.2432 - val_loss: 180.1078\n",
      "Epoch 2110/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 139.5561 - val_loss: 145.7305\n",
      "Epoch 2111/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 148.9101 - val_loss: 152.7635\n",
      "Epoch 2112/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 135.6250 - val_loss: 165.0499\n",
      "Epoch 2113/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 167.0138 - val_loss: 162.2269\n",
      "Epoch 2114/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 151.6413 - val_loss: 182.4373\n",
      "Epoch 2115/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 140.9895 - val_loss: 158.1948\n",
      "Epoch 2116/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 130.7478 - val_loss: 149.1544\n",
      "Epoch 2117/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 147.9196 - val_loss: 174.9981\n",
      "Epoch 2118/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 155.4114 - val_loss: 151.9057\n",
      "Epoch 2119/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 131.5310 - val_loss: 149.2455\n",
      "Epoch 2120/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 126.2817 - val_loss: 146.2858\n",
      "Epoch 2121/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.1764 - val_loss: 144.0447\n",
      "Epoch 2122/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 136.5371 - val_loss: 165.3459\n",
      "Epoch 2123/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 129.1031 - val_loss: 144.5604\n",
      "Epoch 2124/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.6809 - val_loss: 146.5177\n",
      "Epoch 2125/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 140.6159 - val_loss: 143.0202\n",
      "Epoch 2126/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.1832 - val_loss: 173.6154\n",
      "Epoch 2127/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 144.4246 - val_loss: 145.8149\n",
      "Epoch 2128/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 142.2906 - val_loss: 161.2292\n",
      "Epoch 2129/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 138.5059 - val_loss: 185.4817\n",
      "Epoch 2130/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 135.5238 - val_loss: 177.1373\n",
      "Epoch 2131/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.3589 - val_loss: 164.6775\n",
      "Epoch 2132/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 165.5752 - val_loss: 143.5690\n",
      "Epoch 2133/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 132.6255 - val_loss: 152.2100\n",
      "Epoch 2134/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 145.7092 - val_loss: 145.8774\n",
      "Epoch 2135/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 132.8806 - val_loss: 151.8145\n",
      "Epoch 2136/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 131.5336 - val_loss: 143.2539\n",
      "Epoch 2137/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 139.7844 - val_loss: 155.2689\n",
      "Epoch 2138/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 162.2545 - val_loss: 173.9656\n",
      "Epoch 2139/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 133.2423 - val_loss: 168.6059\n",
      "Epoch 2140/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 132.9348 - val_loss: 169.2382\n",
      "Epoch 2141/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 138.3020 - val_loss: 154.2874\n",
      "Epoch 2142/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 128.3315 - val_loss: 144.8657\n",
      "Epoch 2143/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 146.1713 - val_loss: 152.6683\n",
      "Epoch 2144/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 63us/step - loss: 130.5785 - val_loss: 148.6226\n",
      "Epoch 2145/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.1623 - val_loss: 146.2623\n",
      "Epoch 2146/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.6193 - val_loss: 148.7091\n",
      "Epoch 2147/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.9687 - val_loss: 159.0040\n",
      "Epoch 2148/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.7768 - val_loss: 176.5267\n",
      "Epoch 2149/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.9554 - val_loss: 165.4965\n",
      "Epoch 2150/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.0247 - val_loss: 182.5933\n",
      "Epoch 2151/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 170.4923 - val_loss: 153.9401\n",
      "Epoch 2152/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.5821 - val_loss: 163.1142\n",
      "Epoch 2153/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.3448 - val_loss: 155.3075\n",
      "Epoch 2154/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.6041 - val_loss: 139.6338\n",
      "Epoch 2155/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.5872 - val_loss: 165.9367\n",
      "Epoch 2156/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.3500 - val_loss: 161.9917\n",
      "Epoch 2157/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.3906 - val_loss: 165.9312\n",
      "Epoch 2158/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.0179 - val_loss: 165.6130\n",
      "Epoch 2159/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.1385 - val_loss: 148.0571\n",
      "Epoch 2160/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 135.3308 - val_loss: 160.6878\n",
      "Epoch 2161/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 174.5296 - val_loss: 169.0018\n",
      "Epoch 2162/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 140.6390 - val_loss: 163.0846\n",
      "Epoch 2163/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.7406 - val_loss: 214.8313\n",
      "Epoch 2164/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.3460 - val_loss: 187.5035\n",
      "Epoch 2165/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.9116 - val_loss: 172.6980\n",
      "Epoch 2166/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.4481 - val_loss: 158.7222\n",
      "Epoch 2167/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.7675 - val_loss: 173.7550\n",
      "Epoch 2168/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 149.9547 - val_loss: 230.5371\n",
      "Epoch 2169/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.6233 - val_loss: 298.5402\n",
      "Epoch 2170/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.4390 - val_loss: 166.7396\n",
      "Epoch 2171/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.5963 - val_loss: 147.9236\n",
      "Epoch 2172/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.1187 - val_loss: 146.0745\n",
      "Epoch 2173/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.7408 - val_loss: 143.6046\n",
      "Epoch 2174/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.3355 - val_loss: 152.1901\n",
      "Epoch 2175/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.9201 - val_loss: 263.5141\n",
      "Epoch 2176/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.3498 - val_loss: 146.2708\n",
      "Epoch 2177/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.3537 - val_loss: 169.6257\n",
      "Epoch 2178/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.0440 - val_loss: 143.7894\n",
      "Epoch 2179/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.9821 - val_loss: 158.4755\n",
      "Epoch 2180/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.2144 - val_loss: 151.1091\n",
      "Epoch 2181/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.8294 - val_loss: 172.4970\n",
      "Epoch 2182/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.2528 - val_loss: 147.6453\n",
      "Epoch 2183/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.6071 - val_loss: 186.1696\n",
      "Epoch 2184/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.0934 - val_loss: 154.7095\n",
      "Epoch 2185/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 133.7568 - val_loss: 151.3844\n",
      "Epoch 2186/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 131.2958 - val_loss: 154.6861\n",
      "Epoch 2187/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 128.0904 - val_loss: 143.2572\n",
      "Epoch 2188/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.7498 - val_loss: 170.5353\n",
      "Epoch 2189/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.4943 - val_loss: 182.0802\n",
      "Epoch 2190/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.9648 - val_loss: 164.0134\n",
      "Epoch 2191/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 134.1653 - val_loss: 141.6913\n",
      "Epoch 2192/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.4976 - val_loss: 162.4730\n",
      "Epoch 2193/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.7169 - val_loss: 142.9225\n",
      "Epoch 2194/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.2661 - val_loss: 154.1342\n",
      "Epoch 2195/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 171.4863 - val_loss: 495.6197\n",
      "Epoch 2196/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.7754 - val_loss: 148.7622\n",
      "Epoch 2197/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.9046 - val_loss: 144.3034\n",
      "Epoch 2198/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.6285 - val_loss: 166.4268\n",
      "Epoch 2199/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.3630 - val_loss: 144.4845\n",
      "Epoch 2200/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.3137 - val_loss: 151.3473\n",
      "Epoch 2201/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 139.0285 - val_loss: 190.2794\n",
      "Epoch 2202/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.7030 - val_loss: 166.9337\n",
      "Epoch 2203/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.3486 - val_loss: 145.5698\n",
      "Epoch 2204/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 131.0589 - val_loss: 151.5987\n",
      "Epoch 2205/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.3908 - val_loss: 149.7596\n",
      "Epoch 2206/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.7162 - val_loss: 155.3911\n",
      "Epoch 2207/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.0921 - val_loss: 147.3741\n",
      "Epoch 2208/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.4004 - val_loss: 171.8896\n",
      "Epoch 2209/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.1814 - val_loss: 146.4763\n",
      "Epoch 2210/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.8591 - val_loss: 155.1274\n",
      "Epoch 2211/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.4294 - val_loss: 151.1036\n",
      "Epoch 2212/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.0551 - val_loss: 289.5048\n",
      "Epoch 2213/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 177.7035 - val_loss: 162.4123\n",
      "Epoch 2214/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.8919 - val_loss: 147.6572\n",
      "Epoch 2215/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.9293 - val_loss: 151.1461\n",
      "Epoch 2216/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 186.9659 - val_loss: 184.3074\n",
      "Epoch 2217/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.4173 - val_loss: 146.0549\n",
      "Epoch 2218/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.3673 - val_loss: 182.2712\n",
      "Epoch 2219/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.4915 - val_loss: 147.9940\n",
      "Epoch 2220/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.8438 - val_loss: 149.1363\n",
      "Epoch 2221/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.5606 - val_loss: 147.4569\n",
      "Epoch 2222/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.4650 - val_loss: 150.1128\n",
      "Epoch 2223/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.9531 - val_loss: 143.1641\n",
      "Epoch 2224/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.1703 - val_loss: 177.5160\n",
      "Epoch 2225/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.4298 - val_loss: 273.8694\n",
      "Epoch 2226/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.5294 - val_loss: 155.9839\n",
      "Epoch 2227/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.3590 - val_loss: 166.5523\n",
      "Epoch 2228/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.7582 - val_loss: 155.0619\n",
      "Epoch 2229/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.0993 - val_loss: 147.3934\n",
      "Epoch 2230/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.5152 - val_loss: 155.1736\n",
      "Epoch 2231/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.2873 - val_loss: 144.0191\n",
      "Epoch 2232/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.9129 - val_loss: 151.6784\n",
      "Epoch 2233/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.8688 - val_loss: 263.9395\n",
      "Epoch 2234/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.1422 - val_loss: 158.1395\n",
      "Epoch 2235/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.3433 - val_loss: 154.7074\n",
      "Epoch 2236/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.5038 - val_loss: 170.6679\n",
      "Epoch 2237/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.0951 - val_loss: 160.2795\n",
      "Epoch 2238/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 129.0062 - val_loss: 167.2439\n",
      "Epoch 2239/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 132.0438 - val_loss: 171.8333\n",
      "Epoch 2240/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.9617 - val_loss: 152.0125\n",
      "Epoch 2241/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 177.3732 - val_loss: 170.9862\n",
      "Epoch 2242/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.8556 - val_loss: 204.2185\n",
      "Epoch 2243/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.0630 - val_loss: 160.8075\n",
      "Epoch 2244/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.1349 - val_loss: 144.2734\n",
      "Epoch 2245/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 165.1424 - val_loss: 172.8296\n",
      "Epoch 2246/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 168.1565 - val_loss: 209.2105\n",
      "Epoch 2247/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 133.2705 - val_loss: 145.2587\n",
      "Epoch 2248/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 139.2312 - val_loss: 143.6720\n",
      "Epoch 2249/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 126.4499 - val_loss: 168.5056\n",
      "Epoch 2250/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.6220 - val_loss: 145.0639\n",
      "Epoch 2251/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 181.9266 - val_loss: 168.4316\n",
      "Epoch 2252/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.7504 - val_loss: 149.1557\n",
      "Epoch 2253/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.1228 - val_loss: 148.7385\n",
      "Epoch 2254/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 126.6991 - val_loss: 173.2388\n",
      "Epoch 2255/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.2220 - val_loss: 144.9648\n",
      "Epoch 2256/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.9086 - val_loss: 143.5281\n",
      "Epoch 2257/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.7208 - val_loss: 209.7383\n",
      "Epoch 2258/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 137.7468 - val_loss: 160.2358\n",
      "Epoch 2259/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.7296 - val_loss: 146.7597\n",
      "Epoch 2260/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.3425 - val_loss: 158.2338\n",
      "Epoch 2261/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.4858 - val_loss: 162.0751\n",
      "Epoch 2262/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 198.3085 - val_loss: 201.9998\n",
      "Epoch 2263/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.5093 - val_loss: 159.8757\n",
      "Epoch 2264/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.3785 - val_loss: 204.9138\n",
      "Epoch 2265/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.8425 - val_loss: 148.1596\n",
      "Epoch 2266/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.5593 - val_loss: 160.0659\n",
      "Epoch 2267/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.8907 - val_loss: 145.5137\n",
      "Epoch 2268/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.1408 - val_loss: 147.4521\n",
      "Epoch 2269/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.7528 - val_loss: 159.4861\n",
      "Epoch 2270/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.0868 - val_loss: 153.8540\n",
      "Epoch 2271/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.1867 - val_loss: 158.3769\n",
      "Epoch 2272/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.1944 - val_loss: 168.7971\n",
      "Epoch 2273/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.5531 - val_loss: 222.9400\n",
      "Epoch 2274/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.0015 - val_loss: 160.1332\n",
      "Epoch 2275/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.9279 - val_loss: 168.1226\n",
      "Epoch 2276/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.5316 - val_loss: 150.3852\n",
      "Epoch 2277/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.3461 - val_loss: 141.2887\n",
      "Epoch 2278/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.2775 - val_loss: 140.6772\n",
      "Epoch 2279/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.2824 - val_loss: 150.8981\n",
      "Epoch 2280/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.9923 - val_loss: 146.8236\n",
      "Epoch 2281/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.5218 - val_loss: 168.1537\n",
      "Epoch 2282/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.2571 - val_loss: 152.3776\n",
      "Epoch 2283/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.5169 - val_loss: 160.3550\n",
      "Epoch 2284/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.9960 - val_loss: 144.3183\n",
      "Epoch 2285/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.9664 - val_loss: 149.2899\n",
      "Epoch 2286/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.6691 - val_loss: 168.2772\n",
      "Epoch 2287/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.6619 - val_loss: 168.5772\n",
      "Epoch 2288/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 175.9820 - val_loss: 184.3443\n",
      "Epoch 2289/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 159.0199 - val_loss: 146.5347\n",
      "Epoch 2290/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.2721 - val_loss: 146.5959\n",
      "Epoch 2291/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 131.1471 - val_loss: 146.5759\n",
      "Epoch 2292/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.7753 - val_loss: 187.6566\n",
      "Epoch 2293/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 205.0428 - val_loss: 148.3997\n",
      "Epoch 2294/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.3555 - val_loss: 162.4109\n",
      "Epoch 2295/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.0923 - val_loss: 152.1760\n",
      "Epoch 2296/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.8967 - val_loss: 153.8727\n",
      "Epoch 2297/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.7619 - val_loss: 178.9754\n",
      "Epoch 2298/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.9648 - val_loss: 194.7355\n",
      "Epoch 2299/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.3002 - val_loss: 145.1807\n",
      "Epoch 2300/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.3141 - val_loss: 182.6256\n",
      "Epoch 2301/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 227.3984 - val_loss: 149.3542\n",
      "Epoch 2302/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.2837 - val_loss: 162.6194\n",
      "Epoch 2303/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.5280 - val_loss: 139.6754\n",
      "Epoch 2304/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.1481 - val_loss: 199.4706\n",
      "Epoch 2305/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.3592 - val_loss: 151.5307\n",
      "Epoch 2306/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.5936 - val_loss: 155.2497\n",
      "Epoch 2307/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 151.8249 - val_loss: 261.4776\n",
      "Epoch 2308/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 138.0873 - val_loss: 144.4256\n",
      "Epoch 2309/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 135.3485 - val_loss: 151.8835\n",
      "Epoch 2310/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.1970 - val_loss: 157.6450\n",
      "Epoch 2311/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.2297 - val_loss: 149.0098\n",
      "Epoch 2312/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.7285 - val_loss: 196.8562\n",
      "Epoch 2313/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.6705 - val_loss: 202.8728\n",
      "Epoch 2314/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.7702 - val_loss: 179.2410\n",
      "Epoch 2315/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.5536 - val_loss: 151.0682\n",
      "Epoch 2316/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.4803 - val_loss: 160.8467\n",
      "Epoch 2317/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.0628 - val_loss: 146.6168\n",
      "Epoch 2318/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 127.5394 - val_loss: 155.1117\n",
      "Epoch 2319/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.5014 - val_loss: 140.3756\n",
      "Epoch 2320/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.4587 - val_loss: 181.5380\n",
      "Epoch 2321/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.6247 - val_loss: 163.4906\n",
      "Epoch 2322/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 177.4576 - val_loss: 163.8141\n",
      "Epoch 2323/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.5990 - val_loss: 148.9252\n",
      "Epoch 2324/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.3224 - val_loss: 171.1321\n",
      "Epoch 2325/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.2853 - val_loss: 142.2268\n",
      "Epoch 2326/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.4552 - val_loss: 146.4458\n",
      "Epoch 2327/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.4765 - val_loss: 139.5719\n",
      "Epoch 2328/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.2901 - val_loss: 145.3679\n",
      "Epoch 2329/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 157.7118 - val_loss: 176.8650\n",
      "Epoch 2330/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 174.7617 - val_loss: 158.5920\n",
      "Epoch 2331/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.6136 - val_loss: 151.8332\n",
      "Epoch 2332/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.6800 - val_loss: 151.5027\n",
      "Epoch 2333/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 126.3706 - val_loss: 153.1662\n",
      "Epoch 2334/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.3187 - val_loss: 142.4599\n",
      "Epoch 2335/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 129.6079 - val_loss: 146.5858\n",
      "Epoch 2336/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.1687 - val_loss: 147.6928\n",
      "Epoch 2337/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.4573 - val_loss: 193.7546\n",
      "Epoch 2338/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.9813 - val_loss: 147.2122\n",
      "Epoch 2339/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.7939 - val_loss: 208.5878\n",
      "Epoch 2340/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.2987 - val_loss: 157.4679\n",
      "Epoch 2341/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.0880 - val_loss: 152.1955\n",
      "Epoch 2342/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.8702 - val_loss: 168.9456\n",
      "Epoch 2343/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.1351 - val_loss: 148.9567\n",
      "Epoch 2344/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.6058 - val_loss: 141.6574\n",
      "Epoch 2345/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.4915 - val_loss: 148.7150\n",
      "Epoch 2346/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.8527 - val_loss: 142.0214\n",
      "Epoch 2347/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.8847 - val_loss: 149.2202\n",
      "Epoch 2348/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.5287 - val_loss: 189.3218\n",
      "Epoch 2349/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.1557 - val_loss: 230.6179\n",
      "Epoch 2350/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.3777 - val_loss: 176.0753\n",
      "Epoch 2351/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.9405 - val_loss: 146.0553\n",
      "Epoch 2352/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.8140 - val_loss: 143.5935\n",
      "Epoch 2353/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 165.3378 - val_loss: 147.4758\n",
      "Epoch 2354/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.8717 - val_loss: 160.6514\n",
      "Epoch 2355/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.3354 - val_loss: 145.9190\n",
      "Epoch 2356/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.1923 - val_loss: 192.7304\n",
      "Epoch 2357/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.1568 - val_loss: 145.1801\n",
      "Epoch 2358/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.5994 - val_loss: 162.1322\n",
      "Epoch 2359/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.3346 - val_loss: 153.7607\n",
      "Epoch 2360/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.6684 - val_loss: 194.1042\n",
      "Epoch 2361/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 135.8478 - val_loss: 229.0546\n",
      "Epoch 2362/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.7514 - val_loss: 141.0208\n",
      "Epoch 2363/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.7698 - val_loss: 146.6990\n",
      "Epoch 2364/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 176.3309 - val_loss: 174.9557\n",
      "Epoch 2365/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.1658 - val_loss: 170.1729\n",
      "Epoch 2366/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.4366 - val_loss: 142.7958\n",
      "Epoch 2367/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 172.3298 - val_loss: 161.7087\n",
      "Epoch 2368/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.6529 - val_loss: 183.1098\n",
      "Epoch 2369/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.9885 - val_loss: 147.5711\n",
      "Epoch 2370/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 149.3139 - val_loss: 563.1764\n",
      "Epoch 2371/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 149.4239 - val_loss: 165.4367\n",
      "Epoch 2372/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 132.3893 - val_loss: 143.3645\n",
      "Epoch 2373/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 159.1392 - val_loss: 154.4720\n",
      "Epoch 2374/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.0699 - val_loss: 187.9170\n",
      "Epoch 2375/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.9905 - val_loss: 153.9118\n",
      "Epoch 2376/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 130.9909 - val_loss: 160.9789\n",
      "Epoch 2377/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 168.5866 - val_loss: 170.2240\n",
      "Epoch 2378/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.6510 - val_loss: 139.0072\n",
      "Epoch 2379/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.8798 - val_loss: 160.3579\n",
      "Epoch 2380/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 137.3580 - val_loss: 159.3810\n",
      "Epoch 2381/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.3487 - val_loss: 174.7609\n",
      "Epoch 2382/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.7079 - val_loss: 172.8647\n",
      "Epoch 2383/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.0219 - val_loss: 147.0810\n",
      "Epoch 2384/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.0181 - val_loss: 138.1467\n",
      "Epoch 2385/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.6431 - val_loss: 155.3015\n",
      "Epoch 2386/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.8457 - val_loss: 151.6878\n",
      "Epoch 2387/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.3989 - val_loss: 220.3101\n",
      "Epoch 2388/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.8256 - val_loss: 157.7740\n",
      "Epoch 2389/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 139.0267 - val_loss: 143.6802\n",
      "Epoch 2390/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 167.7287 - val_loss: 165.6457\n",
      "Epoch 2391/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.9546 - val_loss: 145.5953\n",
      "Epoch 2392/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.4195 - val_loss: 155.3083\n",
      "Epoch 2393/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.5219 - val_loss: 166.4466\n",
      "Epoch 2394/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.2952 - val_loss: 179.2259\n",
      "Epoch 2395/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.5144 - val_loss: 147.3028\n",
      "Epoch 2396/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.3906 - val_loss: 151.0016\n",
      "Epoch 2397/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.2249 - val_loss: 180.2900\n",
      "Epoch 2398/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.3534 - val_loss: 142.0024\n",
      "Epoch 2399/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.9618 - val_loss: 181.8095\n",
      "Epoch 2400/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.6883 - val_loss: 160.8640\n",
      "Epoch 2401/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.1979 - val_loss: 228.4293\n",
      "Epoch 2402/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.1820 - val_loss: 157.1271\n",
      "Epoch 2403/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.7281 - val_loss: 183.8780\n",
      "Epoch 2404/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.3785 - val_loss: 142.5636\n",
      "Epoch 2405/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.1942 - val_loss: 180.5949\n",
      "Epoch 2406/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.9740 - val_loss: 161.4314\n",
      "Epoch 2407/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.1657 - val_loss: 168.2272\n",
      "Epoch 2408/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.0737 - val_loss: 201.6282\n",
      "Epoch 2409/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.0917 - val_loss: 172.2174\n",
      "Epoch 2410/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 128.6720 - val_loss: 172.6109\n",
      "Epoch 2411/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.0495 - val_loss: 163.6364\n",
      "Epoch 2412/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.0594 - val_loss: 177.7607\n",
      "Epoch 2413/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.9225 - val_loss: 301.6871\n",
      "Epoch 2414/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 160.8790 - val_loss: 198.3795\n",
      "Epoch 2415/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 196.4523 - val_loss: 153.7723\n",
      "Epoch 2416/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.5709 - val_loss: 151.9477\n",
      "Epoch 2417/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.8965 - val_loss: 146.3099\n",
      "Epoch 2418/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.4195 - val_loss: 140.0610\n",
      "Epoch 2419/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.5078 - val_loss: 142.9852\n",
      "Epoch 2420/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.8251 - val_loss: 146.1833\n",
      "Epoch 2421/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 159.156 - 1s 64us/step - loss: 158.2998 - val_loss: 147.2351\n",
      "Epoch 2422/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.3861 - val_loss: 167.7609\n",
      "Epoch 2423/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.7760 - val_loss: 162.3517\n",
      "Epoch 2424/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.9660 - val_loss: 150.3091\n",
      "Epoch 2425/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.8673 - val_loss: 145.3702\n",
      "Epoch 2426/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.8719 - val_loss: 205.1374\n",
      "Epoch 2427/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.1998 - val_loss: 138.4868\n",
      "Epoch 2428/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.3711 - val_loss: 143.8870\n",
      "Epoch 2429/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.5136 - val_loss: 145.8996\n",
      "Epoch 2430/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 129.5055 - val_loss: 144.2288\n",
      "Epoch 2431/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 135.7958 - val_loss: 143.5983\n",
      "Epoch 2432/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 69us/step - loss: 138.9491 - val_loss: 182.2138\n",
      "Epoch 2433/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 131.0208 - val_loss: 148.1705\n",
      "Epoch 2434/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.4876 - val_loss: 205.8446\n",
      "Epoch 2435/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.8390 - val_loss: 145.2665\n",
      "Epoch 2436/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.9388 - val_loss: 143.0169\n",
      "Epoch 2437/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.6593 - val_loss: 153.1913\n",
      "Epoch 2438/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 124.5013 - val_loss: 147.1884\n",
      "Epoch 2439/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.0952 - val_loss: 174.0629\n",
      "Epoch 2440/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.5220 - val_loss: 166.1635\n",
      "Epoch 2441/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.7736 - val_loss: 251.0092\n",
      "Epoch 2442/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.3407 - val_loss: 144.9282\n",
      "Epoch 2443/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.8223 - val_loss: 173.2691\n",
      "Epoch 2444/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.5728 - val_loss: 147.5709\n",
      "Epoch 2445/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.6321 - val_loss: 148.4865\n",
      "Epoch 2446/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.9083 - val_loss: 172.2574\n",
      "Epoch 2447/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.0762 - val_loss: 198.9106\n",
      "Epoch 2448/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.7823 - val_loss: 143.5198\n",
      "Epoch 2449/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.3851 - val_loss: 168.3473\n",
      "Epoch 2450/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.0838 - val_loss: 150.9641\n",
      "Epoch 2451/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 161.5986 - val_loss: 182.6709\n",
      "Epoch 2452/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.2802 - val_loss: 142.2992\n",
      "Epoch 2453/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.5556 - val_loss: 171.2895\n",
      "Epoch 2454/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.9302 - val_loss: 163.0085\n",
      "Epoch 2455/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.9995 - val_loss: 146.5463\n",
      "Epoch 2456/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.9771 - val_loss: 142.1392\n",
      "Epoch 2457/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.2285 - val_loss: 222.4976\n",
      "Epoch 2458/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.0231 - val_loss: 299.2219\n",
      "Epoch 2459/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.0828 - val_loss: 164.5207\n",
      "Epoch 2460/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.3624 - val_loss: 150.9008\n",
      "Epoch 2461/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.0478 - val_loss: 150.2490\n",
      "Epoch 2462/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.3404 - val_loss: 159.2152\n",
      "Epoch 2463/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.1720 - val_loss: 186.1624\n",
      "Epoch 2464/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.9500 - val_loss: 154.3775\n",
      "Epoch 2465/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.4330 - val_loss: 168.3890\n",
      "Epoch 2466/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.5092 - val_loss: 338.8453\n",
      "Epoch 2467/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.5023 - val_loss: 154.4998\n",
      "Epoch 2468/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.3793 - val_loss: 280.1912\n",
      "Epoch 2469/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.9870 - val_loss: 149.9330\n",
      "Epoch 2470/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 126.3912 - val_loss: 149.2092\n",
      "Epoch 2471/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.9418 - val_loss: 163.2505\n",
      "Epoch 2472/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.9362 - val_loss: 148.6083\n",
      "Epoch 2473/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.0607 - val_loss: 145.0136\n",
      "Epoch 2474/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.0647 - val_loss: 150.9471\n",
      "Epoch 2475/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.1592 - val_loss: 155.7144\n",
      "Epoch 2476/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.6220 - val_loss: 154.2241\n",
      "Epoch 2477/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.6244 - val_loss: 142.0158\n",
      "Epoch 2478/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.1259 - val_loss: 143.7172\n",
      "Epoch 2479/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 129.3461 - val_loss: 156.2621\n",
      "Epoch 2480/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.3264 - val_loss: 155.3502\n",
      "Epoch 2481/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.6111 - val_loss: 146.6943\n",
      "Epoch 2482/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.0995 - val_loss: 182.5446\n",
      "Epoch 2483/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 140.7493 - val_loss: 145.1373\n",
      "Epoch 2484/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.8117 - val_loss: 171.9499\n",
      "Epoch 2485/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.1330 - val_loss: 150.8496\n",
      "Epoch 2486/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.1253 - val_loss: 149.1297\n",
      "Epoch 2487/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.3093 - val_loss: 151.6307\n",
      "Epoch 2488/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.5056 - val_loss: 181.7352\n",
      "Epoch 2489/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.1218 - val_loss: 146.7815\n",
      "Epoch 2490/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.9082 - val_loss: 144.3975\n",
      "Epoch 2491/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.9043 - val_loss: 161.4033\n",
      "Epoch 2492/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.5195 - val_loss: 206.8184\n",
      "Epoch 2493/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 130.1098 - val_loss: 161.7751\n",
      "Epoch 2494/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 142.6146 - val_loss: 148.4195\n",
      "Epoch 2495/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 131.6235 - val_loss: 167.7214\n",
      "Epoch 2496/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.1084 - val_loss: 143.6085\n",
      "Epoch 2497/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 127.4242 - val_loss: 158.6603\n",
      "Epoch 2498/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.0614 - val_loss: 145.7568\n",
      "Epoch 2499/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.0399 - val_loss: 152.5531\n",
      "Epoch 2500/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 129.1804 - val_loss: 144.3742\n",
      "Epoch 2501/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.5196 - val_loss: 144.3185\n",
      "Epoch 2502/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 127.5717 - val_loss: 170.2765\n",
      "Epoch 2503/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 165.4181 - val_loss: 167.7287\n",
      "Epoch 2504/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.3150 - val_loss: 169.6771\n",
      "Epoch 2505/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.5338 - val_loss: 166.8184\n",
      "Epoch 2506/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.5596 - val_loss: 183.6710\n",
      "Epoch 2507/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 128.5949 - val_loss: 166.7905\n",
      "Epoch 2508/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.1044 - val_loss: 181.1114\n",
      "Epoch 2509/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 143.9695 - val_loss: 141.5464\n",
      "Epoch 2510/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 138.7790 - val_loss: 153.9986\n",
      "Epoch 2511/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.7903 - val_loss: 147.3667\n",
      "Epoch 2512/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 133.5550 - val_loss: 145.0797\n",
      "Epoch 2513/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 137.0091 - val_loss: 155.8847\n",
      "Epoch 2514/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.8341 - val_loss: 161.2131\n",
      "Epoch 2515/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 164.2224 - val_loss: 141.9027\n",
      "Epoch 2516/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 132.3071 - val_loss: 159.2099\n",
      "Epoch 2517/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.5242 - val_loss: 266.1197\n",
      "Epoch 2518/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.6724 - val_loss: 146.9367\n",
      "Epoch 2519/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.0115 - val_loss: 194.6730\n",
      "Epoch 2520/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 130.0594 - val_loss: 173.1956\n",
      "Epoch 2521/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.8594 - val_loss: 182.2891\n",
      "Epoch 2522/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 132.5647 - val_loss: 183.5532\n",
      "Epoch 2523/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.6544 - val_loss: 143.1268\n",
      "Epoch 2524/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.7972 - val_loss: 157.2620\n",
      "Epoch 2525/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 130.6212 - val_loss: 153.6753\n",
      "Epoch 2526/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 139.2470 - val_loss: 147.6567\n",
      "Epoch 2527/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.4032 - val_loss: 150.1530\n",
      "Epoch 2528/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 131.0128 - val_loss: 148.1882\n",
      "Epoch 2529/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.6648 - val_loss: 138.6398\n",
      "Epoch 2530/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 126.3767 - val_loss: 143.7415\n",
      "Epoch 2531/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.2366 - val_loss: 143.2586\n",
      "Epoch 2532/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.3387 - val_loss: 177.5013\n",
      "Epoch 2533/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.7397 - val_loss: 344.8539\n",
      "Epoch 2534/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 169.0068 - val_loss: 163.9873\n",
      "Epoch 2535/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.3703 - val_loss: 166.1934\n",
      "Epoch 2536/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 127.4686 - val_loss: 139.4195\n",
      "Epoch 2537/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 130.9608 - val_loss: 142.2975\n",
      "Epoch 2538/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.5546 - val_loss: 144.3353\n",
      "Epoch 2539/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 126.9943 - val_loss: 144.1798\n",
      "Epoch 2540/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 131.4049 - val_loss: 170.3719\n",
      "Epoch 2541/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 135.7059 - val_loss: 152.4670\n",
      "Epoch 2542/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.3416 - val_loss: 175.7482\n",
      "Epoch 2543/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.8368 - val_loss: 152.1587\n",
      "Epoch 2544/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.0969 - val_loss: 166.2102\n",
      "Epoch 2545/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 131.6981 - val_loss: 147.9153\n",
      "Epoch 2546/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.5585 - val_loss: 172.5832\n",
      "Epoch 2547/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 126.7717 - val_loss: 159.4987\n",
      "Epoch 2548/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.3477 - val_loss: 164.2356\n",
      "Epoch 2549/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.0747 - val_loss: 161.4719\n",
      "Epoch 2550/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 160.0875 - val_loss: 146.7095\n",
      "Epoch 2551/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 144.0146 - val_loss: 162.9867\n",
      "Epoch 2552/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 132.1516 - val_loss: 199.1435\n",
      "Epoch 2553/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 130.6636 - val_loss: 147.3415\n",
      "Epoch 2554/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.1676 - val_loss: 165.8180\n",
      "Epoch 2555/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.3555 - val_loss: 180.8724\n",
      "Epoch 2556/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.2961 - val_loss: 161.2902\n",
      "Epoch 2557/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 129.0351 - val_loss: 177.9746\n",
      "Epoch 2558/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.9779 - val_loss: 151.3018\n",
      "Epoch 2559/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.2129 - val_loss: 174.1754\n",
      "Epoch 2560/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.5978 - val_loss: 204.4381\n",
      "Epoch 2561/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.9514 - val_loss: 143.4570\n",
      "Epoch 2562/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.2668 - val_loss: 161.3317\n",
      "Epoch 2563/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.2153 - val_loss: 147.1947\n",
      "Epoch 2564/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 166.3643 - val_loss: 152.0366\n",
      "Epoch 2565/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.3470 - val_loss: 339.0593\n",
      "Epoch 2566/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.3130 - val_loss: 554.8216\n",
      "Epoch 2567/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.0396 - val_loss: 161.2736\n",
      "Epoch 2568/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.2012 - val_loss: 167.1575\n",
      "Epoch 2569/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.0648 - val_loss: 228.6797\n",
      "Epoch 2570/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.7307 - val_loss: 153.0388\n",
      "Epoch 2571/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.3298 - val_loss: 140.7738\n",
      "Epoch 2572/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.0223 - val_loss: 150.2833\n",
      "Epoch 2573/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.3763 - val_loss: 167.0585\n",
      "Epoch 2574/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 133.4229 - val_loss: 172.5382\n",
      "Epoch 2575/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 183.1239 - val_loss: 158.3126\n",
      "Epoch 2576/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.9558 - val_loss: 166.1340\n",
      "Epoch 2577/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 125.0349 - val_loss: 147.4103\n",
      "Epoch 2578/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.6170 - val_loss: 178.1771\n",
      "Epoch 2579/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 129.0601 - val_loss: 161.6253\n",
      "Epoch 2580/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.6859 - val_loss: 167.7100\n",
      "Epoch 2581/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.8602 - val_loss: 145.0719\n",
      "Epoch 2582/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.9834 - val_loss: 141.2583\n",
      "Epoch 2583/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.8373 - val_loss: 160.3326\n",
      "Epoch 2584/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.8027 - val_loss: 152.8873\n",
      "Epoch 2585/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.1904 - val_loss: 167.9395\n",
      "Epoch 2586/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.7213 - val_loss: 144.3570\n",
      "Epoch 2587/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.5965 - val_loss: 157.1348\n",
      "Epoch 2588/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.2730 - val_loss: 150.0427\n",
      "Epoch 2589/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.1178 - val_loss: 159.0259\n",
      "Epoch 2590/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.3663 - val_loss: 208.9841\n",
      "Epoch 02590: early stopping\n",
      "Fold score (RMSE): 14.247203826904297\n",
      "Fold #3\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 5922.2977 - val_loss: 5295.6149\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 4853.4379 - val_loss: 4872.4887\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 4526.3236 - val_loss: 4644.7443\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 4345.1483 - val_loss: 4424.8441\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 4242.5633 - val_loss: 4399.6167\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 4122.0174 - val_loss: 4262.7725\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 3979.8213 - val_loss: 4115.8079\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 3889.8389 - val_loss: 4131.7146\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 3748.1473 - val_loss: 3791.6827\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 3486.9632 - val_loss: 3483.5973\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 3265.7072 - val_loss: 3229.4723\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 2835.7410 - val_loss: 2234.0592\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 2530.9731 - val_loss: 1798.2672\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 1885.9235 - val_loss: 2112.2006\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 1952.4165 - val_loss: 1311.1186\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 1613.5944 - val_loss: 1703.3105\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 1232.1112 - val_loss: 1317.0898\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 983.6450 - val_loss: 721.7504\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 813.5426 - val_loss: 588.2541\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 780.2851 - val_loss: 2013.4273\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 781.0937 - val_loss: 638.9954\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 815.0031 - val_loss: 558.6238\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 676.4088 - val_loss: 940.2412\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 716.4143 - val_loss: 609.9099\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 654.3369 - val_loss: 605.4651\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 675.6360 - val_loss: 498.5779\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 654.9025 - val_loss: 493.9106\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 599.9472 - val_loss: 420.4279\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 636.1571 - val_loss: 400.2338\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 602.2517 - val_loss: 542.4294\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 470.8783 - val_loss: 459.4512\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 631.5844 - val_loss: 1089.3529\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 509.9264 - val_loss: 645.5920\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 661.2497 - val_loss: 543.0247\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 552.1471 - val_loss: 1135.6820\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 519.8597 - val_loss: 450.0699\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 447.1481 - val_loss: 336.7808\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 512.2906 - val_loss: 366.3547\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 610.8079 - val_loss: 475.1379\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 463.1588 - val_loss: 511.3368\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 670.7144 - val_loss: 579.8513\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 517.7812 - val_loss: 348.1226\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 429.5650 - val_loss: 480.4207\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 461.3877 - val_loss: 385.2227\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 528.2265 - val_loss: 389.6793\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 458.3796 - val_loss: 368.3829\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 456.7841 - val_loss: 430.7102\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 366.9739 - val_loss: 842.9155\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 467.2484 - val_loss: 328.0008\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 403.9954 - val_loss: 322.0920\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 372.5947 - val_loss: 532.8656\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 489.5199 - val_loss: 745.4447\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 371.3353 - val_loss: 342.8065\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 530.9732 - val_loss: 353.0352\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 421.8880 - val_loss: 988.2863\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 555.9301 - val_loss: 351.9956\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 355.2421 - val_loss: 308.6321\n",
      "Epoch 58/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 474.1525 - val_loss: 348.4880\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 384.2898 - val_loss: 833.2575\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 480.9897 - val_loss: 281.6467\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 366.4499 - val_loss: 274.9465\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 453.6100 - val_loss: 282.0985\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 341.9849 - val_loss: 253.2902\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 367.9065 - val_loss: 555.7923\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 439.5417 - val_loss: 502.3123\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 351.1591 - val_loss: 248.1895\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 501.6068 - val_loss: 531.5346\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 312.6413 - val_loss: 285.5284\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 396.8267 - val_loss: 481.4503\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 312.5600 - val_loss: 485.9045\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 320.0468 - val_loss: 290.2411\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 348.6583 - val_loss: 290.4608\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 399.5074 - val_loss: 300.1402\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 337.7739 - val_loss: 583.6220\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 287.1732 - val_loss: 238.5739\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 325.8842 - val_loss: 254.9002\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 331.8299 - val_loss: 242.9625\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 345.3174 - val_loss: 283.4953\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 340.7552 - val_loss: 542.5758\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 325.7538 - val_loss: 475.8964\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 324.4399 - val_loss: 275.2262\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 367.2146 - val_loss: 532.1928\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 422.2237 - val_loss: 345.8545\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 435.1515 - val_loss: 278.3938\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 344.7239 - val_loss: 306.0100\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 353.6622 - val_loss: 321.9563\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 301.0361 - val_loss: 251.0754\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 332.7487 - val_loss: 295.8444\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 459.6876 - val_loss: 361.3447\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 273.7347 - val_loss: 441.8420\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 392.0659 - val_loss: 408.8385\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 300.0340 - val_loss: 273.6856\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 304.3850 - val_loss: 371.5774\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 329.6881 - val_loss: 226.8125\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 347.7708 - val_loss: 244.2765\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 278.5655 - val_loss: 244.4321\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 345.7448 - val_loss: 277.9253\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 323.8631 - val_loss: 723.9063\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 365.1248 - val_loss: 205.3077\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 252.5705 - val_loss: 271.1674\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 320.8471 - val_loss: 369.4447\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 272.0077 - val_loss: 256.2479\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 585.0974 - val_loss: 472.1889\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 317.4655 - val_loss: 509.4157\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 303.0861 - val_loss: 305.8116\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 272.5988 - val_loss: 374.1242\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 262.9966 - val_loss: 271.8400\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 306.0488 - val_loss: 327.5192\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 294.0961 - val_loss: 541.3722\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 342.5530 - val_loss: 284.3556\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 339.9563 - val_loss: 387.6278\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 290.1529 - val_loss: 237.7468\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 259.8825 - val_loss: 298.2884\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 242.0435 - val_loss: 209.8074\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 284.7799 - val_loss: 484.1192\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 278.4546 - val_loss: 264.2525\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 284.7003 - val_loss: 199.2734\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 324.0429 - val_loss: 197.8045\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 246.0298 - val_loss: 256.9964\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 258.4303 - val_loss: 194.1490\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 493.2888 - val_loss: 352.5413\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 273.0036 - val_loss: 516.3036\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 312.0766 - val_loss: 231.5746\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 257.4395 - val_loss: 222.4241\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 297.0541 - val_loss: 190.3269\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 292.4007 - val_loss: 249.4155\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 278.7275 - val_loss: 201.3621\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 325.5670 - val_loss: 180.3789\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 249.9054 - val_loss: 240.2201\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 290.1167 - val_loss: 239.6470\n",
      "Epoch 131/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 266.5832 - val_loss: 210.0536\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 245.7335 - val_loss: 238.9417\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 438.4870 - val_loss: 206.0833\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 343.1179 - val_loss: 552.9444\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 308.0435 - val_loss: 372.0301\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 236.0596 - val_loss: 273.4965\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 230.5193 - val_loss: 188.1085\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 322.7920 - val_loss: 320.5082\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 240.8212 - val_loss: 201.7690\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 265.4427 - val_loss: 471.1961\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 249.8002 - val_loss: 229.5877\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 250.9812 - val_loss: 204.4243\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 350.0408 - val_loss: 406.5359\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 236.8229 - val_loss: 210.6882\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 477.8877 - val_loss: 193.6865\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 265.7673 - val_loss: 167.9624\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 247.0052 - val_loss: 183.5108\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 250.0830 - val_loss: 234.1982\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 231.8227 - val_loss: 186.2874\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 221.6440 - val_loss: 238.0922\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 236.2531 - val_loss: 265.0113\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 252.2908 - val_loss: 520.2586\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 228.4269 - val_loss: 196.8216\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 419.7934 - val_loss: 316.6845\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 270.1567 - val_loss: 191.9085\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 230.0810 - val_loss: 190.8152\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 242.7419 - val_loss: 178.5563\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 275.4704 - val_loss: 193.3539\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 254.0014 - val_loss: 213.0661\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 414.8808 - val_loss: 267.8709\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 330.5713 - val_loss: 320.7147\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 246.5780 - val_loss: 173.7634\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 266.5499 - val_loss: 212.6736\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 260.9518 - val_loss: 198.6170\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 500.6585 - val_loss: 284.4451\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 345.7830 - val_loss: 201.7559\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 266.2296 - val_loss: 284.9133\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 330.2221 - val_loss: 186.9279\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 281.1824 - val_loss: 227.3333\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 294.4663 - val_loss: 242.2070\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 336.6954 - val_loss: 296.9899\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 304.8889 - val_loss: 189.6526\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 233.3648 - val_loss: 203.2140\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 231.2396 - val_loss: 210.8062\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 242.5052 - val_loss: 198.5734\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 241.4895 - val_loss: 219.8570\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 301.9064 - val_loss: 235.5351\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 306.0678 - val_loss: 218.8429\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 225.8957 - val_loss: 207.7748\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 260.3224 - val_loss: 262.0295\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 237.1779 - val_loss: 185.9716\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 281.6481 - val_loss: 234.9236\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 283.2165 - val_loss: 253.6416\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 289.1555 - val_loss: 446.2562\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 239.5325 - val_loss: 179.0619\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 222.3065 - val_loss: 225.2670\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 229.1087 - val_loss: 318.4071\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 242.1639 - val_loss: 298.5503\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 232.5631 - val_loss: 232.9784\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 246.3887 - val_loss: 198.8755\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 372.6657 - val_loss: 201.7059\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 284.2808 - val_loss: 252.6280\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 266.5479 - val_loss: 175.0470\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 207.8023 - val_loss: 166.0839\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 221.0715 - val_loss: 244.1247\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 250.2575 - val_loss: 192.7985\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 209.1936 - val_loss: 422.4272\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 301.1784 - val_loss: 494.7972\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 222.9038 - val_loss: 506.3109\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 238.5393 - val_loss: 169.7836\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 246.0956 - val_loss: 231.5786\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 242.1680 - val_loss: 170.3932\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 271.1180 - val_loss: 180.0292\n",
      "Epoch 204/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 280.3636 - val_loss: 265.6095\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 241.2728 - val_loss: 184.4633\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 243.1583 - val_loss: 333.3515\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 274.6391 - val_loss: 241.4266\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 202.9573 - val_loss: 165.1123\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 215.5776 - val_loss: 290.0719\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 243.9133 - val_loss: 303.4769\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 208.6294 - val_loss: 157.1694\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 217.5106 - val_loss: 296.5422\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 240.0794 - val_loss: 232.2493\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 381.6975 - val_loss: 316.2791\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 376.8940 - val_loss: 356.1123\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 266.1838 - val_loss: 212.4801\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 216.1996 - val_loss: 176.0433\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 239.8727 - val_loss: 165.8899\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 225.1187 - val_loss: 337.1492\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 240.3838 - val_loss: 192.4926\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 269.6570 - val_loss: 192.9852\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 198.6710 - val_loss: 284.1743\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 230.2148 - val_loss: 172.0245\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 196.9029 - val_loss: 154.4357\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 224.6188 - val_loss: 162.7908\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 221.2085 - val_loss: 188.4137\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 307.2559 - val_loss: 161.1855\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 215.6259 - val_loss: 159.9423\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 268.8302 - val_loss: 196.1464\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 206.3973 - val_loss: 247.4765\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 199.0195 - val_loss: 194.4345\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 231.7761 - val_loss: 253.7593\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 224.4802 - val_loss: 223.3688\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 194.4299 - val_loss: 183.7129\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 331.9170 - val_loss: 268.4201\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 225.9096 - val_loss: 303.4929\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 245.6001 - val_loss: 209.9086\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 273.1141 - val_loss: 193.2924\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 230.5047 - val_loss: 199.2799\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 313.2404 - val_loss: 227.6256\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 220.6587 - val_loss: 159.4739\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.0815 - val_loss: 204.8988\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 209.0449 - val_loss: 158.5278\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 239.4849 - val_loss: 160.5033\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 224.0281 - val_loss: 257.9970\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 276.5153 - val_loss: 187.7857\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 213.7958 - val_loss: 210.0638\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 246.2831 - val_loss: 216.3963\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 200.6174 - val_loss: 240.0219\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 202.2959 - val_loss: 181.2836\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 212.4216 - val_loss: 313.9868\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 235.8795 - val_loss: 242.8989\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 288.2109 - val_loss: 368.3303\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 207.4794 - val_loss: 293.6396\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 196.7206 - val_loss: 170.2619\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 195.1742 - val_loss: 289.9422\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 220.5894 - val_loss: 162.0808\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 405.8453 - val_loss: 184.5571\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 204.8925 - val_loss: 169.5475\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 198.0621 - val_loss: 208.9410\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 217.9001 - val_loss: 185.3522\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 282.4773 - val_loss: 165.0153\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 196.0973 - val_loss: 175.6975\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 297.2729 - val_loss: 214.5262\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 198.8283 - val_loss: 170.1834\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 202.9717 - val_loss: 167.6919\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 210.7830 - val_loss: 196.9554\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 217.6210 - val_loss: 225.6061\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 199.4951 - val_loss: 245.2084\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 212.8965 - val_loss: 162.1202\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 209.0829 - val_loss: 158.0601\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 198.2391 - val_loss: 228.9849\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 204.2019 - val_loss: 180.8728\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 172.6349 - val_loss: 154.5716\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 349.1791 - val_loss: 408.3923\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 208.8579 - val_loss: 207.3128\n",
      "Epoch 277/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 210.8281 - val_loss: 193.8343\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.7966 - val_loss: 203.0894\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 243.1843 - val_loss: 161.6225\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 186.3447 - val_loss: 216.8823\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 234.5748 - val_loss: 220.4924\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 339.1386 - val_loss: 159.8983\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 177.2031 - val_loss: 170.3325\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 205.8568 - val_loss: 227.4013\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 205.3025 - val_loss: 173.0299\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 219.3738 - val_loss: 160.8823\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 192.3780 - val_loss: 178.8808\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 221.7872 - val_loss: 182.0805\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 355.3863 - val_loss: 158.9870\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 222.2729 - val_loss: 263.2437\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.2114 - val_loss: 210.7173\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 206.7160 - val_loss: 207.7919\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.5752 - val_loss: 156.8552\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 194.6372 - val_loss: 152.3939\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 186.7953 - val_loss: 150.4878\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 288.8264 - val_loss: 941.8010\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 317.2505 - val_loss: 177.6129\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 208.5090 - val_loss: 154.4466\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 204.4264 - val_loss: 328.9583\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 197.5072 - val_loss: 171.2823\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 180.7866 - val_loss: 192.6794\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 222.8049 - val_loss: 288.6970\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 208.1527 - val_loss: 465.5919\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 238.2793 - val_loss: 152.5067\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 195.4937 - val_loss: 306.8644\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 282.4668 - val_loss: 167.4409\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 219.0190 - val_loss: 184.9970\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 177.0929 - val_loss: 167.5649\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 235.8790 - val_loss: 176.7833\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 176.1357 - val_loss: 153.2655\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.1050 - val_loss: 230.4473\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 203.0083 - val_loss: 185.2184\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 332.5227 - val_loss: 169.9590\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 233.5573 - val_loss: 191.6070\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 198.4328 - val_loss: 156.7975\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 177.3597 - val_loss: 213.5931\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 195.9990 - val_loss: 208.9617\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 200.1306 - val_loss: 177.8287\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 187.3405 - val_loss: 163.3061\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 179.7383 - val_loss: 150.3506\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 246.5875 - val_loss: 169.9501\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 167.0621 - val_loss: 306.4475\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 186.5553 - val_loss: 182.0875\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 283.3086 - val_loss: 335.6395\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 230.2377 - val_loss: 164.8206\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 243.8155 - val_loss: 205.5678\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 207.7701 - val_loss: 222.3429\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 183.9597 - val_loss: 186.4922\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 192.1143 - val_loss: 164.3375\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 185.1421 - val_loss: 165.0248\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 337.2608 - val_loss: 149.7311\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 229.5327 - val_loss: 252.0849\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 179.1343 - val_loss: 186.0570\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 195.9691 - val_loss: 152.0665\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 186.7979 - val_loss: 233.1647\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 223.3950 - val_loss: 323.6184\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 187.1561 - val_loss: 183.5731\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 308.2383 - val_loss: 332.6374\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 356.2813 - val_loss: 1108.6838\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 398.2157 - val_loss: 278.6673\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 224.5161 - val_loss: 165.5042\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 239.1077 - val_loss: 184.6590\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 260.6786 - val_loss: 259.3430\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 226.3347 - val_loss: 276.0771\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 221.5681 - val_loss: 287.8120\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 227.4846 - val_loss: 161.7379\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 199.7564 - val_loss: 172.6367\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 228.8724 - val_loss: 363.5085\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 242.8628 - val_loss: 176.2425\n",
      "Epoch 350/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 274.0795 - val_loss: 191.3902\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 231.2245 - val_loss: 308.1494\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 237.9330 - val_loss: 277.6244\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 214.0302 - val_loss: 174.2731\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 201.384 - 0s 57us/step - loss: 206.3362 - val_loss: 180.2639\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 286.9954 - val_loss: 180.9602\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 198.8205 - val_loss: 388.7158\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 208.222 - 0s 57us/step - loss: 200.7621 - val_loss: 149.5101\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 229.8306 - val_loss: 643.9462\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 278.9142 - val_loss: 171.5449\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 200.8691 - val_loss: 196.3743\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 278.9188 - val_loss: 175.6384\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 261.6123 - val_loss: 194.3378\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 268.2948 - val_loss: 200.3062\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 211.0223 - val_loss: 256.3125\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 209.1661 - val_loss: 188.3975\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 218.0591 - val_loss: 148.9664\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 261.7405 - val_loss: 146.0847\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 195.0447 - val_loss: 205.5624\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 206.3889 - val_loss: 145.1534\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 206.1376 - val_loss: 201.4831\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 219.2329 - val_loss: 185.8322\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 212.6264 - val_loss: 165.4886\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 227.6153 - val_loss: 163.2326\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 198.3929 - val_loss: 169.0150\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 196.0937 - val_loss: 186.7873\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 191.6412 - val_loss: 149.3503\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 260.5011 - val_loss: 231.4807\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 262.0885 - val_loss: 164.7302\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 179.8527 - val_loss: 147.6640\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 191.4317 - val_loss: 168.9773\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 200.2353 - val_loss: 263.9386\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 299.9194 - val_loss: 152.0721\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 207.2041 - val_loss: 161.6934\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 186.6354 - val_loss: 205.3166\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 187.9670 - val_loss: 175.5769\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 221.0395 - val_loss: 148.1764\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 235.0811 - val_loss: 176.7609\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.0000 - val_loss: 197.8025\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 190.3609 - val_loss: 158.4502\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 190.0361 - val_loss: 280.7375\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 225.2342 - val_loss: 180.0927\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 245.8844 - val_loss: 162.9319\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 179.7469 - val_loss: 174.1408\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 174.1049 - val_loss: 144.4913\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 186.9081 - val_loss: 161.1516\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 178.3160 - val_loss: 149.1510\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 216.0129 - val_loss: 532.2635\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 324.3641 - val_loss: 198.5944\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 325.3534 - val_loss: 438.8139\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 241.9748 - val_loss: 167.0228\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 278.6883 - val_loss: 208.2213\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 251.8082 - val_loss: 511.4270\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 224.6652 - val_loss: 157.4190\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 167.3815 - val_loss: 177.4215\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 321.2929 - val_loss: 313.1714\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 251.4601 - val_loss: 245.6133\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 222.0739 - val_loss: 168.2115\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 240.8155 - val_loss: 217.0809\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 279.8663 - val_loss: 157.0355\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 226.3486 - val_loss: 157.1137\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 239.5016 - val_loss: 305.1857\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 256.0817 - val_loss: 151.5425\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 215.4169 - val_loss: 142.6526\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 224.6572 - val_loss: 161.9140\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 189.8321 - val_loss: 183.6770\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 275.1036 - val_loss: 192.3598\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 224.3202 - val_loss: 147.8650\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 232.2073 - val_loss: 148.3758\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 222.2104 - val_loss: 242.4717\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 221.6197 - val_loss: 198.2634\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 208.4849 - val_loss: 157.4969\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 210.2424 - val_loss: 163.2156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 218.9601 - val_loss: 166.6516\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 228.6787 - val_loss: 315.6310\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 306.4277 - val_loss: 578.3997\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 232.0024 - val_loss: 144.0034\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 201.7450 - val_loss: 248.2119\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 209.7977 - val_loss: 150.8648\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 217.7188 - val_loss: 162.6312\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 252.5578 - val_loss: 424.5613\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 192.6709 - val_loss: 321.1372\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 196.6528 - val_loss: 153.1592\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 169.4096 - val_loss: 149.9649\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 174.2872 - val_loss: 158.7819\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 220.6045 - val_loss: 233.2049\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 201.6179 - val_loss: 211.0351\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 191.4479 - val_loss: 150.0797\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.0729 - val_loss: 249.2029\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.9204 - val_loss: 671.1104\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 458.4892 - val_loss: 322.9699\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 248.4801 - val_loss: 150.6407\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 224.9111 - val_loss: 199.9541\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 203.1399 - val_loss: 145.4403\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 221.2420 - val_loss: 168.2976\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 210.3646 - val_loss: 205.8958\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 275.7549 - val_loss: 192.2353\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 200.8936 - val_loss: 177.1343\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 262.3816 - val_loss: 155.4476\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 197.3257 - val_loss: 216.6974\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 194.0317 - val_loss: 331.5008\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 197.3880 - val_loss: 158.8556\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 187.3950 - val_loss: 236.2315\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 215.7794 - val_loss: 174.4599\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 210.7426 - val_loss: 150.9308\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.6322 - val_loss: 156.4811\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 194.4291 - val_loss: 205.1091\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 228.9312 - val_loss: 226.2862\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 256.5935 - val_loss: 188.9731\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 241.3412 - val_loss: 158.7249\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.9780 - val_loss: 233.6884\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 220.0207 - val_loss: 160.7694\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 191.1255 - val_loss: 137.8116\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 190.2780 - val_loss: 181.0419\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 190.8731 - val_loss: 248.0131\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 199.3675 - val_loss: 263.1482\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 192.0300 - val_loss: 141.9395\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 181.9095 - val_loss: 216.0593\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 199.8491 - val_loss: 154.9405\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 199.9568 - val_loss: 162.8568\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 227.5935 - val_loss: 156.2279\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 184.9761 - val_loss: 157.8136\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.3925 - val_loss: 151.4154\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 185.6401 - val_loss: 235.7029\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 312.7920 - val_loss: 333.9253\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 266.5118 - val_loss: 208.8711\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 192.2200 - val_loss: 156.4378\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.5518 - val_loss: 175.6482\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 172.9175 - val_loss: 179.7097\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 201.3221 - val_loss: 332.6828\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 212.0275 - val_loss: 183.7976\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 271.3750 - val_loss: 216.8950\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 222.0186 - val_loss: 155.5898\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 188.9642 - val_loss: 150.4484\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 202.8030 - val_loss: 166.6327\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.9672 - val_loss: 228.1160\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 177.0912 - val_loss: 142.8195\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 259.5831 - val_loss: 165.2712\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 220.9761 - val_loss: 357.6186\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 185.6479 - val_loss: 195.9898\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.7826 - val_loss: 185.0861\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 192.2453 - val_loss: 153.7638\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 243.7502 - val_loss: 191.7330\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 202.4668 - val_loss: 175.2329\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 276.8458 - val_loss: 146.0700\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 198.2880 - val_loss: 161.9240\n",
      "Epoch 496/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 212.1557 - val_loss: 157.2649\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 207.2839 - val_loss: 180.7393\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 210.0484 - val_loss: 150.2322\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 261.4460 - val_loss: 254.8221\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 351.3520 - val_loss: 183.9913\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 191.6920 - val_loss: 269.0504\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 200.0119 - val_loss: 153.3098\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 209.8312 - val_loss: 163.5913\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 216.5349 - val_loss: 353.8598\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 205.1669 - val_loss: 177.9072\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 282.6889 - val_loss: 214.2887\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 184.5786 - val_loss: 170.3358\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 240.6790 - val_loss: 195.2126\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 222.6444 - val_loss: 187.3979\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 185.0745 - val_loss: 154.3527\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 183.4003 - val_loss: 143.2140\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 285.7437 - val_loss: 263.3339\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 184.9791 - val_loss: 489.7946\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 255.5180 - val_loss: 160.4779\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 181.5643 - val_loss: 164.3440\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 187.8943 - val_loss: 208.9826\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 226.1442 - val_loss: 365.9888\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 224.9095 - val_loss: 175.2746\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 196.2343 - val_loss: 156.6874\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 199.0621 - val_loss: 169.2665\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 196.7186 - val_loss: 169.9410\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 207.7216 - val_loss: 381.7665\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 193.8824 - val_loss: 172.6306\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 252.2661 - val_loss: 148.6279\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 183.6403 - val_loss: 237.0665\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 281.9941 - val_loss: 156.0411\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 185.6581 - val_loss: 142.9198\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.7789 - val_loss: 175.0087\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 178.6014 - val_loss: 164.6903\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 183.8719 - val_loss: 233.0096\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 334.6661 - val_loss: 166.6286\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 180.4251 - val_loss: 163.7421\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 191.8197 - val_loss: 148.4913\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 174.7823 - val_loss: 158.2109\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 176.4272 - val_loss: 159.5129\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 172.7817 - val_loss: 168.5826\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 308.4093 - val_loss: 169.4725\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 217.6785 - val_loss: 166.6765\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.3520 - val_loss: 149.1967\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 196.0333 - val_loss: 151.6172\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 242.3263 - val_loss: 306.2850\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 206.4427 - val_loss: 159.5674\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 192.2572 - val_loss: 145.6541\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 211.8659 - val_loss: 188.2993\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 218.8183 - val_loss: 178.2043\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 177.7920 - val_loss: 146.7219\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 197.4416 - val_loss: 161.8382\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 197.6991 - val_loss: 145.7200\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 190.6581 - val_loss: 300.3426\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 192.9678 - val_loss: 151.3985\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 264.9073 - val_loss: 154.3526\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 177.8763 - val_loss: 155.3979\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.8977 - val_loss: 145.4158\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 233.7458 - val_loss: 145.3799\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 203.6444 - val_loss: 171.2889\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 246.8719 - val_loss: 337.7305\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.2312 - val_loss: 156.0971\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.8500 - val_loss: 213.1050\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 212.2377 - val_loss: 202.5848\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 183.4490 - val_loss: 246.2041\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 222.0975 - val_loss: 163.5748\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 191.9165 - val_loss: 174.1723\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 183.5482 - val_loss: 151.9966\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 234.3175 - val_loss: 146.3576\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 231.9901 - val_loss: 151.3566\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 196.1159 - val_loss: 143.9973\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 185.9828 - val_loss: 147.2184\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 180.5588 - val_loss: 144.2086\n",
      "Epoch 569/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 173.9657 - val_loss: 160.1754\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 191.0564 - val_loss: 162.3213\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 240.6015 - val_loss: 233.0586\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 227.5026 - val_loss: 271.2310\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 313.2380 - val_loss: 159.1044\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 173.4953 - val_loss: 183.3954\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 213.6194 - val_loss: 180.0374\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 180.0053 - val_loss: 149.3179\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 189.1655 - val_loss: 306.7230\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 253.5360 - val_loss: 159.1103\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 177.6688 - val_loss: 153.0515\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 172.8976 - val_loss: 158.4228\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 197.4269 - val_loss: 183.1599\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 215.9177 - val_loss: 176.7216\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 289.6890 - val_loss: 160.7095\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 182.4527 - val_loss: 142.2605\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 209.4040 - val_loss: 230.2174\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.1827 - val_loss: 147.5773\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 201.2965 - val_loss: 176.5620\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 299.4375 - val_loss: 147.1552\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 210.8488 - val_loss: 174.5010\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 190.9221 - val_loss: 143.9521\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 191.9279 - val_loss: 184.9142\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.7791 - val_loss: 151.7141\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 182.6886 - val_loss: 144.4582\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 209.8790 - val_loss: 166.1272\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 185.4218 - val_loss: 158.6056\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.7731 - val_loss: 167.4639\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 291.6139 - val_loss: 156.7142\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 191.8764 - val_loss: 203.7540\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 169.7079 - val_loss: 156.8429\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.5481 - val_loss: 139.6449\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 209.7402 - val_loss: 141.0080\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.1583 - val_loss: 144.6651\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 187.4534 - val_loss: 189.0575\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.2583 - val_loss: 276.4444\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 210.7411 - val_loss: 134.5973\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.3615 - val_loss: 140.1360\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 226.0421 - val_loss: 136.5196\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 171.0315 - val_loss: 137.9034\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.5891 - val_loss: 204.5100\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 254.7280 - val_loss: 138.3872\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 180.6024 - val_loss: 149.4610\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.3114 - val_loss: 162.8272\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 199.9609 - val_loss: 171.9731\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 169.9914 - val_loss: 156.7115\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.4288 - val_loss: 151.2409\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 180.7367 - val_loss: 570.2270\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 210.9986 - val_loss: 193.9003\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 215.3299 - val_loss: 221.7866\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 199.1443 - val_loss: 193.0251\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 190.9668 - val_loss: 190.1704\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.4632 - val_loss: 153.9127\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 201.1658 - val_loss: 148.9621\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 173.8850 - val_loss: 193.3047\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.0057 - val_loss: 196.7763\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 176.9717 - val_loss: 222.8251\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 203.4760 - val_loss: 140.7603\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 187.1681 - val_loss: 613.6922\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 190.3005 - val_loss: 133.9715\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 185.2806 - val_loss: 167.7097\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 188.8879 - val_loss: 164.0731\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 184.7318 - val_loss: 183.9447\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 176.8099 - val_loss: 144.3335\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 191.1422 - val_loss: 178.9929\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 207.0268 - val_loss: 155.5075\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 223.7285 - val_loss: 180.2925\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 176.9055 - val_loss: 304.2824\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 207.1832 - val_loss: 146.3501\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 187.3086 - val_loss: 135.5988\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.6061 - val_loss: 155.3647\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 186.8195 - val_loss: 145.7642\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 171.1384 - val_loss: 162.0865\n",
      "Epoch 642/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 186.2570 - val_loss: 150.8025\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 177.7398 - val_loss: 153.7123\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 177.6799 - val_loss: 135.4235\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 228.5174 - val_loss: 716.4292\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 218.9600 - val_loss: 136.8324\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 168.4561 - val_loss: 160.7840\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 167.9701 - val_loss: 151.9374\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 189.3690 - val_loss: 138.9488\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.5453 - val_loss: 136.0917\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 180.8465 - val_loss: 146.4099\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 174.5876 - val_loss: 170.7128\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 227.2667 - val_loss: 157.3485\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.7803 - val_loss: 172.7327\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 187.8972 - val_loss: 156.7602\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 193.4737 - val_loss: 171.7247\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 178.5862 - val_loss: 141.5804\n",
      "Epoch 658/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 172.7031 - val_loss: 143.5320\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 192.9625 - val_loss: 152.3924\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 225.5457 - val_loss: 137.9703\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.9299 - val_loss: 189.2902\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 184.4647 - val_loss: 144.9086\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.1718 - val_loss: 146.5178\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 185.4966 - val_loss: 133.3112\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 164.6372 - val_loss: 153.0039\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.3607 - val_loss: 208.2713\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 183.8103 - val_loss: 137.4981\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 298.9933 - val_loss: 196.2973\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.6007 - val_loss: 208.0634\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.1474 - val_loss: 150.9194\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 187.6838 - val_loss: 158.0681\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.4160 - val_loss: 137.8901\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 230.7525 - val_loss: 147.1575\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 173.8889 - val_loss: 180.0159\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 163.2692 - val_loss: 153.8142\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.9701 - val_loss: 138.5788\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 171.2840 - val_loss: 173.4379\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.6051 - val_loss: 200.2159\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 436.7778 - val_loss: 1437.7976\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 734.2187 - val_loss: 598.3946\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 489.8124 - val_loss: 471.5424\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 379.7678 - val_loss: 333.9035\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 374.8612 - val_loss: 409.6802\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 291.5835 - val_loss: 253.2483\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 265.8525 - val_loss: 211.1073\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 240.8599 - val_loss: 268.2277\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 247.4881 - val_loss: 204.3334\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 216.8739 - val_loss: 401.4228\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 198.3843 - val_loss: 174.1855\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 202.2320 - val_loss: 248.6945\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 205.2560 - val_loss: 247.4618\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 212.0576 - val_loss: 163.5402\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 201.0438 - val_loss: 152.8687\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 189.0877 - val_loss: 204.7981\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 210.5524 - val_loss: 168.6111\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 220.3956 - val_loss: 179.3960\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 205.8613 - val_loss: 218.0511\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 197.431 - 0s 57us/step - loss: 197.1387 - val_loss: 171.7477\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 183.3009 - val_loss: 159.7902\n",
      "Epoch 700/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 198.5949 - val_loss: 183.2584\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 259.2472 - val_loss: 148.4277\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 187.1756 - val_loss: 193.4633\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 171.7801 - val_loss: 163.7236\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 177.4152 - val_loss: 176.2383\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 203.7236 - val_loss: 146.1384\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 193.5488 - val_loss: 249.2235\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 188.6212 - val_loss: 287.3838\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 207.9375 - val_loss: 160.6127\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 177.9095 - val_loss: 196.9002\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.4893 - val_loss: 255.8264\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 191.1192 - val_loss: 184.5920\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 191.6643 - val_loss: 149.9926\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 191.2075 - val_loss: 152.7181\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 227.5593 - val_loss: 200.0045\n",
      "Epoch 715/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 181.2066 - val_loss: 187.5392\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 191.7640 - val_loss: 139.0419\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 233.7145 - val_loss: 201.1807\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 183.3355 - val_loss: 177.5927\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 182.3576 - val_loss: 143.1876\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 185.2387 - val_loss: 152.8880\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 183.4301 - val_loss: 260.0343\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 218.3484 - val_loss: 162.5237\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 193.3986 - val_loss: 160.2926\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 221.2726 - val_loss: 173.5732\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 219.5756 - val_loss: 152.8934\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 188.8123 - val_loss: 150.7374\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 201.9689 - val_loss: 330.4205\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 199.6520 - val_loss: 140.1687\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.0771 - val_loss: 156.2684\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 203.7547 - val_loss: 375.9186\n",
      "Epoch 731/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 188.6120 - val_loss: 186.6688\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 189.2950 - val_loss: 145.3702\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 217.1196 - val_loss: 183.0358\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.9362 - val_loss: 161.0051\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 171.5233 - val_loss: 183.8447\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 168.6402 - val_loss: 212.2383\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 184.8213 - val_loss: 148.2770\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.6686 - val_loss: 151.4521\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 181.5793 - val_loss: 226.2558\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.5518 - val_loss: 144.4443\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 191.8905 - val_loss: 191.8839\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 183.2866 - val_loss: 147.6376\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 183.8082 - val_loss: 139.6992\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.8874 - val_loss: 156.7555\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 202.1296 - val_loss: 383.2427\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 238.3182 - val_loss: 191.8168\n",
      "Epoch 747/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.6111 - val_loss: 143.3437\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 217.7948 - val_loss: 253.5104\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 185.8905 - val_loss: 147.5763\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 200.5610 - val_loss: 182.6772\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 185.3927 - val_loss: 157.5249\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 172.6976 - val_loss: 163.1462\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 191.6787 - val_loss: 176.0008\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.2979 - val_loss: 171.1906\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 197.2209 - val_loss: 440.3063\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 184.4297 - val_loss: 143.7949\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 209.9429 - val_loss: 344.5964\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 179.8438 - val_loss: 148.5606\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 213.7465 - val_loss: 148.5702\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 172.8019 - val_loss: 169.7572\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 181.9641 - val_loss: 415.8652\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 186.8175 - val_loss: 210.2625\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 184.5282 - val_loss: 150.0633\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 194.4983 - val_loss: 151.1051\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 181.0457 - val_loss: 139.3367\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 201.0532 - val_loss: 158.5972\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 189.9291 - val_loss: 218.8202\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 210.7423 - val_loss: 150.6448\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 204.2510 - val_loss: 153.3594\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 174.0564 - val_loss: 165.5024\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 186.8815 - val_loss: 188.0811\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 192.1790 - val_loss: 324.3815\n",
      "Epoch 773/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 182.4333 - val_loss: 179.4252\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 206.1948 - val_loss: 135.5843\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 196.3716 - val_loss: 147.4988\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.8900 - val_loss: 137.9960\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 176.1028 - val_loss: 307.8991\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 196.6623 - val_loss: 148.7837\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 205.3079 - val_loss: 484.8120\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 198.0046 - val_loss: 178.3354\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 173.8174 - val_loss: 196.5297\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 187.7518 - val_loss: 161.9486\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 186.5161 - val_loss: 148.1261\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.9169 - val_loss: 416.6637\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 182.6164 - val_loss: 269.1113\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.5526 - val_loss: 192.8914\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 190.9970 - val_loss: 152.8790\n",
      "Epoch 788/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.7320 - val_loss: 155.0709\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 204.0411 - val_loss: 173.9018\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 149.0743 - val_loss: 170.4463\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 187.3618 - val_loss: 168.8905\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 206.5059 - val_loss: 357.6529\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 194.1542 - val_loss: 147.3623\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.8365 - val_loss: 158.2908\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.8851 - val_loss: 159.1326\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 161.1738 - val_loss: 153.9631\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 173.0721 - val_loss: 261.4463\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.3419 - val_loss: 165.2860\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 190.4896 - val_loss: 238.0890\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 179.8516 - val_loss: 262.0151\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 166.9594 - val_loss: 164.2821\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 169.2381 - val_loss: 155.6524\n",
      "Epoch 803/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 237.5902 - val_loss: 188.2045\n",
      "Epoch 804/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 178.8542 - val_loss: 149.7592\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 163.9060 - val_loss: 164.7169\n",
      "Epoch 806/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 208.3570 - val_loss: 159.9025\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.8273 - val_loss: 141.7079\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 176.3255 - val_loss: 140.5097\n",
      "Epoch 809/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.3594 - val_loss: 203.5601\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 209.7088 - val_loss: 197.8487\n",
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.9661 - val_loss: 135.2587\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 170.8447 - val_loss: 176.9969\n",
      "Epoch 813/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 167.2100 - val_loss: 148.5121\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 190.7699 - val_loss: 186.5579\n",
      "Epoch 815/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.2626 - val_loss: 137.8825\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 182.5832 - val_loss: 169.1365\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 176.2794 - val_loss: 143.0409\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 167.9623 - val_loss: 198.1921\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 205.9925 - val_loss: 204.7529\n",
      "Epoch 820/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 171.9350 - val_loss: 160.1872\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 176.5334 - val_loss: 144.6929\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 178.3471 - val_loss: 152.2066\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 199.9348 - val_loss: 287.9417\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.4328 - val_loss: 161.3769\n",
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 181.8964 - val_loss: 247.3699\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 172.6005 - val_loss: 148.6988\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.8881 - val_loss: 176.7730\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.4605 - val_loss: 165.9417\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.8030 - val_loss: 146.8718\n",
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 183.9975 - val_loss: 269.5547\n",
      "Epoch 831/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 194.9299 - val_loss: 272.6854\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 174.5771 - val_loss: 172.8899\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.8922 - val_loss: 144.4955\n",
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 186.9512 - val_loss: 163.7792\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 169.9783 - val_loss: 138.2644\n",
      "Epoch 836/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.2812 - val_loss: 134.2660\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 212.1122 - val_loss: 164.0390\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.4101 - val_loss: 156.1022\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.6203 - val_loss: 216.3552\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.9525 - val_loss: 277.7048\n",
      "Epoch 841/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.4852 - val_loss: 148.1061\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.8505 - val_loss: 200.8062\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 204.6902 - val_loss: 154.8689\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 181.4717 - val_loss: 145.1466\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 181.0767 - val_loss: 149.2450\n",
      "Epoch 846/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 169.7575 - val_loss: 151.9599\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 178.0964 - val_loss: 148.8433\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 188.1917 - val_loss: 159.2096\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 155.9753 - val_loss: 140.9367\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 189.0643 - val_loss: 154.2597\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.5066 - val_loss: 184.6636\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.9597 - val_loss: 158.7928\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 181.5619 - val_loss: 143.4944\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.7511 - val_loss: 134.6108\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.1641 - val_loss: 149.5166\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 186.0591 - val_loss: 134.7298\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 187.3379 - val_loss: 139.3483\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 153.1384 - val_loss: 167.7131\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 172.6191 - val_loss: 190.8990\n",
      "Epoch 860/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 179.8652 - val_loss: 190.7038\n",
      "Epoch 861/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 177.8247 - val_loss: 163.2693\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 190.8893 - val_loss: 144.3722\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 171.6577 - val_loss: 186.4835\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.8899 - val_loss: 150.4302\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 193.2860 - val_loss: 191.9910\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 171.5178 - val_loss: 147.0758\n",
      "Epoch 867/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.6888 - val_loss: 258.0844\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 187.5122 - val_loss: 139.7448\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 202.0988 - val_loss: 138.7995\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 208.4629 - val_loss: 254.0714\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.4793 - val_loss: 137.8075\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 161.1039 - val_loss: 142.7888\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.1959 - val_loss: 209.1138\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 185.0494 - val_loss: 153.8092\n",
      "Epoch 875/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 153.0247 - val_loss: 210.3322\n",
      "Epoch 876/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 172.1537 - val_loss: 180.5074\n",
      "Epoch 877/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 162.1083 - val_loss: 138.4863\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.5881 - val_loss: 200.3982\n",
      "Epoch 879/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 168.0450 - val_loss: 141.2432\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.4489 - val_loss: 139.2378\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.5828 - val_loss: 154.6961\n",
      "Epoch 882/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 175.0001 - val_loss: 154.6180\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 157.7757 - val_loss: 139.6401\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 158.0900 - val_loss: 135.7322\n",
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 206.7743 - val_loss: 201.6626\n",
      "Epoch 886/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 166.4386 - val_loss: 183.8890\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 175.9113 - val_loss: 183.3039\n",
      "Epoch 888/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 177.5073 - val_loss: 136.9176\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 208.0518 - val_loss: 156.9483\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.9880 - val_loss: 140.6154\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.3710 - val_loss: 154.6597\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 176.8050 - val_loss: 136.1418\n",
      "Epoch 893/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.8982 - val_loss: 155.4631\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.8684 - val_loss: 155.9533\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.3361 - val_loss: 142.8956\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 181.0859 - val_loss: 201.1722\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 161.9825 - val_loss: 150.0977\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.8308 - val_loss: 148.5609\n",
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.2414 - val_loss: 166.9025\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.5593 - val_loss: 136.7350\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 156.4994 - val_loss: 155.0807\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.4478 - val_loss: 142.3384\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.8994 - val_loss: 140.7301\n",
      "Epoch 904/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.2677 - val_loss: 159.0813\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 177.3199 - val_loss: 145.8387\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.4838 - val_loss: 147.6507\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 185.6798 - val_loss: 136.6679\n",
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.3763 - val_loss: 137.9316\n",
      "Epoch 909/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 195.2784 - val_loss: 311.1327\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.2082 - val_loss: 239.7383\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 223.4789 - val_loss: 158.1698\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 168.3082 - val_loss: 136.1949\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 188.4901 - val_loss: 167.9228\n",
      "Epoch 914/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 196.0942 - val_loss: 135.6222\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.0080 - val_loss: 143.2922\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.2923 - val_loss: 143.2400\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 180.7939 - val_loss: 142.5046\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 170.1396 - val_loss: 177.4950\n",
      "Epoch 919/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.3160 - val_loss: 168.1822\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 181.2446 - val_loss: 147.0768\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 190.0933 - val_loss: 231.0755\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.1990 - val_loss: 146.1837\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.1550 - val_loss: 162.4696\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.8970 - val_loss: 173.0084\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 191.7100 - val_loss: 177.7669\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 171.5757 - val_loss: 193.9488\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 177.8350 - val_loss: 137.4849\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 170.9629 - val_loss: 163.4652\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.5019 - val_loss: 157.2340\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.1426 - val_loss: 146.6261\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 181.1938 - val_loss: 172.7669\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.5401 - val_loss: 180.7232\n",
      "Epoch 933/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 164.4115 - val_loss: 150.7021\n",
      "Epoch 934/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 57us/step - loss: 168.7481 - val_loss: 150.8051\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 161.8212 - val_loss: 137.5903\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 188.2705 - val_loss: 194.5759\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.2980 - val_loss: 136.5757\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 164.9896 - val_loss: 167.9728\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.3787 - val_loss: 299.2979\n",
      "Epoch 940/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 169.5503 - val_loss: 161.0626\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 203.4743 - val_loss: 222.0339\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 169.5624 - val_loss: 265.7336\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.4020 - val_loss: 160.9582\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 166.2725 - val_loss: 140.9724\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 169.7566 - val_loss: 135.2715\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 161.2648 - val_loss: 215.8238\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 221.0045 - val_loss: 295.5942\n",
      "Epoch 948/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.0930 - val_loss: 178.7441\n",
      "Epoch 949/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.5288 - val_loss: 134.3184\n",
      "Epoch 950/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 199.8464 - val_loss: 168.2468\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 169.6785 - val_loss: 145.4935\n",
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.0847 - val_loss: 131.5526\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.6132 - val_loss: 173.1669\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.8823 - val_loss: 175.8252\n",
      "Epoch 955/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.1595 - val_loss: 214.6286\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.8357 - val_loss: 197.8459\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.3249 - val_loss: 162.8125\n",
      "Epoch 958/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 204.3852 - val_loss: 243.8224\n",
      "Epoch 959/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 172.7629 - val_loss: 167.9537\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 168.3598 - val_loss: 173.2804\n",
      "Epoch 961/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.8646 - val_loss: 143.4794\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.1459 - val_loss: 148.2097\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 178.7665 - val_loss: 248.0363\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.0576 - val_loss: 405.1127\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 204.7813 - val_loss: 168.9500\n",
      "Epoch 966/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 175.8980 - val_loss: 234.3673\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 207.9862 - val_loss: 185.6658\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.9087 - val_loss: 207.1870\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 181.4580 - val_loss: 148.4566\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.2302 - val_loss: 134.3162\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 182.6742 - val_loss: 162.9344\n",
      "Epoch 972/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.5339 - val_loss: 167.2031\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.3668 - val_loss: 138.6533\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.1853 - val_loss: 133.4376\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.3226 - val_loss: 151.6115\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.0557 - val_loss: 267.4817\n",
      "Epoch 977/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.9850 - val_loss: 168.9800\n",
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 174.4083 - val_loss: 144.8517\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.8816 - val_loss: 553.6021\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 196.4814 - val_loss: 138.4015\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.9346 - val_loss: 151.9189\n",
      "Epoch 982/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.3466 - val_loss: 209.8451\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 172.0431 - val_loss: 186.8939\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 201.5905 - val_loss: 167.3613\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 208.9112 - val_loss: 152.7555\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.4133 - val_loss: 152.3018\n",
      "Epoch 987/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.7716 - val_loss: 140.9980\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 172.1270 - val_loss: 238.5774\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.4702 - val_loss: 148.3498\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.5562 - val_loss: 159.0235\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 164.3970 - val_loss: 170.7283\n",
      "Epoch 992/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.7587 - val_loss: 137.1091\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 173.7620 - val_loss: 239.6655\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.8094 - val_loss: 268.6105\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.5068 - val_loss: 154.4973\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 169.6166 - val_loss: 171.5399\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 190.1256 - val_loss: 154.7017\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.4385 - val_loss: 159.0429\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 191.9512 - val_loss: 133.6897\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.5858 - val_loss: 194.2779\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 181.6160 - val_loss: 175.7244\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 171.4007 - val_loss: 133.3269\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 165.7813 - val_loss: 148.4257\n",
      "Epoch 1004/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 177.2712 - val_loss: 243.3725\n",
      "Epoch 1005/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 183.3858 - val_loss: 281.5445\n",
      "Epoch 1006/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 162.7009 - val_loss: 152.0271\n",
      "Epoch 1007/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 86us/step - loss: 161.1214 - val_loss: 201.4327\n",
      "Epoch 1008/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 151.7302 - val_loss: 149.2550\n",
      "Epoch 1009/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 153.3074 - val_loss: 268.5751\n",
      "Epoch 1010/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 237.7213 - val_loss: 184.6780\n",
      "Epoch 1011/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 180.0841 - val_loss: 140.3978\n",
      "Epoch 1012/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.6175 - val_loss: 152.2526\n",
      "Epoch 1013/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.3532 - val_loss: 131.9335\n",
      "Epoch 1014/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.8534 - val_loss: 134.0232\n",
      "Epoch 1015/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.7528 - val_loss: 223.4899\n",
      "Epoch 1016/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 164.5173 - val_loss: 312.1272\n",
      "Epoch 1017/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 212.8872 - val_loss: 301.7071\n",
      "Epoch 1018/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 165.4072 - val_loss: 149.4413\n",
      "Epoch 1019/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.2996 - val_loss: 148.8971\n",
      "Epoch 1020/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 169.5133 - val_loss: 145.6613\n",
      "Epoch 1021/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 167.6251 - val_loss: 182.7052\n",
      "Epoch 1022/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.6240 - val_loss: 169.9590\n",
      "Epoch 1023/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 186.0320 - val_loss: 165.1254\n",
      "Epoch 1024/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.9207 - val_loss: 132.6339\n",
      "Epoch 1025/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.6934 - val_loss: 179.4150\n",
      "Epoch 1026/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.8138 - val_loss: 145.4058\n",
      "Epoch 1027/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.8466 - val_loss: 162.7882\n",
      "Epoch 1028/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 160.6871 - val_loss: 157.9565\n",
      "Epoch 1029/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.7340 - val_loss: 151.0008\n",
      "Epoch 1030/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 163.3428 - val_loss: 134.0992\n",
      "Epoch 1031/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.5247 - val_loss: 198.1773\n",
      "Epoch 1032/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.1930 - val_loss: 169.4289\n",
      "Epoch 1033/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.3033 - val_loss: 147.6402\n",
      "Epoch 1034/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.3292 - val_loss: 180.2650\n",
      "Epoch 1035/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 173.2839 - val_loss: 211.3845\n",
      "Epoch 1036/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 164.5938 - val_loss: 437.4149\n",
      "Epoch 1037/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.8356 - val_loss: 156.8898\n",
      "Epoch 1038/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.3393 - val_loss: 178.7632\n",
      "Epoch 1039/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.7105 - val_loss: 147.8219\n",
      "Epoch 1040/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.7772 - val_loss: 146.1581\n",
      "Epoch 1041/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.4530 - val_loss: 139.3238\n",
      "Epoch 1042/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 175.2027 - val_loss: 136.3762\n",
      "Epoch 1043/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.1395 - val_loss: 183.6740\n",
      "Epoch 1044/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.8064 - val_loss: 210.6242\n",
      "Epoch 1045/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.8115 - val_loss: 183.7445\n",
      "Epoch 1046/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 204.8441 - val_loss: 270.3484\n",
      "Epoch 1047/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 165.4258 - val_loss: 134.6190\n",
      "Epoch 1048/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.8672 - val_loss: 159.6975\n",
      "Epoch 1049/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.8304 - val_loss: 140.7661\n",
      "Epoch 1050/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 156.4293 - val_loss: 209.6645\n",
      "Epoch 1051/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 173.4758 - val_loss: 145.3095\n",
      "Epoch 1052/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 162.6103 - val_loss: 142.4551\n",
      "Epoch 1053/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 180.7728 - val_loss: 153.4850\n",
      "Epoch 1054/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.4690 - val_loss: 256.5112\n",
      "Epoch 1055/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 164.9844 - val_loss: 181.0092\n",
      "Epoch 1056/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.3314 - val_loss: 165.3377\n",
      "Epoch 1057/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 183.5915 - val_loss: 167.5536\n",
      "Epoch 1058/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 176.1068 - val_loss: 140.5246\n",
      "Epoch 1059/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 180.4010 - val_loss: 268.2267\n",
      "Epoch 1060/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 171.9607 - val_loss: 204.1107\n",
      "Epoch 1061/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 188.9449 - val_loss: 381.2853\n",
      "Epoch 1062/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 171.6193 - val_loss: 249.5999\n",
      "Epoch 1063/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.2178 - val_loss: 165.8893\n",
      "Epoch 1064/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 167.8020 - val_loss: 203.0631\n",
      "Epoch 1065/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 169.5690 - val_loss: 144.3002\n",
      "Epoch 1066/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 161.3563 - val_loss: 167.7057\n",
      "Epoch 1067/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 146.5214 - val_loss: 145.4662\n",
      "Epoch 1068/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 173.6876 - val_loss: 158.5694\n",
      "Epoch 1069/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.5319 - val_loss: 155.7156\n",
      "Epoch 1070/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 153.1932 - val_loss: 300.6061\n",
      "Epoch 1071/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 197.1787 - val_loss: 215.4640\n",
      "Epoch 1072/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.7017 - val_loss: 162.0173\n",
      "Epoch 1073/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 165.0616 - val_loss: 149.1760\n",
      "Epoch 1074/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 153.0297 - val_loss: 141.6782\n",
      "Epoch 1075/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.0166 - val_loss: 430.9558\n",
      "Epoch 1076/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.6250 - val_loss: 223.3780\n",
      "Epoch 1077/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 178.7032 - val_loss: 133.8118\n",
      "Epoch 1078/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 157.468 - 0s 57us/step - loss: 157.4227 - val_loss: 136.2549\n",
      "Epoch 1079/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.7337 - val_loss: 148.3548\n",
      "Epoch 1080/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.1278 - val_loss: 176.4978\n",
      "Epoch 1081/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 160.2642 - val_loss: 170.9179\n",
      "Epoch 1082/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 171.6226 - val_loss: 139.7857\n",
      "Epoch 1083/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.2509 - val_loss: 160.1558\n",
      "Epoch 1084/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.6109 - val_loss: 230.1532\n",
      "Epoch 1085/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.8517 - val_loss: 235.1196\n",
      "Epoch 1086/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.3480 - val_loss: 198.5379\n",
      "Epoch 1087/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.7177 - val_loss: 145.6974\n",
      "Epoch 1088/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 175.0981 - val_loss: 130.6624\n",
      "Epoch 1089/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 180.1037 - val_loss: 154.7520\n",
      "Epoch 1090/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 166.5432 - val_loss: 143.2951\n",
      "Epoch 1091/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.9714 - val_loss: 198.7211\n",
      "Epoch 1092/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 169.2787 - val_loss: 155.0311\n",
      "Epoch 1093/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.5189 - val_loss: 130.2868\n",
      "Epoch 1094/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.4811 - val_loss: 143.3496\n",
      "Epoch 1095/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.5704 - val_loss: 148.4363\n",
      "Epoch 1096/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 218.5937 - val_loss: 499.7820\n",
      "Epoch 1097/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.8129 - val_loss: 149.4400\n",
      "Epoch 1098/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 174.3887 - val_loss: 164.3491\n",
      "Epoch 1099/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 185.4241 - val_loss: 130.3496\n",
      "Epoch 1100/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.7270 - val_loss: 216.6299\n",
      "Epoch 1101/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 172.2948 - val_loss: 150.7672\n",
      "Epoch 1102/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.5074 - val_loss: 144.9933\n",
      "Epoch 1103/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.2870 - val_loss: 184.7375\n",
      "Epoch 1104/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 175.8118 - val_loss: 195.8976\n",
      "Epoch 1105/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 156.5118 - val_loss: 168.8905\n",
      "Epoch 1106/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.3435 - val_loss: 383.2926\n",
      "Epoch 1107/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 169.9631 - val_loss: 130.0827\n",
      "Epoch 1108/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.8789 - val_loss: 135.2427\n",
      "Epoch 1109/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 170.5160 - val_loss: 147.3351\n",
      "Epoch 1110/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.2629 - val_loss: 139.0728\n",
      "Epoch 1111/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.0142 - val_loss: 204.0848\n",
      "Epoch 1112/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 163.8199 - val_loss: 390.2449\n",
      "Epoch 1113/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.6403 - val_loss: 170.1824\n",
      "Epoch 1114/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.2890 - val_loss: 159.8023\n",
      "Epoch 1115/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 164.1791 - val_loss: 133.0360\n",
      "Epoch 1116/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 217.8615 - val_loss: 165.0179\n",
      "Epoch 1117/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.5366 - val_loss: 230.4954\n",
      "Epoch 1118/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.0096 - val_loss: 178.4461\n",
      "Epoch 1119/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 159.8224 - val_loss: 240.8951\n",
      "Epoch 1120/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 179.5289 - val_loss: 142.5244\n",
      "Epoch 1121/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 164.0892 - val_loss: 220.2484\n",
      "Epoch 1122/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.9263 - val_loss: 171.3710\n",
      "Epoch 1123/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 181.9571 - val_loss: 160.0054\n",
      "Epoch 1124/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.0977 - val_loss: 140.6132\n",
      "Epoch 1125/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.9229 - val_loss: 152.8579\n",
      "Epoch 1126/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.0096 - val_loss: 143.3060\n",
      "Epoch 1127/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.3876 - val_loss: 136.4999\n",
      "Epoch 1128/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 231.2605 - val_loss: 240.5773\n",
      "Epoch 1129/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 150.6278 - val_loss: 209.6865\n",
      "Epoch 1130/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 165.2425 - val_loss: 144.0298\n",
      "Epoch 1131/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 151.6549 - val_loss: 165.9946\n",
      "Epoch 1132/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 159.8413 - val_loss: 128.5486\n",
      "Epoch 1133/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.7727 - val_loss: 141.4315\n",
      "Epoch 1134/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.4867 - val_loss: 139.5451\n",
      "Epoch 1135/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.4047 - val_loss: 135.6383\n",
      "Epoch 1136/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 155.2377 - val_loss: 141.1776\n",
      "Epoch 1137/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.1759 - val_loss: 136.3683\n",
      "Epoch 1138/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.4489 - val_loss: 137.3244\n",
      "Epoch 1139/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.9707 - val_loss: 135.5996\n",
      "Epoch 1140/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.0903 - val_loss: 141.7091\n",
      "Epoch 1141/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 178.3503 - val_loss: 148.1582\n",
      "Epoch 1142/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.5163 - val_loss: 190.7858\n",
      "Epoch 1143/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 158.9616 - val_loss: 150.8607\n",
      "Epoch 1144/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.6327 - val_loss: 166.3877\n",
      "Epoch 1145/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.6603 - val_loss: 130.2803\n",
      "Epoch 1146/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.6068 - val_loss: 164.2104\n",
      "Epoch 1147/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.7002 - val_loss: 138.9984\n",
      "Epoch 1148/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.3279 - val_loss: 213.9489\n",
      "Epoch 1149/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.6779 - val_loss: 153.8669\n",
      "Epoch 1150/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 201.6777 - val_loss: 312.3595\n",
      "Epoch 1151/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.9279 - val_loss: 142.1094\n",
      "Epoch 1152/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.6473 - val_loss: 185.7466\n",
      "Epoch 1153/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 158.5288 - val_loss: 147.9431\n",
      "Epoch 1154/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.0437 - val_loss: 227.2704\n",
      "Epoch 1155/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 182.9195 - val_loss: 140.3120\n",
      "Epoch 1156/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.6960 - val_loss: 136.4567\n",
      "Epoch 1157/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.1455 - val_loss: 136.0225\n",
      "Epoch 1158/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 156.5569 - val_loss: 159.9998\n",
      "Epoch 1159/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.9349 - val_loss: 168.6404\n",
      "Epoch 1160/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.9898 - val_loss: 130.9001\n",
      "Epoch 1161/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 158.0098 - val_loss: 138.4784\n",
      "Epoch 1162/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.4556 - val_loss: 182.1396\n",
      "Epoch 1163/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.5884 - val_loss: 152.8272\n",
      "Epoch 1164/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 226.1255 - val_loss: 308.2724\n",
      "Epoch 1165/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.5285 - val_loss: 137.1186\n",
      "Epoch 1166/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.0093 - val_loss: 179.0803\n",
      "Epoch 1167/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 161.2444 - val_loss: 184.0202\n",
      "Epoch 1168/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 161.9295 - val_loss: 234.3387\n",
      "Epoch 1169/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 150.0261 - val_loss: 237.1928\n",
      "Epoch 1170/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 179.9160 - val_loss: 140.4541\n",
      "Epoch 1171/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.5111 - val_loss: 166.9832\n",
      "Epoch 1172/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.7104 - val_loss: 133.2010\n",
      "Epoch 1173/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.4590 - val_loss: 133.5857\n",
      "Epoch 1174/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.3190 - val_loss: 297.9471\n",
      "Epoch 1175/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 189.8922 - val_loss: 184.3221\n",
      "Epoch 1176/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.2207 - val_loss: 207.6437\n",
      "Epoch 1177/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.9083 - val_loss: 135.8242\n",
      "Epoch 1178/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.6485 - val_loss: 262.8683\n",
      "Epoch 1179/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.7799 - val_loss: 143.0689\n",
      "Epoch 1180/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.9249 - val_loss: 161.5202\n",
      "Epoch 1181/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.2606 - val_loss: 290.8140\n",
      "Epoch 1182/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 169.7861 - val_loss: 145.4332\n",
      "Epoch 1183/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.1502 - val_loss: 157.3477\n",
      "Epoch 1184/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.6216 - val_loss: 143.8595\n",
      "Epoch 1185/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.1408 - val_loss: 147.0612\n",
      "Epoch 1186/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.5752 - val_loss: 141.3408\n",
      "Epoch 1187/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.8315 - val_loss: 208.4494\n",
      "Epoch 1188/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 221.5044 - val_loss: 214.0289\n",
      "Epoch 1189/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 157.4675 - val_loss: 164.8205\n",
      "Epoch 1190/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 160.0231 - val_loss: 153.3166\n",
      "Epoch 1191/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 152.3304 - val_loss: 145.1350\n",
      "Epoch 1192/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 151.8554 - val_loss: 166.7185\n",
      "Epoch 1193/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.0040 - val_loss: 130.2974\n",
      "Epoch 1194/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.7568 - val_loss: 141.0200\n",
      "Epoch 1195/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.7495 - val_loss: 140.9638\n",
      "Epoch 1196/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.7709 - val_loss: 155.0295\n",
      "Epoch 1197/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.0901 - val_loss: 168.5383\n",
      "Epoch 1198/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 202.1388 - val_loss: 133.9695\n",
      "Epoch 1199/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 172.9776 - val_loss: 175.6514\n",
      "Epoch 1200/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.2466 - val_loss: 151.2664\n",
      "Epoch 1201/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.5286 - val_loss: 152.4188\n",
      "Epoch 1202/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 165.9002 - val_loss: 136.8630\n",
      "Epoch 1203/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.5044 - val_loss: 165.4527\n",
      "Epoch 1204/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.2893 - val_loss: 134.8011\n",
      "Epoch 1205/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 160.4694 - val_loss: 130.4574\n",
      "Epoch 1206/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.1515 - val_loss: 136.4325\n",
      "Epoch 1207/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.9367 - val_loss: 140.0173\n",
      "Epoch 1208/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 172.9133 - val_loss: 152.7580\n",
      "Epoch 1209/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.0889 - val_loss: 131.7877\n",
      "Epoch 1210/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 177.7909 - val_loss: 167.2496\n",
      "Epoch 1211/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.9215 - val_loss: 133.5467\n",
      "Epoch 1212/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 148.9573 - val_loss: 163.4126\n",
      "Epoch 1213/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.2063 - val_loss: 149.0347\n",
      "Epoch 1214/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.9998 - val_loss: 133.6409\n",
      "Epoch 1215/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 178.3027 - val_loss: 149.9527\n",
      "Epoch 1216/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.5983 - val_loss: 150.6860\n",
      "Epoch 1217/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.0201 - val_loss: 131.1226\n",
      "Epoch 1218/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.5592 - val_loss: 136.8572\n",
      "Epoch 1219/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 175.9591 - val_loss: 192.6790\n",
      "Epoch 1220/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.4645 - val_loss: 135.4537\n",
      "Epoch 1221/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 207.0635 - val_loss: 194.6710\n",
      "Epoch 1222/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 149.6250 - val_loss: 131.7290\n",
      "Epoch 1223/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 176.1784 - val_loss: 189.9549\n",
      "Epoch 1224/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 212.7878 - val_loss: 147.1195\n",
      "Epoch 1225/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 179.0008 - val_loss: 164.5898\n",
      "Epoch 1226/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.9367 - val_loss: 145.6050\n",
      "Epoch 1227/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.8552 - val_loss: 136.8251\n",
      "Epoch 1228/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.7126 - val_loss: 319.8325\n",
      "Epoch 1229/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 173.0225 - val_loss: 132.7988\n",
      "Epoch 1230/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.7284 - val_loss: 179.3028\n",
      "Epoch 1231/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.7404 - val_loss: 165.9469\n",
      "Epoch 1232/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.8315 - val_loss: 141.3927\n",
      "Epoch 1233/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 188.8388 - val_loss: 159.9862\n",
      "Epoch 1234/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.0582 - val_loss: 171.1369\n",
      "Epoch 1235/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 188.6592 - val_loss: 196.7226\n",
      "Epoch 1236/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.8417 - val_loss: 177.5816\n",
      "Epoch 1237/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 153.6522 - val_loss: 165.6156\n",
      "Epoch 1238/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.2686 - val_loss: 134.8753\n",
      "Epoch 1239/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 169.0344 - val_loss: 264.4483\n",
      "Epoch 1240/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 167.8623 - val_loss: 166.8621\n",
      "Epoch 1241/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.7230 - val_loss: 148.0369\n",
      "Epoch 1242/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 292.8060 - val_loss: 182.3122\n",
      "Epoch 1243/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 201.7742 - val_loss: 268.0667\n",
      "Epoch 1244/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 238.2253 - val_loss: 201.4431\n",
      "Epoch 1245/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 200.4854 - val_loss: 178.8139\n",
      "Epoch 1246/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.7712 - val_loss: 209.9064\n",
      "Epoch 1247/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 184.1024 - val_loss: 265.1463\n",
      "Epoch 1248/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.5779 - val_loss: 160.1905\n",
      "Epoch 1249/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 185.3279 - val_loss: 167.9466\n",
      "Epoch 1250/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 197.6680 - val_loss: 156.8074\n",
      "Epoch 1251/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 176.7267 - val_loss: 284.1478\n",
      "Epoch 1252/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 194.3376 - val_loss: 150.1101\n",
      "Epoch 1253/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 167.2806 - val_loss: 272.4616\n",
      "Epoch 1254/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 187.6724 - val_loss: 154.6045\n",
      "Epoch 1255/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 160.1248 - val_loss: 156.4207\n",
      "Epoch 1256/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 164.6084 - val_loss: 161.5363\n",
      "Epoch 1257/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 162.5211 - val_loss: 298.1556\n",
      "Epoch 1258/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.6742 - val_loss: 151.5316\n",
      "Epoch 1259/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 192.3476 - val_loss: 302.0143\n",
      "Epoch 1260/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.6581 - val_loss: 149.5934\n",
      "Epoch 1261/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 162.8542 - val_loss: 139.7412\n",
      "Epoch 1262/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.2211 - val_loss: 146.2943\n",
      "Epoch 1263/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.2817 - val_loss: 160.0315\n",
      "Epoch 1264/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 192.2546 - val_loss: 142.5013\n",
      "Epoch 1265/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 184.1968 - val_loss: 162.8308\n",
      "Epoch 1266/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.3514 - val_loss: 142.4538\n",
      "Epoch 1267/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 176.5360 - val_loss: 157.1582\n",
      "Epoch 1268/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 179.3045 - val_loss: 153.8179\n",
      "Epoch 1269/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 223.7600 - val_loss: 163.5158\n",
      "Epoch 1270/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 168.1166 - val_loss: 173.9008\n",
      "Epoch 1271/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.0118 - val_loss: 154.4224\n",
      "Epoch 1272/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.3017 - val_loss: 137.2231\n",
      "Epoch 1273/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.2683 - val_loss: 278.3557\n",
      "Epoch 1274/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 237.6270 - val_loss: 173.0604\n",
      "Epoch 1275/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.7456 - val_loss: 141.6190\n",
      "Epoch 1276/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 155.7926 - val_loss: 171.4861\n",
      "Epoch 1277/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.2786 - val_loss: 183.8145\n",
      "Epoch 1278/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.4006 - val_loss: 158.3094\n",
      "Epoch 1279/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 149.3801 - val_loss: 145.1741\n",
      "Epoch 1280/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.8543 - val_loss: 135.1024\n",
      "Epoch 1281/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.6813 - val_loss: 237.7078\n",
      "Epoch 1282/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 169.0186 - val_loss: 141.7424\n",
      "Epoch 1283/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.6350 - val_loss: 131.3048\n",
      "Epoch 1284/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.2060 - val_loss: 326.2266\n",
      "Epoch 1285/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 174.7345 - val_loss: 203.3213\n",
      "Epoch 1286/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.3259 - val_loss: 139.6433\n",
      "Epoch 1287/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.2097 - val_loss: 191.0220\n",
      "Epoch 1288/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.3896 - val_loss: 137.7173\n",
      "Epoch 1289/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.5681 - val_loss: 131.9113\n",
      "Epoch 1290/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.0308 - val_loss: 146.7892\n",
      "Epoch 1291/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.8083 - val_loss: 146.7509\n",
      "Epoch 1292/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.8258 - val_loss: 159.7595\n",
      "Epoch 1293/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.1861 - val_loss: 303.8186\n",
      "Epoch 1294/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 162.9706 - val_loss: 129.5720\n",
      "Epoch 1295/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 161.6606 - val_loss: 244.7392\n",
      "Epoch 1296/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.4100 - val_loss: 151.5729\n",
      "Epoch 1297/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.9649 - val_loss: 163.1457\n",
      "Epoch 1298/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.8003 - val_loss: 138.8518\n",
      "Epoch 1299/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.1354 - val_loss: 137.0919\n",
      "Epoch 1300/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.2442 - val_loss: 228.2200\n",
      "Epoch 1301/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 193.4926 - val_loss: 215.7606\n",
      "Epoch 1302/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.2806 - val_loss: 141.9629\n",
      "Epoch 1303/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 158.6888 - val_loss: 150.5651\n",
      "Epoch 1304/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 160.8985 - val_loss: 167.2553\n",
      "Epoch 1305/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.9077 - val_loss: 131.2313\n",
      "Epoch 1306/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 158.5414 - val_loss: 156.0686\n",
      "Epoch 1307/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.2503 - val_loss: 143.8456\n",
      "Epoch 1308/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.3098 - val_loss: 148.5572\n",
      "Epoch 1309/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 165.2799 - val_loss: 165.0142\n",
      "Epoch 1310/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.2417 - val_loss: 155.0467\n",
      "Epoch 1311/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.2281 - val_loss: 153.7854\n",
      "Epoch 1312/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.3573 - val_loss: 141.5728\n",
      "Epoch 1313/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.1129 - val_loss: 142.0496\n",
      "Epoch 1314/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 199.6142 - val_loss: 144.3376\n",
      "Epoch 1315/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.5843 - val_loss: 137.2803\n",
      "Epoch 1316/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 175.6143 - val_loss: 224.2686\n",
      "Epoch 1317/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.8624 - val_loss: 147.2047\n",
      "Epoch 1318/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.0590 - val_loss: 132.5559\n",
      "Epoch 1319/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.2625 - val_loss: 164.8828\n",
      "Epoch 1320/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.7262 - val_loss: 152.1909\n",
      "Epoch 1321/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.5768 - val_loss: 132.4330\n",
      "Epoch 1322/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.9909 - val_loss: 165.0806\n",
      "Epoch 1323/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.2998 - val_loss: 138.9061\n",
      "Epoch 1324/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 148.4749 - val_loss: 161.2575\n",
      "Epoch 1325/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 169.1752 - val_loss: 130.0676\n",
      "Epoch 1326/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 203.1588 - val_loss: 149.4780\n",
      "Epoch 1327/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 161.2740 - val_loss: 136.0912\n",
      "Epoch 1328/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 178.7438 - val_loss: 128.2194\n",
      "Epoch 1329/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.6712 - val_loss: 130.8367\n",
      "Epoch 1330/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.7134 - val_loss: 131.3315\n",
      "Epoch 1331/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.2828 - val_loss: 143.4685\n",
      "Epoch 1332/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.5892 - val_loss: 140.9038\n",
      "Epoch 1333/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.7502 - val_loss: 130.9873\n",
      "Epoch 1334/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.7885 - val_loss: 149.9192\n",
      "Epoch 1335/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.8174 - val_loss: 185.3879\n",
      "Epoch 1336/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.4197 - val_loss: 154.4376\n",
      "Epoch 1337/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.3784 - val_loss: 179.4844\n",
      "Epoch 1338/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 169.9181 - val_loss: 207.6303\n",
      "Epoch 1339/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.3514 - val_loss: 162.5652\n",
      "Epoch 1340/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.1494 - val_loss: 156.7080\n",
      "Epoch 1341/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 148.9849 - val_loss: 129.8549\n",
      "Epoch 1342/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.8667 - val_loss: 135.4419\n",
      "Epoch 1343/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.6276 - val_loss: 136.6785\n",
      "Epoch 1344/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.8848 - val_loss: 156.0332\n",
      "Epoch 1345/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.0108 - val_loss: 150.1389\n",
      "Epoch 1346/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.4795 - val_loss: 170.8943\n",
      "Epoch 1347/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.1755 - val_loss: 144.6966\n",
      "Epoch 1348/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 180.6012 - val_loss: 159.8484\n",
      "Epoch 1349/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.1193 - val_loss: 184.1914\n",
      "Epoch 1350/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.1652 - val_loss: 136.6541\n",
      "Epoch 1351/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.2397 - val_loss: 143.1236\n",
      "Epoch 1352/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 170.4933 - val_loss: 199.6099\n",
      "Epoch 1353/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.3319 - val_loss: 151.7768\n",
      "Epoch 1354/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.9592 - val_loss: 152.1210\n",
      "Epoch 1355/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.6132 - val_loss: 148.6331\n",
      "Epoch 1356/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.9548 - val_loss: 155.3575\n",
      "Epoch 1357/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 250.5823 - val_loss: 343.2120\n",
      "Epoch 1358/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 222.2426 - val_loss: 209.0350\n",
      "Epoch 1359/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 213.5555 - val_loss: 160.9521\n",
      "Epoch 1360/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 202.2966 - val_loss: 333.8903\n",
      "Epoch 1361/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 177.8735 - val_loss: 265.1598\n",
      "Epoch 1362/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 187.8254 - val_loss: 170.8265\n",
      "Epoch 1363/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.8033 - val_loss: 182.3986\n",
      "Epoch 1364/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 173.9926 - val_loss: 154.1485\n",
      "Epoch 1365/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 176.0466 - val_loss: 168.4754\n",
      "Epoch 1366/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.2149 - val_loss: 143.3422\n",
      "Epoch 1367/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 174.3586 - val_loss: 207.6624\n",
      "Epoch 1368/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 169.8305 - val_loss: 194.2021\n",
      "Epoch 1369/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 172.0287 - val_loss: 150.8905\n",
      "Epoch 1370/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.0406 - val_loss: 176.1346\n",
      "Epoch 1371/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 191.7885 - val_loss: 329.7930\n",
      "Epoch 1372/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 166.6246 - val_loss: 177.4002\n",
      "Epoch 1373/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.9877 - val_loss: 160.9544\n",
      "Epoch 1374/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 178.0916 - val_loss: 147.9038\n",
      "Epoch 1375/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 171.6540 - val_loss: 171.2764\n",
      "Epoch 1376/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 190.4028 - val_loss: 153.5738\n",
      "Epoch 1377/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 157.6794 - val_loss: 154.9051\n",
      "Epoch 1378/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 156.1535 - val_loss: 140.3254\n",
      "Epoch 1379/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 168.0055 - val_loss: 203.5455\n",
      "Epoch 1380/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.8212 - val_loss: 189.8158\n",
      "Epoch 1381/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.6578 - val_loss: 193.3446\n",
      "Epoch 1382/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.1510 - val_loss: 182.0982\n",
      "Epoch 1383/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 151.0739 - val_loss: 193.8816\n",
      "Epoch 1384/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 179.2447 - val_loss: 169.5969\n",
      "Epoch 1385/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.2416 - val_loss: 262.2253\n",
      "Epoch 1386/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 164.0580 - val_loss: 159.4046\n",
      "Epoch 1387/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 164.4696 - val_loss: 155.7153\n",
      "Epoch 1388/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.5025 - val_loss: 159.8204\n",
      "Epoch 1389/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 170.5691 - val_loss: 149.1610\n",
      "Epoch 1390/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.6969 - val_loss: 162.1396\n",
      "Epoch 1391/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.0653 - val_loss: 152.4718\n",
      "Epoch 1392/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 162.3350 - val_loss: 266.6351\n",
      "Epoch 1393/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 169.7634 - val_loss: 141.5554\n",
      "Epoch 1394/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 167.6240 - val_loss: 142.5230\n",
      "Epoch 1395/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 163.5866 - val_loss: 161.3324\n",
      "Epoch 1396/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.6754 - val_loss: 173.4260\n",
      "Epoch 1397/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 157.5722 - val_loss: 139.6414\n",
      "Epoch 1398/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 156.8957 - val_loss: 142.2066\n",
      "Epoch 1399/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 181.2756 - val_loss: 192.9229\n",
      "Epoch 1400/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 158.9113 - val_loss: 136.6946\n",
      "Epoch 1401/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.5729 - val_loss: 205.2871\n",
      "Epoch 1402/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 169.4719 - val_loss: 327.5597\n",
      "Epoch 1403/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 200.1877 - val_loss: 152.9919\n",
      "Epoch 1404/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.7469 - val_loss: 155.2921\n",
      "Epoch 1405/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.4044 - val_loss: 157.9409\n",
      "Epoch 1406/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 181.5132 - val_loss: 165.2776\n",
      "Epoch 1407/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 189.5885 - val_loss: 176.1178\n",
      "Epoch 1408/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.0157 - val_loss: 351.3621\n",
      "Epoch 1409/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 196.6426 - val_loss: 158.8527\n",
      "Epoch 1410/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 161.7587 - val_loss: 156.6177\n",
      "Epoch 1411/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.3963 - val_loss: 154.2454\n",
      "Epoch 1412/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 209.8746 - val_loss: 160.9713\n",
      "Epoch 1413/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.5480 - val_loss: 170.8072\n",
      "Epoch 1414/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 166.1780 - val_loss: 235.0909\n",
      "Epoch 1415/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 163.1263 - val_loss: 249.7184\n",
      "Epoch 1416/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 147.5175 - val_loss: 155.0806\n",
      "Epoch 1417/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.4450 - val_loss: 160.7692\n",
      "Epoch 1418/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 179.4184 - val_loss: 143.9726\n",
      "Epoch 1419/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 183.0062 - val_loss: 135.6789\n",
      "Epoch 1420/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.9350 - val_loss: 171.8364\n",
      "Epoch 1421/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.9050 - val_loss: 176.7779\n",
      "Epoch 1422/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.3929 - val_loss: 161.0884\n",
      "Epoch 1423/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 164.3778 - val_loss: 262.5820\n",
      "Epoch 1424/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 165.2247 - val_loss: 184.6098\n",
      "Epoch 1425/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.7044 - val_loss: 199.9887\n",
      "Epoch 1426/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 176.3216 - val_loss: 282.1387\n",
      "Epoch 1427/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 203.6187 - val_loss: 196.6215\n",
      "Epoch 1428/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.6057 - val_loss: 157.9693\n",
      "Epoch 1429/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.6747 - val_loss: 140.1016\n",
      "Epoch 1430/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.1084 - val_loss: 240.4146\n",
      "Epoch 1431/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.2050 - val_loss: 274.1801\n",
      "Epoch 1432/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 202.1632 - val_loss: 152.1482\n",
      "Epoch 1433/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.6721 - val_loss: 173.5635\n",
      "Epoch 1434/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.6331 - val_loss: 168.5767\n",
      "Epoch 1435/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.3674 - val_loss: 276.9038\n",
      "Epoch 1436/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.3944 - val_loss: 184.2340\n",
      "Epoch 1437/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.7403 - val_loss: 180.5752\n",
      "Epoch 1438/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.0681 - val_loss: 151.6347\n",
      "Epoch 1439/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 175.8095 - val_loss: 157.8492\n",
      "Epoch 1440/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 158.9636 - val_loss: 227.5979\n",
      "Epoch 1441/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 173.7975 - val_loss: 153.3900\n",
      "Epoch 1442/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.1606 - val_loss: 140.5055\n",
      "Epoch 1443/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.6472 - val_loss: 140.3224\n",
      "Epoch 1444/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.2392 - val_loss: 143.3746\n",
      "Epoch 1445/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.9077 - val_loss: 219.4859\n",
      "Epoch 1446/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.4261 - val_loss: 142.4242\n",
      "Epoch 1447/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 156.9218 - val_loss: 158.7339\n",
      "Epoch 1448/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.4023 - val_loss: 188.8776\n",
      "Epoch 1449/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.8901 - val_loss: 136.0480\n",
      "Epoch 1450/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 155.5956 - val_loss: 204.3045\n",
      "Epoch 1451/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.5872 - val_loss: 141.6894\n",
      "Epoch 1452/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.8148 - val_loss: 299.6746\n",
      "Epoch 1453/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 161.7588 - val_loss: 138.3156\n",
      "Epoch 1454/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 246.0341 - val_loss: 153.7809\n",
      "Epoch 1455/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.3501 - val_loss: 213.1592\n",
      "Epoch 1456/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.9808 - val_loss: 196.1145\n",
      "Epoch 1457/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.9426 - val_loss: 148.5940\n",
      "Epoch 1458/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.7972 - val_loss: 131.0411\n",
      "Epoch 1459/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.2392 - val_loss: 153.4184\n",
      "Epoch 1460/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 189.1111 - val_loss: 231.7000\n",
      "Epoch 1461/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.0101 - val_loss: 185.8025\n",
      "Epoch 1462/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.9341 - val_loss: 137.2613\n",
      "Epoch 1463/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 156.8443 - val_loss: 146.3527\n",
      "Epoch 1464/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.8607 - val_loss: 140.0474\n",
      "Epoch 1465/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.9625 - val_loss: 151.2793\n",
      "Epoch 1466/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.6172 - val_loss: 147.4496\n",
      "Epoch 1467/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.6184 - val_loss: 172.7082\n",
      "Epoch 1468/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 172.0761 - val_loss: 247.0052\n",
      "Epoch 1469/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 288.2099 - val_loss: 165.1750\n",
      "Epoch 1470/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 167.0997 - val_loss: 268.1760\n",
      "Epoch 1471/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.5061 - val_loss: 145.0037\n",
      "Epoch 1472/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 237.9699 - val_loss: 182.4518\n",
      "Epoch 1473/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.0252 - val_loss: 153.8591\n",
      "Epoch 1474/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.6280 - val_loss: 172.1866\n",
      "Epoch 1475/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.0778 - val_loss: 147.5952\n",
      "Epoch 1476/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.1904 - val_loss: 135.5409\n",
      "Epoch 1477/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.8103 - val_loss: 139.6285\n",
      "Epoch 1478/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.3404 - val_loss: 180.6402\n",
      "Epoch 1479/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.1438 - val_loss: 161.4062\n",
      "Epoch 1480/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 169.3029 - val_loss: 143.0828\n",
      "Epoch 1481/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.5606 - val_loss: 181.1744\n",
      "Epoch 1482/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 152.5606 - val_loss: 148.5257\n",
      "Epoch 1483/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 179.3371 - val_loss: 175.0604\n",
      "Epoch 1484/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.3045 - val_loss: 139.6603\n",
      "Epoch 1485/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.2527 - val_loss: 158.6359\n",
      "Epoch 1486/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.5702 - val_loss: 158.9392\n",
      "Epoch 1487/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.2380 - val_loss: 147.2419\n",
      "Epoch 1488/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.4890 - val_loss: 188.2452\n",
      "Epoch 1489/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.4811 - val_loss: 136.3605\n",
      "Epoch 1490/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.7147 - val_loss: 186.1728\n",
      "Epoch 1491/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.7345 - val_loss: 166.9780\n",
      "Epoch 1492/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 173.5922 - val_loss: 180.5666\n",
      "Epoch 1493/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 156.1591 - val_loss: 193.6978\n",
      "Epoch 1494/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 156.7902 - val_loss: 193.4153\n",
      "Epoch 1495/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 160.7633 - val_loss: 144.5475\n",
      "Epoch 1496/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 186.9044 - val_loss: 182.3537\n",
      "Epoch 1497/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.1333 - val_loss: 215.8841\n",
      "Epoch 1498/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 150.1566 - val_loss: 141.9792\n",
      "Epoch 1499/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.8612 - val_loss: 174.6433\n",
      "Epoch 1500/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 181.6642 - val_loss: 144.2258\n",
      "Epoch 1501/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 184.6508 - val_loss: 267.0350\n",
      "Epoch 1502/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 149.5575 - val_loss: 205.8684\n",
      "Epoch 1503/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 154.8575 - val_loss: 178.3022\n",
      "Epoch 1504/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.7775 - val_loss: 227.2421\n",
      "Epoch 1505/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.1181 - val_loss: 145.2317\n",
      "Epoch 1506/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.0018 - val_loss: 133.2997\n",
      "Epoch 1507/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 189.1432 - val_loss: 160.6935\n",
      "Epoch 1508/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.8058 - val_loss: 136.1029\n",
      "Epoch 1509/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.3133 - val_loss: 158.6130\n",
      "Epoch 1510/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 174.341 - 0s 57us/step - loss: 182.0477 - val_loss: 307.7019\n",
      "Epoch 1511/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.9464 - val_loss: 167.7898\n",
      "Epoch 1512/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.8023 - val_loss: 134.9569\n",
      "Epoch 1513/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 141.4915 - val_loss: 204.2326\n",
      "Epoch 1514/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.0609 - val_loss: 136.0425\n",
      "Epoch 1515/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.8786 - val_loss: 220.9182\n",
      "Epoch 1516/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.8142 - val_loss: 207.4869\n",
      "Epoch 1517/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.5962 - val_loss: 138.3059\n",
      "Epoch 1518/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.0807 - val_loss: 150.4710\n",
      "Epoch 1519/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.4978 - val_loss: 153.9700\n",
      "Epoch 1520/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.5900 - val_loss: 136.3574\n",
      "Epoch 1521/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.7311 - val_loss: 172.9111\n",
      "Epoch 1522/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.8549 - val_loss: 152.8797\n",
      "Epoch 1523/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 189.8627 - val_loss: 194.3110\n",
      "Epoch 1524/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 172.8593 - val_loss: 168.9683\n",
      "Epoch 1525/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.3594 - val_loss: 133.6104\n",
      "Epoch 1526/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.3463 - val_loss: 190.4836\n",
      "Epoch 1527/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.5701 - val_loss: 159.0036\n",
      "Epoch 1528/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.5674 - val_loss: 159.6517\n",
      "Epoch 1529/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.7396 - val_loss: 174.2806\n",
      "Epoch 1530/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.7420 - val_loss: 190.4181\n",
      "Epoch 1531/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.7865 - val_loss: 170.8124\n",
      "Epoch 1532/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.9569 - val_loss: 146.7879\n",
      "Epoch 1533/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.0019 - val_loss: 148.4153\n",
      "Epoch 1534/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.9718 - val_loss: 155.6151\n",
      "Epoch 1535/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.2552 - val_loss: 201.6090\n",
      "Epoch 1536/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.1925 - val_loss: 196.2315\n",
      "Epoch 1537/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 175.0579 - val_loss: 137.6815\n",
      "Epoch 1538/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.6570 - val_loss: 164.6432\n",
      "Epoch 1539/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.3659 - val_loss: 171.0866\n",
      "Epoch 1540/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.2826 - val_loss: 151.8777\n",
      "Epoch 1541/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.4009 - val_loss: 172.7928\n",
      "Epoch 1542/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.5336 - val_loss: 134.7742\n",
      "Epoch 1543/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.0134 - val_loss: 137.3389\n",
      "Epoch 1544/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 169.7607 - val_loss: 203.2948\n",
      "Epoch 1545/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.2820 - val_loss: 136.1621\n",
      "Epoch 1546/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.0133 - val_loss: 138.6925\n",
      "Epoch 1547/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 167.8182 - val_loss: 189.3688\n",
      "Epoch 1548/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.7988 - val_loss: 156.0165\n",
      "Epoch 1549/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.6894 - val_loss: 145.1731\n",
      "Epoch 1550/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 150.0132 - val_loss: 134.2558\n",
      "Epoch 1551/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.5359 - val_loss: 139.1683\n",
      "Epoch 1552/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.6735 - val_loss: 251.7356\n",
      "Epoch 1553/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 171.4414 - val_loss: 136.4925\n",
      "Epoch 1554/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 162.8748 - val_loss: 157.7113\n",
      "Epoch 1555/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.7289 - val_loss: 150.2189\n",
      "Epoch 1556/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.8066 - val_loss: 250.3263\n",
      "Epoch 1557/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.3583 - val_loss: 266.9249\n",
      "Epoch 1558/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.2595 - val_loss: 157.2152\n",
      "Epoch 1559/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.8380 - val_loss: 132.7221\n",
      "Epoch 1560/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 171.8470 - val_loss: 147.6215\n",
      "Epoch 1561/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.0918 - val_loss: 193.9937\n",
      "Epoch 1562/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 179.6078 - val_loss: 150.5452\n",
      "Epoch 1563/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 165.3094 - val_loss: 179.5402\n",
      "Epoch 1564/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 162.8658 - val_loss: 145.3322\n",
      "Epoch 1565/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 154.5674 - val_loss: 139.0357\n",
      "Epoch 1566/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 153.3262 - val_loss: 162.9991\n",
      "Epoch 1567/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 176.2259 - val_loss: 187.1464\n",
      "Epoch 1568/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.0715 - val_loss: 140.1713\n",
      "Epoch 1569/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.5376 - val_loss: 141.3386\n",
      "Epoch 1570/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.9216 - val_loss: 154.9067\n",
      "Epoch 1571/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.8771 - val_loss: 145.0464\n",
      "Epoch 1572/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 166.4162 - val_loss: 170.5103\n",
      "Epoch 1573/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.8222 - val_loss: 146.5567\n",
      "Epoch 1574/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.8137 - val_loss: 134.9215\n",
      "Epoch 1575/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.5448 - val_loss: 149.3877\n",
      "Epoch 1576/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.2934 - val_loss: 192.7503\n",
      "Epoch 1577/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.4799 - val_loss: 152.3397\n",
      "Epoch 1578/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 153.6793 - val_loss: 133.3398\n",
      "Epoch 1579/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.9616 - val_loss: 131.4452\n",
      "Epoch 1580/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 197.9115 - val_loss: 166.9842\n",
      "Epoch 1581/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 150.2137 - val_loss: 229.9619\n",
      "Epoch 1582/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.0589 - val_loss: 218.4472\n",
      "Epoch 1583/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.0367 - val_loss: 131.4447\n",
      "Epoch 1584/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.5326 - val_loss: 135.2843\n",
      "Epoch 1585/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.6936 - val_loss: 309.6983\n",
      "Epoch 1586/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 167.7957 - val_loss: 151.4621\n",
      "Epoch 1587/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.2602 - val_loss: 150.2220\n",
      "Epoch 1588/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.3510 - val_loss: 162.0380\n",
      "Epoch 1589/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.4007 - val_loss: 146.5775\n",
      "Epoch 1590/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.2109 - val_loss: 133.5698\n",
      "Epoch 1591/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.6935 - val_loss: 147.1039\n",
      "Epoch 1592/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.5783 - val_loss: 148.5328\n",
      "Epoch 1593/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.4186 - val_loss: 141.1450\n",
      "Epoch 1594/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 181.9868 - val_loss: 271.8287\n",
      "Epoch 1595/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 163.2428 - val_loss: 141.5231\n",
      "Epoch 1596/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.3300 - val_loss: 145.2981\n",
      "Epoch 1597/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.5643 - val_loss: 157.5716\n",
      "Epoch 1598/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.0225 - val_loss: 149.6082\n",
      "Epoch 1599/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.8897 - val_loss: 144.6678\n",
      "Epoch 1600/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.7565 - val_loss: 143.7588\n",
      "Epoch 1601/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.8503 - val_loss: 243.9924\n",
      "Epoch 1602/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 162.8845 - val_loss: 132.8014\n",
      "Epoch 1603/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.4123 - val_loss: 132.0877\n",
      "Epoch 1604/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 191.5342 - val_loss: 158.6102\n",
      "Epoch 1605/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.0859 - val_loss: 142.6898\n",
      "Epoch 1606/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.8693 - val_loss: 207.2308\n",
      "Epoch 1607/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.6525 - val_loss: 147.3401\n",
      "Epoch 1608/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.0793 - val_loss: 134.6119\n",
      "Epoch 1609/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.3751 - val_loss: 135.2057\n",
      "Epoch 1610/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.7805 - val_loss: 141.9399\n",
      "Epoch 1611/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.5105 - val_loss: 146.7789\n",
      "Epoch 1612/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 173.4116 - val_loss: 198.2180\n",
      "Epoch 1613/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.2007 - val_loss: 170.2900\n",
      "Epoch 1614/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.3265 - val_loss: 409.1571\n",
      "Epoch 1615/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 177.9588 - val_loss: 148.9589\n",
      "Epoch 1616/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 172.7510 - val_loss: 141.6264\n",
      "Epoch 1617/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.8758 - val_loss: 132.6711\n",
      "Epoch 1618/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 154.0434 - val_loss: 196.6731\n",
      "Epoch 1619/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.4689 - val_loss: 165.0282\n",
      "Epoch 1620/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.3829 - val_loss: 154.0190\n",
      "Epoch 1621/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.1294 - val_loss: 141.7850\n",
      "Epoch 1622/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.6807 - val_loss: 153.4858\n",
      "Epoch 1623/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 161.5475 - val_loss: 153.6101\n",
      "Epoch 1624/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 187.5103 - val_loss: 201.2061\n",
      "Epoch 1625/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 182.3937 - val_loss: 141.9272\n",
      "Epoch 1626/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 162.5494 - val_loss: 184.7413\n",
      "Epoch 1627/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 157.9378 - val_loss: 150.0453\n",
      "Epoch 1628/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 170.4810 - val_loss: 285.3132\n",
      "Epoch 1629/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 156.1799 - val_loss: 164.4651\n",
      "Epoch 1630/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.1864 - val_loss: 152.7062\n",
      "Epoch 1631/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.7901 - val_loss: 150.1018\n",
      "Epoch 1632/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 164.1533 - val_loss: 191.9547\n",
      "Epoch 1633/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.3912 - val_loss: 139.0381\n",
      "Epoch 1634/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.3634 - val_loss: 133.3332\n",
      "Epoch 1635/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 177.9797 - val_loss: 188.1927\n",
      "Epoch 1636/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.4142 - val_loss: 135.5075\n",
      "Epoch 1637/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.7650 - val_loss: 289.8047\n",
      "Epoch 1638/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 161.9106 - val_loss: 151.8373\n",
      "Epoch 1639/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.9381 - val_loss: 139.9298\n",
      "Epoch 1640/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.6064 - val_loss: 133.0765\n",
      "Epoch 1641/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 174.2197 - val_loss: 134.4959\n",
      "Epoch 1642/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.5620 - val_loss: 157.8290\n",
      "Epoch 1643/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 155.4329 - val_loss: 178.3726\n",
      "Epoch 1644/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.5960 - val_loss: 171.4325\n",
      "Epoch 1645/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 156.9151 - val_loss: 142.0177\n",
      "Epoch 1646/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.3743 - val_loss: 133.9052\n",
      "Epoch 1647/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.7556 - val_loss: 160.2757\n",
      "Epoch 1648/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.3874 - val_loss: 196.2477\n",
      "Epoch 1649/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.5996 - val_loss: 172.2575\n",
      "Epoch 1650/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.7640 - val_loss: 165.8516\n",
      "Epoch 1651/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.4329 - val_loss: 233.6377\n",
      "Epoch 1652/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 195.0920 - val_loss: 145.2903\n",
      "Epoch 1653/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.1449 - val_loss: 245.0716\n",
      "Epoch 1654/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.3822 - val_loss: 141.6461\n",
      "Epoch 1655/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.0224 - val_loss: 142.9574\n",
      "Epoch 1656/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.5713 - val_loss: 214.5204\n",
      "Epoch 1657/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.6905 - val_loss: 137.0203\n",
      "Epoch 1658/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 150.1193 - val_loss: 161.9371\n",
      "Epoch 1659/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.5822 - val_loss: 136.0250\n",
      "Epoch 1660/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.9646 - val_loss: 134.5665\n",
      "Epoch 1661/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.9107 - val_loss: 132.9769\n",
      "Epoch 1662/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 141.6769 - val_loss: 152.2477\n",
      "Epoch 1663/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 203.5715 - val_loss: 156.7862\n",
      "Epoch 1664/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.2645 - val_loss: 163.5404\n",
      "Epoch 1665/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.5485 - val_loss: 135.7272\n",
      "Epoch 1666/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.6892 - val_loss: 194.2631\n",
      "Epoch 1667/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.7372 - val_loss: 135.1293\n",
      "Epoch 1668/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 155.3014 - val_loss: 170.9230\n",
      "Epoch 1669/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.8076 - val_loss: 138.7492\n",
      "Epoch 1670/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.3676 - val_loss: 132.3431\n",
      "Epoch 1671/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.5026 - val_loss: 149.3377\n",
      "Epoch 1672/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.7690 - val_loss: 150.5917\n",
      "Epoch 1673/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.1273 - val_loss: 173.9628\n",
      "Epoch 1674/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.9572 - val_loss: 130.5461\n",
      "Epoch 1675/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.3578 - val_loss: 149.4131\n",
      "Epoch 1676/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.7636 - val_loss: 144.1049\n",
      "Epoch 1677/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.0659 - val_loss: 180.6158\n",
      "Epoch 1678/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.7227 - val_loss: 133.8323\n",
      "Epoch 1679/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 170.5419 - val_loss: 156.0645\n",
      "Epoch 1680/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.4826 - val_loss: 137.8703\n",
      "Epoch 1681/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.7584 - val_loss: 141.5021\n",
      "Epoch 1682/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.1212 - val_loss: 134.9162\n",
      "Epoch 1683/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.0731 - val_loss: 148.2594\n",
      "Epoch 1684/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 150.5074 - val_loss: 137.2147\n",
      "Epoch 1685/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.2088 - val_loss: 153.5510\n",
      "Epoch 1686/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.0357 - val_loss: 143.5032\n",
      "Epoch 1687/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.6475 - val_loss: 140.9887\n",
      "Epoch 1688/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 149.8781 - val_loss: 133.4650\n",
      "Epoch 1689/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 157.4307 - val_loss: 136.4957\n",
      "Epoch 1690/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 153.8492 - val_loss: 193.5540\n",
      "Epoch 1691/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 190.5007 - val_loss: 179.0857\n",
      "Epoch 1692/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.4426 - val_loss: 134.8963\n",
      "Epoch 1693/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.5949 - val_loss: 148.1048\n",
      "Epoch 1694/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.3720 - val_loss: 141.2162\n",
      "Epoch 1695/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 164.2281 - val_loss: 133.2758\n",
      "Epoch 1696/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.8801 - val_loss: 158.2308\n",
      "Epoch 1697/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.9563 - val_loss: 129.8704\n",
      "Epoch 1698/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.0699 - val_loss: 182.8341\n",
      "Epoch 1699/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.1203 - val_loss: 189.7025\n",
      "Epoch 1700/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.1353 - val_loss: 131.8731\n",
      "Epoch 1701/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.2733 - val_loss: 150.8190\n",
      "Epoch 1702/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.0164 - val_loss: 132.0134\n",
      "Epoch 1703/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.1606 - val_loss: 212.5376\n",
      "Epoch 1704/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.0841 - val_loss: 142.5115\n",
      "Epoch 1705/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.1368 - val_loss: 132.1896\n",
      "Epoch 1706/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.4708 - val_loss: 255.8181\n",
      "Epoch 1707/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.8615 - val_loss: 138.9319\n",
      "Epoch 1708/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.4441 - val_loss: 135.8240\n",
      "Epoch 1709/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.1094 - val_loss: 142.7830\n",
      "Epoch 1710/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.0679 - val_loss: 142.1331\n",
      "Epoch 1711/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 140.5141 - val_loss: 135.0565\n",
      "Epoch 1712/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.0758 - val_loss: 164.8513\n",
      "Epoch 1713/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 199.7127 - val_loss: 147.9072\n",
      "Epoch 1714/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.5869 - val_loss: 148.6077\n",
      "Epoch 1715/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 152.3325 - val_loss: 237.0496\n",
      "Epoch 1716/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 283.4348 - val_loss: 211.2513\n",
      "Epoch 1717/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 221.0416 - val_loss: 148.9113\n",
      "Epoch 1718/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.2097 - val_loss: 250.9340\n",
      "Epoch 1719/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.2184 - val_loss: 143.9011\n",
      "Epoch 1720/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.7073 - val_loss: 145.3615\n",
      "Epoch 1721/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.2141 - val_loss: 169.7044\n",
      "Epoch 1722/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.9722 - val_loss: 170.2329\n",
      "Epoch 1723/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.2849 - val_loss: 152.2136\n",
      "Epoch 1724/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.1128 - val_loss: 160.9181\n",
      "Epoch 1725/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.8562 - val_loss: 143.7719\n",
      "Epoch 1726/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.6146 - val_loss: 152.6112\n",
      "Epoch 1727/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.0687 - val_loss: 146.8818\n",
      "Epoch 1728/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.1726 - val_loss: 137.5515\n",
      "Epoch 1729/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.9097 - val_loss: 142.3641\n",
      "Epoch 1730/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.2613 - val_loss: 166.9791\n",
      "Epoch 1731/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.3276 - val_loss: 135.6702\n",
      "Epoch 1732/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.0202 - val_loss: 152.1450\n",
      "Epoch 1733/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.6095 - val_loss: 137.4394\n",
      "Epoch 1734/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.8457 - val_loss: 134.9105\n",
      "Epoch 1735/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.0322 - val_loss: 159.8840\n",
      "Epoch 1736/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 202.9189 - val_loss: 136.0844\n",
      "Epoch 1737/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.5864 - val_loss: 142.1493\n",
      "Epoch 1738/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.8804 - val_loss: 136.9948\n",
      "Epoch 1739/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.7984 - val_loss: 141.4569\n",
      "Epoch 1740/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.4671 - val_loss: 149.0390\n",
      "Epoch 1741/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.5218 - val_loss: 171.4407\n",
      "Epoch 1742/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 149.2413 - val_loss: 195.9881\n",
      "Epoch 1743/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.9934 - val_loss: 138.7337\n",
      "Epoch 1744/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.5251 - val_loss: 135.3839\n",
      "Epoch 1745/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.7392 - val_loss: 160.9773\n",
      "Epoch 1746/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 193.7635 - val_loss: 163.0341\n",
      "Epoch 1747/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.7709 - val_loss: 136.9748\n",
      "Epoch 1748/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.1727 - val_loss: 173.5115\n",
      "Epoch 1749/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.6520 - val_loss: 139.3543\n",
      "Epoch 1750/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 148.0023 - val_loss: 140.7107\n",
      "Epoch 1751/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 155.5164 - val_loss: 152.1089\n",
      "Epoch 1752/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 147.9750 - val_loss: 197.7223\n",
      "Epoch 1753/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 173.9568 - val_loss: 145.0304\n",
      "Epoch 1754/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.8711 - val_loss: 141.3605\n",
      "Epoch 1755/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 216.0711 - val_loss: 735.8572\n",
      "Epoch 1756/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 216.5263 - val_loss: 143.6879\n",
      "Epoch 1757/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.3672 - val_loss: 144.1741\n",
      "Epoch 1758/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 148.0008 - val_loss: 142.2443\n",
      "Epoch 1759/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.0939 - val_loss: 137.6000\n",
      "Epoch 1760/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.0627 - val_loss: 137.8140\n",
      "Epoch 1761/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.2013 - val_loss: 143.6010\n",
      "Epoch 1762/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 150.4009 - val_loss: 161.8682\n",
      "Epoch 1763/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.9147 - val_loss: 130.2663\n",
      "Epoch 1764/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.8767 - val_loss: 268.8992\n",
      "Epoch 1765/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 141.9531 - val_loss: 157.5796\n",
      "Epoch 1766/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.3130 - val_loss: 160.4776\n",
      "Epoch 1767/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.5516 - val_loss: 151.7084\n",
      "Epoch 1768/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.3778 - val_loss: 136.1618\n",
      "Epoch 1769/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.5500 - val_loss: 153.2528\n",
      "Epoch 1770/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 183.6297 - val_loss: 141.6203\n",
      "Epoch 1771/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.8674 - val_loss: 174.7322\n",
      "Epoch 1772/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.8161 - val_loss: 143.4197\n",
      "Epoch 1773/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.0825 - val_loss: 154.4141\n",
      "Epoch 1774/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.4890 - val_loss: 178.5465\n",
      "Epoch 1775/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.9765 - val_loss: 128.5388\n",
      "Epoch 1776/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.0987 - val_loss: 163.5302\n",
      "Epoch 1777/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.5047 - val_loss: 166.8729\n",
      "Epoch 1778/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 177.2894 - val_loss: 150.2345\n",
      "Epoch 1779/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.9035 - val_loss: 145.7445\n",
      "Epoch 1780/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 150.3330 - val_loss: 131.7324\n",
      "Epoch 1781/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.4360 - val_loss: 336.0376\n",
      "Epoch 1782/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.6159 - val_loss: 174.9066\n",
      "Epoch 1783/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.9859 - val_loss: 165.6429\n",
      "Epoch 1784/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.3498 - val_loss: 296.7944\n",
      "Epoch 1785/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.1939 - val_loss: 227.7559\n",
      "Epoch 1786/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.0716 - val_loss: 150.7481\n",
      "Epoch 1787/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.8452 - val_loss: 170.4488\n",
      "Epoch 1788/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.6650 - val_loss: 132.9210\n",
      "Epoch 1789/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.7289 - val_loss: 164.6306\n",
      "Epoch 1790/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.2013 - val_loss: 148.5956\n",
      "Epoch 1791/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 222.3815 - val_loss: 136.2502\n",
      "Epoch 1792/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.4030 - val_loss: 144.3913\n",
      "Epoch 1793/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.9857 - val_loss: 164.4297\n",
      "Epoch 1794/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.8168 - val_loss: 138.3032\n",
      "Epoch 1795/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.5096 - val_loss: 164.0974\n",
      "Epoch 1796/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.7181 - val_loss: 133.5191\n",
      "Epoch 1797/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.2664 - val_loss: 134.4194\n",
      "Epoch 1798/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 144.2166 - val_loss: 155.4492\n",
      "Epoch 1799/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.0017 - val_loss: 138.1709\n",
      "Epoch 1800/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.7595 - val_loss: 221.6107\n",
      "Epoch 1801/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 160.1334 - val_loss: 176.7701\n",
      "Epoch 1802/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.7004 - val_loss: 136.5947\n",
      "Epoch 1803/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.3833 - val_loss: 137.2370\n",
      "Epoch 1804/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 148.3519 - val_loss: 152.8066\n",
      "Epoch 1805/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.1961 - val_loss: 162.9760\n",
      "Epoch 1806/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 187.7771 - val_loss: 143.0138\n",
      "Epoch 1807/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 159.1164 - val_loss: 151.9668\n",
      "Epoch 1808/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.1439 - val_loss: 269.5148\n",
      "Epoch 1809/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 185.2970 - val_loss: 144.2285\n",
      "Epoch 1810/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.7062 - val_loss: 135.0510\n",
      "Epoch 1811/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.4674 - val_loss: 148.2248\n",
      "Epoch 1812/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 158.0559 - val_loss: 162.9770\n",
      "Epoch 1813/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 147.4955 - val_loss: 134.3535\n",
      "Epoch 1814/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 154.3489 - val_loss: 132.4198\n",
      "Epoch 1815/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 157.5536 - val_loss: 148.1592\n",
      "Epoch 1816/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.5980 - val_loss: 157.2878\n",
      "Epoch 1817/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.6596 - val_loss: 134.4807\n",
      "Epoch 1818/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.0943 - val_loss: 138.4431\n",
      "Epoch 1819/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.9606 - val_loss: 164.9251\n",
      "Epoch 1820/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.9000 - val_loss: 136.7085\n",
      "Epoch 1821/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 174.6329 - val_loss: 251.9510\n",
      "Epoch 1822/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 152.5865 - val_loss: 170.5234\n",
      "Epoch 1823/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.4439 - val_loss: 281.6969\n",
      "Epoch 1824/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.0877 - val_loss: 132.0553\n",
      "Epoch 1825/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 140.5832 - val_loss: 170.8373\n",
      "Epoch 1826/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.6398 - val_loss: 197.7599\n",
      "Epoch 1827/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 168.6690 - val_loss: 210.2373\n",
      "Epoch 1828/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.4230 - val_loss: 136.2195\n",
      "Epoch 1829/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.4625 - val_loss: 141.2864\n",
      "Epoch 1830/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.7606 - val_loss: 157.1971\n",
      "Epoch 1831/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.4312 - val_loss: 166.9779\n",
      "Epoch 1832/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.8734 - val_loss: 137.3379\n",
      "Epoch 1833/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.8460 - val_loss: 193.5316\n",
      "Epoch 1834/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.0539 - val_loss: 171.0076\n",
      "Epoch 1835/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.2878 - val_loss: 157.7969\n",
      "Epoch 1836/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 140.9219 - val_loss: 130.2089\n",
      "Epoch 1837/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.5705 - val_loss: 144.2586\n",
      "Epoch 1838/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.8509 - val_loss: 146.5020\n",
      "Epoch 1839/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 157.0179 - val_loss: 222.8765\n",
      "Epoch 1840/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.9254 - val_loss: 139.4348\n",
      "Epoch 1841/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.4346 - val_loss: 140.0760\n",
      "Epoch 1842/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.7883 - val_loss: 148.1767\n",
      "Epoch 1843/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.2484 - val_loss: 150.6754\n",
      "Epoch 1844/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.2404 - val_loss: 130.4007\n",
      "Epoch 1845/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.7137 - val_loss: 164.4532\n",
      "Epoch 1846/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 150.9790 - val_loss: 139.8413\n",
      "Epoch 1847/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.4923 - val_loss: 131.9396\n",
      "Epoch 1848/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.5743 - val_loss: 216.0662\n",
      "Epoch 1849/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 159.6384 - val_loss: 131.1653\n",
      "Epoch 1850/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.8101 - val_loss: 153.2524\n",
      "Epoch 1851/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.8900 - val_loss: 146.2319\n",
      "Epoch 1852/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 185.0890 - val_loss: 136.0865\n",
      "Epoch 1853/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.6893 - val_loss: 162.2313\n",
      "Epoch 1854/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.6696 - val_loss: 135.4230\n",
      "Epoch 1855/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.1239 - val_loss: 240.4337\n",
      "Epoch 1856/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.3621 - val_loss: 166.6337\n",
      "Epoch 1857/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.0193 - val_loss: 167.3577\n",
      "Epoch 1858/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 178.0251 - val_loss: 141.7260\n",
      "Epoch 1859/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.5896 - val_loss: 136.2978\n",
      "Epoch 1860/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.1748 - val_loss: 163.7955\n",
      "Epoch 1861/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.4454 - val_loss: 140.0597\n",
      "Epoch 1862/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.1550 - val_loss: 132.1693\n",
      "Epoch 1863/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.7237 - val_loss: 206.5138\n",
      "Epoch 1864/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.4475 - val_loss: 180.0234\n",
      "Epoch 1865/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 164.2600 - val_loss: 138.9693\n",
      "Epoch 1866/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.8184 - val_loss: 142.2838\n",
      "Epoch 1867/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.4025 - val_loss: 248.2001\n",
      "Epoch 1868/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 234.7534 - val_loss: 191.7257\n",
      "Epoch 1869/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.0273 - val_loss: 146.4012\n",
      "Epoch 1870/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.3847 - val_loss: 156.1653\n",
      "Epoch 1871/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.1947 - val_loss: 128.0858\n",
      "Epoch 1872/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.4437 - val_loss: 140.6711\n",
      "Epoch 1873/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.2842 - val_loss: 170.2945\n",
      "Epoch 1874/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.2752 - val_loss: 134.9336\n",
      "Epoch 1875/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 148.2114 - val_loss: 132.3664\n",
      "Epoch 1876/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 145.7533 - val_loss: 234.3265\n",
      "Epoch 1877/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 157.2708 - val_loss: 135.3382\n",
      "Epoch 1878/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.7909 - val_loss: 145.8821\n",
      "Epoch 1879/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.1032 - val_loss: 140.1774\n",
      "Epoch 1880/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 180.9984 - val_loss: 167.3799\n",
      "Epoch 1881/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.5079 - val_loss: 214.1956\n",
      "Epoch 1882/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.0095 - val_loss: 137.0962\n",
      "Epoch 1883/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 182.7873 - val_loss: 158.3479\n",
      "Epoch 1884/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 169.0071 - val_loss: 139.7053\n",
      "Epoch 1885/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.9713 - val_loss: 159.8832\n",
      "Epoch 1886/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 150.4531 - val_loss: 142.6587\n",
      "Epoch 1887/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.7397 - val_loss: 133.3152\n",
      "Epoch 1888/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.6827 - val_loss: 161.4660\n",
      "Epoch 1889/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 180.3280 - val_loss: 127.6616\n",
      "Epoch 1890/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.7309 - val_loss: 164.3950\n",
      "Epoch 1891/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 188.9279 - val_loss: 154.3320\n",
      "Epoch 1892/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.1984 - val_loss: 138.5574\n",
      "Epoch 1893/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.1807 - val_loss: 148.5550\n",
      "Epoch 1894/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.5765 - val_loss: 212.1031\n",
      "Epoch 1895/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.5135 - val_loss: 136.7636\n",
      "Epoch 1896/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.9513 - val_loss: 157.8532\n",
      "Epoch 1897/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.6968 - val_loss: 160.0733\n",
      "Epoch 1898/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.6937 - val_loss: 382.7174\n",
      "Epoch 1899/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.5590 - val_loss: 134.0784\n",
      "Epoch 1900/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 185.0829 - val_loss: 143.5458\n",
      "Epoch 1901/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 166.6617 - val_loss: 237.8370\n",
      "Epoch 1902/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.8820 - val_loss: 149.6403\n",
      "Epoch 1903/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.5451 - val_loss: 142.8447\n",
      "Epoch 1904/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.0475 - val_loss: 140.1642\n",
      "Epoch 1905/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 178.5203 - val_loss: 223.8446\n",
      "Epoch 1906/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 161.7973 - val_loss: 135.2527\n",
      "Epoch 1907/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.2656 - val_loss: 141.4505\n",
      "Epoch 1908/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.6009 - val_loss: 139.2724\n",
      "Epoch 1909/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.5963 - val_loss: 130.6344\n",
      "Epoch 1910/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.9508 - val_loss: 132.7559\n",
      "Epoch 1911/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.7840 - val_loss: 146.7823\n",
      "Epoch 1912/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.0098 - val_loss: 135.1870\n",
      "Epoch 1913/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.9677 - val_loss: 152.8438\n",
      "Epoch 1914/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 184.5235 - val_loss: 151.1476\n",
      "Epoch 1915/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 169.1596 - val_loss: 137.0033\n",
      "Epoch 1916/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.7825 - val_loss: 186.4038\n",
      "Epoch 1917/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.4811 - val_loss: 163.8267\n",
      "Epoch 1918/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.3561 - val_loss: 134.6264\n",
      "Epoch 1919/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.7741 - val_loss: 133.3977\n",
      "Epoch 1920/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.1060 - val_loss: 134.3853\n",
      "Epoch 1921/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.2503 - val_loss: 130.4324\n",
      "Epoch 1922/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.2739 - val_loss: 209.2766\n",
      "Epoch 1923/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.9059 - val_loss: 135.9744\n",
      "Epoch 1924/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.1933 - val_loss: 138.0360\n",
      "Epoch 1925/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.0897 - val_loss: 210.2804\n",
      "Epoch 1926/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 156.1511 - val_loss: 230.9451\n",
      "Epoch 1927/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.8874 - val_loss: 146.6774\n",
      "Epoch 1928/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.8761 - val_loss: 134.2245\n",
      "Epoch 1929/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.7660 - val_loss: 145.8429\n",
      "Epoch 1930/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.2145 - val_loss: 137.1038\n",
      "Epoch 1931/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.3962 - val_loss: 147.4999\n",
      "Epoch 1932/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.0674 - val_loss: 154.5341\n",
      "Epoch 1933/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.9549 - val_loss: 133.1646\n",
      "Epoch 1934/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.6958 - val_loss: 146.9624\n",
      "Epoch 1935/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 188.9790 - val_loss: 148.0682\n",
      "Epoch 1936/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.0454 - val_loss: 146.9426\n",
      "Epoch 1937/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 149.4273 - val_loss: 154.9543\n",
      "Epoch 1938/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.2682 - val_loss: 128.5260\n",
      "Epoch 1939/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 185.6066 - val_loss: 228.9271\n",
      "Epoch 1940/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.1397 - val_loss: 143.9065\n",
      "Epoch 1941/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.8058 - val_loss: 135.4616\n",
      "Epoch 1942/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.1175 - val_loss: 130.6385\n",
      "Epoch 1943/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - ETA: 0s - loss: 156.686 - 0s 57us/step - loss: 161.6620 - val_loss: 175.7544\n",
      "Epoch 1944/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.6322 - val_loss: 243.2488\n",
      "Epoch 1945/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.0995 - val_loss: 140.7723\n",
      "Epoch 1946/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 139.1469 - val_loss: 168.7547\n",
      "Epoch 1947/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 161.2314 - val_loss: 133.9181\n",
      "Epoch 1948/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 160.6483 - val_loss: 143.9756\n",
      "Epoch 1949/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.9522 - val_loss: 170.5182\n",
      "Epoch 1950/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.9842 - val_loss: 142.1860\n",
      "Epoch 1951/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.7165 - val_loss: 139.4411\n",
      "Epoch 1952/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.4287 - val_loss: 141.2184\n",
      "Epoch 1953/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.3867 - val_loss: 149.1468\n",
      "Epoch 1954/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.5410 - val_loss: 136.3490\n",
      "Epoch 1955/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.8254 - val_loss: 136.9950\n",
      "Epoch 1956/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.6176 - val_loss: 231.8745\n",
      "Epoch 1957/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.4537 - val_loss: 206.0193\n",
      "Epoch 1958/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.2447 - val_loss: 138.5480\n",
      "Epoch 1959/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.3842 - val_loss: 165.9802\n",
      "Epoch 1960/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 150.5073 - val_loss: 158.2935\n",
      "Epoch 1961/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.2670 - val_loss: 142.4856\n",
      "Epoch 1962/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.9022 - val_loss: 133.4299\n",
      "Epoch 1963/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.3606 - val_loss: 189.5598\n",
      "Epoch 1964/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.8150 - val_loss: 133.0172\n",
      "Epoch 1965/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.8507 - val_loss: 139.0146\n",
      "Epoch 1966/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.8992 - val_loss: 139.6338\n",
      "Epoch 1967/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.6741 - val_loss: 152.2959\n",
      "Epoch 1968/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.9238 - val_loss: 142.0981\n",
      "Epoch 1969/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.5482 - val_loss: 143.1688\n",
      "Epoch 1970/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 183.3354 - val_loss: 153.1296\n",
      "Epoch 1971/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.4165 - val_loss: 147.5174\n",
      "Epoch 1972/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.8543 - val_loss: 132.7087\n",
      "Epoch 1973/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.3777 - val_loss: 165.9902\n",
      "Epoch 1974/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.6580 - val_loss: 148.9067\n",
      "Epoch 1975/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.6113 - val_loss: 153.0251\n",
      "Epoch 1976/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.6878 - val_loss: 136.9213\n",
      "Epoch 1977/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.8675 - val_loss: 133.9658\n",
      "Epoch 1978/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 188.3108 - val_loss: 147.7281\n",
      "Epoch 1979/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.3892 - val_loss: 128.6333\n",
      "Epoch 1980/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.5433 - val_loss: 209.1569\n",
      "Epoch 1981/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.0222 - val_loss: 149.9054\n",
      "Epoch 1982/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.7527 - val_loss: 151.4543\n",
      "Epoch 1983/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.6955 - val_loss: 136.4365\n",
      "Epoch 1984/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.3764 - val_loss: 131.1759\n",
      "Epoch 1985/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.1820 - val_loss: 177.4766\n",
      "Epoch 1986/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.8035 - val_loss: 139.0264\n",
      "Epoch 1987/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.2822 - val_loss: 139.8454\n",
      "Epoch 1988/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.5942 - val_loss: 138.8800\n",
      "Epoch 1989/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.6577 - val_loss: 143.4433\n",
      "Epoch 1990/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.2448 - val_loss: 137.5240\n",
      "Epoch 1991/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.3487 - val_loss: 148.9878\n",
      "Epoch 1992/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.9745 - val_loss: 130.8217\n",
      "Epoch 1993/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.2999 - val_loss: 208.2119\n",
      "Epoch 1994/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.6092 - val_loss: 155.6202\n",
      "Epoch 1995/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.5143 - val_loss: 129.8452\n",
      "Epoch 1996/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.8581 - val_loss: 141.9125\n",
      "Epoch 1997/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.4876 - val_loss: 131.8985\n",
      "Epoch 1998/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.6230 - val_loss: 151.0447\n",
      "Epoch 1999/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 148.3000 - val_loss: 185.7089\n",
      "Epoch 2000/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 159.6916 - val_loss: 131.1965\n",
      "Epoch 2001/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 132.3672 - val_loss: 151.1635\n",
      "Epoch 2002/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.1956 - val_loss: 148.0605\n",
      "Epoch 2003/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.8139 - val_loss: 166.2242\n",
      "Epoch 2004/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.5721 - val_loss: 135.5151\n",
      "Epoch 2005/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.2863 - val_loss: 133.3807\n",
      "Epoch 2006/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.9656 - val_loss: 142.2754\n",
      "Epoch 2007/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.0420 - val_loss: 145.9487\n",
      "Epoch 2008/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.7782 - val_loss: 140.9330\n",
      "Epoch 2009/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.0473 - val_loss: 149.7670\n",
      "Epoch 2010/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.6603 - val_loss: 144.6315\n",
      "Epoch 2011/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.6485 - val_loss: 172.0991\n",
      "Epoch 2012/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.5537 - val_loss: 137.3077\n",
      "Epoch 2013/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 167.8883 - val_loss: 203.3553\n",
      "Epoch 2014/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.3120 - val_loss: 168.5509\n",
      "Epoch 2015/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.8530 - val_loss: 134.0181\n",
      "Epoch 2016/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.5456 - val_loss: 163.1320\n",
      "Epoch 2017/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.1240 - val_loss: 137.8953\n",
      "Epoch 2018/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.0570 - val_loss: 144.9232\n",
      "Epoch 2019/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.6666 - val_loss: 158.4067\n",
      "Epoch 2020/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.7714 - val_loss: 157.6534\n",
      "Epoch 2021/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.9574 - val_loss: 134.8129\n",
      "Epoch 2022/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.3107 - val_loss: 131.5386\n",
      "Epoch 2023/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.3964 - val_loss: 149.3491\n",
      "Epoch 2024/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.4178 - val_loss: 132.8331\n",
      "Epoch 2025/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.5242 - val_loss: 129.6836\n",
      "Epoch 2026/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.4592 - val_loss: 155.6546\n",
      "Epoch 2027/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.9575 - val_loss: 152.6146\n",
      "Epoch 2028/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.5927 - val_loss: 158.1223\n",
      "Epoch 2029/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.5784 - val_loss: 146.1639\n",
      "Epoch 2030/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 160.6422 - val_loss: 144.5008\n",
      "Epoch 2031/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.7419 - val_loss: 179.7401\n",
      "Epoch 2032/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.8208 - val_loss: 131.6287\n",
      "Epoch 2033/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.3582 - val_loss: 162.5293\n",
      "Epoch 2034/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.7466 - val_loss: 135.6846\n",
      "Epoch 2035/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.9986 - val_loss: 170.4300\n",
      "Epoch 2036/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 155.0411 - val_loss: 149.4907\n",
      "Epoch 2037/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.6106 - val_loss: 130.4129\n",
      "Epoch 2038/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.9717 - val_loss: 137.4348\n",
      "Epoch 2039/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.7835 - val_loss: 200.0496\n",
      "Epoch 2040/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.2590 - val_loss: 139.0888\n",
      "Epoch 2041/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 145.3510 - val_loss: 148.3400\n",
      "Epoch 2042/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.7118 - val_loss: 242.3657\n",
      "Epoch 2043/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.5205 - val_loss: 139.4880\n",
      "Epoch 2044/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.3758 - val_loss: 131.0694\n",
      "Epoch 2045/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.1901 - val_loss: 138.4197\n",
      "Epoch 2046/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.9150 - val_loss: 185.6092\n",
      "Epoch 2047/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.5765 - val_loss: 156.9359\n",
      "Epoch 2048/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.3025 - val_loss: 140.9679\n",
      "Epoch 2049/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.4465 - val_loss: 162.7005\n",
      "Epoch 2050/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.6493 - val_loss: 211.5693\n",
      "Epoch 2051/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.3497 - val_loss: 137.8560\n",
      "Epoch 2052/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.0411 - val_loss: 159.1921\n",
      "Epoch 2053/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 139.6331 - val_loss: 175.1503\n",
      "Epoch 2054/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 187.5860 - val_loss: 171.7466\n",
      "Epoch 2055/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.1611 - val_loss: 138.7196\n",
      "Epoch 2056/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 142.1448 - val_loss: 135.7433\n",
      "Epoch 2057/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.6975 - val_loss: 149.0229\n",
      "Epoch 2058/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.9420 - val_loss: 136.2415\n",
      "Epoch 2059/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.1144 - val_loss: 148.9038\n",
      "Epoch 2060/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.2114 - val_loss: 140.4092\n",
      "Epoch 2061/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 151.2826 - val_loss: 152.0622\n",
      "Epoch 2062/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 160.4051 - val_loss: 129.8373\n",
      "Epoch 2063/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 134.3414 - val_loss: 132.0724\n",
      "Epoch 2064/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.1500 - val_loss: 196.3927\n",
      "Epoch 2065/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.4048 - val_loss: 141.2952\n",
      "Epoch 2066/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.0629 - val_loss: 148.8321\n",
      "Epoch 2067/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.0585 - val_loss: 169.0458\n",
      "Epoch 2068/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 224.1041 - val_loss: 678.7947\n",
      "Epoch 2069/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.7207 - val_loss: 130.0113\n",
      "Epoch 2070/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.6603 - val_loss: 285.2362\n",
      "Epoch 2071/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.1277 - val_loss: 132.1384\n",
      "Epoch 2072/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.3764 - val_loss: 128.8490\n",
      "Epoch 2073/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.2604 - val_loss: 131.6169\n",
      "Epoch 2074/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.9758 - val_loss: 147.2968\n",
      "Epoch 2075/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.8133 - val_loss: 155.9615\n",
      "Epoch 2076/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.4517 - val_loss: 205.0262\n",
      "Epoch 2077/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.2285 - val_loss: 155.9321\n",
      "Epoch 2078/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.1251 - val_loss: 143.6274\n",
      "Epoch 2079/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.1859 - val_loss: 146.4851\n",
      "Epoch 2080/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.9845 - val_loss: 222.2934\n",
      "Epoch 2081/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.0300 - val_loss: 145.2521\n",
      "Epoch 2082/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.7240 - val_loss: 142.2795\n",
      "Epoch 2083/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.2081 - val_loss: 198.2654\n",
      "Epoch 2084/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.3507 - val_loss: 147.2086\n",
      "Epoch 2085/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.4008 - val_loss: 132.6400\n",
      "Epoch 2086/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.1791 - val_loss: 179.6926\n",
      "Epoch 2087/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.9035 - val_loss: 129.3034\n",
      "Epoch 2088/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.6523 - val_loss: 162.2790\n",
      "Epoch 2089/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.8403 - val_loss: 177.5537\n",
      "Epoch 2090/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.5518 - val_loss: 158.5021\n",
      "Epoch 2091/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.0244 - val_loss: 142.6943\n",
      "Epoch 2092/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.8667 - val_loss: 157.3856\n",
      "Epoch 2093/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.1995 - val_loss: 130.8833\n",
      "Epoch 2094/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.9444 - val_loss: 171.6329\n",
      "Epoch 2095/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.6325 - val_loss: 238.7223\n",
      "Epoch 2096/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.3552 - val_loss: 148.0731\n",
      "Epoch 2097/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.6600 - val_loss: 143.6127\n",
      "Epoch 2098/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.2485 - val_loss: 152.0381\n",
      "Epoch 2099/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.2131 - val_loss: 138.4646\n",
      "Epoch 2100/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.9288 - val_loss: 147.2554\n",
      "Epoch 2101/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.8606 - val_loss: 131.9786\n",
      "Epoch 2102/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.8788 - val_loss: 150.9447\n",
      "Epoch 2103/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 175.0726 - val_loss: 133.0575\n",
      "Epoch 2104/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.0597 - val_loss: 147.5362\n",
      "Epoch 2105/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.6617 - val_loss: 177.4273\n",
      "Epoch 2106/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.4554 - val_loss: 149.0865\n",
      "Epoch 2107/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.5475 - val_loss: 137.5723\n",
      "Epoch 2108/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.4516 - val_loss: 217.7790\n",
      "Epoch 2109/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.1290 - val_loss: 167.0666\n",
      "Epoch 2110/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.8664 - val_loss: 158.9160\n",
      "Epoch 2111/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.9385 - val_loss: 126.9397\n",
      "Epoch 2112/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.7409 - val_loss: 151.8505\n",
      "Epoch 2113/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.5632 - val_loss: 157.4655\n",
      "Epoch 2114/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.4051 - val_loss: 136.7859\n",
      "Epoch 2115/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.1594 - val_loss: 139.3729\n",
      "Epoch 2116/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 172.0392 - val_loss: 149.8698\n",
      "Epoch 2117/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.7253 - val_loss: 185.8908\n",
      "Epoch 2118/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.3128 - val_loss: 159.5340\n",
      "Epoch 2119/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 142.9788 - val_loss: 170.0076\n",
      "Epoch 2120/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.5274 - val_loss: 136.2287\n",
      "Epoch 2121/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 182.1317 - val_loss: 155.5780\n",
      "Epoch 2122/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 152.8125 - val_loss: 198.5821\n",
      "Epoch 2123/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 137.0486 - val_loss: 159.7377\n",
      "Epoch 2124/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 137.9569 - val_loss: 151.8232\n",
      "Epoch 2125/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 154.0464 - val_loss: 155.8383\n",
      "Epoch 2126/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.6741 - val_loss: 139.2409\n",
      "Epoch 2127/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 163.2632 - val_loss: 136.6652\n",
      "Epoch 2128/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.9832 - val_loss: 135.2801\n",
      "Epoch 2129/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.4517 - val_loss: 139.7035\n",
      "Epoch 2130/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.8557 - val_loss: 146.6546\n",
      "Epoch 2131/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.6745 - val_loss: 144.5188\n",
      "Epoch 2132/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.2219 - val_loss: 163.7057\n",
      "Epoch 2133/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.3656 - val_loss: 141.5680\n",
      "Epoch 2134/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.6624 - val_loss: 155.0979\n",
      "Epoch 2135/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.6392 - val_loss: 172.8661\n",
      "Epoch 2136/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.9985 - val_loss: 160.6706\n",
      "Epoch 2137/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.1480 - val_loss: 245.6774\n",
      "Epoch 2138/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.7377 - val_loss: 146.2763\n",
      "Epoch 2139/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.7082 - val_loss: 142.5618\n",
      "Epoch 2140/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.9021 - val_loss: 135.6578\n",
      "Epoch 2141/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.9101 - val_loss: 159.4418\n",
      "Epoch 2142/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.9521 - val_loss: 131.1831\n",
      "Epoch 2143/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.2181 - val_loss: 135.9135\n",
      "Epoch 2144/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.3646 - val_loss: 139.6753\n",
      "Epoch 2145/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.3841 - val_loss: 133.7442\n",
      "Epoch 2146/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.9193 - val_loss: 158.4574\n",
      "Epoch 2147/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 180.7016 - val_loss: 139.0875\n",
      "Epoch 2148/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.7989 - val_loss: 143.5827\n",
      "Epoch 2149/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.4621 - val_loss: 135.8639\n",
      "Epoch 2150/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.6665 - val_loss: 136.8599\n",
      "Epoch 2151/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.1988 - val_loss: 152.0593\n",
      "Epoch 2152/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.6205 - val_loss: 136.5268\n",
      "Epoch 2153/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.6346 - val_loss: 142.5656\n",
      "Epoch 2154/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 155.6407 - val_loss: 140.7315\n",
      "Epoch 2155/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.3007 - val_loss: 151.9449\n",
      "Epoch 2156/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.7404 - val_loss: 137.9073\n",
      "Epoch 2157/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.0707 - val_loss: 144.6560\n",
      "Epoch 2158/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.3200 - val_loss: 130.0761\n",
      "Epoch 2159/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.9869 - val_loss: 135.2159\n",
      "Epoch 2160/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 153.8414 - val_loss: 256.9726\n",
      "Epoch 2161/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 172.7635 - val_loss: 153.8646\n",
      "Epoch 2162/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.4836 - val_loss: 140.5202\n",
      "Epoch 2163/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.2932 - val_loss: 147.4533\n",
      "Epoch 2164/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.2654 - val_loss: 132.2883\n",
      "Epoch 2165/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.5680 - val_loss: 137.8312\n",
      "Epoch 2166/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.3415 - val_loss: 132.9675\n",
      "Epoch 2167/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.0504 - val_loss: 132.9611\n",
      "Epoch 2168/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.7423 - val_loss: 328.1999\n",
      "Epoch 2169/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 193.8380 - val_loss: 165.8863\n",
      "Epoch 2170/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.1902 - val_loss: 146.7747\n",
      "Epoch 2171/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.8079 - val_loss: 153.6187\n",
      "Epoch 2172/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.3547 - val_loss: 137.0746\n",
      "Epoch 2173/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.5291 - val_loss: 162.5118\n",
      "Epoch 2174/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.7710 - val_loss: 152.3885\n",
      "Epoch 2175/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.6720 - val_loss: 244.6493\n",
      "Epoch 2176/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 150.8801 - val_loss: 171.3005\n",
      "Epoch 2177/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.2985 - val_loss: 139.3205\n",
      "Epoch 2178/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.6550 - val_loss: 138.8798\n",
      "Epoch 2179/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.4026 - val_loss: 175.3765\n",
      "Epoch 2180/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.4372 - val_loss: 482.6742\n",
      "Epoch 2181/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.7938 - val_loss: 160.0573\n",
      "Epoch 2182/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 174.9493 - val_loss: 134.0875\n",
      "Epoch 2183/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.8404 - val_loss: 135.9556\n",
      "Epoch 2184/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 196.0850 - val_loss: 167.1251\n",
      "Epoch 2185/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 139.9764 - val_loss: 144.1041\n",
      "Epoch 2186/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 139.4764 - val_loss: 168.3866\n",
      "Epoch 2187/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 153.0768 - val_loss: 131.6038\n",
      "Epoch 2188/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.2480 - val_loss: 146.3441\n",
      "Epoch 2189/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.7454 - val_loss: 139.7163\n",
      "Epoch 2190/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.3776 - val_loss: 129.4016\n",
      "Epoch 2191/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.7954 - val_loss: 132.7568\n",
      "Epoch 2192/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.9995 - val_loss: 146.0909\n",
      "Epoch 2193/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.1459 - val_loss: 175.2180\n",
      "Epoch 2194/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.2434 - val_loss: 163.5176\n",
      "Epoch 2195/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 150.0987 - val_loss: 182.7003\n",
      "Epoch 2196/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.7692 - val_loss: 169.0004\n",
      "Epoch 2197/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.6999 - val_loss: 133.2364\n",
      "Epoch 2198/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.5852 - val_loss: 224.1368\n",
      "Epoch 2199/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 137.4602 - val_loss: 143.8909\n",
      "Epoch 2200/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.5666 - val_loss: 146.8142\n",
      "Epoch 2201/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.9132 - val_loss: 136.5698\n",
      "Epoch 2202/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.9303 - val_loss: 130.5240\n",
      "Epoch 2203/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.5252 - val_loss: 136.3799\n",
      "Epoch 2204/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.1355 - val_loss: 134.7178\n",
      "Epoch 2205/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 177.1591 - val_loss: 165.0405\n",
      "Epoch 2206/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.8781 - val_loss: 143.5011\n",
      "Epoch 2207/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.2482 - val_loss: 135.7779\n",
      "Epoch 2208/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.5669 - val_loss: 167.7696\n",
      "Epoch 2209/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.6957 - val_loss: 134.8954\n",
      "Epoch 2210/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.4312 - val_loss: 146.1640\n",
      "Epoch 2211/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.7130 - val_loss: 143.3500\n",
      "Epoch 2212/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.1028 - val_loss: 138.5563\n",
      "Epoch 2213/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.9362 - val_loss: 171.3785\n",
      "Epoch 2214/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.6180 - val_loss: 150.0261\n",
      "Epoch 2215/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.6237 - val_loss: 135.0555\n",
      "Epoch 2216/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.3155 - val_loss: 148.0889\n",
      "Epoch 2217/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.1235 - val_loss: 141.4306\n",
      "Epoch 2218/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.7153 - val_loss: 140.5686\n",
      "Epoch 2219/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.4956 - val_loss: 129.1491\n",
      "Epoch 2220/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.6841 - val_loss: 176.1664\n",
      "Epoch 2221/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.5625 - val_loss: 297.9072\n",
      "Epoch 2222/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 194.7016 - val_loss: 203.6669\n",
      "Epoch 2223/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 208.7199 - val_loss: 137.0666\n",
      "Epoch 2224/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.1676 - val_loss: 149.2969\n",
      "Epoch 2225/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 200.6547 - val_loss: 144.7319\n",
      "Epoch 2226/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.1904 - val_loss: 132.0290\n",
      "Epoch 2227/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.6980 - val_loss: 142.2128\n",
      "Epoch 2228/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.9826 - val_loss: 140.0057\n",
      "Epoch 2229/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.8336 - val_loss: 128.3346\n",
      "Epoch 2230/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.7646 - val_loss: 211.4079\n",
      "Epoch 2231/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 153.4213 - val_loss: 235.5829\n",
      "Epoch 2232/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.0533 - val_loss: 131.9907\n",
      "Epoch 2233/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.8590 - val_loss: 140.8623\n",
      "Epoch 2234/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.6298 - val_loss: 146.6716\n",
      "Epoch 2235/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.1147 - val_loss: 142.9505\n",
      "Epoch 2236/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.1941 - val_loss: 191.5776\n",
      "Epoch 2237/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.6445 - val_loss: 154.5172\n",
      "Epoch 2238/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.5482 - val_loss: 167.7795\n",
      "Epoch 2239/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.5119 - val_loss: 183.8358\n",
      "Epoch 2240/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.1565 - val_loss: 135.9039\n",
      "Epoch 2241/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.6341 - val_loss: 154.6943\n",
      "Epoch 2242/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.4100 - val_loss: 156.9262\n",
      "Epoch 2243/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.5438 - val_loss: 137.4583\n",
      "Epoch 2244/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.5952 - val_loss: 177.2917\n",
      "Epoch 2245/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.5798 - val_loss: 136.6629\n",
      "Epoch 2246/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.4043 - val_loss: 142.5998\n",
      "Epoch 2247/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.2586 - val_loss: 145.4520\n",
      "Epoch 2248/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 154.6871 - val_loss: 148.1364\n",
      "Epoch 2249/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 138.3163 - val_loss: 152.9820\n",
      "Epoch 2250/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 136.9292 - val_loss: 180.9839\n",
      "Epoch 2251/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.2104 - val_loss: 155.3303\n",
      "Epoch 2252/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.4217 - val_loss: 140.3988\n",
      "Epoch 2253/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.9595 - val_loss: 183.3776\n",
      "Epoch 2254/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.6861 - val_loss: 140.1961\n",
      "Epoch 2255/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.4924 - val_loss: 134.6811\n",
      "Epoch 2256/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.7882 - val_loss: 150.1473\n",
      "Epoch 2257/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.3695 - val_loss: 130.1583\n",
      "Epoch 2258/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.5089 - val_loss: 136.5051\n",
      "Epoch 2259/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.6781 - val_loss: 145.5646\n",
      "Epoch 2260/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 131.3104 - val_loss: 144.7755\n",
      "Epoch 2261/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.2098 - val_loss: 140.2155\n",
      "Epoch 2262/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.6502 - val_loss: 140.7573\n",
      "Epoch 2263/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.5437 - val_loss: 143.4499\n",
      "Epoch 2264/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.1838 - val_loss: 165.0050\n",
      "Epoch 2265/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 153.9795 - val_loss: 216.6152\n",
      "Epoch 2266/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.8146 - val_loss: 138.2683\n",
      "Epoch 2267/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.3642 - val_loss: 156.8121\n",
      "Epoch 2268/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 160.5235 - val_loss: 174.4146\n",
      "Epoch 2269/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.7366 - val_loss: 132.6940\n",
      "Epoch 2270/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 178.9227 - val_loss: 235.6583\n",
      "Epoch 2271/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.9787 - val_loss: 137.7185\n",
      "Epoch 2272/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.0336 - val_loss: 153.3404\n",
      "Epoch 2273/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.8644 - val_loss: 182.5533\n",
      "Epoch 2274/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.8312 - val_loss: 160.0718\n",
      "Epoch 2275/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.6698 - val_loss: 139.1356\n",
      "Epoch 2276/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.2097 - val_loss: 145.1484\n",
      "Epoch 2277/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 142.1262 - val_loss: 130.9839\n",
      "Epoch 2278/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.9382 - val_loss: 138.4108\n",
      "Epoch 2279/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.1087 - val_loss: 196.2217\n",
      "Epoch 2280/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.0345 - val_loss: 170.1782\n",
      "Epoch 2281/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.6635 - val_loss: 201.3845\n",
      "Epoch 2282/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.6388 - val_loss: 178.3622\n",
      "Epoch 2283/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 153.8807 - val_loss: 134.5025\n",
      "Epoch 2284/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.7384 - val_loss: 140.4165\n",
      "Epoch 2285/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.4875 - val_loss: 135.2573\n",
      "Epoch 2286/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.3213 - val_loss: 172.2645\n",
      "Epoch 2287/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.8263 - val_loss: 153.7531\n",
      "Epoch 2288/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.2620 - val_loss: 140.4089\n",
      "Epoch 2289/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.8209 - val_loss: 154.8279\n",
      "Epoch 2290/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.6427 - val_loss: 161.4624\n",
      "Epoch 2291/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.0938 - val_loss: 136.6310\n",
      "Epoch 2292/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.8086 - val_loss: 153.6553\n",
      "Epoch 2293/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.7673 - val_loss: 178.2532\n",
      "Epoch 2294/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.6743 - val_loss: 236.7314\n",
      "Epoch 2295/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.3678 - val_loss: 141.3333\n",
      "Epoch 2296/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.2006 - val_loss: 138.0047\n",
      "Epoch 2297/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.8430 - val_loss: 148.9714\n",
      "Epoch 2298/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.1304 - val_loss: 135.0295\n",
      "Epoch 2299/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.6595 - val_loss: 140.8751\n",
      "Epoch 2300/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 214.3659 - val_loss: 197.7924\n",
      "Epoch 2301/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 180.2327 - val_loss: 151.6120\n",
      "Epoch 2302/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.1289 - val_loss: 148.4755\n",
      "Epoch 2303/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.1080 - val_loss: 132.6165\n",
      "Epoch 2304/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.7821 - val_loss: 127.7526\n",
      "Epoch 2305/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.2554 - val_loss: 171.1763\n",
      "Epoch 2306/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.5164 - val_loss: 153.5148\n",
      "Epoch 2307/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.2797 - val_loss: 131.2020\n",
      "Epoch 2308/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.6093 - val_loss: 228.3593\n",
      "Epoch 2309/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.5088 - val_loss: 135.6401\n",
      "Epoch 2310/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 162.9027 - val_loss: 137.5014\n",
      "Epoch 2311/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 132.1545 - val_loss: 138.2542\n",
      "Epoch 2312/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 140.3640 - val_loss: 180.6859\n",
      "Epoch 2313/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.1525 - val_loss: 128.0936\n",
      "Epoch 2314/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.2784 - val_loss: 139.4333\n",
      "Epoch 2315/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.4236 - val_loss: 158.0494\n",
      "Epoch 2316/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.4529 - val_loss: 140.0726\n",
      "Epoch 2317/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.5818 - val_loss: 192.4666\n",
      "Epoch 2318/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.5420 - val_loss: 134.0545\n",
      "Epoch 2319/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.5887 - val_loss: 211.3955\n",
      "Epoch 2320/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.3673 - val_loss: 132.5801\n",
      "Epoch 2321/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.6088 - val_loss: 142.6312\n",
      "Epoch 2322/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.2657 - val_loss: 151.6805\n",
      "Epoch 2323/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.8604 - val_loss: 154.0221\n",
      "Epoch 2324/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.7047 - val_loss: 131.1163\n",
      "Epoch 2325/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.7842 - val_loss: 146.0651\n",
      "Epoch 2326/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.4482 - val_loss: 245.1201\n",
      "Epoch 2327/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.2640 - val_loss: 128.9865\n",
      "Epoch 2328/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 131.6861 - val_loss: 139.3568\n",
      "Epoch 2329/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.3366 - val_loss: 147.1569\n",
      "Epoch 2330/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.6375 - val_loss: 134.5946\n",
      "Epoch 2331/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 182.4693 - val_loss: 217.7111\n",
      "Epoch 2332/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.3325 - val_loss: 146.7120\n",
      "Epoch 2333/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.2322 - val_loss: 151.4098\n",
      "Epoch 2334/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.3072 - val_loss: 142.6921\n",
      "Epoch 2335/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 131.4881 - val_loss: 150.4988\n",
      "Epoch 2336/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 142.5471 - val_loss: 158.3239\n",
      "Epoch 2337/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.8143 - val_loss: 147.2170\n",
      "Epoch 2338/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.7537 - val_loss: 133.1285\n",
      "Epoch 2339/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 185.2032 - val_loss: 243.2691\n",
      "Epoch 2340/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.2453 - val_loss: 247.3106\n",
      "Epoch 2341/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 147.3785 - val_loss: 287.1405\n",
      "Epoch 2342/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.8796 - val_loss: 156.6738\n",
      "Epoch 2343/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.9436 - val_loss: 179.7199\n",
      "Epoch 2344/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.5497 - val_loss: 141.8777\n",
      "Epoch 2345/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.3975 - val_loss: 195.5095\n",
      "Epoch 2346/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.3212 - val_loss: 146.3286\n",
      "Epoch 2347/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.8381 - val_loss: 142.2784\n",
      "Epoch 2348/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 176.6782 - val_loss: 146.4686\n",
      "Epoch 2349/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.3392 - val_loss: 142.0421\n",
      "Epoch 2350/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.7138 - val_loss: 142.1798\n",
      "Epoch 2351/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.7258 - val_loss: 141.0150\n",
      "Epoch 2352/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.7688 - val_loss: 147.0814\n",
      "Epoch 2353/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.4782 - val_loss: 146.4417\n",
      "Epoch 2354/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.1334 - val_loss: 187.3869\n",
      "Epoch 2355/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.0873 - val_loss: 131.4970\n",
      "Epoch 2356/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 189.8065 - val_loss: 149.7828\n",
      "Epoch 2357/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.6164 - val_loss: 174.1141\n",
      "Epoch 2358/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.7424 - val_loss: 253.9296\n",
      "Epoch 2359/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 162.4036 - val_loss: 132.3407\n",
      "Epoch 2360/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.7159 - val_loss: 156.4920\n",
      "Epoch 2361/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 201.6275 - val_loss: 165.2830\n",
      "Epoch 2362/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.4592 - val_loss: 146.4745\n",
      "Epoch 2363/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.7034 - val_loss: 153.3278\n",
      "Epoch 2364/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.8332 - val_loss: 145.3634\n",
      "Epoch 2365/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.4095 - val_loss: 130.5965\n",
      "Epoch 2366/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.3385 - val_loss: 170.6589\n",
      "Epoch 2367/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.1515 - val_loss: 134.1999\n",
      "Epoch 2368/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.8061 - val_loss: 182.0835\n",
      "Epoch 2369/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 161.9304 - val_loss: 177.8127\n",
      "Epoch 2370/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.5509 - val_loss: 157.0087\n",
      "Epoch 2371/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.7084 - val_loss: 144.7695\n",
      "Epoch 2372/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 154.4981 - val_loss: 164.0248\n",
      "Epoch 2373/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 176.7131 - val_loss: 137.0077\n",
      "Epoch 2374/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 163.2599 - val_loss: 176.4388\n",
      "Epoch 2375/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.7780 - val_loss: 128.9853\n",
      "Epoch 2376/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.6311 - val_loss: 129.9609\n",
      "Epoch 2377/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 142.0523 - val_loss: 140.9862\n",
      "Epoch 2378/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.3332 - val_loss: 131.1428\n",
      "Epoch 2379/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.3062 - val_loss: 161.5116\n",
      "Epoch 2380/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.6782 - val_loss: 146.5942\n",
      "Epoch 2381/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.0732 - val_loss: 133.6790\n",
      "Epoch 2382/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.0492 - val_loss: 204.1518\n",
      "Epoch 2383/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.4025 - val_loss: 175.7573\n",
      "Epoch 2384/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.4597 - val_loss: 172.8100\n",
      "Epoch 2385/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.3195 - val_loss: 144.2342\n",
      "Epoch 2386/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.1356 - val_loss: 136.1132\n",
      "Epoch 2387/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 159.7121 - val_loss: 173.1326\n",
      "Epoch 2388/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.1435 - val_loss: 205.9072\n",
      "Epoch 2389/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 191.9471 - val_loss: 157.8196\n",
      "Epoch 2390/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.0037 - val_loss: 209.8938\n",
      "Epoch 2391/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.6894 - val_loss: 139.7287\n",
      "Epoch 2392/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.9832 - val_loss: 160.8929\n",
      "Epoch 2393/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.1957 - val_loss: 140.1066\n",
      "Epoch 2394/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.2428 - val_loss: 165.0226\n",
      "Epoch 2395/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.0535 - val_loss: 145.4737\n",
      "Epoch 2396/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.3873 - val_loss: 138.4688\n",
      "Epoch 2397/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 177.9895 - val_loss: 163.2567\n",
      "Epoch 2398/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.4394 - val_loss: 175.4904\n",
      "Epoch 2399/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.1991 - val_loss: 136.2032\n",
      "Epoch 2400/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.9549 - val_loss: 140.7380\n",
      "Epoch 2401/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.6075 - val_loss: 159.9331\n",
      "Epoch 2402/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.5373 - val_loss: 151.6081\n",
      "Epoch 2403/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.9838 - val_loss: 210.8497\n",
      "Epoch 2404/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.6854 - val_loss: 143.2836\n",
      "Epoch 2405/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.7751 - val_loss: 257.7768\n",
      "Epoch 2406/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.0831 - val_loss: 148.4659\n",
      "Epoch 2407/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.9032 - val_loss: 148.5987\n",
      "Epoch 2408/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.2896 - val_loss: 143.0245\n",
      "Epoch 2409/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.6557 - val_loss: 186.6936\n",
      "Epoch 2410/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.5042 - val_loss: 133.6176\n",
      "Epoch 2411/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.0321 - val_loss: 147.1354\n",
      "Epoch 2412/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.7205 - val_loss: 139.2909\n",
      "Epoch 2413/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.3275 - val_loss: 154.5058\n",
      "Epoch 2414/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.4249 - val_loss: 130.2545\n",
      "Epoch 2415/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 142.7639 - val_loss: 141.1520\n",
      "Epoch 2416/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.8351 - val_loss: 138.6408\n",
      "Epoch 2417/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.5420 - val_loss: 182.9818\n",
      "Epoch 2418/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.2001 - val_loss: 143.1829\n",
      "Epoch 2419/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.5998 - val_loss: 143.3044\n",
      "Epoch 2420/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.2843 - val_loss: 139.5931\n",
      "Epoch 2421/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.0605 - val_loss: 143.9447\n",
      "Epoch 2422/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.5355 - val_loss: 140.2388\n",
      "Epoch 2423/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.4976 - val_loss: 211.9525\n",
      "Epoch 2424/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.9441 - val_loss: 136.0986\n",
      "Epoch 2425/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.9251 - val_loss: 136.0004\n",
      "Epoch 2426/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.4676 - val_loss: 131.2853\n",
      "Epoch 2427/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.5923 - val_loss: 134.9124\n",
      "Epoch 2428/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.3914 - val_loss: 142.2214\n",
      "Epoch 2429/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 153.0756 - val_loss: 226.4374\n",
      "Epoch 2430/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.2320 - val_loss: 136.7851\n",
      "Epoch 2431/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.1908 - val_loss: 143.7450\n",
      "Epoch 2432/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.7037 - val_loss: 150.6605\n",
      "Epoch 2433/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.3195 - val_loss: 139.5698\n",
      "Epoch 2434/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 151.0741 - val_loss: 140.6911\n",
      "Epoch 2435/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 133.7059 - val_loss: 133.6822\n",
      "Epoch 2436/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 140.8132 - val_loss: 165.8552\n",
      "Epoch 2437/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.5590 - val_loss: 138.3973\n",
      "Epoch 2438/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.1816 - val_loss: 132.2292\n",
      "Epoch 2439/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.8981 - val_loss: 182.3465\n",
      "Epoch 2440/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 223.2216 - val_loss: 172.0459\n",
      "Epoch 2441/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.4545 - val_loss: 155.0018\n",
      "Epoch 2442/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.4098 - val_loss: 148.8236\n",
      "Epoch 2443/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.3583 - val_loss: 128.6919\n",
      "Epoch 2444/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 136.4881 - val_loss: 141.7549\n",
      "Epoch 2445/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.1187 - val_loss: 142.6229\n",
      "Epoch 2446/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.8811 - val_loss: 182.1519\n",
      "Epoch 2447/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.7828 - val_loss: 175.8645\n",
      "Epoch 2448/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.8384 - val_loss: 134.7660\n",
      "Epoch 2449/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.2062 - val_loss: 145.9039\n",
      "Epoch 2450/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.9587 - val_loss: 234.5424\n",
      "Epoch 2451/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.1094 - val_loss: 176.7488\n",
      "Epoch 2452/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.7651 - val_loss: 131.8604\n",
      "Epoch 2453/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.3300 - val_loss: 168.5959\n",
      "Epoch 2454/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.4423 - val_loss: 145.5660\n",
      "Epoch 2455/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.6236 - val_loss: 131.2526\n",
      "Epoch 2456/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.4840 - val_loss: 151.7277\n",
      "Epoch 2457/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 131.2073 - val_loss: 144.3788\n",
      "Epoch 2458/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 135.614 - 1s 63us/step - loss: 132.2273 - val_loss: 143.5065\n",
      "Epoch 2459/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.4684 - val_loss: 159.5132\n",
      "Epoch 2460/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.4210 - val_loss: 161.6442\n",
      "Epoch 2461/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.8595 - val_loss: 144.4382\n",
      "Epoch 2462/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 138.9122 - val_loss: 142.8356\n",
      "Epoch 2463/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 138.6848 - val_loss: 132.1251\n",
      "Epoch 2464/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.1131 - val_loss: 168.5297\n",
      "Epoch 2465/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.6878 - val_loss: 131.3507\n",
      "Epoch 2466/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.2168 - val_loss: 148.6314\n",
      "Epoch 2467/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 136.4029 - val_loss: 156.1517\n",
      "Epoch 2468/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 141.9005 - val_loss: 138.8215\n",
      "Epoch 2469/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.0599 - val_loss: 133.8940\n",
      "Epoch 2470/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.8100 - val_loss: 134.5314\n",
      "Epoch 2471/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.8363 - val_loss: 136.2590\n",
      "Epoch 2472/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 139.3275 - val_loss: 149.6580\n",
      "Epoch 2473/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.4178 - val_loss: 159.4473\n",
      "Epoch 2474/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.1141 - val_loss: 134.2162\n",
      "Epoch 2475/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 167.9162 - val_loss: 143.9597\n",
      "Epoch 2476/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.1972 - val_loss: 138.7594\n",
      "Epoch 2477/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.6459 - val_loss: 172.4047\n",
      "Epoch 2478/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.6167 - val_loss: 179.2568\n",
      "Epoch 2479/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.2327 - val_loss: 138.1750\n",
      "Epoch 2480/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.6533 - val_loss: 131.1670\n",
      "Epoch 2481/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.0967 - val_loss: 132.4295\n",
      "Epoch 2482/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.7504 - val_loss: 161.9405\n",
      "Epoch 2483/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 130.5621 - val_loss: 129.1794\n",
      "Epoch 2484/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.9771 - val_loss: 130.6321\n",
      "Epoch 2485/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.5635 - val_loss: 140.3828\n",
      "Epoch 2486/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.6152 - val_loss: 162.2901\n",
      "Epoch 2487/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.6272 - val_loss: 131.8777\n",
      "Epoch 2488/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.3104 - val_loss: 137.2593\n",
      "Epoch 2489/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.6902 - val_loss: 158.6154\n",
      "Epoch 2490/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.4704 - val_loss: 143.4767\n",
      "Epoch 2491/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.7539 - val_loss: 170.2318\n",
      "Epoch 2492/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.1650 - val_loss: 137.9224\n",
      "Epoch 2493/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 158.4130 - val_loss: 267.8660\n",
      "Epoch 2494/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 138.2101 - val_loss: 170.8370\n",
      "Epoch 2495/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 136.7428 - val_loss: 143.0071\n",
      "Epoch 2496/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 134.1404 - val_loss: 134.0783\n",
      "Epoch 2497/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.5141 - val_loss: 140.4637\n",
      "Epoch 2498/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.8603 - val_loss: 215.6934\n",
      "Epoch 2499/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.4125 - val_loss: 137.4303\n",
      "Epoch 2500/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 142.9895 - val_loss: 165.9088\n",
      "Epoch 2501/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.0615 - val_loss: 138.5886\n",
      "Epoch 2502/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.0569 - val_loss: 154.5985\n",
      "Epoch 2503/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.4311 - val_loss: 152.1508\n",
      "Epoch 2504/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.6821 - val_loss: 144.9323\n",
      "Epoch 2505/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.2324 - val_loss: 166.5015\n",
      "Epoch 2506/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.8695 - val_loss: 140.9243\n",
      "Epoch 2507/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.9602 - val_loss: 135.3847\n",
      "Epoch 2508/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.2690 - val_loss: 148.4596\n",
      "Epoch 2509/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.0107 - val_loss: 138.4868\n",
      "Epoch 2510/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.3425 - val_loss: 156.2446\n",
      "Epoch 2511/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 155.9293 - val_loss: 216.6394\n",
      "Epoch 2512/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.3275 - val_loss: 152.8142\n",
      "Epoch 2513/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.3829 - val_loss: 131.1497\n",
      "Epoch 2514/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.1613 - val_loss: 137.9301\n",
      "Epoch 2515/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.4886 - val_loss: 152.7407\n",
      "Epoch 2516/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.6519 - val_loss: 136.8028\n",
      "Epoch 2517/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.1927 - val_loss: 140.9787\n",
      "Epoch 2518/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.9429 - val_loss: 230.3998\n",
      "Epoch 2519/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.2176 - val_loss: 133.7304\n",
      "Epoch 2520/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.1571 - val_loss: 248.1825\n",
      "Epoch 2521/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.2229 - val_loss: 145.7329\n",
      "Epoch 2522/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.3039 - val_loss: 144.9256\n",
      "Epoch 2523/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.8943 - val_loss: 133.5984\n",
      "Epoch 2524/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.0896 - val_loss: 140.8319\n",
      "Epoch 2525/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.7558 - val_loss: 152.0008\n",
      "Epoch 2526/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.4649 - val_loss: 142.0325\n",
      "Epoch 2527/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 156.3272 - val_loss: 192.6992\n",
      "Epoch 2528/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.0666 - val_loss: 185.5602\n",
      "Epoch 2529/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.8717 - val_loss: 130.9095\n",
      "Epoch 2530/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.7633 - val_loss: 135.7271\n",
      "Epoch 2531/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.0120 - val_loss: 145.7325\n",
      "Epoch 2532/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.7607 - val_loss: 154.2454\n",
      "Epoch 2533/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.1143 - val_loss: 153.3634\n",
      "Epoch 2534/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.9322 - val_loss: 139.1355\n",
      "Epoch 2535/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.7329 - val_loss: 130.4516\n",
      "Epoch 2536/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.6628 - val_loss: 138.3767\n",
      "Epoch 2537/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.1402 - val_loss: 159.6316\n",
      "Epoch 2538/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 155.3693 - val_loss: 158.9493\n",
      "Epoch 2539/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.3130 - val_loss: 133.6857\n",
      "Epoch 2540/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.0522 - val_loss: 172.7779\n",
      "Epoch 2541/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.2882 - val_loss: 196.7642\n",
      "Epoch 2542/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.4707 - val_loss: 179.9793\n",
      "Epoch 2543/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.1495 - val_loss: 136.1814\n",
      "Epoch 2544/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.9389 - val_loss: 155.5743\n",
      "Epoch 2545/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.7137 - val_loss: 130.6705\n",
      "Epoch 2546/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.9017 - val_loss: 178.6177\n",
      "Epoch 2547/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 164.9174 - val_loss: 129.9113\n",
      "Epoch 2548/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.1022 - val_loss: 159.0271\n",
      "Epoch 2549/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.7295 - val_loss: 134.4717\n",
      "Epoch 2550/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.6761 - val_loss: 174.2082\n",
      "Epoch 2551/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.3332 - val_loss: 198.1695\n",
      "Epoch 2552/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.1967 - val_loss: 159.3381\n",
      "Epoch 2553/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.7161 - val_loss: 165.9476\n",
      "Epoch 2554/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.4526 - val_loss: 184.9653\n",
      "Epoch 2555/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.5450 - val_loss: 166.1100\n",
      "Epoch 2556/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.8368 - val_loss: 147.8858\n",
      "Epoch 2557/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.9611 - val_loss: 193.4737\n",
      "Epoch 2558/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 131.1715 - val_loss: 176.0567\n",
      "Epoch 2559/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.3362 - val_loss: 156.5447\n",
      "Epoch 2560/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.2117 - val_loss: 142.3167\n",
      "Epoch 2561/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 138.7020 - val_loss: 140.6950\n",
      "Epoch 2562/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.0506 - val_loss: 134.4142\n",
      "Epoch 2563/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.2041 - val_loss: 146.1658\n",
      "Epoch 2564/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.7953 - val_loss: 132.9072\n",
      "Epoch 2565/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 156.0084 - val_loss: 173.9556\n",
      "Epoch 2566/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 204.2060 - val_loss: 146.9496\n",
      "Epoch 2567/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 152.1282 - val_loss: 161.8853\n",
      "Epoch 2568/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.9764 - val_loss: 157.3083\n",
      "Epoch 2569/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.5628 - val_loss: 153.0851\n",
      "Epoch 2570/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.9030 - val_loss: 158.7828\n",
      "Epoch 2571/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.9606 - val_loss: 145.6081\n",
      "Epoch 2572/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.3745 - val_loss: 132.1005\n",
      "Epoch 2573/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 176.9294 - val_loss: 147.5708\n",
      "Epoch 2574/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.9873 - val_loss: 128.8977\n",
      "Epoch 2575/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.8563 - val_loss: 147.4851\n",
      "Epoch 2576/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.4847 - val_loss: 148.1778\n",
      "Epoch 2577/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 179.5325 - val_loss: 131.9355\n",
      "Epoch 2578/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.4122 - val_loss: 128.8223\n",
      "Epoch 2579/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 127.9446 - val_loss: 140.4503\n",
      "Epoch 2580/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.6198 - val_loss: 141.0340\n",
      "Epoch 2581/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.2803 - val_loss: 185.0179\n",
      "Epoch 2582/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.2539 - val_loss: 175.8156\n",
      "Epoch 2583/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.5864 - val_loss: 158.8589\n",
      "Epoch 2584/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.4941 - val_loss: 129.5837\n",
      "Epoch 2585/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.6186 - val_loss: 133.0637\n",
      "Epoch 2586/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.5923 - val_loss: 138.0186\n",
      "Epoch 2587/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 157.4629 - val_loss: 159.6120\n",
      "Epoch 2588/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.6180 - val_loss: 130.1522\n",
      "Epoch 2589/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 144.9072 - val_loss: 126.3314\n",
      "Epoch 2590/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.9951 - val_loss: 133.3623\n",
      "Epoch 2591/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.1724 - val_loss: 143.7607\n",
      "Epoch 2592/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.2878 - val_loss: 140.2029\n",
      "Epoch 2593/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.5717 - val_loss: 136.2966\n",
      "Epoch 2594/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.9405 - val_loss: 130.7759\n",
      "Epoch 2595/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.3360 - val_loss: 128.8168\n",
      "Epoch 2596/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 186.5876 - val_loss: 144.3431\n",
      "Epoch 2597/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.9299 - val_loss: 132.0289\n",
      "Epoch 2598/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 141.8711 - val_loss: 143.3855\n",
      "Epoch 2599/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.0762 - val_loss: 145.5204\n",
      "Epoch 2600/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.1499 - val_loss: 142.3145\n",
      "Epoch 2601/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.8685 - val_loss: 252.2293\n",
      "Epoch 2602/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.4769 - val_loss: 140.0241\n",
      "Epoch 2603/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.8624 - val_loss: 207.2404\n",
      "Epoch 2604/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 136.2757 - val_loss: 184.0769\n",
      "Epoch 2605/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.1816 - val_loss: 140.3693\n",
      "Epoch 2606/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.0110 - val_loss: 159.4321\n",
      "Epoch 2607/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.1429 - val_loss: 138.8869\n",
      "Epoch 2608/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.9205 - val_loss: 162.6290\n",
      "Epoch 2609/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.1845 - val_loss: 135.5051\n",
      "Epoch 2610/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 198.3509 - val_loss: 144.7571\n",
      "Epoch 2611/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.6474 - val_loss: 135.5910\n",
      "Epoch 2612/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.5597 - val_loss: 133.0653\n",
      "Epoch 2613/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.4269 - val_loss: 136.3270\n",
      "Epoch 2614/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.6250 - val_loss: 193.9478\n",
      "Epoch 2615/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.7696 - val_loss: 171.4988\n",
      "Epoch 2616/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.8954 - val_loss: 141.1597\n",
      "Epoch 2617/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.4464 - val_loss: 145.3121\n",
      "Epoch 2618/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 131.4007 - val_loss: 137.0850\n",
      "Epoch 2619/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 137.4847 - val_loss: 133.5399\n",
      "Epoch 2620/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 134.9755 - val_loss: 148.4853\n",
      "Epoch 2621/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.6242 - val_loss: 131.7878\n",
      "Epoch 2622/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.2625 - val_loss: 131.7613\n",
      "Epoch 2623/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.4774 - val_loss: 174.6823\n",
      "Epoch 2624/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.6059 - val_loss: 167.2921\n",
      "Epoch 2625/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.9092 - val_loss: 128.9764\n",
      "Epoch 2626/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.2532 - val_loss: 140.7155\n",
      "Epoch 2627/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 150.6025 - val_loss: 147.0926\n",
      "Epoch 2628/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.6249 - val_loss: 147.5491\n",
      "Epoch 2629/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.6502 - val_loss: 151.9590\n",
      "Epoch 2630/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.2728 - val_loss: 164.6526\n",
      "Epoch 2631/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.0885 - val_loss: 159.9156\n",
      "Epoch 2632/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.3777 - val_loss: 135.3976\n",
      "Epoch 2633/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 134.3810 - val_loss: 138.2717\n",
      "Epoch 2634/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.4774 - val_loss: 193.7039\n",
      "Epoch 2635/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.2810 - val_loss: 137.6511\n",
      "Epoch 2636/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.2388 - val_loss: 158.9997\n",
      "Epoch 2637/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.5032 - val_loss: 142.2017\n",
      "Epoch 2638/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.9972 - val_loss: 249.0543\n",
      "Epoch 2639/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 142.0152 - val_loss: 131.9141\n",
      "Epoch 2640/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.7993 - val_loss: 217.5182\n",
      "Epoch 2641/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.7771 - val_loss: 143.7626\n",
      "Epoch 2642/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.2202 - val_loss: 162.1748\n",
      "Epoch 2643/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.5904 - val_loss: 164.4778\n",
      "Epoch 2644/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.3735 - val_loss: 145.9876\n",
      "Epoch 2645/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.2540 - val_loss: 134.8400\n",
      "Epoch 2646/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.8156 - val_loss: 135.3618\n",
      "Epoch 2647/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.1555 - val_loss: 183.9683\n",
      "Epoch 2648/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.9093 - val_loss: 173.6727\n",
      "Epoch 2649/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.6917 - val_loss: 144.8295\n",
      "Epoch 2650/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.9063 - val_loss: 128.6991\n",
      "Epoch 2651/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.2315 - val_loss: 163.9317\n",
      "Epoch 2652/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.3309 - val_loss: 142.9364\n",
      "Epoch 2653/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.5833 - val_loss: 185.7612\n",
      "Epoch 2654/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.3749 - val_loss: 135.2964\n",
      "Epoch 2655/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.0413 - val_loss: 137.3531\n",
      "Epoch 2656/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.5228 - val_loss: 161.9325\n",
      "Epoch 2657/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.5375 - val_loss: 140.5470\n",
      "Epoch 2658/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.4730 - val_loss: 182.1994\n",
      "Epoch 2659/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.7619 - val_loss: 139.1735\n",
      "Epoch 2660/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.4956 - val_loss: 155.3560\n",
      "Epoch 2661/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.1220 - val_loss: 161.0327\n",
      "Epoch 2662/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.5264 - val_loss: 172.4379\n",
      "Epoch 2663/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.7846 - val_loss: 139.9831\n",
      "Epoch 2664/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 189.0370 - val_loss: 194.8855\n",
      "Epoch 2665/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.6139 - val_loss: 183.7362\n",
      "Epoch 2666/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.7311 - val_loss: 139.1465\n",
      "Epoch 2667/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.4921 - val_loss: 157.4410\n",
      "Epoch 2668/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.5848 - val_loss: 168.3130\n",
      "Epoch 2669/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.0703 - val_loss: 134.7238\n",
      "Epoch 2670/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.1811 - val_loss: 137.1684\n",
      "Epoch 2671/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 142.6909 - val_loss: 176.9065\n",
      "Epoch 2672/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.8826 - val_loss: 158.2894\n",
      "Epoch 2673/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.7842 - val_loss: 135.1353\n",
      "Epoch 2674/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.4704 - val_loss: 143.5483\n",
      "Epoch 2675/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.3535 - val_loss: 128.6448\n",
      "Epoch 2676/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.9521 - val_loss: 148.4204\n",
      "Epoch 2677/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 178.7819 - val_loss: 146.2337\n",
      "Epoch 2678/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.9310 - val_loss: 194.2458\n",
      "Epoch 2679/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.8253 - val_loss: 149.3368\n",
      "Epoch 2680/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 164.7141 - val_loss: 137.6981\n",
      "Epoch 2681/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 135.6985 - val_loss: 142.3703\n",
      "Epoch 2682/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 138.2360 - val_loss: 151.7185\n",
      "Epoch 2683/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.2446 - val_loss: 137.0724\n",
      "Epoch 2684/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.6842 - val_loss: 137.7115\n",
      "Epoch 2685/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.8967 - val_loss: 145.0982\n",
      "Epoch 2686/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.3040 - val_loss: 157.9221\n",
      "Epoch 2687/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.8306 - val_loss: 141.2932\n",
      "Epoch 2688/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.9295 - val_loss: 267.5903\n",
      "Epoch 2689/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.5008 - val_loss: 144.9689\n",
      "Epoch 2690/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.8563 - val_loss: 142.6213\n",
      "Epoch 2691/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.0792 - val_loss: 132.3943\n",
      "Epoch 2692/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.5783 - val_loss: 161.2277\n",
      "Epoch 2693/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 197.2279 - val_loss: 131.2985\n",
      "Epoch 2694/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.4631 - val_loss: 182.3740\n",
      "Epoch 2695/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.3298 - val_loss: 223.9496\n",
      "Epoch 2696/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.2526 - val_loss: 158.1692\n",
      "Epoch 2697/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.2178 - val_loss: 133.6344\n",
      "Epoch 2698/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.7879 - val_loss: 130.0067\n",
      "Epoch 2699/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.2423 - val_loss: 151.2404\n",
      "Epoch 2700/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.4463 - val_loss: 136.6907\n",
      "Epoch 2701/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 162.8910 - val_loss: 215.3551\n",
      "Epoch 2702/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.6282 - val_loss: 175.9603\n",
      "Epoch 2703/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 154.4035 - val_loss: 133.4136\n",
      "Epoch 2704/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 134.1255 - val_loss: 138.4087\n",
      "Epoch 2705/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.5945 - val_loss: 153.5145\n",
      "Epoch 2706/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.0151 - val_loss: 141.0792\n",
      "Epoch 2707/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.3430 - val_loss: 139.3945\n",
      "Epoch 2708/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.6137 - val_loss: 130.4886\n",
      "Epoch 2709/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.1954 - val_loss: 189.2096\n",
      "Epoch 2710/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.8534 - val_loss: 166.5946\n",
      "Epoch 2711/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.1966 - val_loss: 185.9873\n",
      "Epoch 2712/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.6563 - val_loss: 143.4259\n",
      "Epoch 2713/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.2736 - val_loss: 142.7777\n",
      "Epoch 2714/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.9084 - val_loss: 135.5221\n",
      "Epoch 2715/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.3394 - val_loss: 151.3941\n",
      "Epoch 2716/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.5392 - val_loss: 143.2597\n",
      "Epoch 2717/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.7213 - val_loss: 134.9345\n",
      "Epoch 2718/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.9260 - val_loss: 128.5339\n",
      "Epoch 2719/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.2124 - val_loss: 154.2251\n",
      "Epoch 2720/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.8603 - val_loss: 133.2519\n",
      "Epoch 2721/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.5044 - val_loss: 151.9006\n",
      "Epoch 2722/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.9069 - val_loss: 153.7238\n",
      "Epoch 2723/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.4924 - val_loss: 129.7818\n",
      "Epoch 2724/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.9955 - val_loss: 155.1973\n",
      "Epoch 2725/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.2772 - val_loss: 148.7792\n",
      "Epoch 2726/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.5412 - val_loss: 146.2299\n",
      "Epoch 2727/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.6057 - val_loss: 209.6130\n",
      "Epoch 2728/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.6046 - val_loss: 162.5661\n",
      "Epoch 2729/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.9635 - val_loss: 151.7731\n",
      "Epoch 2730/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.6363 - val_loss: 131.2480\n",
      "Epoch 2731/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.6465 - val_loss: 135.2010\n",
      "Epoch 2732/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.5782 - val_loss: 131.5337\n",
      "Epoch 2733/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 148.7551 - val_loss: 161.1553\n",
      "Epoch 2734/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.3522 - val_loss: 132.9365\n",
      "Epoch 2735/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.4824 - val_loss: 165.3827\n",
      "Epoch 2736/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.8461 - val_loss: 143.2056\n",
      "Epoch 2737/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.1510 - val_loss: 137.6800\n",
      "Epoch 2738/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.6536 - val_loss: 156.9786\n",
      "Epoch 2739/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.5435 - val_loss: 144.3076\n",
      "Epoch 2740/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.3551 - val_loss: 133.1552\n",
      "Epoch 2741/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.7290 - val_loss: 153.4175\n",
      "Epoch 2742/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 138.9022 - val_loss: 143.5441\n",
      "Epoch 2743/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 152.0282 - val_loss: 140.5996\n",
      "Epoch 2744/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 136.6579 - val_loss: 161.5148\n",
      "Epoch 2745/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.8492 - val_loss: 136.7330\n",
      "Epoch 2746/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.2209 - val_loss: 164.9949\n",
      "Epoch 2747/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.8456 - val_loss: 134.3197\n",
      "Epoch 2748/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.6238 - val_loss: 136.0806\n",
      "Epoch 2749/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.2171 - val_loss: 173.1504\n",
      "Epoch 2750/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.3413 - val_loss: 177.7537\n",
      "Epoch 2751/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.6294 - val_loss: 149.7001\n",
      "Epoch 2752/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.9691 - val_loss: 131.7528\n",
      "Epoch 2753/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.2562 - val_loss: 141.6097\n",
      "Epoch 2754/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.8229 - val_loss: 169.2340\n",
      "Epoch 2755/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 125.8253 - val_loss: 128.4034\n",
      "Epoch 2756/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.6278 - val_loss: 133.9987\n",
      "Epoch 2757/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.3135 - val_loss: 128.3342\n",
      "Epoch 2758/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.6851 - val_loss: 182.0202\n",
      "Epoch 2759/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.1583 - val_loss: 167.3562\n",
      "Epoch 2760/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.2530 - val_loss: 172.3149\n",
      "Epoch 2761/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 126.9887 - val_loss: 132.8124\n",
      "Epoch 2762/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.1482 - val_loss: 138.8885\n",
      "Epoch 2763/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.5269 - val_loss: 138.9926\n",
      "Epoch 2764/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 134.2087 - val_loss: 175.8631\n",
      "Epoch 2765/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.3878 - val_loss: 225.8817\n",
      "Epoch 2766/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.7979 - val_loss: 137.2948\n",
      "Epoch 2767/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.9543 - val_loss: 140.6506\n",
      "Epoch 2768/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.4781 - val_loss: 136.4400\n",
      "Epoch 2769/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.0844 - val_loss: 181.3221\n",
      "Epoch 2770/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 128.0447 - val_loss: 181.0084\n",
      "Epoch 2771/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.8667 - val_loss: 135.9152\n",
      "Epoch 2772/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.5857 - val_loss: 137.0949\n",
      "Epoch 2773/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.9434 - val_loss: 219.1188\n",
      "Epoch 2774/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.5895 - val_loss: 195.7753\n",
      "Epoch 2775/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.5278 - val_loss: 141.6256\n",
      "Epoch 2776/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.1997 - val_loss: 242.4750\n",
      "Epoch 2777/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 133.8023 - val_loss: 134.2780\n",
      "Epoch 2778/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.5466 - val_loss: 129.5556\n",
      "Epoch 2779/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.9611 - val_loss: 143.3474\n",
      "Epoch 2780/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.6117 - val_loss: 132.3261\n",
      "Epoch 2781/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.6799 - val_loss: 134.3011\n",
      "Epoch 2782/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.2609 - val_loss: 147.0084\n",
      "Epoch 2783/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.6117 - val_loss: 147.9426\n",
      "Epoch 2784/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.5036 - val_loss: 158.3763\n",
      "Epoch 2785/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.3538 - val_loss: 136.0140\n",
      "Epoch 2786/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 240.7931 - val_loss: 143.4731\n",
      "Epoch 2787/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 203.0731 - val_loss: 173.3185\n",
      "Epoch 2788/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 156.6956 - val_loss: 159.9002\n",
      "Epoch 2789/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.7627 - val_loss: 152.6720\n",
      "Epoch 2790/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.2125 - val_loss: 135.0298\n",
      "Epoch 2791/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.0509 - val_loss: 151.3633\n",
      "Epoch 2792/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.1208 - val_loss: 132.0702\n",
      "Epoch 2793/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.5254 - val_loss: 185.9226\n",
      "Epoch 2794/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.7548 - val_loss: 222.2562\n",
      "Epoch 2795/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.1305 - val_loss: 137.8925\n",
      "Epoch 2796/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.8959 - val_loss: 140.5692\n",
      "Epoch 2797/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.2339 - val_loss: 165.9531\n",
      "Epoch 2798/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 126.3088 - val_loss: 135.7637\n",
      "Epoch 2799/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.5269 - val_loss: 136.3406\n",
      "Epoch 2800/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.2876 - val_loss: 145.0792\n",
      "Epoch 2801/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 134.6154 - val_loss: 141.7659\n",
      "Epoch 2802/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 191.4757 - val_loss: 143.3190\n",
      "Epoch 2803/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.2272 - val_loss: 167.5665\n",
      "Epoch 2804/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 132.2277 - val_loss: 125.5085\n",
      "Epoch 2805/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 126.9393 - val_loss: 151.6375\n",
      "Epoch 2806/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 175.2805 - val_loss: 136.8811\n",
      "Epoch 2807/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.1797 - val_loss: 179.7930\n",
      "Epoch 2808/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 127.9430 - val_loss: 142.2061\n",
      "Epoch 2809/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.3149 - val_loss: 147.5973\n",
      "Epoch 2810/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.4361 - val_loss: 167.9828\n",
      "Epoch 2811/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.7547 - val_loss: 145.9760\n",
      "Epoch 2812/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.1837 - val_loss: 133.6785\n",
      "Epoch 2813/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.2205 - val_loss: 150.5672\n",
      "Epoch 2814/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.2403 - val_loss: 147.7213\n",
      "Epoch 2815/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.0791 - val_loss: 134.0051\n",
      "Epoch 2816/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.4434 - val_loss: 137.7586\n",
      "Epoch 2817/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.2978 - val_loss: 139.9122\n",
      "Epoch 2818/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.1172 - val_loss: 164.3096\n",
      "Epoch 2819/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.3346 - val_loss: 144.2879\n",
      "Epoch 2820/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.4488 - val_loss: 203.1632\n",
      "Epoch 2821/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.1138 - val_loss: 155.4212\n",
      "Epoch 2822/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.9552 - val_loss: 140.7484\n",
      "Epoch 2823/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.7450 - val_loss: 199.1255\n",
      "Epoch 2824/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.6888 - val_loss: 135.7490\n",
      "Epoch 2825/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.0990 - val_loss: 145.0373\n",
      "Epoch 2826/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.0924 - val_loss: 139.7678\n",
      "Epoch 2827/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.1650 - val_loss: 144.5492\n",
      "Epoch 2828/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.9683 - val_loss: 142.2897\n",
      "Epoch 2829/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.6076 - val_loss: 136.7387\n",
      "Epoch 2830/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.3266 - val_loss: 158.9138\n",
      "Epoch 2831/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.2388 - val_loss: 151.2367\n",
      "Epoch 2832/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.7587 - val_loss: 166.5474\n",
      "Epoch 2833/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.3163 - val_loss: 135.8511\n",
      "Epoch 2834/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.4855 - val_loss: 151.6827\n",
      "Epoch 2835/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.2172 - val_loss: 159.4868\n",
      "Epoch 2836/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.7637 - val_loss: 141.4416\n",
      "Epoch 2837/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 129.5240 - val_loss: 147.1693\n",
      "Epoch 2838/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 128.1749 - val_loss: 180.1714\n",
      "Epoch 2839/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.5956 - val_loss: 138.0066\n",
      "Epoch 2840/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.3903 - val_loss: 134.8227\n",
      "Epoch 2841/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.3662 - val_loss: 135.0997\n",
      "Epoch 2842/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.5058 - val_loss: 173.7602\n",
      "Epoch 2843/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.8049 - val_loss: 144.4986\n",
      "Epoch 2844/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.4298 - val_loss: 146.4744\n",
      "Epoch 2845/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.8118 - val_loss: 131.3339\n",
      "Epoch 2846/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.5546 - val_loss: 166.7403\n",
      "Epoch 2847/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.2337 - val_loss: 143.9984\n",
      "Epoch 2848/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.7526 - val_loss: 198.3708\n",
      "Epoch 2849/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.0419 - val_loss: 148.3785\n",
      "Epoch 2850/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.1870 - val_loss: 274.3133\n",
      "Epoch 2851/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.5823 - val_loss: 139.7149\n",
      "Epoch 2852/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.4208 - val_loss: 167.9684\n",
      "Epoch 2853/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.1833 - val_loss: 143.6073\n",
      "Epoch 2854/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.4331 - val_loss: 131.6910\n",
      "Epoch 2855/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.3604 - val_loss: 135.4648\n",
      "Epoch 2856/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.6423 - val_loss: 136.1556\n",
      "Epoch 2857/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.4244 - val_loss: 133.4452\n",
      "Epoch 2858/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.9054 - val_loss: 143.2925\n",
      "Epoch 2859/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.8511 - val_loss: 167.1168\n",
      "Epoch 2860/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.0251 - val_loss: 204.9797\n",
      "Epoch 2861/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.7502 - val_loss: 280.7637\n",
      "Epoch 2862/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.4254 - val_loss: 144.8728\n",
      "Epoch 2863/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.9964 - val_loss: 141.7772\n",
      "Epoch 2864/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.3366 - val_loss: 135.4923\n",
      "Epoch 2865/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.0835 - val_loss: 161.6300\n",
      "Epoch 2866/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 126.4464 - val_loss: 139.3698\n",
      "Epoch 2867/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 147.6119 - val_loss: 150.4005\n",
      "Epoch 2868/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 160.5331 - val_loss: 244.4306\n",
      "Epoch 2869/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.6937 - val_loss: 165.6052\n",
      "Epoch 2870/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.3990 - val_loss: 168.5188\n",
      "Epoch 2871/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 185.5788 - val_loss: 135.5769\n",
      "Epoch 2872/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.3195 - val_loss: 204.6934\n",
      "Epoch 2873/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.1245 - val_loss: 133.6135\n",
      "Epoch 2874/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.4327 - val_loss: 168.3483\n",
      "Epoch 2875/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.0034 - val_loss: 172.6876\n",
      "Epoch 2876/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.5305 - val_loss: 144.2778\n",
      "Epoch 2877/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.8519 - val_loss: 174.3313\n",
      "Epoch 2878/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.3460 - val_loss: 147.6366\n",
      "Epoch 2879/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.8935 - val_loss: 146.2996\n",
      "Epoch 2880/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 135.9244 - val_loss: 130.0364\n",
      "Epoch 2881/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.9917 - val_loss: 128.0147\n",
      "Epoch 2882/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.2357 - val_loss: 138.5504\n",
      "Epoch 2883/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.7370 - val_loss: 212.1395\n",
      "Epoch 2884/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.0852 - val_loss: 160.5370\n",
      "Epoch 2885/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.0410 - val_loss: 128.9474\n",
      "Epoch 2886/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 123.2212 - val_loss: 147.8657\n",
      "Epoch 2887/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.7627 - val_loss: 142.1618\n",
      "Epoch 2888/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.7684 - val_loss: 157.0343\n",
      "Epoch 2889/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.5074 - val_loss: 161.0761\n",
      "Epoch 2890/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.0974 - val_loss: 196.2521\n",
      "Epoch 2891/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.9733 - val_loss: 141.3996\n",
      "Epoch 2892/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.8394 - val_loss: 132.5945\n",
      "Epoch 2893/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.0890 - val_loss: 170.1057\n",
      "Epoch 2894/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.0941 - val_loss: 147.2506\n",
      "Epoch 2895/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.3047 - val_loss: 139.4697\n",
      "Epoch 2896/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.6646 - val_loss: 149.9960\n",
      "Epoch 2897/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.0041 - val_loss: 132.9824\n",
      "Epoch 2898/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.8852 - val_loss: 142.1473\n",
      "Epoch 2899/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.3036 - val_loss: 150.9498\n",
      "Epoch 2900/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.5044 - val_loss: 190.6503\n",
      "Epoch 2901/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.7042 - val_loss: 129.8981\n",
      "Epoch 2902/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 132.4141 - val_loss: 179.7159\n",
      "Epoch 2903/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.0152 - val_loss: 287.7228\n",
      "Epoch 2904/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 184.5115 - val_loss: 152.2987\n",
      "Epoch 2905/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.0994 - val_loss: 143.8010\n",
      "Epoch 2906/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.1377 - val_loss: 135.2537\n",
      "Epoch 2907/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.3279 - val_loss: 193.4804\n",
      "Epoch 2908/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.6755 - val_loss: 167.8025\n",
      "Epoch 2909/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.6757 - val_loss: 145.9106\n",
      "Epoch 2910/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.9188 - val_loss: 175.8681\n",
      "Epoch 2911/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 131.0146 - val_loss: 243.1090\n",
      "Epoch 2912/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.1746 - val_loss: 145.5519\n",
      "Epoch 2913/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.4816 - val_loss: 144.6393\n",
      "Epoch 2914/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.6532 - val_loss: 139.1746\n",
      "Epoch 2915/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.0983 - val_loss: 135.2042\n",
      "Epoch 2916/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.1580 - val_loss: 141.2198\n",
      "Epoch 2917/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.3542 - val_loss: 143.3268\n",
      "Epoch 2918/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.6258 - val_loss: 134.9816\n",
      "Epoch 2919/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.4876 - val_loss: 138.3962\n",
      "Epoch 2920/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.0235 - val_loss: 143.9080\n",
      "Epoch 2921/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.7363 - val_loss: 422.0631\n",
      "Epoch 2922/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 181.4394 - val_loss: 137.9166\n",
      "Epoch 2923/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.3865 - val_loss: 223.5784\n",
      "Epoch 2924/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.5480 - val_loss: 128.5017\n",
      "Epoch 2925/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.0003 - val_loss: 139.9612\n",
      "Epoch 2926/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.3407 - val_loss: 135.1831\n",
      "Epoch 2927/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.6998 - val_loss: 134.6629\n",
      "Epoch 2928/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.2267 - val_loss: 225.1752\n",
      "Epoch 2929/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 142.1747 - val_loss: 146.9919\n",
      "Epoch 2930/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 140.2203 - val_loss: 153.6923\n",
      "Epoch 2931/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 134.4831 - val_loss: 153.9790\n",
      "Epoch 2932/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.9742 - val_loss: 136.3564\n",
      "Epoch 2933/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.4020 - val_loss: 143.2841\n",
      "Epoch 2934/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.6065 - val_loss: 176.1975\n",
      "Epoch 2935/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.4017 - val_loss: 160.0195\n",
      "Epoch 2936/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.2809 - val_loss: 164.5861\n",
      "Epoch 2937/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 153.2504 - val_loss: 429.0568\n",
      "Epoch 2938/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.1474 - val_loss: 144.3396\n",
      "Epoch 2939/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.6617 - val_loss: 176.3253\n",
      "Epoch 2940/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.5016 - val_loss: 139.0582\n",
      "Epoch 2941/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.3860 - val_loss: 178.5701\n",
      "Epoch 2942/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.5780 - val_loss: 142.5720\n",
      "Epoch 2943/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 155.7951 - val_loss: 180.6316\n",
      "Epoch 2944/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.9776 - val_loss: 134.9918\n",
      "Epoch 2945/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 138.7221 - val_loss: 142.3440\n",
      "Epoch 2946/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.2983 - val_loss: 189.1853\n",
      "Epoch 2947/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 130.3639 - val_loss: 145.5107\n",
      "Epoch 2948/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 156.2710 - val_loss: 143.3411\n",
      "Epoch 2949/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.2012 - val_loss: 180.8799\n",
      "Epoch 2950/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 128.1198 - val_loss: 150.9978\n",
      "Epoch 2951/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.2913 - val_loss: 162.5309\n",
      "Epoch 2952/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.9743 - val_loss: 149.8549\n",
      "Epoch 2953/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.9013 - val_loss: 142.9313\n",
      "Epoch 2954/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.3120 - val_loss: 163.9306\n",
      "Epoch 2955/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.8328 - val_loss: 133.7975\n",
      "Epoch 2956/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.0765 - val_loss: 135.2461\n",
      "Epoch 2957/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.8004 - val_loss: 160.6332\n",
      "Epoch 2958/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.6676 - val_loss: 144.1381\n",
      "Epoch 2959/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.5891 - val_loss: 135.5195\n",
      "Epoch 2960/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.7980 - val_loss: 143.9115\n",
      "Epoch 2961/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.2128 - val_loss: 142.1780\n",
      "Epoch 2962/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.0067 - val_loss: 137.1404\n",
      "Epoch 2963/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.1081 - val_loss: 134.5817\n",
      "Epoch 2964/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.1556 - val_loss: 141.5152\n",
      "Epoch 2965/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.7104 - val_loss: 204.0193\n",
      "Epoch 2966/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.3523 - val_loss: 162.7688\n",
      "Epoch 2967/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.2403 - val_loss: 138.7917\n",
      "Epoch 2968/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.4282 - val_loss: 132.3822\n",
      "Epoch 2969/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.8684 - val_loss: 158.4427\n",
      "Epoch 2970/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.4151 - val_loss: 217.4323\n",
      "Epoch 2971/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 142.0741 - val_loss: 136.5390\n",
      "Epoch 2972/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.2953 - val_loss: 140.8524\n",
      "Epoch 2973/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.7116 - val_loss: 138.3398\n",
      "Epoch 2974/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.9861 - val_loss: 136.0384\n",
      "Epoch 2975/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.8171 - val_loss: 169.3754\n",
      "Epoch 2976/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.0239 - val_loss: 150.7663\n",
      "Epoch 2977/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.1363 - val_loss: 138.0553\n",
      "Epoch 2978/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 130.9700 - val_loss: 156.1080\n",
      "Epoch 2979/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.5080 - val_loss: 151.4433\n",
      "Epoch 2980/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.0690 - val_loss: 136.6739\n",
      "Epoch 2981/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.2675 - val_loss: 273.0974\n",
      "Epoch 2982/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.6612 - val_loss: 135.4271\n",
      "Epoch 2983/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.2572 - val_loss: 182.9902\n",
      "Epoch 2984/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.9729 - val_loss: 178.7014\n",
      "Epoch 2985/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.5961 - val_loss: 136.7263\n",
      "Epoch 2986/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 129.1075 - val_loss: 134.0733\n",
      "Epoch 2987/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.2974 - val_loss: 129.8703\n",
      "Epoch 2988/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.0186 - val_loss: 136.0013\n",
      "Epoch 2989/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 128.7741 - val_loss: 151.3305\n",
      "Epoch 2990/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 146.6219 - val_loss: 168.3562\n",
      "Epoch 2991/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 136.2175 - val_loss: 143.0951\n",
      "Epoch 2992/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 141.9714 - val_loss: 128.2014\n",
      "Epoch 2993/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.6354 - val_loss: 125.7696\n",
      "Epoch 2994/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.3575 - val_loss: 157.6067\n",
      "Epoch 2995/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.8402 - val_loss: 142.3547\n",
      "Epoch 2996/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.5397 - val_loss: 150.6776\n",
      "Epoch 2997/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.0576 - val_loss: 147.4054\n",
      "Epoch 2998/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 156.0385 - val_loss: 157.3762\n",
      "Epoch 2999/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.3008 - val_loss: 135.2289\n",
      "Epoch 3000/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.3232 - val_loss: 167.7159\n",
      "Epoch 3001/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.7852 - val_loss: 164.2750\n",
      "Epoch 3002/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.5384 - val_loss: 141.3111\n",
      "Epoch 3003/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.5102 - val_loss: 151.8097\n",
      "Epoch 3004/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.2685 - val_loss: 150.2708\n",
      "Epoch 3005/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.1536 - val_loss: 143.2526\n",
      "Epoch 3006/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.7614 - val_loss: 148.0842\n",
      "Epoch 3007/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.0078 - val_loss: 180.6838\n",
      "Epoch 3008/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 129.2761 - val_loss: 158.6702\n",
      "Epoch 3009/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.8710 - val_loss: 130.0997\n",
      "Epoch 3010/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.2353 - val_loss: 152.8114\n",
      "Epoch 3011/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.4844 - val_loss: 141.5589\n",
      "Epoch 3012/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.7907 - val_loss: 132.5625\n",
      "Epoch 3013/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.0143 - val_loss: 132.3908\n",
      "Epoch 3014/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.9648 - val_loss: 166.6112\n",
      "Epoch 3015/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.6220 - val_loss: 143.7229\n",
      "Epoch 3016/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 125.8017 - val_loss: 188.5872\n",
      "Epoch 3017/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.5611 - val_loss: 145.4174\n",
      "Epoch 3018/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.2280 - val_loss: 129.4564\n",
      "Epoch 3019/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.8630 - val_loss: 145.2697\n",
      "Epoch 3020/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.9762 - val_loss: 151.2212\n",
      "Epoch 3021/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 132.0445 - val_loss: 161.2585\n",
      "Epoch 3022/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.4330 - val_loss: 134.6553\n",
      "Epoch 3023/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.5682 - val_loss: 144.4559\n",
      "Epoch 3024/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.9858 - val_loss: 136.4053\n",
      "Epoch 3025/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.8840 - val_loss: 143.5106\n",
      "Epoch 3026/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.6244 - val_loss: 145.2661\n",
      "Epoch 3027/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.6570 - val_loss: 145.3400\n",
      "Epoch 3028/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.6791 - val_loss: 131.2641\n",
      "Epoch 3029/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 171.6626 - val_loss: 156.3543\n",
      "Epoch 3030/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.7875 - val_loss: 145.8248\n",
      "Epoch 3031/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.9704 - val_loss: 132.9278\n",
      "Epoch 3032/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.5880 - val_loss: 146.5662\n",
      "Epoch 3033/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.0008 - val_loss: 145.0220\n",
      "Epoch 3034/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.8690 - val_loss: 153.9320\n",
      "Epoch 3035/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.2720 - val_loss: 143.5992\n",
      "Epoch 3036/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.8176 - val_loss: 148.2366\n",
      "Epoch 3037/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.2285 - val_loss: 139.1781\n",
      "Epoch 3038/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.5966 - val_loss: 160.0301\n",
      "Epoch 3039/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.0466 - val_loss: 138.7876\n",
      "Epoch 3040/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.3384 - val_loss: 139.3406\n",
      "Epoch 3041/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.0148 - val_loss: 203.8965\n",
      "Epoch 3042/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.9794 - val_loss: 133.1148\n",
      "Epoch 3043/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 129.7589 - val_loss: 132.1670\n",
      "Epoch 3044/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.7456 - val_loss: 139.4318\n",
      "Epoch 3045/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.1710 - val_loss: 135.7532\n",
      "Epoch 3046/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.3552 - val_loss: 132.9536\n",
      "Epoch 3047/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.7163 - val_loss: 164.6879\n",
      "Epoch 3048/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.3399 - val_loss: 159.8068\n",
      "Epoch 3049/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.1321 - val_loss: 159.5066\n",
      "Epoch 3050/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.8101 - val_loss: 157.8805\n",
      "Epoch 3051/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.3286 - val_loss: 153.0076\n",
      "Epoch 3052/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 129.8284 - val_loss: 137.3918\n",
      "Epoch 3053/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 148.2236 - val_loss: 134.3412\n",
      "Epoch 3054/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 134.2878 - val_loss: 134.4197\n",
      "Epoch 3055/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 136.3150 - val_loss: 218.9698\n",
      "Epoch 3056/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.2525 - val_loss: 136.8513\n",
      "Epoch 3057/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.4808 - val_loss: 136.6748\n",
      "Epoch 3058/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.7457 - val_loss: 133.4531\n",
      "Epoch 3059/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.7924 - val_loss: 128.8771\n",
      "Epoch 3060/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.9106 - val_loss: 134.6053\n",
      "Epoch 3061/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.7832 - val_loss: 144.6628\n",
      "Epoch 3062/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.1488 - val_loss: 155.6389\n",
      "Epoch 3063/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.0982 - val_loss: 143.1692\n",
      "Epoch 3064/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.8369 - val_loss: 137.9768\n",
      "Epoch 3065/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.2704 - val_loss: 151.7941\n",
      "Epoch 3066/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.5778 - val_loss: 141.5757\n",
      "Epoch 3067/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.2835 - val_loss: 227.1065\n",
      "Epoch 3068/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.3543 - val_loss: 137.3133\n",
      "Epoch 3069/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.0667 - val_loss: 146.5861\n",
      "Epoch 3070/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.4919 - val_loss: 145.4986\n",
      "Epoch 3071/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 134.7041 - val_loss: 138.6853\n",
      "Epoch 3072/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 127.8036 - val_loss: 153.4361\n",
      "Epoch 3073/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.7502 - val_loss: 184.5188\n",
      "Epoch 3074/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.8986 - val_loss: 145.3369\n",
      "Epoch 3075/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.2143 - val_loss: 274.1721\n",
      "Epoch 3076/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.7511 - val_loss: 141.5296\n",
      "Epoch 3077/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 206.1603 - val_loss: 190.3388\n",
      "Epoch 3078/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.4301 - val_loss: 135.0589\n",
      "Epoch 3079/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 127.2946 - val_loss: 139.7027\n",
      "Epoch 3080/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.0660 - val_loss: 148.7921\n",
      "Epoch 3081/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.6191 - val_loss: 145.4005\n",
      "Epoch 3082/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.9011 - val_loss: 135.5078\n",
      "Epoch 3083/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.5221 - val_loss: 131.2687\n",
      "Epoch 3084/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.5838 - val_loss: 251.7964\n",
      "Epoch 3085/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.2551 - val_loss: 142.4346\n",
      "Epoch 3086/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 128.1364 - val_loss: 135.4998\n",
      "Epoch 3087/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.3574 - val_loss: 148.9850\n",
      "Epoch 3088/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.8502 - val_loss: 141.9631\n",
      "Epoch 3089/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 126.9918 - val_loss: 133.2179\n",
      "Epoch 3090/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.9337 - val_loss: 145.7579\n",
      "Epoch 3091/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.4608 - val_loss: 135.1448\n",
      "Epoch 3092/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 131.2223 - val_loss: 139.3825\n",
      "Epoch 3093/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.2746 - val_loss: 166.9653\n",
      "Epoch 3094/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.5597 - val_loss: 141.1426\n",
      "Epoch 3095/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.1486 - val_loss: 135.0122\n",
      "Epoch 3096/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.3032 - val_loss: 137.1167\n",
      "Epoch 3097/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.0600 - val_loss: 156.8473\n",
      "Epoch 3098/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.5275 - val_loss: 144.8274\n",
      "Epoch 3099/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.3744 - val_loss: 151.8612\n",
      "Epoch 3100/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.5311 - val_loss: 134.4198\n",
      "Epoch 3101/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.5849 - val_loss: 162.1519\n",
      "Epoch 3102/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.2547 - val_loss: 225.6222\n",
      "Epoch 3103/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.7220 - val_loss: 148.0951\n",
      "Epoch 3104/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 193.9922 - val_loss: 156.7621\n",
      "Epoch 3105/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.5547 - val_loss: 131.9807\n",
      "Epoch 3106/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 131.7635 - val_loss: 138.5013\n",
      "Epoch 3107/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.2198 - val_loss: 140.0461\n",
      "Epoch 3108/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.7889 - val_loss: 134.1393\n",
      "Epoch 3109/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.2480 - val_loss: 164.3044\n",
      "Epoch 3110/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.2337 - val_loss: 131.3190\n",
      "Epoch 3111/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.1903 - val_loss: 194.3492\n",
      "Epoch 3112/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 144.9451 - val_loss: 141.8052\n",
      "Epoch 3113/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.8916 - val_loss: 148.8660\n",
      "Epoch 3114/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.0178 - val_loss: 140.7845\n",
      "Epoch 3115/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 128.3830 - val_loss: 150.8139\n",
      "Epoch 3116/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 137.4476 - val_loss: 141.0286\n",
      "Epoch 3117/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.4880 - val_loss: 144.2740\n",
      "Epoch 3118/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.8459 - val_loss: 133.7561\n",
      "Epoch 3119/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.0865 - val_loss: 138.2515\n",
      "Epoch 3120/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.8227 - val_loss: 146.8850\n",
      "Epoch 3121/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 134.4889 - val_loss: 151.7867\n",
      "Epoch 3122/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.9444 - val_loss: 136.6475\n",
      "Epoch 3123/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.4394 - val_loss: 152.6883\n",
      "Epoch 3124/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.4475 - val_loss: 154.5254\n",
      "Epoch 3125/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.4810 - val_loss: 138.2051\n",
      "Epoch 3126/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.7015 - val_loss: 218.2646\n",
      "Epoch 3127/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.4160 - val_loss: 193.4720\n",
      "Epoch 3128/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.1717 - val_loss: 136.9312\n",
      "Epoch 3129/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.0989 - val_loss: 143.4738\n",
      "Epoch 3130/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.3216 - val_loss: 154.4798\n",
      "Epoch 3131/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.6605 - val_loss: 135.3377\n",
      "Epoch 3132/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.1079 - val_loss: 134.0947\n",
      "Epoch 3133/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 128.3001 - val_loss: 130.0966\n",
      "Epoch 3134/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.7402 - val_loss: 142.5887\n",
      "Epoch 3135/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.7333 - val_loss: 138.6192\n",
      "Epoch 3136/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 172.0770 - val_loss: 155.1549\n",
      "Epoch 3137/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.4230 - val_loss: 171.9240\n",
      "Epoch 3138/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 128.8098 - val_loss: 163.3188\n",
      "Epoch 3139/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.9179 - val_loss: 145.7670\n",
      "Epoch 3140/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 125.6869 - val_loss: 130.4786\n",
      "Epoch 3141/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 128.0376 - val_loss: 136.7401\n",
      "Epoch 3142/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.5242 - val_loss: 156.6571\n",
      "Epoch 3143/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.4543 - val_loss: 177.8274\n",
      "Epoch 3144/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.7431 - val_loss: 217.8184\n",
      "Epoch 3145/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.5338 - val_loss: 153.0234\n",
      "Epoch 3146/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.2810 - val_loss: 137.0490\n",
      "Epoch 3147/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.2703 - val_loss: 156.1331\n",
      "Epoch 3148/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.5462 - val_loss: 140.5095\n",
      "Epoch 3149/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.3855 - val_loss: 143.2028\n",
      "Epoch 3150/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.7262 - val_loss: 206.8666\n",
      "Epoch 3151/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.8063 - val_loss: 201.7503\n",
      "Epoch 3152/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.9049 - val_loss: 135.8527\n",
      "Epoch 3153/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.6097 - val_loss: 157.6820\n",
      "Epoch 3154/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.9561 - val_loss: 176.0873\n",
      "Epoch 3155/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.3168 - val_loss: 182.1579\n",
      "Epoch 3156/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 154.834 - 1s 63us/step - loss: 151.7903 - val_loss: 138.4572\n",
      "Epoch 3157/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.4677 - val_loss: 165.4859\n",
      "Epoch 3158/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 146.6661 - val_loss: 136.7575\n",
      "Epoch 3159/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.9619 - val_loss: 152.8951\n",
      "Epoch 3160/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.9371 - val_loss: 136.3666\n",
      "Epoch 3161/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.0273 - val_loss: 148.8212\n",
      "Epoch 3162/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.2770 - val_loss: 151.0824\n",
      "Epoch 3163/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.0453 - val_loss: 164.4511\n",
      "Epoch 3164/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 203.5252 - val_loss: 145.9660\n",
      "Epoch 3165/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.9174 - val_loss: 170.9956\n",
      "Epoch 3166/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 131.7248 - val_loss: 133.5049\n",
      "Epoch 3167/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.3153 - val_loss: 131.7781\n",
      "Epoch 3168/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.1218 - val_loss: 145.3177\n",
      "Epoch 3169/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.4817 - val_loss: 142.6965\n",
      "Epoch 3170/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.4888 - val_loss: 153.8754\n",
      "Epoch 3171/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.8617 - val_loss: 146.2017\n",
      "Epoch 3172/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.8981 - val_loss: 138.1871\n",
      "Epoch 3173/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.2503 - val_loss: 150.4353\n",
      "Epoch 3174/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.2826 - val_loss: 135.1668\n",
      "Epoch 3175/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 191.1123 - val_loss: 158.3002\n",
      "Epoch 3176/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.1105 - val_loss: 143.9077\n",
      "Epoch 3177/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.0596 - val_loss: 148.7953\n",
      "Epoch 3178/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.0922 - val_loss: 152.9197\n",
      "Epoch 3179/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.7341 - val_loss: 226.0250\n",
      "Epoch 3180/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.4432 - val_loss: 155.5030\n",
      "Epoch 3181/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.8902 - val_loss: 142.9578\n",
      "Epoch 3182/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.4509 - val_loss: 135.8277\n",
      "Epoch 3183/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.3715 - val_loss: 141.0439\n",
      "Epoch 3184/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 131.0201 - val_loss: 159.8851\n",
      "Epoch 3185/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 139.2896 - val_loss: 127.1543\n",
      "Epoch 3186/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 154.3438 - val_loss: 137.3199\n",
      "Epoch 3187/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 143.4837 - val_loss: 135.0459\n",
      "Epoch 3188/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 135.9069 - val_loss: 138.5584\n",
      "Epoch 3189/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.7123 - val_loss: 147.6916\n",
      "Epoch 3190/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.3814 - val_loss: 136.9343\n",
      "Epoch 3191/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.5509 - val_loss: 182.5423\n",
      "Epoch 3192/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 178.8880 - val_loss: 167.4527\n",
      "Epoch 3193/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.2913 - val_loss: 141.3288\n",
      "Epoch 3194/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.1356 - val_loss: 152.4466\n",
      "Epoch 3195/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.7827 - val_loss: 134.4067\n",
      "Epoch 3196/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.1225 - val_loss: 130.6107\n",
      "Epoch 3197/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.4059 - val_loss: 138.8265\n",
      "Epoch 3198/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.6198 - val_loss: 133.5664\n",
      "Epoch 3199/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.8830 - val_loss: 234.5010\n",
      "Epoch 3200/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.9260 - val_loss: 157.9142\n",
      "Epoch 3201/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 136.4681 - val_loss: 137.8394\n",
      "Epoch 3202/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.9659 - val_loss: 140.8212\n",
      "Epoch 3203/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.5549 - val_loss: 161.9089\n",
      "Epoch 3204/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.8681 - val_loss: 136.9216\n",
      "Epoch 3205/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 132.4620 - val_loss: 140.4081\n",
      "Epoch 3206/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.6918 - val_loss: 155.4899\n",
      "Epoch 3207/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.0704 - val_loss: 144.3571\n",
      "Epoch 3208/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.7903 - val_loss: 181.7844\n",
      "Epoch 3209/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 131.552 - 0s 57us/step - loss: 132.5959 - val_loss: 158.8718\n",
      "Epoch 3210/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.3748 - val_loss: 140.5351\n",
      "Epoch 3211/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.4264 - val_loss: 163.3367\n",
      "Epoch 3212/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 142.4209 - val_loss: 169.5197\n",
      "Epoch 3213/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.5729 - val_loss: 139.4379\n",
      "Epoch 3214/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.8988 - val_loss: 157.2531\n",
      "Epoch 3215/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 165.7898 - val_loss: 143.3114\n",
      "Epoch 3216/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.5995 - val_loss: 140.1643\n",
      "Epoch 3217/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.1721 - val_loss: 152.0369\n",
      "Epoch 3218/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.2237 - val_loss: 141.0776\n",
      "Epoch 3219/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.2715 - val_loss: 171.2179\n",
      "Epoch 3220/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.3168 - val_loss: 147.6793\n",
      "Epoch 3221/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.2407 - val_loss: 140.3863\n",
      "Epoch 3222/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.2002 - val_loss: 134.9065\n",
      "Epoch 3223/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.9657 - val_loss: 161.6743\n",
      "Epoch 3224/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.9883 - val_loss: 151.2845\n",
      "Epoch 3225/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.5174 - val_loss: 147.2021\n",
      "Epoch 3226/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 128.4825 - val_loss: 130.9087\n",
      "Epoch 3227/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.5955 - val_loss: 145.8142\n",
      "Epoch 3228/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.8713 - val_loss: 191.6817\n",
      "Epoch 3229/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.9560 - val_loss: 156.9752\n",
      "Epoch 3230/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.0060 - val_loss: 137.2588\n",
      "Epoch 3231/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.5235 - val_loss: 143.2301\n",
      "Epoch 3232/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.2324 - val_loss: 139.1493\n",
      "Epoch 3233/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.1877 - val_loss: 163.7706\n",
      "Epoch 3234/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.4067 - val_loss: 210.9702\n",
      "Epoch 3235/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 189.0829 - val_loss: 140.5111\n",
      "Epoch 3236/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.3142 - val_loss: 132.9864\n",
      "Epoch 3237/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.3196 - val_loss: 139.7518\n",
      "Epoch 3238/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.6397 - val_loss: 131.6238\n",
      "Epoch 3239/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 82us/step - loss: 131.7471 - val_loss: 141.4329\n",
      "Epoch 3240/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 137.5613 - val_loss: 132.8574\n",
      "Epoch 3241/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 128.1840 - val_loss: 140.5190\n",
      "Epoch 3242/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.3538 - val_loss: 159.0300\n",
      "Epoch 3243/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.1747 - val_loss: 167.8195\n",
      "Epoch 3244/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.9627 - val_loss: 136.7593\n",
      "Epoch 3245/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.3895 - val_loss: 184.3637\n",
      "Epoch 3246/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.3071 - val_loss: 192.3186\n",
      "Epoch 3247/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.8716 - val_loss: 134.9855\n",
      "Epoch 3248/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.7978 - val_loss: 131.1504\n",
      "Epoch 3249/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.0978 - val_loss: 218.1423\n",
      "Epoch 3250/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.2950 - val_loss: 138.8335\n",
      "Epoch 3251/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.7524 - val_loss: 148.1014\n",
      "Epoch 3252/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.9863 - val_loss: 331.5691\n",
      "Epoch 3253/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.2672 - val_loss: 184.4280\n",
      "Epoch 3254/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.4583 - val_loss: 137.6857\n",
      "Epoch 3255/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.6971 - val_loss: 158.1465\n",
      "Epoch 3256/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.2548 - val_loss: 135.0990\n",
      "Epoch 3257/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.3192 - val_loss: 156.3981\n",
      "Epoch 3258/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.5078 - val_loss: 156.5127\n",
      "Epoch 3259/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.6968 - val_loss: 139.1053\n",
      "Epoch 3260/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 128.2792 - val_loss: 176.3176\n",
      "Epoch 3261/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.8259 - val_loss: 152.2503\n",
      "Epoch 3262/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.0051 - val_loss: 215.7312\n",
      "Epoch 3263/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.9807 - val_loss: 147.2523\n",
      "Epoch 3264/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.3955 - val_loss: 233.3564\n",
      "Epoch 3265/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.6966 - val_loss: 128.8561\n",
      "Epoch 3266/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.3335 - val_loss: 136.6532\n",
      "Epoch 3267/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.3864 - val_loss: 141.2269\n",
      "Epoch 3268/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.3252 - val_loss: 141.4266\n",
      "Epoch 3269/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.7993 - val_loss: 148.1775\n",
      "Epoch 3270/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.3671 - val_loss: 133.6603\n",
      "Epoch 3271/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.4335 - val_loss: 159.3988\n",
      "Epoch 3272/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.8919 - val_loss: 170.7484\n",
      "Epoch 3273/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.6534 - val_loss: 185.5849\n",
      "Epoch 3274/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.1071 - val_loss: 135.4925\n",
      "Epoch 3275/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.8823 - val_loss: 153.2260\n",
      "Epoch 3276/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.0004 - val_loss: 167.5467\n",
      "Epoch 3277/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.1246 - val_loss: 136.4573\n",
      "Epoch 3278/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.5867 - val_loss: 137.9403\n",
      "Epoch 3279/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 131.8157 - val_loss: 151.0516\n",
      "Epoch 3280/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.2185 - val_loss: 135.3116\n",
      "Epoch 3281/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.4342 - val_loss: 158.6525\n",
      "Epoch 3282/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.7821 - val_loss: 160.1624\n",
      "Epoch 3283/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.6286 - val_loss: 201.2035\n",
      "Epoch 3284/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 128.9186 - val_loss: 134.1553\n",
      "Epoch 3285/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.1042 - val_loss: 167.1497\n",
      "Epoch 3286/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.6716 - val_loss: 148.1185\n",
      "Epoch 3287/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.0844 - val_loss: 137.3482\n",
      "Epoch 3288/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 132.4358 - val_loss: 140.4608\n",
      "Epoch 3289/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.2973 - val_loss: 142.6088\n",
      "Epoch 3290/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.8162 - val_loss: 134.7161\n",
      "Epoch 3291/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.1636 - val_loss: 156.8738\n",
      "Epoch 3292/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.2422 - val_loss: 159.6580\n",
      "Epoch 3293/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.0597 - val_loss: 139.0034\n",
      "Epoch 3294/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.1688 - val_loss: 155.9869\n",
      "Epoch 3295/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.7307 - val_loss: 139.1266\n",
      "Epoch 3296/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.7461 - val_loss: 139.1049\n",
      "Epoch 3297/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.2804 - val_loss: 137.1869\n",
      "Epoch 3298/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.5370 - val_loss: 135.7395\n",
      "Epoch 3299/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.3668 - val_loss: 142.6587\n",
      "Epoch 3300/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 145.5148 - val_loss: 254.0632\n",
      "Epoch 3301/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 141.5584 - val_loss: 260.9343\n",
      "Epoch 3302/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 134.5014 - val_loss: 176.1375\n",
      "Epoch 3303/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.7856 - val_loss: 146.1803\n",
      "Epoch 3304/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 127.6824 - val_loss: 157.2813\n",
      "Epoch 3305/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.2300 - val_loss: 166.3329\n",
      "Epoch 3306/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.8565 - val_loss: 146.9167\n",
      "Epoch 3307/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.2539 - val_loss: 134.6794\n",
      "Epoch 3308/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.6416 - val_loss: 142.0243\n",
      "Epoch 3309/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.6305 - val_loss: 142.4561\n",
      "Epoch 3310/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 130.6426 - val_loss: 154.7123\n",
      "Epoch 3311/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.0953 - val_loss: 147.3516\n",
      "Epoch 3312/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.3437 - val_loss: 156.4129\n",
      "Epoch 3313/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.8067 - val_loss: 138.7710\n",
      "Epoch 3314/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.3789 - val_loss: 135.2627\n",
      "Epoch 3315/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.5576 - val_loss: 169.2785\n",
      "Epoch 3316/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 211.7984 - val_loss: 351.7771\n",
      "Epoch 3317/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.5608 - val_loss: 140.6603\n",
      "Epoch 3318/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.7744 - val_loss: 135.8566\n",
      "Epoch 3319/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.8770 - val_loss: 153.2155\n",
      "Epoch 3320/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.8739 - val_loss: 149.0395\n",
      "Epoch 3321/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.6519 - val_loss: 133.5946\n",
      "Epoch 3322/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 142.2018 - val_loss: 138.2634\n",
      "Epoch 3323/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.4396 - val_loss: 145.4919\n",
      "Epoch 3324/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.1305 - val_loss: 143.8146\n",
      "Epoch 3325/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.9877 - val_loss: 176.5523\n",
      "Epoch 3326/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.4343 - val_loss: 143.1053\n",
      "Epoch 3327/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.2863 - val_loss: 157.9450\n",
      "Epoch 3328/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.8340 - val_loss: 137.4061\n",
      "Epoch 3329/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 124.9832 - val_loss: 139.7673\n",
      "Epoch 3330/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.3523 - val_loss: 140.3658\n",
      "Epoch 3331/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.5514 - val_loss: 145.7164\n",
      "Epoch 3332/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.4882 - val_loss: 134.5188\n",
      "Epoch 3333/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.0567 - val_loss: 132.2171\n",
      "Epoch 3334/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.3310 - val_loss: 158.1185\n",
      "Epoch 3335/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.5754 - val_loss: 156.2323\n",
      "Epoch 3336/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.9419 - val_loss: 183.4927\n",
      "Epoch 3337/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.7295 - val_loss: 168.8904\n",
      "Epoch 3338/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.2126 - val_loss: 146.1580\n",
      "Epoch 3339/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.6696 - val_loss: 239.2429\n",
      "Epoch 3340/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.5682 - val_loss: 162.9056\n",
      "Epoch 3341/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.4051 - val_loss: 197.7015\n",
      "Epoch 3342/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.6742 - val_loss: 137.2439\n",
      "Epoch 3343/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.8969 - val_loss: 134.9259\n",
      "Epoch 3344/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 184.2049 - val_loss: 166.3447\n",
      "Epoch 3345/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.7777 - val_loss: 140.5070\n",
      "Epoch 3346/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.8036 - val_loss: 216.1759\n",
      "Epoch 3347/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.0671 - val_loss: 147.0625\n",
      "Epoch 3348/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.5256 - val_loss: 133.1220\n",
      "Epoch 3349/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.9065 - val_loss: 144.3630\n",
      "Epoch 3350/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 124.3166 - val_loss: 198.9216\n",
      "Epoch 3351/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.6719 - val_loss: 150.8988\n",
      "Epoch 3352/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 128.9168 - val_loss: 157.9967\n",
      "Epoch 3353/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.1519 - val_loss: 142.3509\n",
      "Epoch 3354/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.1007 - val_loss: 144.2509\n",
      "Epoch 3355/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.0401 - val_loss: 138.1856\n",
      "Epoch 3356/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.9317 - val_loss: 132.3164\n",
      "Epoch 3357/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.1097 - val_loss: 140.8681\n",
      "Epoch 3358/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.0131 - val_loss: 165.5656\n",
      "Epoch 3359/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.8520 - val_loss: 227.1441\n",
      "Epoch 3360/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.8228 - val_loss: 154.1155\n",
      "Epoch 3361/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.3696 - val_loss: 153.0548\n",
      "Epoch 3362/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 139.0955 - val_loss: 144.0118\n",
      "Epoch 3363/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 158.4134 - val_loss: 256.7166\n",
      "Epoch 3364/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 149.663 - 1s 77us/step - loss: 149.1060 - val_loss: 140.8803\n",
      "Epoch 3365/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.8502 - val_loss: 134.5483\n",
      "Epoch 3366/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.7724 - val_loss: 132.9053\n",
      "Epoch 3367/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.5775 - val_loss: 195.5278\n",
      "Epoch 3368/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.6100 - val_loss: 161.9415\n",
      "Epoch 3369/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.1864 - val_loss: 131.4443\n",
      "Epoch 3370/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 126.8149 - val_loss: 154.3723\n",
      "Epoch 3371/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.6564 - val_loss: 136.1708\n",
      "Epoch 3372/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.5038 - val_loss: 132.8841\n",
      "Epoch 3373/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.1160 - val_loss: 167.6903\n",
      "Epoch 3374/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.6030 - val_loss: 170.5002\n",
      "Epoch 3375/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.0232 - val_loss: 165.0966\n",
      "Epoch 3376/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.1409 - val_loss: 144.0248\n",
      "Epoch 3377/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.9521 - val_loss: 157.9629\n",
      "Epoch 3378/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.8533 - val_loss: 190.9501\n",
      "Epoch 3379/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.8991 - val_loss: 149.0556\n",
      "Epoch 3380/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.1168 - val_loss: 145.4184\n",
      "Epoch 3381/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.0039 - val_loss: 147.5930\n",
      "Epoch 3382/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.9312 - val_loss: 135.3480\n",
      "Epoch 3383/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.6637 - val_loss: 141.5783\n",
      "Epoch 3384/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.2991 - val_loss: 163.1375\n",
      "Epoch 3385/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.3849 - val_loss: 139.8843\n",
      "Epoch 3386/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.0562 - val_loss: 172.5420\n",
      "Epoch 3387/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.0885 - val_loss: 134.9330\n",
      "Epoch 3388/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.2579 - val_loss: 137.5270\n",
      "Epoch 3389/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.7691 - val_loss: 227.4856\n",
      "Epoch 3390/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.3820 - val_loss: 146.0562\n",
      "Epoch 3391/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.7782 - val_loss: 137.8860\n",
      "Epoch 3392/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 126.1531 - val_loss: 203.5589\n",
      "Epoch 3393/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 127.8342 - val_loss: 140.3882\n",
      "Epoch 3394/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.3645 - val_loss: 211.3444\n",
      "Epoch 3395/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.5170 - val_loss: 158.4104\n",
      "Epoch 3396/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.0323 - val_loss: 140.9675\n",
      "Epoch 3397/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.5850 - val_loss: 138.0242\n",
      "Epoch 3398/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.1099 - val_loss: 132.9397\n",
      "Epoch 3399/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 128.4579 - val_loss: 127.9507\n",
      "Epoch 3400/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.6278 - val_loss: 202.8233\n",
      "Epoch 3401/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.6557 - val_loss: 198.3749\n",
      "Epoch 3402/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.7226 - val_loss: 145.5408\n",
      "Epoch 3403/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.9990 - val_loss: 137.3816\n",
      "Epoch 3404/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 125.3325 - val_loss: 242.4744\n",
      "Epoch 3405/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.4910 - val_loss: 134.7208\n",
      "Epoch 3406/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.6833 - val_loss: 171.7608\n",
      "Epoch 3407/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.8945 - val_loss: 143.0366\n",
      "Epoch 3408/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 131.0376 - val_loss: 153.2449\n",
      "Epoch 3409/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.8043 - val_loss: 150.0413\n",
      "Epoch 3410/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.4045 - val_loss: 139.3240\n",
      "Epoch 3411/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 128.2949 - val_loss: 158.2923\n",
      "Epoch 3412/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.0261 - val_loss: 154.2765\n",
      "Epoch 3413/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.7369 - val_loss: 132.2840\n",
      "Epoch 3414/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 126.4640 - val_loss: 139.3460\n",
      "Epoch 3415/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.4539 - val_loss: 171.5497\n",
      "Epoch 3416/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.7952 - val_loss: 138.9533\n",
      "Epoch 3417/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.4985 - val_loss: 134.3688\n",
      "Epoch 3418/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.6739 - val_loss: 150.0483\n",
      "Epoch 3419/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.0284 - val_loss: 146.4639\n",
      "Epoch 3420/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.3124 - val_loss: 192.2767\n",
      "Epoch 3421/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.8620 - val_loss: 130.6570\n",
      "Epoch 3422/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.9731 - val_loss: 176.8566\n",
      "Epoch 3423/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 130.4282 - val_loss: 129.5537\n",
      "Epoch 3424/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.8367 - val_loss: 148.4242\n",
      "Epoch 3425/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 147.8174 - val_loss: 149.5091\n",
      "Epoch 3426/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 136.7803 - val_loss: 137.7125\n",
      "Epoch 3427/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 132.5620 - val_loss: 151.3393\n",
      "Epoch 3428/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.9315 - val_loss: 190.3527\n",
      "Epoch 3429/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.9269 - val_loss: 214.9742\n",
      "Epoch 3430/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.6035 - val_loss: 149.1960\n",
      "Epoch 3431/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.0299 - val_loss: 145.0975\n",
      "Epoch 3432/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 131.8675 - val_loss: 151.3992\n",
      "Epoch 3433/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.0280 - val_loss: 148.5620\n",
      "Epoch 3434/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.5786 - val_loss: 135.8820\n",
      "Epoch 3435/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.9827 - val_loss: 137.6306\n",
      "Epoch 3436/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.4890 - val_loss: 196.8883\n",
      "Epoch 3437/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.4460 - val_loss: 137.5792\n",
      "Epoch 3438/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 135.8073 - val_loss: 134.6064\n",
      "Epoch 3439/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.4640 - val_loss: 178.8039\n",
      "Epoch 3440/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 179.6576 - val_loss: 159.6994\n",
      "Epoch 3441/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.3801 - val_loss: 137.5004\n",
      "Epoch 3442/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 126.5896 - val_loss: 134.1525\n",
      "Epoch 3443/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.6424 - val_loss: 138.8565\n",
      "Epoch 3444/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 129.1675 - val_loss: 143.0705\n",
      "Epoch 3445/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 126.2109 - val_loss: 140.7199\n",
      "Epoch 3446/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 125.9991 - val_loss: 137.1416\n",
      "Epoch 3447/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.5610 - val_loss: 153.1876\n",
      "Epoch 3448/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.2097 - val_loss: 200.3837\n",
      "Epoch 3449/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 198.4950 - val_loss: 334.7379\n",
      "Epoch 3450/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.2283 - val_loss: 143.1823\n",
      "Epoch 3451/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.4764 - val_loss: 142.7627\n",
      "Epoch 3452/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.2089 - val_loss: 184.8276\n",
      "Epoch 3453/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.3205 - val_loss: 131.1477\n",
      "Epoch 3454/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.6874 - val_loss: 197.9178\n",
      "Epoch 3455/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.6414 - val_loss: 136.6839\n",
      "Epoch 3456/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.8769 - val_loss: 140.0095\n",
      "Epoch 3457/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.6268 - val_loss: 133.5777\n",
      "Epoch 3458/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.1935 - val_loss: 144.4778\n",
      "Epoch 3459/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 126.8553 - val_loss: 139.6323\n",
      "Epoch 3460/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.6647 - val_loss: 481.2977\n",
      "Epoch 3461/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.4602 - val_loss: 132.2589\n",
      "Epoch 3462/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.8652 - val_loss: 134.6815\n",
      "Epoch 3463/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 126.7683 - val_loss: 156.4075\n",
      "Epoch 3464/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.2909 - val_loss: 164.9347\n",
      "Epoch 3465/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.4259 - val_loss: 138.5753\n",
      "Epoch 3466/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.8190 - val_loss: 136.5314\n",
      "Epoch 3467/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.4952 - val_loss: 180.1428\n",
      "Epoch 3468/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.0421 - val_loss: 153.2985\n",
      "Epoch 3469/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.2578 - val_loss: 168.3356\n",
      "Epoch 3470/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.5176 - val_loss: 152.8025\n",
      "Epoch 3471/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.5150 - val_loss: 134.8213\n",
      "Epoch 3472/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.5559 - val_loss: 157.0956\n",
      "Epoch 3473/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 127.3286 - val_loss: 175.2874\n",
      "Epoch 3474/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.6895 - val_loss: 180.0519\n",
      "Epoch 3475/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 127.2691 - val_loss: 164.5599\n",
      "Epoch 3476/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.1214 - val_loss: 139.1731\n",
      "Epoch 3477/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 194.7661 - val_loss: 148.5521\n",
      "Epoch 3478/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.0477 - val_loss: 158.7362\n",
      "Epoch 3479/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.5364 - val_loss: 145.4358\n",
      "Epoch 3480/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 129.7768 - val_loss: 136.1087\n",
      "Epoch 3481/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.3683 - val_loss: 161.7713\n",
      "Epoch 3482/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.8098 - val_loss: 145.6875\n",
      "Epoch 3483/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.7863 - val_loss: 136.7661\n",
      "Epoch 3484/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.3586 - val_loss: 156.1426\n",
      "Epoch 3485/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.7663 - val_loss: 173.1212\n",
      "Epoch 3486/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 144.1464 - val_loss: 183.9019\n",
      "Epoch 3487/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 180.6342 - val_loss: 142.5219\n",
      "Epoch 3488/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 135.8307 - val_loss: 149.6658\n",
      "Epoch 3489/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 135.4797 - val_loss: 145.0226\n",
      "Epoch 3490/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.5525 - val_loss: 163.6019\n",
      "Epoch 3491/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.1630 - val_loss: 143.5228\n",
      "Epoch 3492/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.9891 - val_loss: 157.2663\n",
      "Epoch 3493/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 127.0846 - val_loss: 130.9061\n",
      "Epoch 3494/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.1170 - val_loss: 168.5227\n",
      "Epoch 3495/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.4220 - val_loss: 140.3493\n",
      "Epoch 3496/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.5918 - val_loss: 170.9184\n",
      "Epoch 3497/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.4086 - val_loss: 142.0284\n",
      "Epoch 3498/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.1389 - val_loss: 145.5721\n",
      "Epoch 3499/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.8715 - val_loss: 191.9562\n",
      "Epoch 3500/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.3230 - val_loss: 134.0302\n",
      "Epoch 3501/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.1529 - val_loss: 157.2868\n",
      "Epoch 3502/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.1743 - val_loss: 166.9070\n",
      "Epoch 3503/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.7914 - val_loss: 140.3701\n",
      "Epoch 3504/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.4682 - val_loss: 172.5307\n",
      "Epoch 3505/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.7125 - val_loss: 195.5767\n",
      "Epoch 3506/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 196.6670 - val_loss: 139.9904\n",
      "Epoch 3507/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.4217 - val_loss: 158.7787\n",
      "Epoch 3508/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.9893 - val_loss: 145.7092\n",
      "Epoch 3509/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.0363 - val_loss: 138.6604\n",
      "Epoch 3510/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.6078 - val_loss: 172.9191\n",
      "Epoch 3511/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.3270 - val_loss: 141.4458\n",
      "Epoch 3512/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.6648 - val_loss: 148.2057\n",
      "Epoch 3513/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.4897 - val_loss: 138.9371\n",
      "Epoch 3514/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.3532 - val_loss: 141.1785\n",
      "Epoch 3515/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.3529 - val_loss: 187.0972\n",
      "Epoch 3516/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.5983 - val_loss: 145.4739\n",
      "Epoch 3517/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.3521 - val_loss: 140.1535\n",
      "Epoch 3518/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.3032 - val_loss: 147.9394\n",
      "Epoch 3519/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.0575 - val_loss: 147.8500\n",
      "Epoch 3520/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.7953 - val_loss: 139.2717\n",
      "Epoch 3521/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.0622 - val_loss: 142.0076\n",
      "Epoch 3522/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.8875 - val_loss: 161.5484\n",
      "Epoch 3523/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.6519 - val_loss: 185.5904\n",
      "Epoch 3524/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.6660 - val_loss: 280.3655\n",
      "Epoch 3525/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.1370 - val_loss: 160.8503\n",
      "Epoch 3526/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.4144 - val_loss: 127.9829\n",
      "Epoch 3527/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.2461 - val_loss: 139.5856\n",
      "Epoch 3528/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.1044 - val_loss: 176.5296\n",
      "Epoch 3529/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.2459 - val_loss: 156.4615\n",
      "Epoch 3530/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.8929 - val_loss: 131.0939\n",
      "Epoch 3531/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.2542 - val_loss: 133.5531\n",
      "Epoch 3532/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.8412 - val_loss: 152.4152\n",
      "Epoch 3533/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.4666 - val_loss: 129.7979\n",
      "Epoch 3534/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.8871 - val_loss: 140.1383\n",
      "Epoch 3535/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 127.0709 - val_loss: 149.9848\n",
      "Epoch 3536/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.7900 - val_loss: 149.7443\n",
      "Epoch 3537/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.4003 - val_loss: 131.9588\n",
      "Epoch 3538/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.7331 - val_loss: 149.8752\n",
      "Epoch 3539/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.9918 - val_loss: 164.9391\n",
      "Epoch 3540/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.5231 - val_loss: 138.1537\n",
      "Epoch 3541/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 125.7806 - val_loss: 166.5104\n",
      "Epoch 3542/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 129.1306 - val_loss: 139.7197\n",
      "Epoch 3543/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 127.4268 - val_loss: 214.4938\n",
      "Epoch 3544/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.0530 - val_loss: 188.8244\n",
      "Epoch 3545/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.4661 - val_loss: 149.2346\n",
      "Epoch 3546/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 128.2745 - val_loss: 139.0649\n",
      "Epoch 3547/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.2725 - val_loss: 136.6593\n",
      "Epoch 3548/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 150.9447 - val_loss: 203.3133\n",
      "Epoch 3549/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 131.2856 - val_loss: 167.7077\n",
      "Epoch 3550/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 212.0273 - val_loss: 138.5229\n",
      "Epoch 3551/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.5362 - val_loss: 138.0996\n",
      "Epoch 3552/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 126.6262 - val_loss: 135.3686\n",
      "Epoch 3553/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.1007 - val_loss: 138.1972\n",
      "Epoch 3554/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.2547 - val_loss: 134.6711\n",
      "Epoch 3555/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 131.8848 - val_loss: 162.9864\n",
      "Epoch 3556/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.1155 - val_loss: 189.2296\n",
      "Epoch 3557/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.4154 - val_loss: 247.2675\n",
      "Epoch 3558/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 131.5673 - val_loss: 154.2257\n",
      "Epoch 3559/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.8426 - val_loss: 134.4142\n",
      "Epoch 3560/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.1961 - val_loss: 133.6205\n",
      "Epoch 3561/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.5536 - val_loss: 134.0950\n",
      "Epoch 3562/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.1695 - val_loss: 133.4747\n",
      "Epoch 3563/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.7233 - val_loss: 136.1392\n",
      "Epoch 3564/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.9538 - val_loss: 150.6991\n",
      "Epoch 3565/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.6084 - val_loss: 129.8832\n",
      "Epoch 3566/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.2628 - val_loss: 131.6136\n",
      "Epoch 3567/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 126.7677 - val_loss: 171.1617\n",
      "Epoch 3568/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 145.0164 - val_loss: 199.4339\n",
      "Epoch 3569/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.3904 - val_loss: 133.4439\n",
      "Epoch 3570/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.8672 - val_loss: 184.7102\n",
      "Epoch 3571/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.1829 - val_loss: 137.2880\n",
      "Epoch 3572/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.4281 - val_loss: 132.0235\n",
      "Epoch 3573/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.8392 - val_loss: 139.9479\n",
      "Epoch 3574/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.7285 - val_loss: 172.1944\n",
      "Epoch 3575/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.0771 - val_loss: 129.8844\n",
      "Epoch 3576/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 129.0274 - val_loss: 137.8828\n",
      "Epoch 3577/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.0813 - val_loss: 143.3625\n",
      "Epoch 3578/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.9404 - val_loss: 157.3972\n",
      "Epoch 3579/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.1443 - val_loss: 142.7292\n",
      "Epoch 3580/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 183.3963 - val_loss: 140.2996\n",
      "Epoch 3581/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.1842 - val_loss: 139.4043\n",
      "Epoch 3582/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.8879 - val_loss: 150.5410\n",
      "Epoch 3583/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.9414 - val_loss: 164.1134\n",
      "Epoch 3584/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.3920 - val_loss: 151.8523\n",
      "Epoch 3585/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.8057 - val_loss: 143.6746\n",
      "Epoch 3586/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 132.1800 - val_loss: 143.4753\n",
      "Epoch 3587/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.2290 - val_loss: 172.2204\n",
      "Epoch 3588/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.6669 - val_loss: 134.7194\n",
      "Epoch 3589/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.9348 - val_loss: 181.9796\n",
      "Epoch 3590/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.8914 - val_loss: 133.9436\n",
      "Epoch 3591/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.8726 - val_loss: 155.1002\n",
      "Epoch 3592/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.7003 - val_loss: 141.3447\n",
      "Epoch 3593/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.4362 - val_loss: 147.8634\n",
      "Epoch 3594/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.4618 - val_loss: 148.3666\n",
      "Epoch 3595/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.1223 - val_loss: 141.2630\n",
      "Epoch 3596/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.9209 - val_loss: 151.4887\n",
      "Epoch 3597/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.9043 - val_loss: 137.5766\n",
      "Epoch 3598/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.2103 - val_loss: 193.1680\n",
      "Epoch 3599/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.8631 - val_loss: 142.6920\n",
      "Epoch 3600/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.2789 - val_loss: 139.4285\n",
      "Epoch 3601/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.8270 - val_loss: 142.7031\n",
      "Epoch 3602/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 140.9072 - val_loss: 160.3802\n",
      "Epoch 3603/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.2684 - val_loss: 136.2181\n",
      "Epoch 3604/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 126.9929 - val_loss: 134.7947\n",
      "Epoch 3605/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.0615 - val_loss: 138.7975\n",
      "Epoch 3606/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.0927 - val_loss: 136.0472\n",
      "Epoch 3607/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.2854 - val_loss: 138.0184\n",
      "Epoch 3608/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.6820 - val_loss: 175.4493\n",
      "Epoch 3609/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.2394 - val_loss: 236.2052\n",
      "Epoch 3610/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 159.0822 - val_loss: 162.1916\n",
      "Epoch 3611/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 127.4671 - val_loss: 142.0076\n",
      "Epoch 3612/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 127.5791 - val_loss: 171.5277\n",
      "Epoch 3613/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.3146 - val_loss: 131.0531\n",
      "Epoch 3614/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.1045 - val_loss: 137.9104\n",
      "Epoch 3615/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.3562 - val_loss: 136.2307\n",
      "Epoch 3616/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.4739 - val_loss: 148.0831\n",
      "Epoch 3617/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.9209 - val_loss: 133.0582\n",
      "Epoch 3618/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.9455 - val_loss: 146.9536\n",
      "Epoch 3619/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 129.7650 - val_loss: 139.9210\n",
      "Epoch 3620/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.9460 - val_loss: 162.0008\n",
      "Epoch 3621/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.9487 - val_loss: 142.0997\n",
      "Epoch 3622/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.7598 - val_loss: 147.6872\n",
      "Epoch 3623/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.9044 - val_loss: 180.1155\n",
      "Epoch 3624/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.4161 - val_loss: 165.9415\n",
      "Epoch 3625/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.2989 - val_loss: 152.3946\n",
      "Epoch 3626/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 131.3183 - val_loss: 134.8740\n",
      "Epoch 3627/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.5716 - val_loss: 132.6741\n",
      "Epoch 3628/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.3545 - val_loss: 171.4685\n",
      "Epoch 3629/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 128.7114 - val_loss: 139.0093\n",
      "Epoch 3630/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.1041 - val_loss: 152.6995\n",
      "Epoch 3631/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.3279 - val_loss: 138.5672\n",
      "Epoch 3632/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 128.5089 - val_loss: 162.0354\n",
      "Epoch 3633/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 128.6232 - val_loss: 138.9730\n",
      "Epoch 3634/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.4827 - val_loss: 144.4099\n",
      "Epoch 3635/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.5492 - val_loss: 144.0152\n",
      "Epoch 3636/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.5261 - val_loss: 162.8253\n",
      "Epoch 3637/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.5279 - val_loss: 158.0473\n",
      "Epoch 3638/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.9688 - val_loss: 146.6575\n",
      "Epoch 3639/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.8794 - val_loss: 144.0998\n",
      "Epoch 3640/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.1680 - val_loss: 136.4254\n",
      "Epoch 3641/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.9213 - val_loss: 137.8378\n",
      "Epoch 3642/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.4643 - val_loss: 132.3880\n",
      "Epoch 3643/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 166.8823 - val_loss: 183.5788\n",
      "Epoch 3644/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.7555 - val_loss: 135.3473\n",
      "Epoch 3645/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 122.8736 - val_loss: 150.1600\n",
      "Epoch 3646/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 129.1063 - val_loss: 154.7383\n",
      "Epoch 3647/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.8059 - val_loss: 148.0388\n",
      "Epoch 3648/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.1805 - val_loss: 134.7879\n",
      "Epoch 3649/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 133.4946 - val_loss: 132.0350\n",
      "Epoch 3650/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.6579 - val_loss: 142.5963\n",
      "Epoch 3651/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.0754 - val_loss: 137.3480\n",
      "Epoch 3652/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.6168 - val_loss: 139.1486\n",
      "Epoch 3653/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.6937 - val_loss: 151.0181\n",
      "Epoch 3654/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.9600 - val_loss: 135.0897\n",
      "Epoch 3655/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.1729 - val_loss: 138.5055\n",
      "Epoch 3656/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 128.2977 - val_loss: 153.6357\n",
      "Epoch 3657/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 142.9843 - val_loss: 137.3164\n",
      "Epoch 3658/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.0832 - val_loss: 136.7069\n",
      "Epoch 3659/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.8738 - val_loss: 148.0748\n",
      "Epoch 3660/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.5338 - val_loss: 138.3922\n",
      "Epoch 3661/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 128.8057 - val_loss: 135.5169\n",
      "Epoch 3662/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.0069 - val_loss: 274.9997\n",
      "Epoch 3663/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.8770 - val_loss: 193.0970\n",
      "Epoch 3664/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.1350 - val_loss: 140.8000\n",
      "Epoch 3665/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.6268 - val_loss: 137.3698\n",
      "Epoch 3666/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 131.2803 - val_loss: 147.3309\n",
      "Epoch 3667/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.1806 - val_loss: 140.4789\n",
      "Epoch 3668/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.2644 - val_loss: 132.7656\n",
      "Epoch 3669/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.3455 - val_loss: 163.9629\n",
      "Epoch 3670/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.2303 - val_loss: 131.7215\n",
      "Epoch 3671/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.3025 - val_loss: 134.0208\n",
      "Epoch 3672/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 138.5537 - val_loss: 154.0738\n",
      "Epoch 3673/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 130.2856 - val_loss: 134.6392\n",
      "Epoch 3674/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 135.9006 - val_loss: 144.0233\n",
      "Epoch 3675/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.5560 - val_loss: 142.0984\n",
      "Epoch 3676/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.5864 - val_loss: 138.1895\n",
      "Epoch 3677/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 136.2354 - val_loss: 135.9828\n",
      "Epoch 3678/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 128.4726 - val_loss: 138.0400\n",
      "Epoch 3679/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.9448 - val_loss: 171.8728\n",
      "Epoch 3680/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 134.0336 - val_loss: 181.1059\n",
      "Epoch 3681/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.8368 - val_loss: 137.0474\n",
      "Epoch 3682/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.9602 - val_loss: 196.9394\n",
      "Epoch 3683/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.8931 - val_loss: 156.7956\n",
      "Epoch 3684/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.5773 - val_loss: 138.7422\n",
      "Epoch 3685/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.1483 - val_loss: 148.3486\n",
      "Epoch 3686/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.3354 - val_loss: 136.5005\n",
      "Epoch 3687/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.9793 - val_loss: 165.1282\n",
      "Epoch 3688/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.9401 - val_loss: 170.8549\n",
      "Epoch 3689/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.9462 - val_loss: 145.4815\n",
      "Epoch 3690/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 125.4094 - val_loss: 134.3472\n",
      "Epoch 3691/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.4051 - val_loss: 153.6353\n",
      "Epoch 3692/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.6466 - val_loss: 143.3694\n",
      "Epoch 3693/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.9103 - val_loss: 147.4195\n",
      "Epoch 3694/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.6648 - val_loss: 159.9344\n",
      "Epoch 3695/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 129.0427 - val_loss: 155.3793\n",
      "Epoch 3696/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 134.7240 - val_loss: 164.0441\n",
      "Epoch 3697/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 131.6510 - val_loss: 289.5443\n",
      "Epoch 3698/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.3857 - val_loss: 196.7487\n",
      "Epoch 3699/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 156.4680 - val_loss: 278.9793\n",
      "Epoch 3700/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.7874 - val_loss: 370.2018\n",
      "Epoch 3701/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 129.9760 - val_loss: 153.8790\n",
      "Epoch 3702/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.3996 - val_loss: 143.1705\n",
      "Epoch 3703/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.5273 - val_loss: 138.6910\n",
      "Epoch 3704/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 135.3797 - val_loss: 137.9071\n",
      "Epoch 3705/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.3736 - val_loss: 135.7140\n",
      "Epoch 3706/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 128.1945 - val_loss: 138.6474\n",
      "Epoch 3707/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.6552 - val_loss: 141.9895\n",
      "Epoch 3708/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.3796 - val_loss: 162.4192\n",
      "Epoch 3709/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.1530 - val_loss: 156.6848\n",
      "Epoch 3710/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.3337 - val_loss: 231.9024\n",
      "Epoch 3711/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.0184 - val_loss: 143.1055\n",
      "Epoch 3712/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.3267 - val_loss: 139.0621\n",
      "Epoch 3713/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.3775 - val_loss: 256.0478\n",
      "Epoch 3714/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 133.0798 - val_loss: 151.1590\n",
      "Epoch 3715/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.3614 - val_loss: 139.9750\n",
      "Epoch 3716/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.7387 - val_loss: 173.3550\n",
      "Epoch 3717/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 134.2289 - val_loss: 135.3508\n",
      "Epoch 3718/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.3432 - val_loss: 139.4875\n",
      "Epoch 3719/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.2091 - val_loss: 150.1584\n",
      "Epoch 3720/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.9160 - val_loss: 154.4744\n",
      "Epoch 3721/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.3920 - val_loss: 136.5319\n",
      "Epoch 3722/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.8150 - val_loss: 139.3340\n",
      "Epoch 3723/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.8831 - val_loss: 131.9640\n",
      "Epoch 3724/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.4169 - val_loss: 156.7915\n",
      "Epoch 3725/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 147.8457 - val_loss: 231.1009\n",
      "Epoch 3726/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 137.4359 - val_loss: 141.0040\n",
      "Epoch 3727/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.6026 - val_loss: 132.6582\n",
      "Epoch 3728/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.9628 - val_loss: 178.1092\n",
      "Epoch 3729/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 177.6554 - val_loss: 227.4249\n",
      "Epoch 3730/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.8767 - val_loss: 183.0770\n",
      "Epoch 3731/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 143.8946 - val_loss: 146.2192\n",
      "Epoch 3732/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 124.5154 - val_loss: 136.4023\n",
      "Epoch 3733/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 131.5719 - val_loss: 148.2028\n",
      "Epoch 3734/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 130.3173 - val_loss: 139.5283\n",
      "Epoch 3735/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 138.9233 - val_loss: 150.7188\n",
      "Epoch 3736/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 139.5795 - val_loss: 146.0827\n",
      "Epoch 3737/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 132.9054 - val_loss: 133.7543\n",
      "Epoch 3738/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.8871 - val_loss: 169.1686\n",
      "Epoch 3739/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.7597 - val_loss: 147.0785\n",
      "Epoch 3740/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.5418 - val_loss: 144.8276\n",
      "Epoch 3741/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 131.5366 - val_loss: 147.1025\n",
      "Epoch 3742/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.2959 - val_loss: 198.4543\n",
      "Epoch 3743/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.1395 - val_loss: 138.4770\n",
      "Epoch 3744/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 161.0269 - val_loss: 135.4377\n",
      "Epoch 3745/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 124.3746 - val_loss: 137.6533\n",
      "Epoch 3746/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 128.8084 - val_loss: 141.9689\n",
      "Epoch 3747/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.2281 - val_loss: 134.4701\n",
      "Epoch 3748/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 128.9520 - val_loss: 170.3654\n",
      "Epoch 3749/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 170.7125 - val_loss: 154.0264\n",
      "Epoch 3750/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.3667 - val_loss: 165.0890\n",
      "Epoch 3751/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.2647 - val_loss: 141.9506\n",
      "Epoch 3752/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.1854 - val_loss: 136.0194\n",
      "Epoch 3753/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.5776 - val_loss: 139.1061\n",
      "Epoch 3754/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.3058 - val_loss: 133.8547\n",
      "Epoch 3755/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 131.6887 - val_loss: 156.5030\n",
      "Epoch 3756/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 128.8510 - val_loss: 138.0044\n",
      "Epoch 3757/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 126.0724 - val_loss: 158.5886\n",
      "Epoch 3758/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.3403 - val_loss: 147.7626\n",
      "Epoch 3759/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 131.8764 - val_loss: 134.0058\n",
      "Epoch 3760/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 128.7979 - val_loss: 135.0564\n",
      "Epoch 3761/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.4353 - val_loss: 139.5413\n",
      "Epoch 3762/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 126.8865 - val_loss: 144.1229\n",
      "Epoch 3763/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 131.9961 - val_loss: 148.6561\n",
      "Epoch 3764/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.0784 - val_loss: 149.5469\n",
      "Epoch 3765/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.9588 - val_loss: 149.2458\n",
      "Epoch 3766/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.9180 - val_loss: 136.9872\n",
      "Epoch 3767/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.5292 - val_loss: 138.7587\n",
      "Epoch 3768/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.3243 - val_loss: 170.2568\n",
      "Epoch 3769/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.8605 - val_loss: 147.0932\n",
      "Epoch 3770/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.1483 - val_loss: 138.3107\n",
      "Epoch 3771/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 125.6365 - val_loss: 175.1076\n",
      "Epoch 3772/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.5558 - val_loss: 152.4690\n",
      "Epoch 3773/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.6535 - val_loss: 134.2290\n",
      "Epoch 3774/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 133.1773 - val_loss: 155.6300\n",
      "Epoch 3775/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.0037 - val_loss: 148.3453\n",
      "Epoch 3776/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.7735 - val_loss: 138.2638\n",
      "Epoch 3777/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.7264 - val_loss: 156.0102\n",
      "Epoch 3778/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.7292 - val_loss: 163.3408\n",
      "Epoch 3779/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.0750 - val_loss: 140.4457\n",
      "Epoch 3780/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 127.8562 - val_loss: 133.1583\n",
      "Epoch 3781/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.3384 - val_loss: 232.2664\n",
      "Epoch 3782/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 129.7792 - val_loss: 166.4635\n",
      "Epoch 3783/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.5739 - val_loss: 275.8106\n",
      "Epoch 3784/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.8600 - val_loss: 158.2550\n",
      "Epoch 3785/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.4677 - val_loss: 148.7496\n",
      "Epoch 3786/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 131.7937 - val_loss: 203.9751\n",
      "Epoch 3787/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.4944 - val_loss: 153.7608\n",
      "Epoch 3788/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 129.4483 - val_loss: 194.7536\n",
      "Epoch 3789/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 137.1566 - val_loss: 213.7819\n",
      "Epoch 3790/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.5173 - val_loss: 147.6401\n",
      "Epoch 3791/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.4993 - val_loss: 149.5084\n",
      "Epoch 3792/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.2442 - val_loss: 175.4648\n",
      "Epoch 3793/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 135.1780 - val_loss: 135.3347\n",
      "Epoch 3794/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.0022 - val_loss: 141.5063\n",
      "Epoch 3795/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.4712 - val_loss: 142.6396\n",
      "Epoch 3796/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.1718 - val_loss: 151.5772\n",
      "Epoch 3797/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.2798 - val_loss: 154.7920\n",
      "Epoch 3798/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.7170 - val_loss: 151.3972\n",
      "Epoch 3799/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.5199 - val_loss: 148.2033\n",
      "Epoch 3800/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.4803 - val_loss: 159.1698\n",
      "Epoch 3801/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 131.2768 - val_loss: 143.8424\n",
      "Epoch 3802/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 151.7314 - val_loss: 165.4648\n",
      "Epoch 3803/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 138.1575 - val_loss: 146.0121\n",
      "Epoch 3804/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 129.5414 - val_loss: 189.0826\n",
      "Epoch 03804: early stopping\n",
      "Fold score (RMSE): 13.514042854309082\n",
      "Fold #4\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 2s 279us/step - loss: 9215.0057 - val_loss: 5227.1152\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 4947.7398 - val_loss: 4770.3255\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 4548.5539 - val_loss: 4353.0454\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 4275.8551 - val_loss: 4247.9623\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 4218.4506 - val_loss: 4252.9999\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 4181.7269 - val_loss: 4116.0680\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 4120.1702 - val_loss: 4100.4526\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 4022.6089 - val_loss: 3999.9936\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 4021.9517 - val_loss: 4387.8778\n",
      "Epoch 10/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 3954.5111 - val_loss: 4005.7556\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 3791.6698 - val_loss: 3897.0065\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 3518.9590 - val_loss: 3559.7002\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 3340.7878 - val_loss: 3144.5250\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 3152.1015 - val_loss: 2757.1479\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 2945.7516 - val_loss: 2434.8998\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 2746.9991 - val_loss: 2178.1302\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 2205.0653 - val_loss: 2288.8059\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 2149.8771 - val_loss: 1430.6261\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 1980.5315 - val_loss: 3333.7839\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 1882.6278 - val_loss: 2427.7109\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 1243.5303 - val_loss: 1009.0277\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 1102.7033 - val_loss: 1306.4592\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 1174.3071 - val_loss: 886.8145\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 1016.5305 - val_loss: 514.8634\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 822.5834 - val_loss: 752.8157\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 808.0225 - val_loss: 466.2003\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 702.8779 - val_loss: 472.0529\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 753.6239 - val_loss: 392.0450\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 728.0912 - val_loss: 497.5118\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 656.2212 - val_loss: 756.7095\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 689.1389 - val_loss: 612.5926\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 740.2599 - val_loss: 375.8249\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 690.5586 - val_loss: 436.8271\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 518.7449 - val_loss: 890.7211\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 539.0503 - val_loss: 556.0541\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 556.2686 - val_loss: 418.5650\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 682.4602 - val_loss: 449.0536\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 578.9948 - val_loss: 310.0882\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 533.9124 - val_loss: 410.8055\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 539.8476 - val_loss: 298.7879\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 516.2889 - val_loss: 590.6247\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 658.1265 - val_loss: 404.0161\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 460.4042 - val_loss: 313.3700\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 413.1949 - val_loss: 334.6717\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 674.9964 - val_loss: 545.1523\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 586.5792 - val_loss: 460.3345\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 481.5299 - val_loss: 254.1929\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 565.9627 - val_loss: 454.3970\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 427.0776 - val_loss: 271.3419\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 424.5063 - val_loss: 281.1864\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 431.8641 - val_loss: 289.5299\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 430.6054 - val_loss: 267.5216\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 532.8196 - val_loss: 277.5347\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 417.9788 - val_loss: 426.4432\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 376.1791 - val_loss: 302.8954\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 532.9943 - val_loss: 401.5871\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 378.4588 - val_loss: 232.8762\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 436.4143 - val_loss: 273.4011\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 416.7568 - val_loss: 276.3576\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 370.8587 - val_loss: 244.1882\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 444.5778 - val_loss: 643.5236\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 376.5540 - val_loss: 667.9263\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 387.5658 - val_loss: 472.9871\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 440.7404 - val_loss: 277.0870\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 453.4880 - val_loss: 237.9580\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 360.1457 - val_loss: 531.5383\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 452.9453 - val_loss: 238.5491\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 437.1410 - val_loss: 420.0675\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 360.3696 - val_loss: 231.9209\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 378.4454 - val_loss: 225.8616\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 298.8504 - val_loss: 338.3167\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 370.2985 - val_loss: 240.9004\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 451.0796 - val_loss: 1473.8550\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 401.8146 - val_loss: 212.1821\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 381.2587 - val_loss: 285.3817\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 403.4205 - val_loss: 354.5184\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 436.4934 - val_loss: 403.3985\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 331.1224 - val_loss: 218.3244\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 329.6926 - val_loss: 197.3640\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 309.8534 - val_loss: 206.2182\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 321.0763 - val_loss: 233.3350\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 341.8706 - val_loss: 190.2856\n",
      "Epoch 83/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 66us/step - loss: 426.5751 - val_loss: 241.4528\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 365.5250 - val_loss: 305.7200\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 376.6565 - val_loss: 209.7432\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 342.8260 - val_loss: 189.4019\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 325.0897 - val_loss: 208.2946\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 343.4294 - val_loss: 256.3303\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 346.5793 - val_loss: 249.6443\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 350.1006 - val_loss: 217.6228\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 378.9431 - val_loss: 203.2200\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 308.4133 - val_loss: 210.3404\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 400.8665 - val_loss: 297.6376\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 324.6722 - val_loss: 264.8571\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 416.8094 - val_loss: 233.2405\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 390.8108 - val_loss: 201.4018\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 343.9237 - val_loss: 261.9390\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 298.1265 - val_loss: 271.9766\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 354.5044 - val_loss: 386.5461\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 319.6974 - val_loss: 266.6980\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 301.4838 - val_loss: 191.0533\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 317.1761 - val_loss: 336.9812\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 359.6925 - val_loss: 447.1936\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 340.5076 - val_loss: 180.3764\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 282.2238 - val_loss: 198.7629\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 303.3513 - val_loss: 1110.9748\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 337.5516 - val_loss: 439.0380\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 391.1156 - val_loss: 166.0210\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 282.9755 - val_loss: 200.6838\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 318.6120 - val_loss: 305.2146\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 322.6035 - val_loss: 241.1945\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 346.8699 - val_loss: 425.3613\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 289.5769 - val_loss: 335.6853\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 308.2941 - val_loss: 188.3670\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 276.3645 - val_loss: 182.0819\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 258.0337 - val_loss: 199.4254\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 404.0155 - val_loss: 372.5033\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 309.1490 - val_loss: 175.1444\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 311.7434 - val_loss: 187.9980\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 265.5937 - val_loss: 204.2122\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 288.6177 - val_loss: 217.0112\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 295.9925 - val_loss: 164.4622\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 343.2532 - val_loss: 292.1561\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 280.2499 - val_loss: 195.9572\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 409.3865 - val_loss: 221.7156\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 293.4720 - val_loss: 921.0236\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 275.3377 - val_loss: 192.1791\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 269.0753 - val_loss: 245.7463\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 296.3160 - val_loss: 310.2871\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 305.4007 - val_loss: 251.1968\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 414.3436 - val_loss: 175.5422\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 274.6756 - val_loss: 161.3595\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 265.7917 - val_loss: 196.5183\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 263.6641 - val_loss: 192.3309\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 294.9047 - val_loss: 163.5889\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 304.1508 - val_loss: 187.9594\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 264.5248 - val_loss: 330.0859\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 289.0833 - val_loss: 160.4227\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 324.6501 - val_loss: 193.1369\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 294.7223 - val_loss: 250.5119\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 253.5546 - val_loss: 172.6829\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 248.2254 - val_loss: 185.9174\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 240.7845 - val_loss: 176.5263\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 274.7370 - val_loss: 168.3072\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 303.7872 - val_loss: 199.4138\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 345.1706 - val_loss: 304.8408\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 264.7691 - val_loss: 227.1360\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 304.4416 - val_loss: 292.1613\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 336.5438 - val_loss: 208.1135\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 276.4210 - val_loss: 195.4242\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 230.3873 - val_loss: 233.0073\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 288.5081 - val_loss: 156.6779\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 237.6674 - val_loss: 212.8564\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 279.5149 - val_loss: 179.1962\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 298.1268 - val_loss: 154.3639\n",
      "Epoch 156/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 268.6127 - val_loss: 266.4288\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 312.2708 - val_loss: 317.9598\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 279.0604 - val_loss: 197.3314\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 285.0026 - val_loss: 147.6506\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 222.4427 - val_loss: 289.0851\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 285.6542 - val_loss: 223.0181\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 266.0373 - val_loss: 183.0666\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 258.5164 - val_loss: 205.0549\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 242.3415 - val_loss: 180.5869\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 296.1158 - val_loss: 174.7998\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 322.3960 - val_loss: 159.9356\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 257.5487 - val_loss: 236.6528\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 252.0175 - val_loss: 169.3996\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 240.8376 - val_loss: 198.5489\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 241.2733 - val_loss: 150.6552\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 257.5910 - val_loss: 159.7527\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 218.4381 - val_loss: 144.5077\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 313.2037 - val_loss: 227.8954\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 298.8885 - val_loss: 331.8627\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 227.0903 - val_loss: 178.9020\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 236.5877 - val_loss: 156.7816\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 261.8752 - val_loss: 199.4874\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 402.7764 - val_loss: 151.7152\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 198.5331 - val_loss: 176.1143\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 218.4749 - val_loss: 147.3485\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 392.0502 - val_loss: 199.4840\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 234.0681 - val_loss: 206.0612\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 237.7096 - val_loss: 262.1691\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 276.0766 - val_loss: 522.3673\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 256.4847 - val_loss: 199.3354\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 222.735 - 1s 63us/step - loss: 226.8002 - val_loss: 253.1686\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 418.2196 - val_loss: 656.8320\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 299.1519 - val_loss: 140.4584\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 402.0501 - val_loss: 172.7516\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 236.2314 - val_loss: 142.7737\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 255.7416 - val_loss: 146.4782\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 217.9192 - val_loss: 138.9704\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 212.3642 - val_loss: 149.4696\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 230.0715 - val_loss: 170.3679\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 291.4498 - val_loss: 238.1241\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 227.6904 - val_loss: 211.7738\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 265.0326 - val_loss: 190.0394\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 253.3642 - val_loss: 220.7440\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 297.0139 - val_loss: 270.2843\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 240.6511 - val_loss: 180.3631\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 276.1052 - val_loss: 250.3458\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 247.4661 - val_loss: 153.6876\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 225.2856 - val_loss: 147.7842\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 240.6790 - val_loss: 163.7948\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.7016 - val_loss: 142.4509\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 233.5775 - val_loss: 181.1357\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 294.0758 - val_loss: 159.5149\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 416.3247 - val_loss: 568.6770\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 415.9093 - val_loss: 148.7729\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 261.8126 - val_loss: 150.8473\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 197.5920 - val_loss: 201.2503\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 205.2948 - val_loss: 165.5373\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 219.9729 - val_loss: 154.1543\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 222.7437 - val_loss: 182.9700\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 338.6831 - val_loss: 266.4432\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 222.1174 - val_loss: 186.3445\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 220.8524 - val_loss: 150.5883\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 287.1976 - val_loss: 197.9064\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 443.5956 - val_loss: 165.7052\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 224.2407 - val_loss: 133.5844\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 184.7548 - val_loss: 418.7655\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 269.2977 - val_loss: 156.2683\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 200.2529 - val_loss: 156.4640\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 224.9786 - val_loss: 340.2034\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 399.4308 - val_loss: 140.9487\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 220.1270 - val_loss: 141.1308\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 313.3283 - val_loss: 351.1550\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 332.1453 - val_loss: 162.9125\n",
      "Epoch 229/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 66us/step - loss: 453.9536 - val_loss: 169.4713\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 213.3173 - val_loss: 218.4088\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 271.9884 - val_loss: 136.6038\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 294.5456 - val_loss: 143.1268\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.3190 - val_loss: 146.8503\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 222.3196 - val_loss: 225.3905\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 237.8777 - val_loss: 206.7465\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 363.0446 - val_loss: 257.2406\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 244.0757 - val_loss: 143.6416\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 222.1632 - val_loss: 187.6011\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 202.3036 - val_loss: 145.5489\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 234.9292 - val_loss: 186.5609\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 275.8483 - val_loss: 155.2796\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 200.7841 - val_loss: 165.6784\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 212.8928 - val_loss: 165.3628\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 199.8615 - val_loss: 154.5770\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 261.9905 - val_loss: 641.7242\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 225.2475 - val_loss: 164.2414\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 236.5909 - val_loss: 186.5508\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 371.9206 - val_loss: 228.4674\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 260.0265 - val_loss: 402.8602\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 255.3675 - val_loss: 270.6119\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 265.0452 - val_loss: 271.9364\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 234.5325 - val_loss: 165.1883\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 234.7465 - val_loss: 162.1840\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 273.0087 - val_loss: 167.5245\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 230.2664 - val_loss: 135.9415\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 278.9230 - val_loss: 335.7467\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 337.7560 - val_loss: 138.9292\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 227.4495 - val_loss: 168.0275\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 201.2151 - val_loss: 152.0263\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 229.5045 - val_loss: 157.0125\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 439.3673 - val_loss: 135.1536\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 232.4405 - val_loss: 146.2928\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 221.3764 - val_loss: 152.3291\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 196.9717 - val_loss: 172.5361\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 228.3425 - val_loss: 205.9074\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 227.3041 - val_loss: 134.1185\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 203.7558 - val_loss: 168.0628\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 227.7340 - val_loss: 134.6452\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 238.2356 - val_loss: 139.1260\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 205.9037 - val_loss: 187.3127\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 227.4010 - val_loss: 129.4666\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 284.5478 - val_loss: 170.2160\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 339.5651 - val_loss: 135.7847\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 230.8450 - val_loss: 188.3254\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 203.7908 - val_loss: 129.3456\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 189.8223 - val_loss: 199.1685\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 234.3464 - val_loss: 173.9642\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 262.6954 - val_loss: 131.4474\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 219.3513 - val_loss: 182.1374\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 306.7228 - val_loss: 956.2544\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 224.1201 - val_loss: 134.9123\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.8921 - val_loss: 160.5896\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 209.8779 - val_loss: 136.4022\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 228.1469 - val_loss: 132.0286\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 186.3954 - val_loss: 349.6186\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 214.6086 - val_loss: 257.3539\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 367.1593 - val_loss: 163.7547\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 459.9874 - val_loss: 656.1722\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 252.9006 - val_loss: 149.1839\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 189.7336 - val_loss: 145.4284\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 204.2109 - val_loss: 209.7546\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 245.7083 - val_loss: 255.5362\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 240.3858 - val_loss: 143.6445\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 201.2271 - val_loss: 239.9674\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 213.9721 - val_loss: 139.3276\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 196.5292 - val_loss: 164.6985\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 397.0812 - val_loss: 205.3589\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 208.0619 - val_loss: 130.8800\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.9656 - val_loss: 130.6705\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 199.9608 - val_loss: 356.1370\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 206.2965 - val_loss: 179.5660\n",
      "Epoch 302/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 238.5655 - val_loss: 131.8385\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 221.6257 - val_loss: 164.3737\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 204.2323 - val_loss: 141.3882\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 268.7853 - val_loss: 163.0756\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 279.3873 - val_loss: 223.2790\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 269.4220 - val_loss: 200.6041\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.6946 - val_loss: 143.3133\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 238.3977 - val_loss: 126.0322\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 207.7879 - val_loss: 126.5211\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.2903 - val_loss: 139.0960\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 258.1387 - val_loss: 321.7565\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 384.3762 - val_loss: 260.9839\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 245.4291 - val_loss: 256.6275\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 189.1829 - val_loss: 167.1363\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 209.4195 - val_loss: 138.9878\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.2379 - val_loss: 142.1813\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 176.5546 - val_loss: 203.6701\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 239.8433 - val_loss: 1108.3764\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 240.7113 - val_loss: 157.9364\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 267.0174 - val_loss: 140.0946\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 231.8657 - val_loss: 297.0358\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 235.2003 - val_loss: 129.7200\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 220.3195 - val_loss: 125.5723\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.9957 - val_loss: 142.8823\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 235.3718 - val_loss: 130.1609\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 217.2005 - val_loss: 165.0603\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 257.4428 - val_loss: 305.8864\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 218.6630 - val_loss: 190.5719\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 194.7297 - val_loss: 142.0775\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 207.3078 - val_loss: 291.8803\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 192.8139 - val_loss: 162.6795\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 177.9703 - val_loss: 137.6822\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 194.8998 - val_loss: 121.5074\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 219.2141 - val_loss: 194.2226\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 200.5241 - val_loss: 126.2577\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.9335 - val_loss: 186.1316\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 217.7827 - val_loss: 168.6956\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 229.5452 - val_loss: 253.9504\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 192.3616 - val_loss: 139.4963\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 286.7813 - val_loss: 151.6518\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 228.7780 - val_loss: 161.3211\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 190.4013 - val_loss: 153.6882\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 227.4183 - val_loss: 187.4721\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 175.4399 - val_loss: 141.1262\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 194.1013 - val_loss: 149.4898\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.6067 - val_loss: 174.2993\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.3299 - val_loss: 332.8076\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 252.6187 - val_loss: 158.9416\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 260.3043 - val_loss: 421.0437\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 216.9870 - val_loss: 164.4330\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 204.4398 - val_loss: 149.3503\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 184.3983 - val_loss: 156.5624\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 190.9076 - val_loss: 124.2654\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 175.9743 - val_loss: 170.6856\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.2235 - val_loss: 334.9705\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 222.9138 - val_loss: 297.2143\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 359.2067 - val_loss: 150.0302\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 169.3659 - val_loss: 123.9657\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 237.7130 - val_loss: 128.4852\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 227.1542 - val_loss: 134.2920\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 189.3191 - val_loss: 125.3451\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.0375 - val_loss: 121.8755\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 164.1004 - val_loss: 136.5854\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 197.2610 - val_loss: 120.2440\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 196.5361 - val_loss: 1938.6805\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 334.2925 - val_loss: 131.4930\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.9765 - val_loss: 147.3215\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.2454 - val_loss: 294.1255\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 167.4995 - val_loss: 136.7948\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 175.0504 - val_loss: 155.1703\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 194.3542 - val_loss: 142.0855\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 293.5552 - val_loss: 161.1524\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 233.9257 - val_loss: 136.7446\n",
      "Epoch 375/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 178.8658 - val_loss: 135.6188\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.4919 - val_loss: 126.3184\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.5439 - val_loss: 149.8458\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 193.4130 - val_loss: 156.4371\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 186.9096 - val_loss: 155.1503\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 206.1238 - val_loss: 203.5389\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 230.9111 - val_loss: 147.5136\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 183.2073 - val_loss: 125.8672\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 523.6137 - val_loss: 551.7852\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 204.5794 - val_loss: 134.7898\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 176.6577 - val_loss: 158.9951\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 174.7453 - val_loss: 141.2487\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.8056 - val_loss: 121.5587\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.1813 - val_loss: 138.2661\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 168.3273 - val_loss: 237.0710\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 194.4090 - val_loss: 347.6701\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 194.1199 - val_loss: 200.1301\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 397.4234 - val_loss: 122.8104\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.4286 - val_loss: 184.2678\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 168.7425 - val_loss: 119.7073\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 174.8118 - val_loss: 143.5857\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 175.4883 - val_loss: 163.4864\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.7985 - val_loss: 121.1396\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.8136 - val_loss: 134.1588\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 252.2221 - val_loss: 213.9126\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 193.6264 - val_loss: 265.3422\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 205.2868 - val_loss: 164.2522\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 187.7634 - val_loss: 131.4742\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 185.1532 - val_loss: 411.7395\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 182.7675 - val_loss: 122.0354\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 200.8194 - val_loss: 133.6964\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 234.1948 - val_loss: 132.9290\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.0231 - val_loss: 200.5248\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.0940 - val_loss: 155.2377\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 248.6332 - val_loss: 139.0663\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 243.0949 - val_loss: 125.2059\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 162.4288 - val_loss: 119.8789\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 171.2588 - val_loss: 122.9303\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.8089 - val_loss: 194.6261\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 214.5662 - val_loss: 128.3015\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 255.1431 - val_loss: 129.6455\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.1403 - val_loss: 124.3112\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.6819 - val_loss: 125.4497\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 207.3227 - val_loss: 168.7968\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.3232 - val_loss: 125.0453\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 201.2504 - val_loss: 150.6205\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 227.9727 - val_loss: 127.2156\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 615.8513 - val_loss: 438.1721\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 344.0716 - val_loss: 169.7276\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 262.3282 - val_loss: 210.8691\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 233.0743 - val_loss: 153.1418\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 241.3473 - val_loss: 361.6562\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 217.1263 - val_loss: 136.9488\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 217.1311 - val_loss: 187.7793\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 212.7096 - val_loss: 143.5709\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 246.7933 - val_loss: 172.4154\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 199.9713 - val_loss: 204.7953\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 213.2488 - val_loss: 177.5160\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 190.9178 - val_loss: 162.6646\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 212.9665 - val_loss: 179.5961\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 193.7429 - val_loss: 142.9528\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 216.9343 - val_loss: 169.2051\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 191.3862 - val_loss: 139.7671\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 213.3781 - val_loss: 151.7375\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 264.5026 - val_loss: 181.3412\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 191.3890 - val_loss: 175.1336\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 201.8275 - val_loss: 135.9974\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 201.5644 - val_loss: 127.5681\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 218.9984 - val_loss: 144.8295\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 234.8362 - val_loss: 141.0087\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 179.2465 - val_loss: 130.8954\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 215.4476 - val_loss: 179.1431\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 208.4700 - val_loss: 245.7127\n",
      "Epoch 448/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 201.2151 - val_loss: 130.5956\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.3105 - val_loss: 149.8902\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 206.7138 - val_loss: 411.3604\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 231.5458 - val_loss: 140.2764\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 189.0489 - val_loss: 132.4570\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 209.1929 - val_loss: 136.0600\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 240.2514 - val_loss: 322.5427\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 194.0556 - val_loss: 143.1667\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 206.8087 - val_loss: 130.4347\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 201.8747 - val_loss: 153.2741\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 171.2466 - val_loss: 140.0928\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 183.3793 - val_loss: 172.5396\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 234.4699 - val_loss: 145.2759\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 190.4537 - val_loss: 133.8365\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 179.0265 - val_loss: 140.1516\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 194.4944 - val_loss: 181.4856\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 178.5661 - val_loss: 131.5557\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 206.5265 - val_loss: 233.4438\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 222.2261 - val_loss: 137.0022\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 197.4301 - val_loss: 162.1618\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 284.6372 - val_loss: 147.5946\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 184.3693 - val_loss: 165.1050\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 200.4276 - val_loss: 132.7328\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 174.6908 - val_loss: 164.2180\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 226.1638 - val_loss: 130.4091\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.4003 - val_loss: 162.3227\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 202.7025 - val_loss: 129.9843\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 186.2027 - val_loss: 128.0936\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.9055 - val_loss: 213.7468\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 190.7728 - val_loss: 125.5205\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 186.9318 - val_loss: 131.0687\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 179.1797 - val_loss: 155.6179\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.8274 - val_loss: 125.3375\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 185.4879 - val_loss: 336.3299\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 188.6790 - val_loss: 139.4177\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 185.1386 - val_loss: 212.4627\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 216.2112 - val_loss: 151.2776\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.9690 - val_loss: 133.5258\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 196.2075 - val_loss: 128.5163\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.4314 - val_loss: 166.2758\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 182.0267 - val_loss: 143.5677\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 177.7197 - val_loss: 156.8059\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 213.2229 - val_loss: 190.8334\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 183.0606 - val_loss: 312.6606\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 283.7995 - val_loss: 147.5077\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.2030 - val_loss: 159.5720\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 232.0221 - val_loss: 155.6242\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.4385 - val_loss: 134.6965\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 193.4767 - val_loss: 166.9555\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 252.6739 - val_loss: 187.0062\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 222.5413 - val_loss: 183.7289\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 208.4700 - val_loss: 162.4434\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 185.3677 - val_loss: 139.3003\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 186.6991 - val_loss: 126.4625\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 327.7992 - val_loss: 146.7801\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.1061 - val_loss: 227.1293\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.3534 - val_loss: 139.4569\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.9462 - val_loss: 123.3484\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.1410 - val_loss: 147.4120\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.0894 - val_loss: 120.0862\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.5882 - val_loss: 145.2925\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 189.4450 - val_loss: 134.6589\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.9243 - val_loss: 351.3105\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 211.0320 - val_loss: 121.7907\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.3937 - val_loss: 126.2707\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.5932 - val_loss: 142.8368\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 202.2837 - val_loss: 507.6952\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 212.8738 - val_loss: 119.8842\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 246.1198 - val_loss: 265.5166\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.9516 - val_loss: 142.1869\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.2647 - val_loss: 177.6734\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.1356 - val_loss: 126.7740\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.1048 - val_loss: 140.2983\n",
      "Epoch 521/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 70us/step - loss: 182.3378 - val_loss: 189.8558\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 196.5714 - val_loss: 117.5478\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 169.7881 - val_loss: 141.7129\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 195.2988 - val_loss: 124.2668\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 178.0565 - val_loss: 133.7758\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 178.8391 - val_loss: 162.0800\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.8124 - val_loss: 120.8780\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.9704 - val_loss: 120.7419\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 327.4508 - val_loss: 217.4592\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 198.8581 - val_loss: 174.4858\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.0119 - val_loss: 127.0350\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.6993 - val_loss: 129.3492\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 178.9506 - val_loss: 127.3354\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 156.4597 - val_loss: 116.9801\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.2837 - val_loss: 143.1040\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.8053 - val_loss: 192.7822\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 156.7956 - val_loss: 116.2616\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.5386 - val_loss: 129.6213\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 196.1591 - val_loss: 142.4919\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.8269 - val_loss: 199.8847\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 190.6891 - val_loss: 133.3150\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 193.0094 - val_loss: 299.2371\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 209.0528 - val_loss: 123.8727\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.4937 - val_loss: 178.2014\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 189.5478 - val_loss: 142.4634\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 161.8219 - val_loss: 169.3209\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.1830 - val_loss: 139.9753\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 315.8323 - val_loss: 220.5683\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.6559 - val_loss: 120.1647\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 197.5984 - val_loss: 136.2806\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.3061 - val_loss: 130.6428\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.1538 - val_loss: 129.0313\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.7538 - val_loss: 116.3544\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 179.4603 - val_loss: 178.2401\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.5423 - val_loss: 288.9417\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 177.8781 - val_loss: 175.6608\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 231.4970 - val_loss: 158.4722\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 171.2346 - val_loss: 118.5617\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.2915 - val_loss: 159.2445\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.3427 - val_loss: 131.0012\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 170.7338 - val_loss: 124.8373\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.6226 - val_loss: 138.6274\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.0327 - val_loss: 152.6046\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.8637 - val_loss: 130.5106\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 187.7471 - val_loss: 189.5904\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.2093 - val_loss: 136.5156\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.5631 - val_loss: 134.1728\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.3714 - val_loss: 120.9586\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 262.3807 - val_loss: 140.1110\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.7395 - val_loss: 133.8444\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 152.3672 - val_loss: 113.5568\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.3336 - val_loss: 194.6591\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.3939 - val_loss: 173.7818\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.8009 - val_loss: 125.4324\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 163.0766 - val_loss: 123.7953\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 211.6613 - val_loss: 141.7478\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.3453 - val_loss: 361.4494\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 319.1429 - val_loss: 124.7664\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.0888 - val_loss: 163.9169\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.7388 - val_loss: 153.9758\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 172.1962 - val_loss: 136.2800\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.5427 - val_loss: 185.1672\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.4241 - val_loss: 132.6931\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 184.7693 - val_loss: 125.4731\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 193.1374 - val_loss: 161.0495\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 160.8397 - val_loss: 130.2381\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.0250 - val_loss: 122.3967\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 195.7458 - val_loss: 200.8298\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 180.7252 - val_loss: 268.6475\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 163.7360 - val_loss: 137.1145\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 178.1536 - val_loss: 239.3942\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 165.6865 - val_loss: 203.8380\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.8374 - val_loss: 139.2257\n",
      "Epoch 594/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 223.3408 - val_loss: 138.6026\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 180.0830 - val_loss: 191.8807\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 173.7929 - val_loss: 303.5057\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 207.7372 - val_loss: 183.1510\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 217.4120 - val_loss: 123.7280\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 171.6423 - val_loss: 143.0128\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 171.6756 - val_loss: 138.7894\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 216.7585 - val_loss: 128.6068\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.5427 - val_loss: 115.2519\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 190.8632 - val_loss: 145.2807\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 172.7715 - val_loss: 120.5362\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 232.8365 - val_loss: 132.1894\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.9112 - val_loss: 117.3164\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.1165 - val_loss: 116.6241\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.3605 - val_loss: 123.7473\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.1651 - val_loss: 119.9928\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.8203 - val_loss: 222.2386\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 174.9299 - val_loss: 507.4190\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 191.1585 - val_loss: 120.7381\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 262.6415 - val_loss: 128.0810\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.2084 - val_loss: 122.6249\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 204.6536 - val_loss: 115.8994\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.0148 - val_loss: 170.2457\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.1688 - val_loss: 154.8583\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 194.5895 - val_loss: 122.3217\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 173.3454 - val_loss: 125.7884\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 163.6077 - val_loss: 160.7528\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 250.4331 - val_loss: 493.3393\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 194.6540 - val_loss: 128.0650\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 204.7549 - val_loss: 139.1939\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.4563 - val_loss: 180.4636\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.3112 - val_loss: 151.6455\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.5905 - val_loss: 224.1031\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.0933 - val_loss: 177.4599\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.5433 - val_loss: 138.4164\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 415.4690 - val_loss: 165.3178\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 202.1806 - val_loss: 246.3829\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.4930 - val_loss: 128.2464\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 207.6106 - val_loss: 381.0533\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 166.5306 - val_loss: 134.1468\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.2671 - val_loss: 169.4645\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 224.3463 - val_loss: 146.1283\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 202.1363 - val_loss: 130.1398\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 187.1370 - val_loss: 160.9037\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 219.2441 - val_loss: 120.8595\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 170.8668 - val_loss: 177.2746\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.2564 - val_loss: 117.4550\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 203.4967 - val_loss: 129.3458\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 154.2740 - val_loss: 125.1392\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 158.0535 - val_loss: 123.9794\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.8664 - val_loss: 135.6179\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 163.3902 - val_loss: 135.0573\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.8472 - val_loss: 119.0729\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.1392 - val_loss: 172.3521\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 206.7643 - val_loss: 147.0916\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 203.5988 - val_loss: 120.6295\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.5693 - val_loss: 167.8178\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 173.4840 - val_loss: 131.7274\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 219.3596 - val_loss: 128.4141\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.8126 - val_loss: 114.9151\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 183.9346 - val_loss: 132.9303\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.6233 - val_loss: 140.6139\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.2224 - val_loss: 205.0011\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.1963 - val_loss: 178.4150\n",
      "Epoch 658/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 188.2536 - val_loss: 117.5874\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 165.6662 - val_loss: 118.6797\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 168.2750 - val_loss: 195.6816\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 197.9778 - val_loss: 121.6785\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.2769 - val_loss: 118.2494\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.4918 - val_loss: 118.8050\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.2736 - val_loss: 251.0566\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 184.9480 - val_loss: 115.0826\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.9492 - val_loss: 114.5425\n",
      "Epoch 667/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.3930 - val_loss: 157.8922\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.1022 - val_loss: 150.2590\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.1808 - val_loss: 146.5573\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.0668 - val_loss: 127.1342\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.7443 - val_loss: 153.7800\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.5940 - val_loss: 115.2044\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 246.3374 - val_loss: 165.9971\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 194.8399 - val_loss: 162.4198\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.6141 - val_loss: 205.4609\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.7090 - val_loss: 213.8783\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.5606 - val_loss: 138.5759\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.0044 - val_loss: 137.5805\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.2926 - val_loss: 144.0943\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.9123 - val_loss: 115.9367\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.6604 - val_loss: 128.5933\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.1676 - val_loss: 125.8758\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.8085 - val_loss: 120.8031\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.5952 - val_loss: 117.3794\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.1834 - val_loss: 202.3251\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.3668 - val_loss: 116.0957\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 257.9934 - val_loss: 190.3702\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.3870 - val_loss: 151.6044\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 196.5004 - val_loss: 128.2175\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.9292 - val_loss: 132.0611\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.3010 - val_loss: 125.9041\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.4114 - val_loss: 131.2973\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.5525 - val_loss: 121.3787\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 254.0974 - val_loss: 152.5044\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 187.0130 - val_loss: 120.6571\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 323.3641 - val_loss: 141.9486\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 166.4442 - val_loss: 148.1315\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.2054 - val_loss: 149.1767\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.4232 - val_loss: 166.4978\n",
      "Epoch 700/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 195.7720 - val_loss: 155.3255\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 161.7680 - val_loss: 115.0325\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 179.7213 - val_loss: 118.4731\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 163.3952 - val_loss: 117.6999\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 153.7297 - val_loss: 117.0746\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.6324 - val_loss: 136.6448\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.8457 - val_loss: 128.7057\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.5577 - val_loss: 119.2443\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 159.2570 - val_loss: 177.6915\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.1368 - val_loss: 122.9802\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.2905 - val_loss: 134.7635\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 210.1168 - val_loss: 131.3706\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.3589 - val_loss: 132.0325\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 191.5524 - val_loss: 176.9516\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.3082 - val_loss: 113.9086\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.9610 - val_loss: 114.1867\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.8249 - val_loss: 116.6417\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.4718 - val_loss: 118.7043\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.2607 - val_loss: 156.2531\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 165.4042 - val_loss: 138.2931\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 259.5830 - val_loss: 143.3238\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 201.3193 - val_loss: 427.4694\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.2110 - val_loss: 115.3099\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 185.7645 - val_loss: 121.4558\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.5133 - val_loss: 125.4287\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.8558 - val_loss: 136.8061\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 210.3003 - val_loss: 310.2391\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 179.2915 - val_loss: 121.8900\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.9975 - val_loss: 166.5348\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.9281 - val_loss: 269.8365\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 188.4080 - val_loss: 168.0872\n",
      "Epoch 731/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.7057 - val_loss: 114.9592\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 220.7395 - val_loss: 180.5406\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.0676 - val_loss: 114.6536\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 154.4047 - val_loss: 137.6100\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.9026 - val_loss: 194.3365\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.9589 - val_loss: 135.4234\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.3995 - val_loss: 114.5936\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.2086 - val_loss: 127.9195\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 170.9253 - val_loss: 165.3184\n",
      "Epoch 740/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.6175 - val_loss: 135.7463\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 161.1475 - val_loss: 126.1872\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 245.1452 - val_loss: 157.5301\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.2168 - val_loss: 116.3354\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 184.4320 - val_loss: 146.7838\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.9461 - val_loss: 136.9069\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.7778 - val_loss: 175.6910\n",
      "Epoch 747/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 202.9596 - val_loss: 124.5241\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.0470 - val_loss: 116.0704\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 162.6591 - val_loss: 136.9379\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.7431 - val_loss: 294.0114\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 156.9371 - val_loss: 122.8234\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.8587 - val_loss: 119.4958\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 250.5986 - val_loss: 197.2635\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 238.2342 - val_loss: 169.2071\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 208.5347 - val_loss: 139.4814\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 226.7910 - val_loss: 257.9130\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 198.6801 - val_loss: 220.1681\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 187.3998 - val_loss: 199.7647\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 174.9512 - val_loss: 124.6764\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.2643 - val_loss: 135.8924\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 178.2143 - val_loss: 126.4988\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 201.3373 - val_loss: 123.1308\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 179.2354 - val_loss: 127.1190\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 209.4907 - val_loss: 204.2212\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 200.2141 - val_loss: 146.5017\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.4370 - val_loss: 166.9645\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.3775 - val_loss: 132.7652\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 214.0271 - val_loss: 163.5269\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.4664 - val_loss: 119.0973\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 182.6535 - val_loss: 145.1256\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.4702 - val_loss: 118.2216\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.4246 - val_loss: 162.9668\n",
      "Epoch 773/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 183.2479 - val_loss: 156.2116\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 166.3117 - val_loss: 126.2648\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.2559 - val_loss: 132.4983\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 170.2719 - val_loss: 119.0377\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.7604 - val_loss: 127.2049\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 160.0528 - val_loss: 117.9853\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.0939 - val_loss: 126.4816\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 163.7571 - val_loss: 130.5146\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 175.0113 - val_loss: 132.2788\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 188.1999 - val_loss: 125.8040\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.2618 - val_loss: 121.6117\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.7280 - val_loss: 125.9520\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 167.088 - 1s 63us/step - loss: 170.3039 - val_loss: 147.1411\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 202.7356 - val_loss: 121.6928\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.2608 - val_loss: 128.7574\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.8886 - val_loss: 147.4935\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.6743 - val_loss: 114.0625\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.7303 - val_loss: 183.6741\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.3592 - val_loss: 116.6006\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.2489 - val_loss: 187.7868\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 160.8257 - val_loss: 113.9080\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 171.3598 - val_loss: 135.7761\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.4562 - val_loss: 125.3751\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.5610 - val_loss: 150.5378\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.4225 - val_loss: 189.3627\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 213.1096 - val_loss: 187.6468\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.7109 - val_loss: 134.6312\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.2276 - val_loss: 151.6354\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 166.8723 - val_loss: 115.4452\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.9039 - val_loss: 153.5016\n",
      "Epoch 803/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 186.1302 - val_loss: 164.9566\n",
      "Epoch 804/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 154.9266 - val_loss: 112.9429\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.4269 - val_loss: 127.9899\n",
      "Epoch 806/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 175.5267 - val_loss: 142.1589\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.3091 - val_loss: 136.2023\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.8849 - val_loss: 114.7183\n",
      "Epoch 809/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.7558 - val_loss: 170.2429\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 252.3968 - val_loss: 149.8073\n",
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.8547 - val_loss: 140.0865\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 158.9048 - val_loss: 112.7270\n",
      "Epoch 813/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 64us/step - loss: 146.9382 - val_loss: 156.3826\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 211.6383 - val_loss: 129.1390\n",
      "Epoch 815/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.9307 - val_loss: 201.3227\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.9663 - val_loss: 118.1201\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.8196 - val_loss: 212.3495\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.2653 - val_loss: 121.6834\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.1791 - val_loss: 121.9468\n",
      "Epoch 820/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.3947 - val_loss: 112.7426\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 209.0472 - val_loss: 121.7208\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 143.4429 - val_loss: 121.2978\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 148.9484 - val_loss: 119.7466\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.0866 - val_loss: 124.1447\n",
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.4978 - val_loss: 118.9674\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.7612 - val_loss: 133.7442\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.2239 - val_loss: 118.8166\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.0394 - val_loss: 114.9843\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 184.6573 - val_loss: 114.9940\n",
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.3682 - val_loss: 123.9182\n",
      "Epoch 831/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 190.4917 - val_loss: 127.0072\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 170.2846 - val_loss: 128.3410\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.9561 - val_loss: 171.9641\n",
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.1231 - val_loss: 129.6249\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.2103 - val_loss: 118.9437\n",
      "Epoch 836/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 158.7425 - val_loss: 156.5940\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 166.7228 - val_loss: 148.9419\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.2689 - val_loss: 117.2493\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.3358 - val_loss: 116.4146\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.4734 - val_loss: 135.4577\n",
      "Epoch 841/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.8155 - val_loss: 192.5738\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 292.7638 - val_loss: 227.0078\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 189.0364 - val_loss: 126.1841\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.6841 - val_loss: 120.8426\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.0652 - val_loss: 117.2595\n",
      "Epoch 846/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 164.4843 - val_loss: 111.4791\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.2516 - val_loss: 129.4792\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.9476 - val_loss: 154.6889\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.2833 - val_loss: 133.0005\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.6469 - val_loss: 225.2646\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.4011 - val_loss: 179.6305\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 159.1248 - val_loss: 134.6250\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 163.2727 - val_loss: 115.9819\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.4192 - val_loss: 337.0410\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 185.2006 - val_loss: 128.4159\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.3852 - val_loss: 146.9446\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.5067 - val_loss: 123.5269\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.7843 - val_loss: 146.8300\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.3303 - val_loss: 119.3949\n",
      "Epoch 860/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.7740 - val_loss: 123.6827\n",
      "Epoch 861/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 171.1598 - val_loss: 122.8566\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.5825 - val_loss: 124.8733\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 182.1030 - val_loss: 268.1531\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.3047 - val_loss: 120.0146\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.2686 - val_loss: 117.3523\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 171.2460 - val_loss: 125.5474\n",
      "Epoch 867/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.1424 - val_loss: 150.0570\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.3816 - val_loss: 116.0035\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 251.1892 - val_loss: 270.8359\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 185.7618 - val_loss: 123.0031\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.3856 - val_loss: 126.2852\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.7334 - val_loss: 163.2544\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 175.2674 - val_loss: 126.4503\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.0945 - val_loss: 112.8128\n",
      "Epoch 875/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 169.1240 - val_loss: 311.7999\n",
      "Epoch 876/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.7452 - val_loss: 120.6072\n",
      "Epoch 877/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.8258 - val_loss: 117.2410\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 183.5873 - val_loss: 174.0242\n",
      "Epoch 879/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.2992 - val_loss: 134.4331\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 163.2105 - val_loss: 151.8106\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 169.6944 - val_loss: 114.5662\n",
      "Epoch 882/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 160.1971 - val_loss: 179.0072\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.1978 - val_loss: 115.3651\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.8847 - val_loss: 128.8339\n",
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.7169 - val_loss: 141.6694\n",
      "Epoch 886/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.8569 - val_loss: 124.5197\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.2545 - val_loss: 117.6140\n",
      "Epoch 888/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.6256 - val_loss: 130.3232\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 174.9515 - val_loss: 632.2531\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 182.2745 - val_loss: 133.9704\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.8220 - val_loss: 155.1381\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.3535 - val_loss: 120.2004\n",
      "Epoch 893/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 166.9701 - val_loss: 116.6707\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.5356 - val_loss: 234.4829\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 206.3652 - val_loss: 139.2847\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.9402 - val_loss: 200.8022\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.7158 - val_loss: 115.7910\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 201.3961 - val_loss: 231.8133\n",
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.9817 - val_loss: 150.2194\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.9375 - val_loss: 179.3675\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.4610 - val_loss: 144.4322\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.3804 - val_loss: 113.4933\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.8201 - val_loss: 117.7259\n",
      "Epoch 904/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 189.9787 - val_loss: 157.7955\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.3795 - val_loss: 285.7688\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.4594 - val_loss: 119.2856\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.7598 - val_loss: 129.7164\n",
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.9776 - val_loss: 131.9661\n",
      "Epoch 909/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.1647 - val_loss: 165.2120\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.7090 - val_loss: 136.1419\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.6807 - val_loss: 146.2742\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.1829 - val_loss: 130.1105\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.7313 - val_loss: 118.2370\n",
      "Epoch 914/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.1605 - val_loss: 138.9783\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 192.6969 - val_loss: 128.4907\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.9259 - val_loss: 125.8967\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.0297 - val_loss: 114.3295\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.4594 - val_loss: 124.7904\n",
      "Epoch 919/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.7049 - val_loss: 178.6619\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.7324 - val_loss: 119.7928\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.0804 - val_loss: 116.8148\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.9069 - val_loss: 112.5022\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.0970 - val_loss: 144.6448\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.2370 - val_loss: 112.2487\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.2458 - val_loss: 127.6936\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.3312 - val_loss: 135.6681\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 184.9129 - val_loss: 151.7361\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.0105 - val_loss: 116.4504\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.6364 - val_loss: 126.1125\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.3378 - val_loss: 116.2297\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.6360 - val_loss: 126.6563\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.1535 - val_loss: 111.8054\n",
      "Epoch 933/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.4695 - val_loss: 215.7978\n",
      "Epoch 934/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 166.5179 - val_loss: 156.0624\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.3611 - val_loss: 129.7999\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.8423 - val_loss: 204.0819\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 356.1774 - val_loss: 127.4679\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 169.9162 - val_loss: 116.8388\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.4038 - val_loss: 113.7253\n",
      "Epoch 940/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.0489 - val_loss: 197.9205\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 156.7813 - val_loss: 124.3873\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 149.1037 - val_loss: 112.3287\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 196.1905 - val_loss: 201.5669\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.4601 - val_loss: 143.3284\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.8940 - val_loss: 130.0653\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.3429 - val_loss: 175.6681\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.7810 - val_loss: 191.0415\n",
      "Epoch 948/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.5823 - val_loss: 151.9729\n",
      "Epoch 949/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.7780 - val_loss: 153.7642\n",
      "Epoch 950/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.3737 - val_loss: 125.3456\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.0049 - val_loss: 111.9806\n",
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.2470 - val_loss: 136.2516\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.9443 - val_loss: 132.7642\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.4171 - val_loss: 118.9411\n",
      "Epoch 955/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.3158 - val_loss: 141.7824\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 175.8550 - val_loss: 119.6004\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.8010 - val_loss: 112.0102\n",
      "Epoch 958/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 279.4990 - val_loss: 115.8322\n",
      "Epoch 959/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.4296 - val_loss: 164.6282\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.8164 - val_loss: 131.1202\n",
      "Epoch 961/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.5654 - val_loss: 118.6252\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.2069 - val_loss: 132.4097\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 169.2482 - val_loss: 154.0720\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.1253 - val_loss: 118.9976\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 168.4095 - val_loss: 144.2610\n",
      "Epoch 966/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.6986 - val_loss: 116.2944\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.8851 - val_loss: 120.0812\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 196.7490 - val_loss: 635.3893\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 212.6301 - val_loss: 119.7953\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.9057 - val_loss: 113.6915\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.5812 - val_loss: 113.2891\n",
      "Epoch 972/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.3247 - val_loss: 124.1761\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.4427 - val_loss: 112.9280\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.4863 - val_loss: 154.4805\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.6627 - val_loss: 116.9233\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.4999 - val_loss: 138.8143\n",
      "Epoch 977/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.9352 - val_loss: 128.1740\n",
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.0009 - val_loss: 122.1943\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.2399 - val_loss: 127.8789\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 242.1058 - val_loss: 148.5370\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.8020 - val_loss: 124.3594\n",
      "Epoch 982/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.2713 - val_loss: 156.8937\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.0485 - val_loss: 119.9591\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.4416 - val_loss: 115.7711\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.3866 - val_loss: 145.9961\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.0528 - val_loss: 158.9219\n",
      "Epoch 987/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.2364 - val_loss: 114.8888\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.6317 - val_loss: 129.8904\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.8600 - val_loss: 119.9156\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.1872 - val_loss: 120.5099\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 170.2131 - val_loss: 134.6729\n",
      "Epoch 992/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.9235 - val_loss: 118.2339\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.4219 - val_loss: 133.0864\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.5157 - val_loss: 309.3665\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.6137 - val_loss: 119.6293\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.1219 - val_loss: 146.8292\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 229.7699 - val_loss: 164.5347\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.1719 - val_loss: 119.2917\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.2029 - val_loss: 134.7401\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.3177 - val_loss: 121.3598\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 155.6053 - val_loss: 127.6678\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 156.7777 - val_loss: 170.8096\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 141.572 - 1s 76us/step - loss: 146.8468 - val_loss: 125.5173\n",
      "Epoch 1004/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.1286 - val_loss: 302.9070\n",
      "Epoch 1005/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.2274 - val_loss: 196.2188\n",
      "Epoch 1006/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.5250 - val_loss: 140.5493\n",
      "Epoch 1007/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 189.5526 - val_loss: 116.1729\n",
      "Epoch 1008/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.1139 - val_loss: 124.5505\n",
      "Epoch 1009/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.4186 - val_loss: 135.4258\n",
      "Epoch 1010/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.8700 - val_loss: 134.9607\n",
      "Epoch 1011/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.7614 - val_loss: 112.7084\n",
      "Epoch 1012/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.6963 - val_loss: 163.6087\n",
      "Epoch 1013/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.9597 - val_loss: 112.0818\n",
      "Epoch 1014/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.5821 - val_loss: 120.0812\n",
      "Epoch 1015/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 210.6121 - val_loss: 140.2954\n",
      "Epoch 1016/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 239.1805 - val_loss: 114.4441\n",
      "Epoch 1017/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 180.5391 - val_loss: 121.5604\n",
      "Epoch 1018/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.3437 - val_loss: 133.5267\n",
      "Epoch 1019/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.9911 - val_loss: 170.8480\n",
      "Epoch 1020/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.5032 - val_loss: 135.2589\n",
      "Epoch 1021/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.4291 - val_loss: 129.7106\n",
      "Epoch 1022/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.5643 - val_loss: 134.7174\n",
      "Epoch 1023/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.9206 - val_loss: 114.2947\n",
      "Epoch 1024/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.8916 - val_loss: 128.0607\n",
      "Epoch 1025/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 178.2631 - val_loss: 122.6602\n",
      "Epoch 1026/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.2641 - val_loss: 150.9506\n",
      "Epoch 1027/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.0306 - val_loss: 130.4337\n",
      "Epoch 1028/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.9226 - val_loss: 174.2926\n",
      "Epoch 1029/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.6399 - val_loss: 143.0206\n",
      "Epoch 1030/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 201.2702 - val_loss: 138.0543\n",
      "Epoch 1031/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 156.2975 - val_loss: 123.2851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1032/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.3452 - val_loss: 154.9733\n",
      "Epoch 1033/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.1831 - val_loss: 128.6929\n",
      "Epoch 1034/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.6929 - val_loss: 124.3914\n",
      "Epoch 1035/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.5421 - val_loss: 114.2204\n",
      "Epoch 1036/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.7961 - val_loss: 133.5835\n",
      "Epoch 1037/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.6438 - val_loss: 185.7049\n",
      "Epoch 1038/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.8147 - val_loss: 161.2882\n",
      "Epoch 1039/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.2831 - val_loss: 158.5641\n",
      "Epoch 1040/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 318.4133 - val_loss: 170.1388\n",
      "Epoch 1041/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 175.6658 - val_loss: 160.6345\n",
      "Epoch 1042/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.1508 - val_loss: 193.3796\n",
      "Epoch 1043/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.5195 - val_loss: 143.7272\n",
      "Epoch 1044/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 189.2424 - val_loss: 139.8989\n",
      "Epoch 1045/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.5398 - val_loss: 155.5863\n",
      "Epoch 1046/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.6546 - val_loss: 133.3924\n",
      "Epoch 1047/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.4504 - val_loss: 111.6919\n",
      "Epoch 1048/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 197.2444 - val_loss: 149.6317\n",
      "Epoch 1049/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.2609 - val_loss: 126.4228\n",
      "Epoch 1050/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.6523 - val_loss: 127.4183\n",
      "Epoch 1051/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.0413 - val_loss: 128.5107\n",
      "Epoch 1052/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.6743 - val_loss: 272.6223\n",
      "Epoch 1053/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.7998 - val_loss: 123.2953\n",
      "Epoch 1054/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.8787 - val_loss: 161.5422\n",
      "Epoch 1055/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.9790 - val_loss: 225.9207\n",
      "Epoch 1056/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.0154 - val_loss: 121.2846\n",
      "Epoch 1057/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.3583 - val_loss: 116.4277\n",
      "Epoch 1058/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.4993 - val_loss: 112.0843\n",
      "Epoch 1059/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.5929 - val_loss: 141.0625\n",
      "Epoch 1060/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.2879 - val_loss: 115.2885\n",
      "Epoch 1061/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 216.4996 - val_loss: 199.6664\n",
      "Epoch 1062/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 144.3507 - val_loss: 116.7871\n",
      "Epoch 1063/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 148.9033 - val_loss: 238.7200\n",
      "Epoch 1064/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.0557 - val_loss: 129.8989\n",
      "Epoch 1065/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.9556 - val_loss: 136.2487\n",
      "Epoch 1066/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.7148 - val_loss: 154.5123\n",
      "Epoch 1067/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 223.7953 - val_loss: 213.6858\n",
      "Epoch 1068/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.4219 - val_loss: 132.4519\n",
      "Epoch 1069/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.4795 - val_loss: 128.8111\n",
      "Epoch 1070/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.6905 - val_loss: 139.8284\n",
      "Epoch 1071/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 150.8536 - val_loss: 123.5533\n",
      "Epoch 1072/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.2639 - val_loss: 162.1848\n",
      "Epoch 1073/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 181.7160 - val_loss: 145.5367\n",
      "Epoch 1074/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.8875 - val_loss: 138.5744\n",
      "Epoch 1075/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 155.0737 - val_loss: 125.3059\n",
      "Epoch 1076/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 156.7418 - val_loss: 136.5410\n",
      "Epoch 1077/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.0536 - val_loss: 146.3334\n",
      "Epoch 1078/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 219.5151 - val_loss: 112.6949\n",
      "Epoch 1079/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.3628 - val_loss: 116.6022\n",
      "Epoch 1080/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 158.0858 - val_loss: 124.2099\n",
      "Epoch 1081/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.9449 - val_loss: 120.0542\n",
      "Epoch 1082/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 174.9934 - val_loss: 125.9643\n",
      "Epoch 1083/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.1899 - val_loss: 116.4449\n",
      "Epoch 1084/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 147.8697 - val_loss: 115.8535\n",
      "Epoch 1085/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.0143 - val_loss: 140.4729\n",
      "Epoch 1086/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.4926 - val_loss: 153.4216\n",
      "Epoch 1087/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.1770 - val_loss: 115.4926\n",
      "Epoch 1088/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.8092 - val_loss: 193.5415\n",
      "Epoch 1089/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.2618 - val_loss: 121.8380\n",
      "Epoch 1090/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 154.489 - 1s 63us/step - loss: 153.7730 - val_loss: 121.5585\n",
      "Epoch 1091/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.0604 - val_loss: 140.6253\n",
      "Epoch 1092/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.6362 - val_loss: 115.4413\n",
      "Epoch 1093/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.8026 - val_loss: 156.6563\n",
      "Epoch 1094/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 204.1287 - val_loss: 200.8092\n",
      "Epoch 1095/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.3533 - val_loss: 120.5123\n",
      "Epoch 1096/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 158.7445 - val_loss: 120.9677\n",
      "Epoch 1097/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.3513 - val_loss: 137.5227\n",
      "Epoch 1098/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.4802 - val_loss: 130.1148\n",
      "Epoch 1099/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.4295 - val_loss: 186.8909\n",
      "Epoch 1100/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 174.2954 - val_loss: 127.9316\n",
      "Epoch 1101/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.1434 - val_loss: 117.3349\n",
      "Epoch 1102/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.5420 - val_loss: 183.2359\n",
      "Epoch 1103/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.9754 - val_loss: 129.9928\n",
      "Epoch 1104/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.5196 - val_loss: 114.9270\n",
      "Epoch 1105/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 160.0280 - val_loss: 144.1267\n",
      "Epoch 1106/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.3801 - val_loss: 132.2383\n",
      "Epoch 1107/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 281.8305 - val_loss: 130.8215\n",
      "Epoch 1108/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.2607 - val_loss: 115.2744\n",
      "Epoch 1109/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.6845 - val_loss: 127.2658\n",
      "Epoch 1110/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.7003 - val_loss: 114.3857\n",
      "Epoch 1111/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.4465 - val_loss: 120.3046\n",
      "Epoch 1112/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.0229 - val_loss: 114.3563\n",
      "Epoch 1113/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.6833 - val_loss: 188.0008\n",
      "Epoch 1114/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 185.1692 - val_loss: 125.9065\n",
      "Epoch 1115/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.2676 - val_loss: 116.7061\n",
      "Epoch 1116/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.2204 - val_loss: 114.7389\n",
      "Epoch 1117/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.2809 - val_loss: 121.1856\n",
      "Epoch 1118/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.9403 - val_loss: 128.6811\n",
      "Epoch 1119/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.8986 - val_loss: 149.0655\n",
      "Epoch 1120/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 159.2046 - val_loss: 119.9036\n",
      "Epoch 1121/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 267.6773 - val_loss: 117.4244\n",
      "Epoch 1122/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 144.6412 - val_loss: 119.8764\n",
      "Epoch 1123/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 138.2472 - val_loss: 117.1980\n",
      "Epoch 1124/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.0981 - val_loss: 118.1087\n",
      "Epoch 1125/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.9211 - val_loss: 181.4044\n",
      "Epoch 1126/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.1827 - val_loss: 118.8562\n",
      "Epoch 1127/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.5319 - val_loss: 187.6757\n",
      "Epoch 1128/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.2308 - val_loss: 112.5587\n",
      "Epoch 1129/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.0793 - val_loss: 135.5659\n",
      "Epoch 1130/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.5414 - val_loss: 120.4447\n",
      "Epoch 1131/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.9879 - val_loss: 189.7651\n",
      "Epoch 1132/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.7144 - val_loss: 124.4442\n",
      "Epoch 1133/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.8806 - val_loss: 113.2435\n",
      "Epoch 1134/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.9021 - val_loss: 116.8585\n",
      "Epoch 1135/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 188.6466 - val_loss: 126.6663\n",
      "Epoch 1136/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 152.4621 - val_loss: 119.5418\n",
      "Epoch 1137/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 164.4935 - val_loss: 111.6313\n",
      "Epoch 1138/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.1515 - val_loss: 132.4065\n",
      "Epoch 1139/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.5590 - val_loss: 156.1497\n",
      "Epoch 1140/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 173.6070 - val_loss: 148.9537\n",
      "Epoch 1141/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.9666 - val_loss: 123.0836\n",
      "Epoch 1142/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 197.0349 - val_loss: 115.6514\n",
      "Epoch 1143/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.4973 - val_loss: 122.9249\n",
      "Epoch 1144/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.6567 - val_loss: 118.7242\n",
      "Epoch 1145/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 157.7094 - val_loss: 126.6042\n",
      "Epoch 1146/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 156.5848 - val_loss: 168.9168\n",
      "Epoch 1147/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 183.7493 - val_loss: 342.7993\n",
      "Epoch 1148/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 157.7134 - val_loss: 111.0459\n",
      "Epoch 1149/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 152.0990 - val_loss: 134.5051\n",
      "Epoch 1150/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 155.2971 - val_loss: 181.2435\n",
      "Epoch 1151/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 146.4275 - val_loss: 129.1865\n",
      "Epoch 1152/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 143.4640 - val_loss: 111.7104\n",
      "Epoch 1153/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.1560 - val_loss: 130.1092\n",
      "Epoch 1154/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 179.4727 - val_loss: 123.9197\n",
      "Epoch 1155/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 158.6211 - val_loss: 185.1500\n",
      "Epoch 1156/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.7727 - val_loss: 169.8015\n",
      "Epoch 1157/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 146.9783 - val_loss: 265.5844\n",
      "Epoch 1158/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 166.9447 - val_loss: 114.1441\n",
      "Epoch 1159/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 221.0384 - val_loss: 148.6534\n",
      "Epoch 1160/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 173.0823 - val_loss: 126.4469\n",
      "Epoch 1161/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.2863 - val_loss: 194.7497\n",
      "Epoch 1162/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 154.5866 - val_loss: 127.3313\n",
      "Epoch 1163/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 141.4885 - val_loss: 121.0858\n",
      "Epoch 1164/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 165.9312 - val_loss: 558.2174\n",
      "Epoch 1165/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 158.9120 - val_loss: 126.1341\n",
      "Epoch 1166/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 149.2927 - val_loss: 151.3881\n",
      "Epoch 1167/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.2255 - val_loss: 118.6810\n",
      "Epoch 1168/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 158.1179 - val_loss: 126.6181\n",
      "Epoch 1169/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.7185 - val_loss: 117.0253\n",
      "Epoch 1170/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.7060 - val_loss: 127.7296\n",
      "Epoch 1171/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.1815 - val_loss: 193.1176\n",
      "Epoch 1172/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.4008 - val_loss: 131.5462\n",
      "Epoch 1173/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.6751 - val_loss: 125.0114\n",
      "Epoch 1174/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 173.0982 - val_loss: 125.1666\n",
      "Epoch 1175/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.4748 - val_loss: 120.9765\n",
      "Epoch 1176/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 201.4126 - val_loss: 155.8125\n",
      "Epoch 1177/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.3535 - val_loss: 123.4637\n",
      "Epoch 1178/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.2688 - val_loss: 154.8203\n",
      "Epoch 1179/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 160.6758 - val_loss: 120.5632\n",
      "Epoch 1180/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.4906 - val_loss: 139.9116\n",
      "Epoch 1181/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.2771 - val_loss: 144.8793\n",
      "Epoch 1182/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.7745 - val_loss: 159.4545\n",
      "Epoch 1183/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 190.7470 - val_loss: 127.8319\n",
      "Epoch 1184/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.1875 - val_loss: 132.4152\n",
      "Epoch 1185/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 194.1437 - val_loss: 149.1036\n",
      "Epoch 1186/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.6125 - val_loss: 116.6211\n",
      "Epoch 1187/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.7154 - val_loss: 134.6122\n",
      "Epoch 1188/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 171.0808 - val_loss: 123.5921\n",
      "Epoch 1189/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 152.1842 - val_loss: 142.4925\n",
      "Epoch 1190/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 151.9318 - val_loss: 126.6225\n",
      "Epoch 1191/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.8593 - val_loss: 119.7376\n",
      "Epoch 1192/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 153.7708 - val_loss: 161.9774\n",
      "Epoch 1193/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.5019 - val_loss: 117.9407\n",
      "Epoch 1194/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.8050 - val_loss: 161.3703\n",
      "Epoch 1195/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 180.3245 - val_loss: 124.6996\n",
      "Epoch 1196/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.7897 - val_loss: 126.3761\n",
      "Epoch 1197/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.7732 - val_loss: 160.0187\n",
      "Epoch 1198/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.1515 - val_loss: 132.9576\n",
      "Epoch 1199/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.7891 - val_loss: 115.6117\n",
      "Epoch 1200/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.5466 - val_loss: 145.2356\n",
      "Epoch 1201/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.4014 - val_loss: 322.5019\n",
      "Epoch 1202/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 182.1277 - val_loss: 139.7645\n",
      "Epoch 1203/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 162.4144 - val_loss: 197.8972\n",
      "Epoch 1204/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.3912 - val_loss: 138.1026\n",
      "Epoch 1205/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 143.1240 - val_loss: 110.9642\n",
      "Epoch 1206/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.8423 - val_loss: 129.7067\n",
      "Epoch 1207/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.1704 - val_loss: 117.2787\n",
      "Epoch 1208/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.7954 - val_loss: 116.6587\n",
      "Epoch 1209/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.4491 - val_loss: 129.0958\n",
      "Epoch 1210/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.8993 - val_loss: 119.1895\n",
      "Epoch 1211/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.3249 - val_loss: 113.8489\n",
      "Epoch 1212/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.6164 - val_loss: 119.9394\n",
      "Epoch 1213/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.7622 - val_loss: 157.9568\n",
      "Epoch 1214/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 186.9617 - val_loss: 136.2135\n",
      "Epoch 1215/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.1830 - val_loss: 132.1434\n",
      "Epoch 1216/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 203.7684 - val_loss: 122.2651\n",
      "Epoch 1217/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.4633 - val_loss: 115.2340\n",
      "Epoch 1218/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.5923 - val_loss: 112.2598\n",
      "Epoch 1219/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.0351 - val_loss: 120.4038\n",
      "Epoch 1220/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.0957 - val_loss: 179.4611\n",
      "Epoch 1221/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.4026 - val_loss: 113.2107\n",
      "Epoch 1222/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.4218 - val_loss: 114.9869\n",
      "Epoch 1223/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 159.7389 - val_loss: 155.0377\n",
      "Epoch 1224/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.6036 - val_loss: 124.0271\n",
      "Epoch 1225/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.0722 - val_loss: 133.7921\n",
      "Epoch 1226/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.2846 - val_loss: 127.5968\n",
      "Epoch 1227/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.8139 - val_loss: 132.1908\n",
      "Epoch 1228/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.4607 - val_loss: 128.2287\n",
      "Epoch 1229/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.9110 - val_loss: 121.8370\n",
      "Epoch 1230/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.2018 - val_loss: 638.7813\n",
      "Epoch 1231/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 326.5742 - val_loss: 117.6572\n",
      "Epoch 1232/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.0570 - val_loss: 121.2586\n",
      "Epoch 1233/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.3368 - val_loss: 146.6407\n",
      "Epoch 1234/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.1727 - val_loss: 117.5305\n",
      "Epoch 1235/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.4897 - val_loss: 115.9532\n",
      "Epoch 1236/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.7397 - val_loss: 123.8487\n",
      "Epoch 1237/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.1502 - val_loss: 146.7324\n",
      "Epoch 1238/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.7640 - val_loss: 123.8602\n",
      "Epoch 1239/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 154.5909 - val_loss: 114.5043\n",
      "Epoch 1240/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 156.2083 - val_loss: 175.6093\n",
      "Epoch 1241/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 143.4263 - val_loss: 125.3243\n",
      "Epoch 1242/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.5481 - val_loss: 116.8605\n",
      "Epoch 1243/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.7080 - val_loss: 121.4916\n",
      "Epoch 1244/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.2007 - val_loss: 110.0720\n",
      "Epoch 1245/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.5411 - val_loss: 139.9810\n",
      "Epoch 1246/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.6410 - val_loss: 133.2597\n",
      "Epoch 1247/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.6981 - val_loss: 124.5786\n",
      "Epoch 1248/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 65us/step - loss: 167.1327 - val_loss: 115.4881\n",
      "Epoch 1249/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.2772 - val_loss: 120.1598\n",
      "Epoch 1250/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.2393 - val_loss: 142.6005\n",
      "Epoch 1251/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.5956 - val_loss: 116.6248\n",
      "Epoch 1252/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 143.1081 - val_loss: 111.8030\n",
      "Epoch 1253/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.7899 - val_loss: 123.5004\n",
      "Epoch 1254/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.7261 - val_loss: 128.4061\n",
      "Epoch 1255/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.2159 - val_loss: 117.7670\n",
      "Epoch 1256/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.7631 - val_loss: 132.8839\n",
      "Epoch 1257/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.1344 - val_loss: 111.7368\n",
      "Epoch 1258/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.8293 - val_loss: 242.0443\n",
      "Epoch 1259/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.3065 - val_loss: 145.0262\n",
      "Epoch 1260/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.7113 - val_loss: 114.7428\n",
      "Epoch 1261/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 312.2525 - val_loss: 122.3467\n",
      "Epoch 1262/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.7841 - val_loss: 127.7929\n",
      "Epoch 1263/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.8457 - val_loss: 118.2164\n",
      "Epoch 1264/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.8660 - val_loss: 120.7770\n",
      "Epoch 1265/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.5190 - val_loss: 123.5182\n",
      "Epoch 1266/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.8948 - val_loss: 115.1212\n",
      "Epoch 1267/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.3017 - val_loss: 113.0600\n",
      "Epoch 1268/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.1262 - val_loss: 137.0357\n",
      "Epoch 1269/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.6742 - val_loss: 112.8925\n",
      "Epoch 1270/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.4708 - val_loss: 147.7157\n",
      "Epoch 1271/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.7921 - val_loss: 114.0368\n",
      "Epoch 1272/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.2385 - val_loss: 154.2893\n",
      "Epoch 1273/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.7673 - val_loss: 114.8038\n",
      "Epoch 1274/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.6616 - val_loss: 115.9382\n",
      "Epoch 1275/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.7217 - val_loss: 120.5117\n",
      "Epoch 1276/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.5110 - val_loss: 115.1835\n",
      "Epoch 1277/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.6627 - val_loss: 159.8046\n",
      "Epoch 1278/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.9478 - val_loss: 115.9173\n",
      "Epoch 1279/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.9807 - val_loss: 128.3365\n",
      "Epoch 1280/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.4441 - val_loss: 111.4018\n",
      "Epoch 1281/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.4124 - val_loss: 153.2361\n",
      "Epoch 1282/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 151.2498 - val_loss: 117.9353\n",
      "Epoch 1283/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.0333 - val_loss: 161.6270\n",
      "Epoch 1284/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.3170 - val_loss: 126.4901\n",
      "Epoch 1285/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.6355 - val_loss: 145.5834\n",
      "Epoch 1286/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.6066 - val_loss: 137.1262\n",
      "Epoch 1287/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.5165 - val_loss: 331.5581\n",
      "Epoch 1288/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 253.6843 - val_loss: 118.1826\n",
      "Epoch 1289/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.1538 - val_loss: 119.4655\n",
      "Epoch 1290/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.1137 - val_loss: 111.2294\n",
      "Epoch 1291/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.6951 - val_loss: 123.9145\n",
      "Epoch 1292/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.7717 - val_loss: 127.5669\n",
      "Epoch 1293/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.9273 - val_loss: 134.7679\n",
      "Epoch 1294/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.7605 - val_loss: 135.7628\n",
      "Epoch 1295/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.1773 - val_loss: 127.6365\n",
      "Epoch 1296/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.2315 - val_loss: 155.2254\n",
      "Epoch 1297/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 174.5054 - val_loss: 113.9721\n",
      "Epoch 1298/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.3769 - val_loss: 160.5390\n",
      "Epoch 1299/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 149.2852 - val_loss: 125.2029\n",
      "Epoch 1300/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 139.7472 - val_loss: 149.9377\n",
      "Epoch 1301/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 171.0363 - val_loss: 117.0118\n",
      "Epoch 1302/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.9846 - val_loss: 169.7822\n",
      "Epoch 1303/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.8245 - val_loss: 118.5735\n",
      "Epoch 1304/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 153.6790 - val_loss: 123.7723\n",
      "Epoch 1305/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.1019 - val_loss: 136.9342\n",
      "Epoch 1306/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 198.5065 - val_loss: 121.6932\n",
      "Epoch 1307/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.4193 - val_loss: 111.6227\n",
      "Epoch 1308/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.9941 - val_loss: 132.0602\n",
      "Epoch 1309/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.3751 - val_loss: 128.2297\n",
      "Epoch 1310/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 175.1755 - val_loss: 121.5463\n",
      "Epoch 1311/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.2491 - val_loss: 139.8340\n",
      "Epoch 1312/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.0556 - val_loss: 121.2235\n",
      "Epoch 1313/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.0588 - val_loss: 116.7766\n",
      "Epoch 1314/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.6228 - val_loss: 134.1882\n",
      "Epoch 1315/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 159.3144 - val_loss: 111.9043\n",
      "Epoch 1316/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.8506 - val_loss: 124.5518\n",
      "Epoch 1317/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.3574 - val_loss: 137.7651\n",
      "Epoch 1318/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.8470 - val_loss: 138.3387\n",
      "Epoch 1319/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.2423 - val_loss: 111.4592\n",
      "Epoch 1320/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.0354 - val_loss: 133.1407\n",
      "Epoch 1321/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 194.7066 - val_loss: 239.4645\n",
      "Epoch 1322/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 176.5543 - val_loss: 124.6090\n",
      "Epoch 1323/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.9352 - val_loss: 129.0540\n",
      "Epoch 1324/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.3403 - val_loss: 184.3434\n",
      "Epoch 1325/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.2485 - val_loss: 210.9318\n",
      "Epoch 1326/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.6026 - val_loss: 236.9730\n",
      "Epoch 1327/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.0950 - val_loss: 116.6670\n",
      "Epoch 1328/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.5827 - val_loss: 127.0318\n",
      "Epoch 1329/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.2564 - val_loss: 122.6401\n",
      "Epoch 1330/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.4440 - val_loss: 121.3874\n",
      "Epoch 1331/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.9246 - val_loss: 115.0466\n",
      "Epoch 1332/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.3821 - val_loss: 140.5033\n",
      "Epoch 1333/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.0794 - val_loss: 162.7592\n",
      "Epoch 1334/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.7018 - val_loss: 170.9887\n",
      "Epoch 1335/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 196.1259 - val_loss: 116.8025\n",
      "Epoch 1336/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.4937 - val_loss: 134.9393\n",
      "Epoch 1337/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.9512 - val_loss: 117.6482\n",
      "Epoch 1338/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.8623 - val_loss: 112.5491\n",
      "Epoch 1339/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.8639 - val_loss: 120.5153\n",
      "Epoch 1340/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.1869 - val_loss: 122.5372\n",
      "Epoch 1341/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.0933 - val_loss: 122.9612\n",
      "Epoch 1342/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.3115 - val_loss: 132.9562\n",
      "Epoch 1343/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 233.3492 - val_loss: 117.4216\n",
      "Epoch 1344/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.5193 - val_loss: 123.9838\n",
      "Epoch 1345/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.0395 - val_loss: 113.1085\n",
      "Epoch 1346/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.6121 - val_loss: 127.6525\n",
      "Epoch 1347/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.8525 - val_loss: 116.4365\n",
      "Epoch 1348/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.9488 - val_loss: 122.7280\n",
      "Epoch 1349/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.2678 - val_loss: 219.3191\n",
      "Epoch 1350/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.4344 - val_loss: 125.5426\n",
      "Epoch 1351/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 153.4017 - val_loss: 142.9022\n",
      "Epoch 1352/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.7177 - val_loss: 146.3717\n",
      "Epoch 1353/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.7228 - val_loss: 117.2057\n",
      "Epoch 1354/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.7796 - val_loss: 218.9609\n",
      "Epoch 1355/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.5852 - val_loss: 114.6732\n",
      "Epoch 1356/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.4717 - val_loss: 142.5119\n",
      "Epoch 1357/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 151.2843 - val_loss: 122.6155\n",
      "Epoch 1358/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 171.6180 - val_loss: 161.3419\n",
      "Epoch 1359/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 198.4274 - val_loss: 118.6513\n",
      "Epoch 1360/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 152.4273 - val_loss: 111.5936\n",
      "Epoch 1361/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 140.6165 - val_loss: 125.3722\n",
      "Epoch 1362/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.3174 - val_loss: 139.7921\n",
      "Epoch 1363/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.0124 - val_loss: 127.6450\n",
      "Epoch 1364/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.3685 - val_loss: 117.8274\n",
      "Epoch 1365/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.0252 - val_loss: 122.9176\n",
      "Epoch 1366/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.0899 - val_loss: 171.0621\n",
      "Epoch 1367/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.1546 - val_loss: 174.7643\n",
      "Epoch 1368/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.3520 - val_loss: 133.4195\n",
      "Epoch 1369/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.9531 - val_loss: 129.8695\n",
      "Epoch 1370/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.0708 - val_loss: 123.9146\n",
      "Epoch 1371/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.8053 - val_loss: 133.7151\n",
      "Epoch 1372/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 259.7505 - val_loss: 141.2763\n",
      "Epoch 1373/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.2293 - val_loss: 309.3170\n",
      "Epoch 1374/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.1637 - val_loss: 135.4996\n",
      "Epoch 1375/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.2892 - val_loss: 124.2676\n",
      "Epoch 1376/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.5602 - val_loss: 133.1047\n",
      "Epoch 1377/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 151.6552 - val_loss: 119.3043\n",
      "Epoch 1378/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 214.9403 - val_loss: 151.5363\n",
      "Epoch 1379/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 227.1481 - val_loss: 145.3439\n",
      "Epoch 1380/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 183.1936 - val_loss: 125.7504\n",
      "Epoch 1381/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 199.1508 - val_loss: 137.2397\n",
      "Epoch 1382/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 183.1923 - val_loss: 122.7557\n",
      "Epoch 1383/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.6873 - val_loss: 121.6513\n",
      "Epoch 1384/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 162.4213 - val_loss: 149.4157\n",
      "Epoch 1385/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 171.7938 - val_loss: 123.0233\n",
      "Epoch 1386/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 140.9653 - val_loss: 116.2620\n",
      "Epoch 1387/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.8544 - val_loss: 133.5684\n",
      "Epoch 1388/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 226.7208 - val_loss: 261.6518\n",
      "Epoch 1389/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 203.5570 - val_loss: 130.2587\n",
      "Epoch 1390/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.7267 - val_loss: 130.2348\n",
      "Epoch 1391/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.2007 - val_loss: 119.7370\n",
      "Epoch 1392/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.4493 - val_loss: 255.8917\n",
      "Epoch 1393/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.0187 - val_loss: 125.0324\n",
      "Epoch 1394/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.8834 - val_loss: 115.9578\n",
      "Epoch 1395/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.7969 - val_loss: 285.2501\n",
      "Epoch 1396/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.1031 - val_loss: 117.8656\n",
      "Epoch 1397/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.9410 - val_loss: 137.3452\n",
      "Epoch 1398/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.1301 - val_loss: 146.7950\n",
      "Epoch 1399/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.6970 - val_loss: 111.3800\n",
      "Epoch 1400/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.9876 - val_loss: 123.5039\n",
      "Epoch 1401/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.5615 - val_loss: 199.6260\n",
      "Epoch 1402/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.7560 - val_loss: 123.4213\n",
      "Epoch 1403/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 167.2917 - val_loss: 118.1857\n",
      "Epoch 1404/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.6271 - val_loss: 130.8744\n",
      "Epoch 1405/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.7324 - val_loss: 119.6157\n",
      "Epoch 1406/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.1577 - val_loss: 134.3189\n",
      "Epoch 1407/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.1595 - val_loss: 125.2544\n",
      "Epoch 1408/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.6140 - val_loss: 131.4232\n",
      "Epoch 1409/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.4075 - val_loss: 116.8442\n",
      "Epoch 1410/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.8727 - val_loss: 127.8852\n",
      "Epoch 1411/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.0487 - val_loss: 136.0432\n",
      "Epoch 1412/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.0939 - val_loss: 125.2249\n",
      "Epoch 1413/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.3609 - val_loss: 127.1000\n",
      "Epoch 1414/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.3090 - val_loss: 116.1975\n",
      "Epoch 1415/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.1324 - val_loss: 111.8482\n",
      "Epoch 1416/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.1873 - val_loss: 140.4389\n",
      "Epoch 1417/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 205.0914 - val_loss: 135.5267\n",
      "Epoch 1418/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 162.2607 - val_loss: 141.2711\n",
      "Epoch 1419/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 144.8682 - val_loss: 204.2943\n",
      "Epoch 1420/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 149.5910 - val_loss: 114.0956\n",
      "Epoch 1421/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 145.9734 - val_loss: 270.6392\n",
      "Epoch 1422/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 144.7932 - val_loss: 117.9231\n",
      "Epoch 1423/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.0556 - val_loss: 110.8811\n",
      "Epoch 1424/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.4032 - val_loss: 125.7518\n",
      "Epoch 1425/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 216.9284 - val_loss: 132.9855\n",
      "Epoch 1426/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.8605 - val_loss: 170.4981\n",
      "Epoch 1427/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.6961 - val_loss: 120.5065\n",
      "Epoch 1428/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.2389 - val_loss: 118.2152\n",
      "Epoch 1429/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.5017 - val_loss: 131.9708\n",
      "Epoch 1430/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.6184 - val_loss: 118.1649\n",
      "Epoch 1431/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.7850 - val_loss: 152.3268\n",
      "Epoch 1432/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 186.5209 - val_loss: 247.2365\n",
      "Epoch 1433/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.5834 - val_loss: 157.7247\n",
      "Epoch 1434/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.2248 - val_loss: 130.7025\n",
      "Epoch 1435/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.0405 - val_loss: 146.9876\n",
      "Epoch 1436/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.3739 - val_loss: 130.8897\n",
      "Epoch 1437/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 198.8275 - val_loss: 181.2283\n",
      "Epoch 1438/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 177.8001 - val_loss: 123.6695\n",
      "Epoch 1439/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 177.6574 - val_loss: 143.1838\n",
      "Epoch 1440/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.0102 - val_loss: 116.1765\n",
      "Epoch 1441/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.2735 - val_loss: 145.1126\n",
      "Epoch 1442/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.2512 - val_loss: 134.4439\n",
      "Epoch 1443/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 171.6471 - val_loss: 118.5693\n",
      "Epoch 1444/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.3143 - val_loss: 127.5074\n",
      "Epoch 1445/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.3876 - val_loss: 156.6563\n",
      "Epoch 1446/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.7372 - val_loss: 124.9623\n",
      "Epoch 1447/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.9863 - val_loss: 118.3223\n",
      "Epoch 1448/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.1094 - val_loss: 145.0701\n",
      "Epoch 1449/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.6987 - val_loss: 115.4477\n",
      "Epoch 1450/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.1461 - val_loss: 125.8355\n",
      "Epoch 1451/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.5637 - val_loss: 198.1355\n",
      "Epoch 1452/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.2512 - val_loss: 159.6480\n",
      "Epoch 1453/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.9003 - val_loss: 120.1423\n",
      "Epoch 1454/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.0138 - val_loss: 166.3353\n",
      "Epoch 1455/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.3797 - val_loss: 140.2009\n",
      "Epoch 1456/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.9333 - val_loss: 162.3172\n",
      "Epoch 1457/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.8775 - val_loss: 137.8418\n",
      "Epoch 1458/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.7371 - val_loss: 134.5526\n",
      "Epoch 1459/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 194.4683 - val_loss: 126.8411\n",
      "Epoch 1460/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 185.9259 - val_loss: 172.6496\n",
      "Epoch 1461/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.3516 - val_loss: 129.1454\n",
      "Epoch 1462/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.4848 - val_loss: 124.6828\n",
      "Epoch 1463/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.7626 - val_loss: 121.3487\n",
      "Epoch 1464/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.8991 - val_loss: 135.0590\n",
      "Epoch 1465/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.4927 - val_loss: 254.2119\n",
      "Epoch 1466/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.5538 - val_loss: 148.9417\n",
      "Epoch 1467/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.6890 - val_loss: 139.7122\n",
      "Epoch 1468/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 176.8780 - val_loss: 165.7012\n",
      "Epoch 1469/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.9774 - val_loss: 253.0253\n",
      "Epoch 1470/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.8210 - val_loss: 122.6627\n",
      "Epoch 1471/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 138.1672 - val_loss: 161.8227\n",
      "Epoch 1472/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.7797 - val_loss: 113.7628\n",
      "Epoch 1473/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.6103 - val_loss: 114.4470\n",
      "Epoch 1474/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.9167 - val_loss: 130.2498\n",
      "Epoch 1475/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.9086 - val_loss: 171.4018\n",
      "Epoch 1476/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.5846 - val_loss: 163.7939\n",
      "Epoch 1477/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.8963 - val_loss: 121.8862\n",
      "Epoch 1478/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.4134 - val_loss: 121.3009\n",
      "Epoch 1479/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.0943 - val_loss: 155.1302\n",
      "Epoch 1480/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 159.3587 - val_loss: 124.2384\n",
      "Epoch 1481/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 148.1780 - val_loss: 115.9155\n",
      "Epoch 1482/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 167.4757 - val_loss: 118.6710\n",
      "Epoch 1483/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.2358 - val_loss: 122.1731\n",
      "Epoch 1484/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.5371 - val_loss: 112.5719\n",
      "Epoch 1485/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.5317 - val_loss: 143.5467\n",
      "Epoch 1486/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.5678 - val_loss: 137.9910\n",
      "Epoch 1487/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.3404 - val_loss: 135.0218\n",
      "Epoch 1488/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.5701 - val_loss: 145.4243\n",
      "Epoch 1489/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.3823 - val_loss: 138.1961\n",
      "Epoch 1490/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.3460 - val_loss: 151.2600\n",
      "Epoch 1491/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.8921 - val_loss: 122.2640\n",
      "Epoch 1492/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.0292 - val_loss: 211.1841\n",
      "Epoch 1493/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.4594 - val_loss: 139.6952\n",
      "Epoch 1494/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.3452 - val_loss: 131.0768\n",
      "Epoch 1495/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 183.5297 - val_loss: 127.9887\n",
      "Epoch 1496/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 142.6461 - val_loss: 109.7900\n",
      "Epoch 1497/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 164.9704 - val_loss: 119.3699\n",
      "Epoch 1498/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 162.9731 - val_loss: 121.2358\n",
      "Epoch 1499/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 144.1583 - val_loss: 111.4631\n",
      "Epoch 1500/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 155.7444 - val_loss: 112.0220\n",
      "Epoch 1501/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.0313 - val_loss: 128.1441\n",
      "Epoch 1502/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.1854 - val_loss: 127.6217\n",
      "Epoch 1503/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.6977 - val_loss: 116.2998\n",
      "Epoch 1504/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.1412 - val_loss: 150.0956\n",
      "Epoch 1505/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.2468 - val_loss: 124.1108\n",
      "Epoch 1506/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.3515 - val_loss: 112.7968\n",
      "Epoch 1507/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.1299 - val_loss: 162.6643\n",
      "Epoch 1508/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 175.9397 - val_loss: 144.0989\n",
      "Epoch 1509/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.1980 - val_loss: 117.9227\n",
      "Epoch 1510/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.6618 - val_loss: 120.4558\n",
      "Epoch 1511/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.0022 - val_loss: 119.0812\n",
      "Epoch 1512/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.0230 - val_loss: 113.1221\n",
      "Epoch 1513/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.7444 - val_loss: 129.7716\n",
      "Epoch 1514/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.9522 - val_loss: 136.6981\n",
      "Epoch 1515/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 269.2301 - val_loss: 160.5702\n",
      "Epoch 1516/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.6107 - val_loss: 128.2474\n",
      "Epoch 1517/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.3646 - val_loss: 114.0960\n",
      "Epoch 1518/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.8675 - val_loss: 114.1726\n",
      "Epoch 1519/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.6140 - val_loss: 138.1729\n",
      "Epoch 1520/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 173.8557 - val_loss: 121.7098\n",
      "Epoch 1521/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.6325 - val_loss: 122.9676\n",
      "Epoch 1522/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.8837 - val_loss: 321.4335\n",
      "Epoch 1523/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.5779 - val_loss: 133.4669\n",
      "Epoch 1524/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.3203 - val_loss: 113.8713\n",
      "Epoch 1525/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 177.8203 - val_loss: 466.1260\n",
      "Epoch 1526/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.4457 - val_loss: 114.5077\n",
      "Epoch 1527/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.9744 - val_loss: 114.2980\n",
      "Epoch 1528/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.4814 - val_loss: 120.2738\n",
      "Epoch 1529/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.2995 - val_loss: 118.3746\n",
      "Epoch 1530/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.3438 - val_loss: 121.0729\n",
      "Epoch 1531/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.7793 - val_loss: 189.0729\n",
      "Epoch 1532/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.2152 - val_loss: 175.2464\n",
      "Epoch 1533/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.5475 - val_loss: 120.9531\n",
      "Epoch 1534/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.6520 - val_loss: 114.4781\n",
      "Epoch 1535/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.8923 - val_loss: 120.1575\n",
      "Epoch 1536/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.0206 - val_loss: 177.8347\n",
      "Epoch 1537/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 304.2474 - val_loss: 163.4011\n",
      "Epoch 1538/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.1804 - val_loss: 115.4080\n",
      "Epoch 1539/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.7939 - val_loss: 115.8122\n",
      "Epoch 1540/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 161.6748 - val_loss: 113.4470\n",
      "Epoch 1541/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 139.6965 - val_loss: 113.1875\n",
      "Epoch 1542/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 149.9072 - val_loss: 198.9462\n",
      "Epoch 1543/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.9505 - val_loss: 119.0992\n",
      "Epoch 1544/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.9245 - val_loss: 114.5024\n",
      "Epoch 1545/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.3678 - val_loss: 119.1078\n",
      "Epoch 1546/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.5548 - val_loss: 119.4496\n",
      "Epoch 1547/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 177.1240 - val_loss: 115.1532\n",
      "Epoch 1548/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.4924 - val_loss: 245.6304\n",
      "Epoch 1549/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 157.5551 - val_loss: 113.6045\n",
      "Epoch 1550/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.8739 - val_loss: 112.0560\n",
      "Epoch 1551/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.8866 - val_loss: 118.9811\n",
      "Epoch 1552/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 157.0014 - val_loss: 124.5508\n",
      "Epoch 1553/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.2279 - val_loss: 128.6549\n",
      "Epoch 1554/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.1418 - val_loss: 159.8630\n",
      "Epoch 1555/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 146.9104 - val_loss: 123.1840\n",
      "Epoch 1556/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.3293 - val_loss: 117.1515\n",
      "Epoch 1557/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.9938 - val_loss: 148.9722\n",
      "Epoch 1558/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 183.1278 - val_loss: 124.4608\n",
      "Epoch 1559/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.9816 - val_loss: 132.5179\n",
      "Epoch 1560/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.9672 - val_loss: 122.6459\n",
      "Epoch 1561/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.1827 - val_loss: 135.3775\n",
      "Epoch 1562/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 146.7921 - val_loss: 124.8721\n",
      "Epoch 1563/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.9630 - val_loss: 114.6284\n",
      "Epoch 1564/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 228.7407 - val_loss: 141.5075\n",
      "Epoch 1565/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.3397 - val_loss: 128.9537\n",
      "Epoch 1566/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.1761 - val_loss: 146.7064\n",
      "Epoch 1567/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.1074 - val_loss: 128.7812\n",
      "Epoch 1568/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 215.0380 - val_loss: 125.1945\n",
      "Epoch 1569/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.6638 - val_loss: 128.7043\n",
      "Epoch 1570/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.7632 - val_loss: 124.8974\n",
      "Epoch 1571/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 181.7452 - val_loss: 129.1290\n",
      "Epoch 1572/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 196.8082 - val_loss: 142.9954\n",
      "Epoch 1573/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 178.5987 - val_loss: 175.0697\n",
      "Epoch 1574/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.7756 - val_loss: 116.0530\n",
      "Epoch 1575/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.5627 - val_loss: 161.4770\n",
      "Epoch 1576/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 186.1321 - val_loss: 122.8681\n",
      "Epoch 1577/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.2342 - val_loss: 119.7511\n",
      "Epoch 1578/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.5618 - val_loss: 112.5455\n",
      "Epoch 1579/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 152.2552 - val_loss: 129.3377\n",
      "Epoch 1580/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.3289 - val_loss: 118.7507\n",
      "Epoch 1581/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.8651 - val_loss: 129.1245\n",
      "Epoch 1582/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.7866 - val_loss: 122.3077\n",
      "Epoch 1583/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.1457 - val_loss: 142.8286\n",
      "Epoch 1584/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 178.5852 - val_loss: 121.1591\n",
      "Epoch 1585/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.1285 - val_loss: 116.5302\n",
      "Epoch 1586/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.5402 - val_loss: 123.8687\n",
      "Epoch 1587/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.0940 - val_loss: 133.8663\n",
      "Epoch 1588/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 269.1070 - val_loss: 135.5804\n",
      "Epoch 1589/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.4621 - val_loss: 131.4749\n",
      "Epoch 1590/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 159.6505 - val_loss: 115.4515\n",
      "Epoch 1591/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.9320 - val_loss: 121.7546\n",
      "Epoch 1592/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.9098 - val_loss: 127.4930\n",
      "Epoch 1593/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.4883 - val_loss: 118.3925\n",
      "Epoch 1594/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 158.3034 - val_loss: 125.2022\n",
      "Epoch 1595/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.3106 - val_loss: 115.6448\n",
      "Epoch 1596/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.1916 - val_loss: 110.7087\n",
      "Epoch 1597/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 183.3362 - val_loss: 159.6461\n",
      "Epoch 1598/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.6001 - val_loss: 113.9200\n",
      "Epoch 1599/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.1532 - val_loss: 139.6478\n",
      "Epoch 1600/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 140.7359 - val_loss: 126.0930\n",
      "Epoch 1601/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 142.9494 - val_loss: 129.3310\n",
      "Epoch 1602/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 139.5487 - val_loss: 152.0301\n",
      "Epoch 1603/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.7034 - val_loss: 126.7258\n",
      "Epoch 1604/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.4151 - val_loss: 115.9425\n",
      "Epoch 1605/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.1426 - val_loss: 124.3445\n",
      "Epoch 1606/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.0871 - val_loss: 186.8901\n",
      "Epoch 1607/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.8754 - val_loss: 150.9300\n",
      "Epoch 1608/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.9706 - val_loss: 175.2770\n",
      "Epoch 1609/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.5941 - val_loss: 133.2826\n",
      "Epoch 1610/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.5871 - val_loss: 114.6789\n",
      "Epoch 1611/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.1255 - val_loss: 191.0687\n",
      "Epoch 1612/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.4003 - val_loss: 117.9672\n",
      "Epoch 1613/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.0620 - val_loss: 118.3783\n",
      "Epoch 1614/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.3437 - val_loss: 134.1020\n",
      "Epoch 1615/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.2922 - val_loss: 192.5788\n",
      "Epoch 1616/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.3097 - val_loss: 123.6897\n",
      "Epoch 1617/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.7515 - val_loss: 135.1658\n",
      "Epoch 1618/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.7444 - val_loss: 142.4073\n",
      "Epoch 1619/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 143.5457 - val_loss: 166.6680\n",
      "Epoch 1620/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.7230 - val_loss: 119.7534\n",
      "Epoch 1621/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.3688 - val_loss: 145.7481\n",
      "Epoch 1622/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.2255 - val_loss: 116.0284\n",
      "Epoch 1623/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.5082 - val_loss: 115.0606\n",
      "Epoch 1624/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.3538 - val_loss: 111.8446\n",
      "Epoch 1625/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.9397 - val_loss: 130.3790\n",
      "Epoch 1626/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.3331 - val_loss: 123.1505\n",
      "Epoch 1627/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.5967 - val_loss: 111.8829\n",
      "Epoch 1628/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.0644 - val_loss: 113.1220\n",
      "Epoch 1629/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.4379 - val_loss: 115.9223\n",
      "Epoch 1630/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.4241 - val_loss: 118.9858\n",
      "Epoch 1631/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.6717 - val_loss: 142.0904\n",
      "Epoch 1632/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.9576 - val_loss: 149.2009\n",
      "Epoch 1633/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.8304 - val_loss: 184.6596\n",
      "Epoch 1634/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 205.5330 - val_loss: 256.0828\n",
      "Epoch 1635/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 189.4389 - val_loss: 155.6249\n",
      "Epoch 1636/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.9328 - val_loss: 145.1092\n",
      "Epoch 1637/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.7675 - val_loss: 117.9110\n",
      "Epoch 1638/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.7836 - val_loss: 137.9429\n",
      "Epoch 1639/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.4327 - val_loss: 114.9773\n",
      "Epoch 1640/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.7670 - val_loss: 126.0202\n",
      "Epoch 1641/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.5308 - val_loss: 111.2715\n",
      "Epoch 1642/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.3766 - val_loss: 116.4049\n",
      "Epoch 1643/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.9739 - val_loss: 135.1436\n",
      "Epoch 1644/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.0182 - val_loss: 126.0839\n",
      "Epoch 1645/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 198.6171 - val_loss: 225.5710\n",
      "Epoch 1646/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 171.0968 - val_loss: 122.1990\n",
      "Epoch 1647/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.9645 - val_loss: 139.1698\n",
      "Epoch 1648/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.3481 - val_loss: 133.7198\n",
      "Epoch 1649/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.4145 - val_loss: 128.2328\n",
      "Epoch 1650/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 202.9729 - val_loss: 116.5976\n",
      "Epoch 1651/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 188.3156 - val_loss: 138.4169\n",
      "Epoch 1652/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.4369 - val_loss: 113.2936\n",
      "Epoch 1653/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.8285 - val_loss: 121.8485\n",
      "Epoch 1654/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.0365 - val_loss: 129.6309\n",
      "Epoch 1655/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.3487 - val_loss: 131.8585\n",
      "Epoch 1656/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.3130 - val_loss: 118.5392\n",
      "Epoch 1657/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.5505 - val_loss: 290.6770\n",
      "Epoch 1658/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.9351 - val_loss: 128.8463\n",
      "Epoch 1659/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.8086 - val_loss: 144.7618\n",
      "Epoch 1660/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 167.4767 - val_loss: 139.6073\n",
      "Epoch 1661/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 140.6572 - val_loss: 118.4930\n",
      "Epoch 1662/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 222.4033 - val_loss: 133.7302\n",
      "Epoch 1663/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.6556 - val_loss: 124.4140\n",
      "Epoch 1664/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.4105 - val_loss: 111.7522\n",
      "Epoch 1665/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.3767 - val_loss: 110.3198\n",
      "Epoch 1666/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.9149 - val_loss: 127.4225\n",
      "Epoch 1667/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.7055 - val_loss: 118.4649\n",
      "Epoch 1668/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.9705 - val_loss: 129.1166\n",
      "Epoch 1669/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.1238 - val_loss: 143.9624\n",
      "Epoch 1670/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.6201 - val_loss: 117.8203\n",
      "Epoch 1671/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.4714 - val_loss: 144.3496\n",
      "Epoch 1672/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.9891 - val_loss: 115.9971\n",
      "Epoch 1673/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.4644 - val_loss: 139.5841\n",
      "Epoch 1674/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.0391 - val_loss: 135.0593\n",
      "Epoch 1675/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.3343 - val_loss: 112.5067\n",
      "Epoch 1676/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.5437 - val_loss: 127.1741\n",
      "Epoch 1677/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.4394 - val_loss: 116.3884\n",
      "Epoch 1678/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.2660 - val_loss: 115.0489\n",
      "Epoch 1679/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.0514 - val_loss: 120.5448\n",
      "Epoch 1680/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.0941 - val_loss: 134.5668\n",
      "Epoch 1681/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.4435 - val_loss: 119.4564\n",
      "Epoch 1682/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.8260 - val_loss: 118.1092\n",
      "Epoch 1683/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.8363 - val_loss: 126.8464\n",
      "Epoch 1684/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 164.1995 - val_loss: 116.8522\n",
      "Epoch 1685/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.5816 - val_loss: 117.4950\n",
      "Epoch 1686/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.8822 - val_loss: 126.5176\n",
      "Epoch 1687/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.5667 - val_loss: 122.1548\n",
      "Epoch 1688/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.4087 - val_loss: 127.6360\n",
      "Epoch 1689/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.6468 - val_loss: 125.7573\n",
      "Epoch 1690/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.1340 - val_loss: 117.6801\n",
      "Epoch 1691/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.9966 - val_loss: 120.0280\n",
      "Epoch 1692/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.6345 - val_loss: 165.2898\n",
      "Epoch 1693/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.7661 - val_loss: 111.4133\n",
      "Epoch 1694/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 144.9015 - val_loss: 131.3959\n",
      "Epoch 1695/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 154.9970 - val_loss: 268.8100\n",
      "Epoch 1696/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.0251 - val_loss: 129.7557\n",
      "Epoch 1697/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.2959 - val_loss: 117.5935\n",
      "Epoch 1698/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.5137 - val_loss: 141.4860\n",
      "Epoch 1699/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 144.1263 - val_loss: 158.3613\n",
      "Epoch 1700/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 143.0659 - val_loss: 120.3621\n",
      "Epoch 1701/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.3618 - val_loss: 126.4350\n",
      "Epoch 1702/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.6454 - val_loss: 131.5098\n",
      "Epoch 1703/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.3115 - val_loss: 176.1871\n",
      "Epoch 1704/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.7448 - val_loss: 127.1958\n",
      "Epoch 1705/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.7087 - val_loss: 114.3734\n",
      "Epoch 1706/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.9252 - val_loss: 261.3796\n",
      "Epoch 1707/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.6903 - val_loss: 179.6832\n",
      "Epoch 1708/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 160.7562 - val_loss: 159.9881\n",
      "Epoch 1709/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.0641 - val_loss: 132.4495\n",
      "Epoch 1710/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 189.4050 - val_loss: 119.3546\n",
      "Epoch 1711/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.4428 - val_loss: 116.9678\n",
      "Epoch 1712/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.6001 - val_loss: 143.2793\n",
      "Epoch 1713/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.5740 - val_loss: 114.1378\n",
      "Epoch 1714/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.8375 - val_loss: 117.1122\n",
      "Epoch 1715/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.3344 - val_loss: 121.7108\n",
      "Epoch 1716/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.5783 - val_loss: 128.1732\n",
      "Epoch 1717/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.3166 - val_loss: 130.3511\n",
      "Epoch 1718/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.4978 - val_loss: 133.4249\n",
      "Epoch 1719/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.1330 - val_loss: 122.2110\n",
      "Epoch 1720/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 142.9159 - val_loss: 124.1286\n",
      "Epoch 1721/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 156.4044 - val_loss: 146.9537\n",
      "Epoch 1722/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.4761 - val_loss: 135.4915\n",
      "Epoch 1723/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.3642 - val_loss: 120.2875\n",
      "Epoch 1724/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.0326 - val_loss: 113.2995\n",
      "Epoch 1725/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.4346 - val_loss: 131.6946\n",
      "Epoch 1726/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.4959 - val_loss: 114.6544\n",
      "Epoch 1727/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.3736 - val_loss: 122.9548\n",
      "Epoch 1728/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 258.7910 - val_loss: 122.3702\n",
      "Epoch 1729/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.0976 - val_loss: 111.5288\n",
      "Epoch 1730/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 142.6753 - val_loss: 109.6884\n",
      "Epoch 1731/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.3350 - val_loss: 113.8977\n",
      "Epoch 1732/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.8954 - val_loss: 110.3567\n",
      "Epoch 1733/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.5791 - val_loss: 120.5710\n",
      "Epoch 1734/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.4366 - val_loss: 130.3676\n",
      "Epoch 1735/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.8429 - val_loss: 126.6403\n",
      "Epoch 1736/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.6867 - val_loss: 110.9312\n",
      "Epoch 1737/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.8486 - val_loss: 182.1278\n",
      "Epoch 1738/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.6628 - val_loss: 111.6499\n",
      "Epoch 1739/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.7939 - val_loss: 126.1530\n",
      "Epoch 1740/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 191.1078 - val_loss: 124.1949\n",
      "Epoch 1741/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 178.1516 - val_loss: 139.6982\n",
      "Epoch 1742/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.9097 - val_loss: 141.4536\n",
      "Epoch 1743/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.9476 - val_loss: 120.5389\n",
      "Epoch 1744/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 162.3301 - val_loss: 146.2932\n",
      "Epoch 1745/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.2545 - val_loss: 118.1048\n",
      "Epoch 1746/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.5903 - val_loss: 123.6998\n",
      "Epoch 1747/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.6669 - val_loss: 135.1677\n",
      "Epoch 1748/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.1075 - val_loss: 115.4110\n",
      "Epoch 1749/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.9218 - val_loss: 191.9984\n",
      "Epoch 1750/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.5512 - val_loss: 125.7382\n",
      "Epoch 1751/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.3680 - val_loss: 119.3793\n",
      "Epoch 1752/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.6145 - val_loss: 126.3169\n",
      "Epoch 1753/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.3406 - val_loss: 134.5960\n",
      "Epoch 1754/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.2834 - val_loss: 121.3536\n",
      "Epoch 1755/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.2414 - val_loss: 165.9519\n",
      "Epoch 1756/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.6302 - val_loss: 182.3468\n",
      "Epoch 1757/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.7618 - val_loss: 118.7628\n",
      "Epoch 1758/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.3782 - val_loss: 151.1653\n",
      "Epoch 1759/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.5817 - val_loss: 111.4011\n",
      "Epoch 1760/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.2046 - val_loss: 118.7861\n",
      "Epoch 1761/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 195.0137 - val_loss: 174.5943\n",
      "Epoch 1762/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.2218 - val_loss: 128.2723\n",
      "Epoch 1763/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.3228 - val_loss: 119.6147\n",
      "Epoch 1764/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.7379 - val_loss: 115.5988\n",
      "Epoch 1765/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.9633 - val_loss: 118.4045\n",
      "Epoch 1766/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.2081 - val_loss: 171.3410\n",
      "Epoch 1767/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.9947 - val_loss: 119.3623\n",
      "Epoch 1768/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 220.9374 - val_loss: 118.2007\n",
      "Epoch 1769/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.6269 - val_loss: 131.8629\n",
      "Epoch 1770/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.9187 - val_loss: 116.2237\n",
      "Epoch 1771/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.4243 - val_loss: 165.5131\n",
      "Epoch 1772/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.3831 - val_loss: 116.5276\n",
      "Epoch 1773/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.7243 - val_loss: 129.8708\n",
      "Epoch 1774/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 189.5656 - val_loss: 124.8017\n",
      "Epoch 1775/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.6369 - val_loss: 120.9399\n",
      "Epoch 1776/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.8228 - val_loss: 120.0407\n",
      "Epoch 1777/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.9634 - val_loss: 191.3384\n",
      "Epoch 1778/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 186.0705 - val_loss: 135.6521\n",
      "Epoch 1779/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.8561 - val_loss: 112.8847\n",
      "Epoch 1780/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 150.4039 - val_loss: 122.8741\n",
      "Epoch 1781/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.8312 - val_loss: 196.4265\n",
      "Epoch 1782/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.4797 - val_loss: 118.9770\n",
      "Epoch 1783/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.2235 - val_loss: 145.4397\n",
      "Epoch 1784/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.8457 - val_loss: 120.9888\n",
      "Epoch 1785/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.0174 - val_loss: 194.8618\n",
      "Epoch 1786/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.9614 - val_loss: 116.3789\n",
      "Epoch 1787/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.9267 - val_loss: 217.2625\n",
      "Epoch 1788/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 198.0777 - val_loss: 125.1408\n",
      "Epoch 1789/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 178.7276 - val_loss: 191.4887\n",
      "Epoch 1790/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 204.7726 - val_loss: 133.2786\n",
      "Epoch 1791/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 167.9080 - val_loss: 119.4839\n",
      "Epoch 1792/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.3264 - val_loss: 110.6027\n",
      "Epoch 1793/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 143.7324 - val_loss: 118.8057\n",
      "Epoch 1794/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.2551 - val_loss: 123.5751\n",
      "Epoch 1795/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 156.4161 - val_loss: 127.0501\n",
      "Epoch 1796/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 144.5551 - val_loss: 226.0890\n",
      "Epoch 1797/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.4305 - val_loss: 111.4113\n",
      "Epoch 1798/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.0837 - val_loss: 123.4682\n",
      "Epoch 1799/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.1580 - val_loss: 142.6284\n",
      "Epoch 1800/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 184.3316 - val_loss: 151.1884\n",
      "Epoch 1801/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 187.1319 - val_loss: 140.4736\n",
      "Epoch 1802/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 178.2436 - val_loss: 116.8860\n",
      "Epoch 1803/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 180.2137 - val_loss: 114.1630\n",
      "Epoch 1804/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.3228 - val_loss: 123.7217\n",
      "Epoch 1805/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 179.2543 - val_loss: 150.0196\n",
      "Epoch 1806/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.5758 - val_loss: 145.2915\n",
      "Epoch 1807/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.6134 - val_loss: 172.2686\n",
      "Epoch 1808/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.9992 - val_loss: 131.0355\n",
      "Epoch 1809/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.8139 - val_loss: 116.5435\n",
      "Epoch 1810/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.7439 - val_loss: 120.3772\n",
      "Epoch 1811/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.2890 - val_loss: 120.9622\n",
      "Epoch 1812/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.7268 - val_loss: 124.9811\n",
      "Epoch 1813/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.1352 - val_loss: 142.8714\n",
      "Epoch 1814/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.3918 - val_loss: 125.3071\n",
      "Epoch 1815/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.5844 - val_loss: 112.0069\n",
      "Epoch 1816/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 183.3404 - val_loss: 944.8534\n",
      "Epoch 1817/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.7167 - val_loss: 117.6751\n",
      "Epoch 1818/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 135.7564 - val_loss: 117.3381\n",
      "Epoch 1819/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.4183 - val_loss: 150.1958\n",
      "Epoch 1820/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.5471 - val_loss: 162.3788\n",
      "Epoch 1821/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.9107 - val_loss: 140.8312\n",
      "Epoch 1822/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.2223 - val_loss: 123.8171\n",
      "Epoch 1823/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.9663 - val_loss: 115.1857\n",
      "Epoch 1824/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.7192 - val_loss: 132.7524\n",
      "Epoch 1825/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.8648 - val_loss: 117.0024\n",
      "Epoch 1826/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.7610 - val_loss: 120.3762\n",
      "Epoch 1827/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.1495 - val_loss: 130.1187\n",
      "Epoch 1828/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.5619 - val_loss: 141.4869\n",
      "Epoch 1829/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.4075 - val_loss: 114.7451\n",
      "Epoch 1830/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 142.0902 - val_loss: 143.7946\n",
      "Epoch 1831/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 148.2269 - val_loss: 134.4370\n",
      "Epoch 1832/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.9165 - val_loss: 121.6850\n",
      "Epoch 1833/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.0177 - val_loss: 431.6311\n",
      "Epoch 1834/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.0747 - val_loss: 116.1349\n",
      "Epoch 1835/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.1704 - val_loss: 122.0162\n",
      "Epoch 1836/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.5241 - val_loss: 132.5190\n",
      "Epoch 1837/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 192.9928 - val_loss: 121.1993\n",
      "Epoch 1838/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.2087 - val_loss: 132.0099\n",
      "Epoch 1839/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 142.2971 - val_loss: 129.9501\n",
      "Epoch 1840/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 139.0047 - val_loss: 122.1521\n",
      "Epoch 1841/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 153.7836 - val_loss: 180.6795\n",
      "Epoch 1842/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.5093 - val_loss: 115.1468\n",
      "Epoch 1843/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.6815 - val_loss: 117.2592\n",
      "Epoch 1844/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.2133 - val_loss: 115.5018\n",
      "Epoch 1845/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.1998 - val_loss: 127.7279\n",
      "Epoch 1846/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.0456 - val_loss: 119.2045\n",
      "Epoch 1847/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.7816 - val_loss: 123.1055\n",
      "Epoch 1848/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.3216 - val_loss: 140.7634\n",
      "Epoch 1849/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 157.3686 - val_loss: 156.1166\n",
      "Epoch 1850/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.1441 - val_loss: 127.0256\n",
      "Epoch 1851/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.9325 - val_loss: 115.1051\n",
      "Epoch 1852/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.4157 - val_loss: 1464.9873\n",
      "Epoch 1853/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 354.8959 - val_loss: 199.7373\n",
      "Epoch 1854/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.4903 - val_loss: 138.8845\n",
      "Epoch 1855/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.8957 - val_loss: 125.1654\n",
      "Epoch 1856/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.1754 - val_loss: 232.2847\n",
      "Epoch 1857/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.3260 - val_loss: 127.5182\n",
      "Epoch 1858/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.8474 - val_loss: 130.2861\n",
      "Epoch 1859/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.3494 - val_loss: 111.7213\n",
      "Epoch 1860/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 155.3731 - val_loss: 118.5798\n",
      "Epoch 1861/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.0552 - val_loss: 180.1916\n",
      "Epoch 1862/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 189.8363 - val_loss: 179.4080\n",
      "Epoch 1863/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 153.5712 - val_loss: 119.4029\n",
      "Epoch 1864/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.3037 - val_loss: 112.0598\n",
      "Epoch 1865/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.4022 - val_loss: 117.3761\n",
      "Epoch 1866/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.1352 - val_loss: 232.5394\n",
      "Epoch 1867/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 179.6707 - val_loss: 139.5075\n",
      "Epoch 1868/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.1711 - val_loss: 147.5462\n",
      "Epoch 1869/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.1599 - val_loss: 136.6647\n",
      "Epoch 1870/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.8065 - val_loss: 193.9789\n",
      "Epoch 1871/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.3905 - val_loss: 128.9444\n",
      "Epoch 1872/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.8843 - val_loss: 114.0604\n",
      "Epoch 1873/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.6455 - val_loss: 180.3335\n",
      "Epoch 1874/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.4518 - val_loss: 118.6892\n",
      "Epoch 1875/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 229.7247 - val_loss: 123.4264\n",
      "Epoch 1876/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.9266 - val_loss: 117.7935\n",
      "Epoch 1877/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.0796 - val_loss: 141.1795\n",
      "Epoch 1878/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.9042 - val_loss: 112.0596\n",
      "Epoch 1879/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.1198 - val_loss: 115.6827\n",
      "Epoch 1880/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.0554 - val_loss: 130.6032\n",
      "Epoch 1881/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.3610 - val_loss: 113.9456\n",
      "Epoch 1882/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.4993 - val_loss: 332.0685\n",
      "Epoch 1883/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.5096 - val_loss: 118.5733\n",
      "Epoch 1884/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.2155 - val_loss: 134.5702\n",
      "Epoch 1885/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.4029 - val_loss: 125.9015\n",
      "Epoch 1886/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.3559 - val_loss: 140.1367\n",
      "Epoch 1887/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.9707 - val_loss: 115.1302\n",
      "Epoch 1888/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.4125 - val_loss: 132.0616\n",
      "Epoch 1889/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.4742 - val_loss: 112.2965\n",
      "Epoch 1890/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.8645 - val_loss: 194.6883\n",
      "Epoch 1891/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.7425 - val_loss: 136.3170\n",
      "Epoch 1892/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.4776 - val_loss: 114.8863\n",
      "Epoch 1893/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.3869 - val_loss: 127.9287\n",
      "Epoch 1894/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 217.0159 - val_loss: 115.2850\n",
      "Epoch 1895/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.0203 - val_loss: 110.9838\n",
      "Epoch 1896/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.9988 - val_loss: 110.7608\n",
      "Epoch 1897/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.2763 - val_loss: 130.1407\n",
      "Epoch 1898/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.0395 - val_loss: 120.3463\n",
      "Epoch 1899/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 152.0771 - val_loss: 120.8775\n",
      "Epoch 1900/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 138.4658 - val_loss: 117.9307\n",
      "Epoch 1901/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 155.5911 - val_loss: 115.0282\n",
      "Epoch 1902/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.2792 - val_loss: 155.4763\n",
      "Epoch 1903/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.4671 - val_loss: 120.2465\n",
      "Epoch 1904/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.4309 - val_loss: 143.9340\n",
      "Epoch 1905/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 183.3909 - val_loss: 113.0218\n",
      "Epoch 1906/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.7642 - val_loss: 138.4355\n",
      "Epoch 1907/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.6743 - val_loss: 125.8652\n",
      "Epoch 1908/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.7001 - val_loss: 113.7982\n",
      "Epoch 1909/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.5597 - val_loss: 259.3293\n",
      "Epoch 1910/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.9626 - val_loss: 116.2094\n",
      "Epoch 1911/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.7839 - val_loss: 116.4512\n",
      "Epoch 1912/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.1132 - val_loss: 122.9138\n",
      "Epoch 1913/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.6232 - val_loss: 119.9319\n",
      "Epoch 1914/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.1138 - val_loss: 114.7161\n",
      "Epoch 1915/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.6610 - val_loss: 132.9530\n",
      "Epoch 1916/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 156.9772 - val_loss: 147.3871\n",
      "Epoch 1917/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.6266 - val_loss: 207.2530\n",
      "Epoch 1918/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.0006 - val_loss: 114.2667\n",
      "Epoch 1919/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 235.0120 - val_loss: 288.2228\n",
      "Epoch 1920/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 185.1308 - val_loss: 117.0408\n",
      "Epoch 1921/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 148.6216 - val_loss: 122.0246\n",
      "Epoch 1922/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.5955 - val_loss: 113.7788\n",
      "Epoch 1923/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.1407 - val_loss: 122.9923\n",
      "Epoch 1924/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 153.5873 - val_loss: 118.7141\n",
      "Epoch 1925/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.1457 - val_loss: 130.6095\n",
      "Epoch 1926/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.5717 - val_loss: 130.5763\n",
      "Epoch 1927/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.4512 - val_loss: 142.9027\n",
      "Epoch 1928/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.0377 - val_loss: 118.9998\n",
      "Epoch 1929/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.0194 - val_loss: 116.2278\n",
      "Epoch 1930/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.6675 - val_loss: 121.5875\n",
      "Epoch 1931/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.2514 - val_loss: 136.5130\n",
      "Epoch 1932/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.8311 - val_loss: 114.9669\n",
      "Epoch 1933/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.8357 - val_loss: 156.1695\n",
      "Epoch 1934/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.8907 - val_loss: 140.2607\n",
      "Epoch 1935/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.0432 - val_loss: 185.5783\n",
      "Epoch 1936/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 162.2430 - val_loss: 115.9571\n",
      "Epoch 1937/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.0792 - val_loss: 132.8220\n",
      "Epoch 1938/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.5666 - val_loss: 122.0055\n",
      "Epoch 1939/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.1135 - val_loss: 161.3585\n",
      "Epoch 1940/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.3590 - val_loss: 115.8419\n",
      "Epoch 1941/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.6055 - val_loss: 115.7149\n",
      "Epoch 1942/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.3532 - val_loss: 158.1989\n",
      "Epoch 1943/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.1039 - val_loss: 151.2082\n",
      "Epoch 1944/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.8732 - val_loss: 151.5424\n",
      "Epoch 1945/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.9818 - val_loss: 115.2760\n",
      "Epoch 1946/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.1493 - val_loss: 116.6757\n",
      "Epoch 1947/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.1661 - val_loss: 116.4456\n",
      "Epoch 1948/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.8423 - val_loss: 112.5900\n",
      "Epoch 1949/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.1473 - val_loss: 211.9455\n",
      "Epoch 1950/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.6709 - val_loss: 113.8086\n",
      "Epoch 1951/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 146.1374 - val_loss: 119.0343\n",
      "Epoch 1952/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.0295 - val_loss: 143.6313\n",
      "Epoch 1953/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.5415 - val_loss: 133.4482\n",
      "Epoch 1954/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.4486 - val_loss: 122.8393\n",
      "Epoch 1955/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.4960 - val_loss: 113.2687\n",
      "Epoch 1956/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.4230 - val_loss: 154.8216\n",
      "Epoch 1957/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.1367 - val_loss: 129.4506\n",
      "Epoch 1958/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.1146 - val_loss: 116.0798\n",
      "Epoch 1959/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 136.9595 - val_loss: 134.2563\n",
      "Epoch 1960/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 148.4541 - val_loss: 125.2469\n",
      "Epoch 1961/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 153.3488 - val_loss: 114.3218\n",
      "Epoch 1962/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.2475 - val_loss: 128.0192\n",
      "Epoch 1963/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.1752 - val_loss: 184.8351\n",
      "Epoch 1964/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.4853 - val_loss: 110.6590\n",
      "Epoch 1965/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 156.2118 - val_loss: 125.3473\n",
      "Epoch 1966/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.0799 - val_loss: 142.0197\n",
      "Epoch 1967/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.9030 - val_loss: 144.3177\n",
      "Epoch 1968/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 185.0324 - val_loss: 119.8731\n",
      "Epoch 1969/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.3032 - val_loss: 129.9495\n",
      "Epoch 1970/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.6520 - val_loss: 129.5936\n",
      "Epoch 1971/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.5350 - val_loss: 114.6492\n",
      "Epoch 1972/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.4296 - val_loss: 126.3857\n",
      "Epoch 1973/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.4774 - val_loss: 114.6072\n",
      "Epoch 1974/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.4965 - val_loss: 129.3658\n",
      "Epoch 1975/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.0159 - val_loss: 120.9880\n",
      "Epoch 1976/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.9719 - val_loss: 137.3873\n",
      "Epoch 1977/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.7425 - val_loss: 131.3691\n",
      "Epoch 1978/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.1151 - val_loss: 136.8362\n",
      "Epoch 1979/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.6258 - val_loss: 140.4216\n",
      "Epoch 1980/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.9175 - val_loss: 110.1160\n",
      "Epoch 1981/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.5110 - val_loss: 151.0901\n",
      "Epoch 1982/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.0058 - val_loss: 188.6857\n",
      "Epoch 1983/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.6040 - val_loss: 144.5311\n",
      "Epoch 1984/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.9971 - val_loss: 129.6163\n",
      "Epoch 1985/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.0884 - val_loss: 111.0185\n",
      "Epoch 1986/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.9181 - val_loss: 131.7501\n",
      "Epoch 1987/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.5705 - val_loss: 123.5939\n",
      "Epoch 1988/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.4432 - val_loss: 136.5021\n",
      "Epoch 1989/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 157.8402 - val_loss: 113.7374\n",
      "Epoch 1990/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.8639 - val_loss: 118.9181\n",
      "Epoch 1991/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.8365 - val_loss: 128.6328\n",
      "Epoch 1992/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 185.7056 - val_loss: 134.7699\n",
      "Epoch 1993/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.2321 - val_loss: 119.3665\n",
      "Epoch 1994/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.5002 - val_loss: 123.2572\n",
      "Epoch 1995/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 181.1496 - val_loss: 125.9622\n",
      "Epoch 1996/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.4940 - val_loss: 134.6809\n",
      "Epoch 1997/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.8311 - val_loss: 188.9924\n",
      "Epoch 1998/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.5315 - val_loss: 115.4371\n",
      "Epoch 1999/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.3732 - val_loss: 124.3052\n",
      "Epoch 2000/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.6808 - val_loss: 111.3697\n",
      "Epoch 2001/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.9737 - val_loss: 111.3365\n",
      "Epoch 2002/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.0778 - val_loss: 110.7167\n",
      "Epoch 2003/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.6002 - val_loss: 153.4229\n",
      "Epoch 2004/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.8031 - val_loss: 123.1036\n",
      "Epoch 2005/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.2186 - val_loss: 132.7999\n",
      "Epoch 2006/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.0234 - val_loss: 135.8783\n",
      "Epoch 2007/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.6211 - val_loss: 147.2012\n",
      "Epoch 2008/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.6711 - val_loss: 117.9092\n",
      "Epoch 2009/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.7596 - val_loss: 136.6780\n",
      "Epoch 2010/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.5404 - val_loss: 111.2251\n",
      "Epoch 2011/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 178.2434 - val_loss: 173.1406\n",
      "Epoch 2012/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.8729 - val_loss: 114.0865\n",
      "Epoch 2013/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.3751 - val_loss: 117.3430\n",
      "Epoch 2014/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.2218 - val_loss: 184.6930\n",
      "Epoch 2015/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.6669 - val_loss: 112.9475\n",
      "Epoch 2016/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.0897 - val_loss: 112.3570\n",
      "Epoch 2017/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.9185 - val_loss: 119.3949\n",
      "Epoch 2018/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.6649 - val_loss: 172.4411\n",
      "Epoch 2019/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 147.5842 - val_loss: 129.6044\n",
      "Epoch 2020/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 137.6776 - val_loss: 144.8288\n",
      "Epoch 2021/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 149.9666 - val_loss: 123.1767\n",
      "Epoch 2022/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.6718 - val_loss: 129.1182\n",
      "Epoch 2023/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.2714 - val_loss: 136.0843\n",
      "Epoch 2024/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.3218 - val_loss: 122.2498\n",
      "Epoch 2025/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.8048 - val_loss: 128.8167\n",
      "Epoch 2026/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.5989 - val_loss: 162.3174\n",
      "Epoch 2027/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.8261 - val_loss: 128.7683\n",
      "Epoch 2028/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.7391 - val_loss: 127.7338\n",
      "Epoch 2029/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 194.1329 - val_loss: 156.9697\n",
      "Epoch 2030/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.7277 - val_loss: 140.3463\n",
      "Epoch 2031/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.6294 - val_loss: 117.2153\n",
      "Epoch 2032/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.0378 - val_loss: 112.9744\n",
      "Epoch 2033/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.2057 - val_loss: 112.1077\n",
      "Epoch 2034/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.4526 - val_loss: 135.3307\n",
      "Epoch 2035/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 217.7910 - val_loss: 417.2720\n",
      "Epoch 2036/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.5216 - val_loss: 126.0520\n",
      "Epoch 2037/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.2355 - val_loss: 126.5716\n",
      "Epoch 2038/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 131.1117 - val_loss: 168.8398\n",
      "Epoch 2039/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.5884 - val_loss: 119.3840\n",
      "Epoch 2040/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.7904 - val_loss: 144.7914\n",
      "Epoch 2041/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 135.0627 - val_loss: 138.5926\n",
      "Epoch 2042/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.7653 - val_loss: 191.2486\n",
      "Epoch 2043/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.4913 - val_loss: 125.4267\n",
      "Epoch 2044/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.6983 - val_loss: 120.9346\n",
      "Epoch 2045/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.7034 - val_loss: 114.8422\n",
      "Epoch 2046/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.0122 - val_loss: 325.5632\n",
      "Epoch 2047/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.7212 - val_loss: 120.8999\n",
      "Epoch 2048/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.2283 - val_loss: 133.3819\n",
      "Epoch 2049/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.5824 - val_loss: 127.2559\n",
      "Epoch 2050/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.7230 - val_loss: 128.6082\n",
      "Epoch 2051/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 205.2580 - val_loss: 146.7400\n",
      "Epoch 2052/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.1424 - val_loss: 126.7068\n",
      "Epoch 2053/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.6150 - val_loss: 127.0004\n",
      "Epoch 2054/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.1946 - val_loss: 114.6607\n",
      "Epoch 2055/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.8397 - val_loss: 121.8008\n",
      "Epoch 2056/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.0364 - val_loss: 123.1935\n",
      "Epoch 2057/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.5168 - val_loss: 117.3760\n",
      "Epoch 2058/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.8143 - val_loss: 129.1122\n",
      "Epoch 2059/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.7252 - val_loss: 119.3828\n",
      "Epoch 2060/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.2224 - val_loss: 140.6402\n",
      "Epoch 2061/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.9996 - val_loss: 126.7423\n",
      "Epoch 2062/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.3926 - val_loss: 122.3311\n",
      "Epoch 2063/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.3404 - val_loss: 142.8468\n",
      "Epoch 2064/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.0659 - val_loss: 609.3525\n",
      "Epoch 2065/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 227.9587 - val_loss: 125.9144\n",
      "Epoch 2066/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.1207 - val_loss: 117.2944\n",
      "Epoch 2067/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.9776 - val_loss: 114.3152\n",
      "Epoch 2068/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.9598 - val_loss: 126.9209\n",
      "Epoch 2069/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.3718 - val_loss: 118.2234\n",
      "Epoch 2070/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.0181 - val_loss: 120.6662\n",
      "Epoch 2071/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.2209 - val_loss: 120.4420\n",
      "Epoch 2072/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 144.0909 - val_loss: 117.7707\n",
      "Epoch 2073/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.3379 - val_loss: 122.6961\n",
      "Epoch 2074/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.9942 - val_loss: 112.4164\n",
      "Epoch 2075/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.6775 - val_loss: 120.6651\n",
      "Epoch 2076/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.1502 - val_loss: 116.1255\n",
      "Epoch 2077/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.2535 - val_loss: 122.6472\n",
      "Epoch 2078/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.9616 - val_loss: 125.0912\n",
      "Epoch 2079/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 158.3831 - val_loss: 122.6482\n",
      "Epoch 2080/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 177.1176 - val_loss: 167.7644\n",
      "Epoch 2081/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 157.8029 - val_loss: 119.7006\n",
      "Epoch 2082/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.4774 - val_loss: 111.4173\n",
      "Epoch 2083/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.6994 - val_loss: 115.8284\n",
      "Epoch 2084/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 144.4724 - val_loss: 123.0274\n",
      "Epoch 2085/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.9836 - val_loss: 113.5841\n",
      "Epoch 2086/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.8525 - val_loss: 111.1217\n",
      "Epoch 2087/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.5999 - val_loss: 120.8367\n",
      "Epoch 2088/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.0881 - val_loss: 180.3161\n",
      "Epoch 2089/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.7398 - val_loss: 277.4217\n",
      "Epoch 2090/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 194.4277 - val_loss: 120.5230\n",
      "Epoch 2091/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.3642 - val_loss: 124.6094\n",
      "Epoch 2092/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 146.2829 - val_loss: 108.9060\n",
      "Epoch 2093/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.3760 - val_loss: 113.3668\n",
      "Epoch 2094/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.2604 - val_loss: 199.3950\n",
      "Epoch 2095/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.7026 - val_loss: 117.2117\n",
      "Epoch 2096/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.9331 - val_loss: 116.7704\n",
      "Epoch 2097/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.1747 - val_loss: 121.0422\n",
      "Epoch 2098/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.3551 - val_loss: 140.7002\n",
      "Epoch 2099/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 147.2902 - val_loss: 116.4745\n",
      "Epoch 2100/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.8495 - val_loss: 112.8860\n",
      "Epoch 2101/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.3417 - val_loss: 142.0867\n",
      "Epoch 2102/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.8313 - val_loss: 113.0887\n",
      "Epoch 2103/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.3073 - val_loss: 119.4915\n",
      "Epoch 2104/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.1161 - val_loss: 114.6360\n",
      "Epoch 2105/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.7383 - val_loss: 111.8179\n",
      "Epoch 2106/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.2332 - val_loss: 122.8507\n",
      "Epoch 2107/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.1225 - val_loss: 114.6750\n",
      "Epoch 2108/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.7029 - val_loss: 112.3360\n",
      "Epoch 2109/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.4417 - val_loss: 164.8930\n",
      "Epoch 2110/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.0333 - val_loss: 143.1460\n",
      "Epoch 2111/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.8337 - val_loss: 168.4965\n",
      "Epoch 2112/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.1787 - val_loss: 120.2590\n",
      "Epoch 2113/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.4122 - val_loss: 119.2938\n",
      "Epoch 2114/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 147.0234 - val_loss: 146.8125\n",
      "Epoch 2115/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.4185 - val_loss: 120.4678\n",
      "Epoch 2116/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.3785 - val_loss: 169.9435\n",
      "Epoch 2117/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.0565 - val_loss: 155.2756\n",
      "Epoch 2118/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.7501 - val_loss: 119.1208\n",
      "Epoch 2119/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.4788 - val_loss: 115.8319\n",
      "Epoch 2120/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.8875 - val_loss: 133.9210\n",
      "Epoch 2121/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.1502 - val_loss: 112.2599\n",
      "Epoch 2122/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.8655 - val_loss: 119.0921\n",
      "Epoch 2123/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.6478 - val_loss: 160.0755\n",
      "Epoch 2124/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 178.3790 - val_loss: 123.7833\n",
      "Epoch 2125/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.9965 - val_loss: 124.2417\n",
      "Epoch 2126/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.7573 - val_loss: 118.8975\n",
      "Epoch 2127/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.2840 - val_loss: 124.6772\n",
      "Epoch 2128/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.2908 - val_loss: 131.1791\n",
      "Epoch 2129/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 144.9470 - val_loss: 110.5420\n",
      "Epoch 2130/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.8532 - val_loss: 114.8836\n",
      "Epoch 2131/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.6412 - val_loss: 338.6806\n",
      "Epoch 2132/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.9640 - val_loss: 110.1206\n",
      "Epoch 2133/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.6785 - val_loss: 116.6865\n",
      "Epoch 2134/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.7027 - val_loss: 133.5351\n",
      "Epoch 2135/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.0767 - val_loss: 111.9493\n",
      "Epoch 2136/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.6653 - val_loss: 126.6865\n",
      "Epoch 2137/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.0332 - val_loss: 111.1992\n",
      "Epoch 2138/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.0654 - val_loss: 119.9576\n",
      "Epoch 2139/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 147.7732 - val_loss: 127.8458\n",
      "Epoch 2140/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 171.8608 - val_loss: 181.1008\n",
      "Epoch 2141/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 156.1185 - val_loss: 112.8368\n",
      "Epoch 2142/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 149.1717 - val_loss: 119.4941\n",
      "Epoch 2143/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.8584 - val_loss: 174.1464\n",
      "Epoch 2144/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.6348 - val_loss: 126.0971\n",
      "Epoch 2145/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.9409 - val_loss: 121.6434\n",
      "Epoch 2146/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 180.2888 - val_loss: 129.1031\n",
      "Epoch 2147/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 182.8095 - val_loss: 114.8353\n",
      "Epoch 2148/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.4728 - val_loss: 144.2917\n",
      "Epoch 2149/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.0610 - val_loss: 125.2457\n",
      "Epoch 2150/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.2581 - val_loss: 115.9605\n",
      "Epoch 2151/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.4105 - val_loss: 121.7241\n",
      "Epoch 2152/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.4970 - val_loss: 118.8685\n",
      "Epoch 2153/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.0100 - val_loss: 116.8882\n",
      "Epoch 2154/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.9475 - val_loss: 126.0174\n",
      "Epoch 2155/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.1682 - val_loss: 128.5333\n",
      "Epoch 2156/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.5486 - val_loss: 116.4636\n",
      "Epoch 2157/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.7089 - val_loss: 128.9569\n",
      "Epoch 2158/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.3621 - val_loss: 115.0790\n",
      "Epoch 2159/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.1994 - val_loss: 127.1148\n",
      "Epoch 2160/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.0874 - val_loss: 125.9231\n",
      "Epoch 2161/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.4801 - val_loss: 111.4637\n",
      "Epoch 2162/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.4592 - val_loss: 135.3797\n",
      "Epoch 2163/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.0434 - val_loss: 128.7815\n",
      "Epoch 2164/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.5646 - val_loss: 144.7417\n",
      "Epoch 2165/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.0805 - val_loss: 120.3907\n",
      "Epoch 2166/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.5636 - val_loss: 109.6595\n",
      "Epoch 2167/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.8414 - val_loss: 198.4029\n",
      "Epoch 2168/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.1288 - val_loss: 115.5809\n",
      "Epoch 2169/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.0246 - val_loss: 180.9339\n",
      "Epoch 2170/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 157.0750 - val_loss: 111.7267\n",
      "Epoch 2171/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.1081 - val_loss: 119.0807\n",
      "Epoch 2172/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.5214 - val_loss: 133.1675\n",
      "Epoch 2173/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.4627 - val_loss: 137.3247\n",
      "Epoch 2174/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.1635 - val_loss: 114.3194\n",
      "Epoch 2175/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.6222 - val_loss: 170.4576\n",
      "Epoch 2176/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.0923 - val_loss: 125.5916\n",
      "Epoch 2177/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 195.8270 - val_loss: 125.8549\n",
      "Epoch 2178/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.4179 - val_loss: 122.9566\n",
      "Epoch 2179/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.2683 - val_loss: 154.7497\n",
      "Epoch 2180/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.0300 - val_loss: 176.1459\n",
      "Epoch 2181/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.5372 - val_loss: 109.8103\n",
      "Epoch 2182/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.2179 - val_loss: 128.4109\n",
      "Epoch 2183/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.1569 - val_loss: 122.6340\n",
      "Epoch 2184/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.6790 - val_loss: 147.2302\n",
      "Epoch 2185/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.2162 - val_loss: 135.9713\n",
      "Epoch 2186/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.9004 - val_loss: 156.8886\n",
      "Epoch 2187/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.8982 - val_loss: 111.6848\n",
      "Epoch 2188/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.6292 - val_loss: 119.6838\n",
      "Epoch 2189/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 142.8633 - val_loss: 119.2978\n",
      "Epoch 2190/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 194.0821 - val_loss: 149.1023\n",
      "Epoch 2191/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 150.1257 - val_loss: 122.9583\n",
      "Epoch 2192/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.2431 - val_loss: 114.5787\n",
      "Epoch 2193/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.8360 - val_loss: 115.9588\n",
      "Epoch 2194/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.0254 - val_loss: 181.1512\n",
      "Epoch 2195/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.4016 - val_loss: 116.9268\n",
      "Epoch 2196/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.9379 - val_loss: 131.0299\n",
      "Epoch 2197/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.2278 - val_loss: 131.4798\n",
      "Epoch 2198/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.9858 - val_loss: 123.4168\n",
      "Epoch 2199/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 141.6019 - val_loss: 118.4710\n",
      "Epoch 2200/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 131.9889 - val_loss: 122.9082\n",
      "Epoch 2201/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 142.9070 - val_loss: 125.5766\n",
      "Epoch 2202/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.7284 - val_loss: 133.9848\n",
      "Epoch 2203/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.7104 - val_loss: 136.8534\n",
      "Epoch 2204/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.7228 - val_loss: 116.8682\n",
      "Epoch 2205/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.4043 - val_loss: 115.0613\n",
      "Epoch 2206/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.4018 - val_loss: 144.2132\n",
      "Epoch 2207/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.8593 - val_loss: 123.8112\n",
      "Epoch 2208/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.3363 - val_loss: 167.4347\n",
      "Epoch 2209/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.3763 - val_loss: 163.1646\n",
      "Epoch 2210/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.1244 - val_loss: 123.3937\n",
      "Epoch 2211/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.7380 - val_loss: 143.8795\n",
      "Epoch 2212/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.5208 - val_loss: 156.3118\n",
      "Epoch 2213/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.8780 - val_loss: 125.5064\n",
      "Epoch 2214/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.0659 - val_loss: 137.2637\n",
      "Epoch 2215/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.7492 - val_loss: 111.1392\n",
      "Epoch 2216/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.8532 - val_loss: 131.3424\n",
      "Epoch 2217/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.6904 - val_loss: 112.9472\n",
      "Epoch 2218/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 207.3601 - val_loss: 320.1446\n",
      "Epoch 2219/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 186.0373 - val_loss: 157.1696\n",
      "Epoch 2220/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.9834 - val_loss: 113.9971\n",
      "Epoch 2221/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.0239 - val_loss: 123.6859\n",
      "Epoch 2222/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.3124 - val_loss: 120.6227\n",
      "Epoch 2223/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.6172 - val_loss: 164.6993\n",
      "Epoch 2224/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.7090 - val_loss: 140.4970\n",
      "Epoch 2225/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 140.0516 - val_loss: 135.6008\n",
      "Epoch 2226/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.2492 - val_loss: 122.0396\n",
      "Epoch 2227/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.2519 - val_loss: 219.2597\n",
      "Epoch 2228/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.5119 - val_loss: 124.7727\n",
      "Epoch 2229/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.6069 - val_loss: 155.2905\n",
      "Epoch 2230/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.4528 - val_loss: 117.9755\n",
      "Epoch 2231/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.9017 - val_loss: 124.2358\n",
      "Epoch 2232/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 176.0725 - val_loss: 130.7807\n",
      "Epoch 2233/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 271.2990 - val_loss: 179.6775\n",
      "Epoch 2234/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.8316 - val_loss: 158.5236\n",
      "Epoch 2235/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.3549 - val_loss: 119.5630\n",
      "Epoch 2236/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.1430 - val_loss: 122.9058\n",
      "Epoch 2237/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.9613 - val_loss: 119.2515\n",
      "Epoch 2238/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.9088 - val_loss: 244.8431\n",
      "Epoch 2239/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.5545 - val_loss: 178.0979\n",
      "Epoch 2240/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.1785 - val_loss: 112.4267\n",
      "Epoch 2241/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.2151 - val_loss: 141.2493\n",
      "Epoch 2242/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.5617 - val_loss: 116.9317\n",
      "Epoch 2243/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.7680 - val_loss: 148.2318\n",
      "Epoch 2244/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.4586 - val_loss: 145.4882\n",
      "Epoch 2245/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.5327 - val_loss: 110.4775\n",
      "Epoch 2246/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.8095 - val_loss: 180.5902\n",
      "Epoch 2247/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.5900 - val_loss: 113.2508\n",
      "Epoch 2248/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.9222 - val_loss: 117.6810\n",
      "Epoch 2249/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.4557 - val_loss: 129.8195\n",
      "Epoch 2250/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.3197 - val_loss: 170.0871\n",
      "Epoch 2251/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.1933 - val_loss: 126.5842\n",
      "Epoch 2252/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.9314 - val_loss: 115.1598\n",
      "Epoch 2253/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.4068 - val_loss: 145.2215\n",
      "Epoch 2254/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.5177 - val_loss: 121.3155\n",
      "Epoch 2255/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.1120 - val_loss: 297.0163\n",
      "Epoch 2256/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.2221 - val_loss: 117.5221\n",
      "Epoch 2257/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 133.7646 - val_loss: 110.3326\n",
      "Epoch 2258/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.5965 - val_loss: 115.3179\n",
      "Epoch 2259/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 148.6741 - val_loss: 172.5158\n",
      "Epoch 2260/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 148.4100 - val_loss: 117.6546\n",
      "Epoch 2261/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 139.1727 - val_loss: 135.9016\n",
      "Epoch 2262/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 161.0636 - val_loss: 128.9592\n",
      "Epoch 2263/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 140.9799 - val_loss: 115.6431\n",
      "Epoch 2264/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.0816 - val_loss: 647.6411\n",
      "Epoch 2265/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.5102 - val_loss: 138.5469\n",
      "Epoch 2266/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.8275 - val_loss: 122.7386\n",
      "Epoch 2267/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.0371 - val_loss: 112.9013\n",
      "Epoch 2268/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.7337 - val_loss: 119.7992\n",
      "Epoch 2269/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.9976 - val_loss: 128.9130\n",
      "Epoch 2270/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.6928 - val_loss: 122.1667\n",
      "Epoch 2271/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.6526 - val_loss: 119.8965\n",
      "Epoch 2272/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.4595 - val_loss: 118.5962\n",
      "Epoch 2273/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.6927 - val_loss: 193.4232\n",
      "Epoch 2274/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.8447 - val_loss: 123.4716\n",
      "Epoch 2275/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 142.4068 - val_loss: 130.3140\n",
      "Epoch 2276/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.6078 - val_loss: 151.2353\n",
      "Epoch 2277/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.2328 - val_loss: 119.2931\n",
      "Epoch 2278/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 190.3841 - val_loss: 120.7518\n",
      "Epoch 2279/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 139.4826 - val_loss: 127.3384\n",
      "Epoch 2280/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.3315 - val_loss: 165.8119\n",
      "Epoch 2281/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 132.7011 - val_loss: 122.5053\n",
      "Epoch 2282/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.0546 - val_loss: 159.9678\n",
      "Epoch 2283/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 183.3188 - val_loss: 153.7569\n",
      "Epoch 2284/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.6504 - val_loss: 144.5072\n",
      "Epoch 2285/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.0117 - val_loss: 142.7717\n",
      "Epoch 2286/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.4812 - val_loss: 116.0990\n",
      "Epoch 2287/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.8253 - val_loss: 114.1229\n",
      "Epoch 2288/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.8647 - val_loss: 131.8414\n",
      "Epoch 2289/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.5634 - val_loss: 125.7986\n",
      "Epoch 2290/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.1885 - val_loss: 118.3609\n",
      "Epoch 2291/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.2671 - val_loss: 157.0309\n",
      "Epoch 2292/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.6283 - val_loss: 123.1082\n",
      "Epoch 2293/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.5382 - val_loss: 209.0014\n",
      "Epoch 2294/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.0545 - val_loss: 168.2791\n",
      "Epoch 2295/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.8503 - val_loss: 122.0871\n",
      "Epoch 2296/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 192.3585 - val_loss: 226.1868\n",
      "Epoch 2297/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.6088 - val_loss: 129.1355\n",
      "Epoch 2298/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 135.0212 - val_loss: 110.1960\n",
      "Epoch 2299/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.1303 - val_loss: 155.6209\n",
      "Epoch 2300/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 143.0337 - val_loss: 128.8708\n",
      "Epoch 2301/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 145.4848 - val_loss: 109.2734\n",
      "Epoch 2302/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.5013 - val_loss: 117.8365\n",
      "Epoch 2303/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.3078 - val_loss: 122.5159\n",
      "Epoch 2304/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.7386 - val_loss: 139.9236\n",
      "Epoch 2305/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.9877 - val_loss: 119.8759\n",
      "Epoch 2306/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.6735 - val_loss: 115.9760\n",
      "Epoch 2307/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 131.4040 - val_loss: 112.3608\n",
      "Epoch 2308/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.6566 - val_loss: 119.6621\n",
      "Epoch 2309/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 139.3834 - val_loss: 120.9671\n",
      "Epoch 2310/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.6248 - val_loss: 112.9480\n",
      "Epoch 2311/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.5926 - val_loss: 118.7447\n",
      "Epoch 2312/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 172.9417 - val_loss: 127.3959\n",
      "Epoch 2313/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 186.0189 - val_loss: 196.5458\n",
      "Epoch 2314/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.3379 - val_loss: 134.1153\n",
      "Epoch 2315/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.1251 - val_loss: 137.3353\n",
      "Epoch 2316/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.6618 - val_loss: 118.5959\n",
      "Epoch 2317/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.0628 - val_loss: 134.0041\n",
      "Epoch 2318/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.3085 - val_loss: 122.7898\n",
      "Epoch 2319/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 156.7618 - val_loss: 131.8697\n",
      "Epoch 2320/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 144.1825 - val_loss: 112.9897\n",
      "Epoch 2321/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 142.0569 - val_loss: 128.6385\n",
      "Epoch 2322/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 174.9128 - val_loss: 113.6467\n",
      "Epoch 2323/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.7366 - val_loss: 119.0781\n",
      "Epoch 2324/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.3345 - val_loss: 131.5528\n",
      "Epoch 2325/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.4159 - val_loss: 130.9100\n",
      "Epoch 2326/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.3444 - val_loss: 130.1986\n",
      "Epoch 2327/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.5700 - val_loss: 121.6945\n",
      "Epoch 2328/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.1388 - val_loss: 109.1318\n",
      "Epoch 2329/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.6327 - val_loss: 113.9579\n",
      "Epoch 2330/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.4094 - val_loss: 118.3974\n",
      "Epoch 2331/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.4026 - val_loss: 179.7111\n",
      "Epoch 2332/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.3669 - val_loss: 130.9685\n",
      "Epoch 2333/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.2449 - val_loss: 138.4096\n",
      "Epoch 2334/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.8428 - val_loss: 128.7474\n",
      "Epoch 2335/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.8265 - val_loss: 122.2357\n",
      "Epoch 2336/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.2791 - val_loss: 115.1756\n",
      "Epoch 2337/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 142.6752 - val_loss: 220.9331\n",
      "Epoch 2338/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 196.6770 - val_loss: 117.2844\n",
      "Epoch 2339/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.2313 - val_loss: 122.7827\n",
      "Epoch 2340/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.2118 - val_loss: 116.9681\n",
      "Epoch 2341/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.6429 - val_loss: 133.3709\n",
      "Epoch 2342/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.1710 - val_loss: 112.6634\n",
      "Epoch 2343/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 148.1268 - val_loss: 114.5453\n",
      "Epoch 2344/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.0277 - val_loss: 141.2862\n",
      "Epoch 2345/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.4165 - val_loss: 126.8486\n",
      "Epoch 2346/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 267.4506 - val_loss: 119.1443\n",
      "Epoch 2347/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 159.6723 - val_loss: 115.9333\n",
      "Epoch 2348/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 135.7914 - val_loss: 124.0181\n",
      "Epoch 2349/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 134.6752 - val_loss: 124.5700\n",
      "Epoch 2350/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.1112 - val_loss: 137.5035\n",
      "Epoch 2351/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 134.1994 - val_loss: 131.7800\n",
      "Epoch 2352/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.4592 - val_loss: 127.5610\n",
      "Epoch 2353/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.4003 - val_loss: 155.7004\n",
      "Epoch 2354/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.4000 - val_loss: 120.2229\n",
      "Epoch 2355/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.2774 - val_loss: 120.2829\n",
      "Epoch 2356/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.1554 - val_loss: 147.5592\n",
      "Epoch 2357/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.1476 - val_loss: 116.9596\n",
      "Epoch 2358/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 131.5422 - val_loss: 117.1165\n",
      "Epoch 2359/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 147.2535 - val_loss: 118.8689\n",
      "Epoch 2360/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 140.0332 - val_loss: 118.2606\n",
      "Epoch 2361/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 134.4223 - val_loss: 120.5400\n",
      "Epoch 2362/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.0849 - val_loss: 114.0939\n",
      "Epoch 2363/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 151.3627 - val_loss: 119.6184\n",
      "Epoch 2364/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 155.7233 - val_loss: 155.0164\n",
      "Epoch 2365/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.8148 - val_loss: 207.2437\n",
      "Epoch 2366/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.6000 - val_loss: 123.2902\n",
      "Epoch 2367/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 141.3834 - val_loss: 120.0307\n",
      "Epoch 2368/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 148.1570 - val_loss: 120.6309\n",
      "Epoch 2369/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.5021 - val_loss: 152.8006\n",
      "Epoch 2370/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.8567 - val_loss: 142.6850\n",
      "Epoch 2371/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 142.5825 - val_loss: 115.8827\n",
      "Epoch 2372/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 136.4105 - val_loss: 115.7015\n",
      "Epoch 2373/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.4468 - val_loss: 125.8485\n",
      "Epoch 2374/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.4045 - val_loss: 113.1557\n",
      "Epoch 2375/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 223.3118 - val_loss: 133.4744\n",
      "Epoch 2376/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.6036 - val_loss: 112.8528\n",
      "Epoch 2377/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 141.6058 - val_loss: 137.6376\n",
      "Epoch 2378/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.6089 - val_loss: 124.1829\n",
      "Epoch 2379/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 133.6321 - val_loss: 121.5282\n",
      "Epoch 2380/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.9447 - val_loss: 127.7861\n",
      "Epoch 2381/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 170.8543 - val_loss: 162.1454\n",
      "Epoch 2382/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 195.6202 - val_loss: 130.2424\n",
      "Epoch 2383/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 153.4732 - val_loss: 114.0408\n",
      "Epoch 2384/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.8262 - val_loss: 160.8383\n",
      "Epoch 2385/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.5445 - val_loss: 123.8537\n",
      "Epoch 2386/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 152.0221 - val_loss: 127.5514\n",
      "Epoch 2387/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 135.2290 - val_loss: 120.3188\n",
      "Epoch 2388/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 148.9576 - val_loss: 122.4984\n",
      "Epoch 2389/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 133.7693 - val_loss: 165.8140\n",
      "Epoch 2390/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.2201 - val_loss: 135.3963\n",
      "Epoch 2391/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.5202 - val_loss: 116.4640\n",
      "Epoch 2392/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 189.5791 - val_loss: 214.3277\n",
      "Epoch 2393/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 164.0869 - val_loss: 190.8037\n",
      "Epoch 2394/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.1977 - val_loss: 120.2603\n",
      "Epoch 2395/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.3054 - val_loss: 128.2038\n",
      "Epoch 2396/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.6977 - val_loss: 139.2501\n",
      "Epoch 2397/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 200.5066 - val_loss: 115.0304\n",
      "Epoch 2398/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.7854 - val_loss: 154.1201\n",
      "Epoch 2399/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.3591 - val_loss: 112.4186\n",
      "Epoch 2400/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.2360 - val_loss: 115.3267\n",
      "Epoch 2401/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.6003 - val_loss: 115.2344\n",
      "Epoch 2402/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.0699 - val_loss: 117.1335\n",
      "Epoch 2403/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.6100 - val_loss: 113.1391\n",
      "Epoch 2404/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.2617 - val_loss: 133.2037\n",
      "Epoch 2405/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.9261 - val_loss: 121.1706\n",
      "Epoch 2406/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.7479 - val_loss: 115.1001\n",
      "Epoch 2407/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.5853 - val_loss: 109.8546\n",
      "Epoch 2408/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.4908 - val_loss: 111.8713\n",
      "Epoch 2409/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.7803 - val_loss: 112.6077\n",
      "Epoch 2410/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.7898 - val_loss: 120.0559\n",
      "Epoch 2411/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.7755 - val_loss: 117.6500\n",
      "Epoch 2412/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.2306 - val_loss: 117.8191\n",
      "Epoch 2413/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.5033 - val_loss: 130.8032\n",
      "Epoch 2414/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.4653 - val_loss: 132.5837\n",
      "Epoch 2415/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.4732 - val_loss: 147.7433\n",
      "Epoch 2416/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.8997 - val_loss: 143.2435\n",
      "Epoch 2417/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.6805 - val_loss: 116.3545\n",
      "Epoch 2418/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.0489 - val_loss: 122.6785\n",
      "Epoch 2419/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.9043 - val_loss: 131.2470\n",
      "Epoch 2420/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.4034 - val_loss: 120.9687\n",
      "Epoch 2421/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.5205 - val_loss: 116.2459\n",
      "Epoch 2422/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.8536 - val_loss: 133.1816\n",
      "Epoch 2423/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.5133 - val_loss: 114.9145\n",
      "Epoch 2424/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.5984 - val_loss: 123.5309\n",
      "Epoch 2425/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.5099 - val_loss: 121.4988\n",
      "Epoch 2426/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.2044 - val_loss: 120.5997\n",
      "Epoch 2427/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.0549 - val_loss: 196.4941\n",
      "Epoch 2428/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.6465 - val_loss: 116.0197\n",
      "Epoch 2429/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 158.0483 - val_loss: 128.3799\n",
      "Epoch 2430/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.1917 - val_loss: 116.4281\n",
      "Epoch 2431/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.7134 - val_loss: 120.4470\n",
      "Epoch 2432/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.5308 - val_loss: 116.2774\n",
      "Epoch 2433/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.6108 - val_loss: 177.8839\n",
      "Epoch 2434/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.2663 - val_loss: 124.5143\n",
      "Epoch 2435/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.9261 - val_loss: 114.1031\n",
      "Epoch 2436/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.5951 - val_loss: 116.2371\n",
      "Epoch 2437/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 132.1587 - val_loss: 159.5641\n",
      "Epoch 2438/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 144.1952 - val_loss: 167.8750\n",
      "Epoch 2439/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 159.7559 - val_loss: 113.8233\n",
      "Epoch 2440/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.7847 - val_loss: 121.2535\n",
      "Epoch 2441/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.6053 - val_loss: 110.7551\n",
      "Epoch 2442/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.6678 - val_loss: 120.6058\n",
      "Epoch 2443/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.0364 - val_loss: 112.0028\n",
      "Epoch 2444/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.3470 - val_loss: 114.7623\n",
      "Epoch 2445/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 136.9193 - val_loss: 129.9503\n",
      "Epoch 2446/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.0883 - val_loss: 174.1376\n",
      "Epoch 2447/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 139.7587 - val_loss: 114.6543\n",
      "Epoch 2448/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.9275 - val_loss: 115.4782\n",
      "Epoch 2449/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.1525 - val_loss: 160.7407\n",
      "Epoch 2450/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.7453 - val_loss: 121.8328\n",
      "Epoch 2451/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.2169 - val_loss: 135.5495\n",
      "Epoch 2452/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.2095 - val_loss: 113.5709\n",
      "Epoch 2453/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 142.9520 - val_loss: 117.4542\n",
      "Epoch 2454/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.8017 - val_loss: 179.8717\n",
      "Epoch 2455/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.7601 - val_loss: 149.9428\n",
      "Epoch 2456/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.9628 - val_loss: 160.4249\n",
      "Epoch 2457/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.4466 - val_loss: 205.2790\n",
      "Epoch 2458/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.8295 - val_loss: 141.6185\n",
      "Epoch 2459/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.2629 - val_loss: 122.9413\n",
      "Epoch 2460/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 193.5923 - val_loss: 123.9105\n",
      "Epoch 2461/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.9325 - val_loss: 125.1547\n",
      "Epoch 2462/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.9063 - val_loss: 115.8727\n",
      "Epoch 2463/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.0199 - val_loss: 147.5414\n",
      "Epoch 2464/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.6367 - val_loss: 124.7237\n",
      "Epoch 2465/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.9865 - val_loss: 201.8598\n",
      "Epoch 2466/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.8716 - val_loss: 118.1735\n",
      "Epoch 2467/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.0362 - val_loss: 131.7683\n",
      "Epoch 2468/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.3891 - val_loss: 155.5780\n",
      "Epoch 2469/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.5487 - val_loss: 376.5745\n",
      "Epoch 2470/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.6892 - val_loss: 109.7531\n",
      "Epoch 2471/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.5990 - val_loss: 117.0708\n",
      "Epoch 2472/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.7380 - val_loss: 136.7870\n",
      "Epoch 2473/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.7121 - val_loss: 124.2338\n",
      "Epoch 2474/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.1103 - val_loss: 117.1588\n",
      "Epoch 2475/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.3822 - val_loss: 119.1819\n",
      "Epoch 2476/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.2047 - val_loss: 112.8407\n",
      "Epoch 2477/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 138.6451 - val_loss: 117.5663\n",
      "Epoch 2478/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.5214 - val_loss: 196.6197\n",
      "Epoch 2479/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.4379 - val_loss: 121.1336\n",
      "Epoch 2480/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.7698 - val_loss: 163.9180\n",
      "Epoch 2481/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.8701 - val_loss: 134.2590\n",
      "Epoch 2482/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.4593 - val_loss: 150.8926\n",
      "Epoch 2483/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.0866 - val_loss: 219.9735\n",
      "Epoch 2484/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.1278 - val_loss: 121.0273\n",
      "Epoch 2485/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.3105 - val_loss: 121.8574\n",
      "Epoch 2486/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.5052 - val_loss: 111.6534\n",
      "Epoch 2487/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.2053 - val_loss: 122.2497\n",
      "Epoch 2488/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.3406 - val_loss: 117.9122\n",
      "Epoch 2489/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.7128 - val_loss: 116.2562\n",
      "Epoch 2490/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.7645 - val_loss: 133.0332\n",
      "Epoch 2491/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.6288 - val_loss: 141.1825\n",
      "Epoch 2492/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.1492 - val_loss: 113.3650\n",
      "Epoch 2493/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 184.7162 - val_loss: 138.9352\n",
      "Epoch 2494/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.4371 - val_loss: 117.2740\n",
      "Epoch 2495/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.6335 - val_loss: 110.7580\n",
      "Epoch 2496/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.2786 - val_loss: 112.1216\n",
      "Epoch 2497/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 141.6549 - val_loss: 152.8816\n",
      "Epoch 2498/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 171.0438 - val_loss: 142.7887\n",
      "Epoch 2499/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 132.8386 - val_loss: 111.5577\n",
      "Epoch 2500/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.2641 - val_loss: 122.9275\n",
      "Epoch 2501/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.9438 - val_loss: 122.1040\n",
      "Epoch 2502/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.3484 - val_loss: 154.0443\n",
      "Epoch 2503/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 133.9135 - val_loss: 120.6763\n",
      "Epoch 2504/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.0655 - val_loss: 220.4981\n",
      "Epoch 2505/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.6835 - val_loss: 123.0205\n",
      "Epoch 2506/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.6834 - val_loss: 126.8715\n",
      "Epoch 2507/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.4165 - val_loss: 120.4169\n",
      "Epoch 2508/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.0709 - val_loss: 110.0522\n",
      "Epoch 2509/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.7419 - val_loss: 118.5296\n",
      "Epoch 2510/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.8003 - val_loss: 123.1715\n",
      "Epoch 2511/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.3149 - val_loss: 137.6104\n",
      "Epoch 2512/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 152.2794 - val_loss: 142.5530\n",
      "Epoch 2513/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.3412 - val_loss: 137.3263\n",
      "Epoch 2514/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.6857 - val_loss: 121.2653\n",
      "Epoch 2515/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 142.6219 - val_loss: 123.3734\n",
      "Epoch 2516/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.7217 - val_loss: 116.4645\n",
      "Epoch 2517/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.2008 - val_loss: 123.2337\n",
      "Epoch 2518/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 164.2653 - val_loss: 145.5407\n",
      "Epoch 2519/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 145.8335 - val_loss: 117.2618\n",
      "Epoch 2520/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.7364 - val_loss: 197.4434\n",
      "Epoch 2521/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 144.8614 - val_loss: 136.3626\n",
      "Epoch 2522/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.1359 - val_loss: 113.4273\n",
      "Epoch 2523/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.6183 - val_loss: 121.7731\n",
      "Epoch 2524/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.6253 - val_loss: 121.6373\n",
      "Epoch 2525/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.3583 - val_loss: 116.5397\n",
      "Epoch 2526/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.8644 - val_loss: 112.7883\n",
      "Epoch 2527/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.4824 - val_loss: 319.6611\n",
      "Epoch 2528/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.2270 - val_loss: 150.1948\n",
      "Epoch 2529/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.9870 - val_loss: 113.5233\n",
      "Epoch 2530/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.5108 - val_loss: 124.9744\n",
      "Epoch 2531/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.6563 - val_loss: 158.3240\n",
      "Epoch 2532/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.8777 - val_loss: 120.0142\n",
      "Epoch 2533/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.0580 - val_loss: 136.5208\n",
      "Epoch 2534/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.5384 - val_loss: 113.4005\n",
      "Epoch 2535/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.3007 - val_loss: 120.0931\n",
      "Epoch 2536/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.8519 - val_loss: 123.8810\n",
      "Epoch 2537/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.8603 - val_loss: 115.0718\n",
      "Epoch 2538/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.5806 - val_loss: 119.2636\n",
      "Epoch 2539/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.1989 - val_loss: 118.0304\n",
      "Epoch 2540/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.1803 - val_loss: 112.5797\n",
      "Epoch 2541/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 131.9486 - val_loss: 119.7956\n",
      "Epoch 2542/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.0100 - val_loss: 148.0119\n",
      "Epoch 2543/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.0435 - val_loss: 117.6264\n",
      "Epoch 2544/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 64us/step - loss: 132.7582 - val_loss: 164.2290\n",
      "Epoch 2545/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.4992 - val_loss: 139.7603\n",
      "Epoch 2546/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.4196 - val_loss: 116.4866\n",
      "Epoch 2547/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 154.9745 - val_loss: 114.7558\n",
      "Epoch 2548/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 140.3145 - val_loss: 126.5392\n",
      "Epoch 2549/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.0875 - val_loss: 116.1698\n",
      "Epoch 2550/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.5050 - val_loss: 127.9264\n",
      "Epoch 2551/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.2687 - val_loss: 127.6870\n",
      "Epoch 2552/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.9995 - val_loss: 125.8594\n",
      "Epoch 2553/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.8586 - val_loss: 153.5016\n",
      "Epoch 2554/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.3052 - val_loss: 133.7416\n",
      "Epoch 2555/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.1141 - val_loss: 113.4388\n",
      "Epoch 2556/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 160.4302 - val_loss: 111.0556\n",
      "Epoch 2557/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 146.3806 - val_loss: 122.1924\n",
      "Epoch 2558/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 141.9413 - val_loss: 109.7818\n",
      "Epoch 2559/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 137.4106 - val_loss: 128.2380\n",
      "Epoch 2560/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.7692 - val_loss: 137.9740\n",
      "Epoch 2561/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.6530 - val_loss: 138.7774\n",
      "Epoch 2562/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.6623 - val_loss: 115.6212\n",
      "Epoch 2563/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.8296 - val_loss: 136.4138\n",
      "Epoch 2564/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.9320 - val_loss: 128.9033\n",
      "Epoch 2565/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.5103 - val_loss: 156.6959\n",
      "Epoch 2566/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 213.7948 - val_loss: 159.3329\n",
      "Epoch 2567/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.3400 - val_loss: 130.9925\n",
      "Epoch 2568/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.7836 - val_loss: 119.2605\n",
      "Epoch 2569/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.5457 - val_loss: 135.1671\n",
      "Epoch 2570/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.5644 - val_loss: 139.8465\n",
      "Epoch 2571/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.5466 - val_loss: 136.4598\n",
      "Epoch 2572/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 178.4681 - val_loss: 309.1879\n",
      "Epoch 2573/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.9062 - val_loss: 116.9699\n",
      "Epoch 2574/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.8429 - val_loss: 115.7289\n",
      "Epoch 2575/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.4569 - val_loss: 122.7140\n",
      "Epoch 2576/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.5775 - val_loss: 122.5033\n",
      "Epoch 2577/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.0235 - val_loss: 133.8941\n",
      "Epoch 2578/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.5207 - val_loss: 187.9504\n",
      "Epoch 2579/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.1488 - val_loss: 119.8166\n",
      "Epoch 2580/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.5977 - val_loss: 120.4121\n",
      "Epoch 2581/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.3813 - val_loss: 116.2108\n",
      "Epoch 2582/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.4107 - val_loss: 122.1646\n",
      "Epoch 2583/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.0753 - val_loss: 119.4456\n",
      "Epoch 2584/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.5824 - val_loss: 139.5932\n",
      "Epoch 2585/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 142.4919 - val_loss: 150.0603\n",
      "Epoch 2586/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 133.6802 - val_loss: 110.3481\n",
      "Epoch 2587/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.7176 - val_loss: 136.5157\n",
      "Epoch 2588/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.4177 - val_loss: 111.0786\n",
      "Epoch 2589/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.5981 - val_loss: 138.2813\n",
      "Epoch 2590/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.7093 - val_loss: 109.8859\n",
      "Epoch 2591/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 163.9120 - val_loss: 114.5579\n",
      "Epoch 2592/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.9876 - val_loss: 113.4315\n",
      "Epoch 2593/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.6509 - val_loss: 118.3270\n",
      "Epoch 2594/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.9259 - val_loss: 113.4161\n",
      "Epoch 2595/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.9014 - val_loss: 117.5119\n",
      "Epoch 2596/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.9761 - val_loss: 120.7483\n",
      "Epoch 2597/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.0463 - val_loss: 128.2589\n",
      "Epoch 2598/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.5702 - val_loss: 118.4728\n",
      "Epoch 2599/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.5965 - val_loss: 136.6797\n",
      "Epoch 2600/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.8472 - val_loss: 189.7916\n",
      "Epoch 2601/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.1390 - val_loss: 110.9244\n",
      "Epoch 2602/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.8096 - val_loss: 110.8018\n",
      "Epoch 2603/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.6140 - val_loss: 152.5257\n",
      "Epoch 2604/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.9541 - val_loss: 113.5580\n",
      "Epoch 2605/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.1951 - val_loss: 133.7070\n",
      "Epoch 2606/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.2207 - val_loss: 123.5328\n",
      "Epoch 2607/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.3907 - val_loss: 142.5609\n",
      "Epoch 2608/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.0141 - val_loss: 314.1064\n",
      "Epoch 2609/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.4437 - val_loss: 149.2276\n",
      "Epoch 2610/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 229.2849 - val_loss: 164.4890\n",
      "Epoch 2611/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.6998 - val_loss: 117.3137\n",
      "Epoch 2612/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.7030 - val_loss: 131.9652\n",
      "Epoch 2613/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 129.6047 - val_loss: 114.0202\n",
      "Epoch 2614/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.3589 - val_loss: 149.7407\n",
      "Epoch 2615/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.0209 - val_loss: 180.9323\n",
      "Epoch 2616/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 68us/step - loss: 138.1829 - val_loss: 128.2218\n",
      "Epoch 2617/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 143.5606 - val_loss: 132.6592\n",
      "Epoch 2618/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 140.3358 - val_loss: 114.2519\n",
      "Epoch 2619/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 136.3851 - val_loss: 112.7524\n",
      "Epoch 2620/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.0619 - val_loss: 124.8060\n",
      "Epoch 2621/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.8010 - val_loss: 152.1707\n",
      "Epoch 2622/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.7894 - val_loss: 121.0384\n",
      "Epoch 2623/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.5342 - val_loss: 117.1235\n",
      "Epoch 2624/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 169.8593 - val_loss: 131.6009\n",
      "Epoch 2625/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.9030 - val_loss: 181.8772\n",
      "Epoch 2626/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 297.1641 - val_loss: 140.7560\n",
      "Epoch 2627/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 182.0215 - val_loss: 119.1631\n",
      "Epoch 2628/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.9883 - val_loss: 119.2932\n",
      "Epoch 2629/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.1122 - val_loss: 117.3675\n",
      "Epoch 2630/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.8923 - val_loss: 120.4751\n",
      "Epoch 2631/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.3780 - val_loss: 116.6371\n",
      "Epoch 2632/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.0777 - val_loss: 112.3090\n",
      "Epoch 2633/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.1510 - val_loss: 117.9679\n",
      "Epoch 2634/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.2474 - val_loss: 113.3259\n",
      "Epoch 2635/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.6286 - val_loss: 165.0411\n",
      "Epoch 2636/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.6015 - val_loss: 161.0409\n",
      "Epoch 2637/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 135.1145 - val_loss: 137.6334\n",
      "Epoch 2638/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.3979 - val_loss: 114.2175\n",
      "Epoch 2639/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.4941 - val_loss: 125.3373\n",
      "Epoch 2640/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.2396 - val_loss: 136.6176\n",
      "Epoch 2641/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.3492 - val_loss: 115.1394\n",
      "Epoch 2642/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.5196 - val_loss: 118.0736\n",
      "Epoch 2643/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.7126 - val_loss: 114.9948\n",
      "Epoch 2644/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.0585 - val_loss: 116.6645\n",
      "Epoch 2645/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.7300 - val_loss: 121.6689\n",
      "Epoch 2646/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 182.5840 - val_loss: 141.9637\n",
      "Epoch 2647/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.9881 - val_loss: 203.9834\n",
      "Epoch 2648/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.2824 - val_loss: 117.6064\n",
      "Epoch 2649/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.7297 - val_loss: 115.6881\n",
      "Epoch 2650/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.0572 - val_loss: 137.7300\n",
      "Epoch 2651/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.5158 - val_loss: 113.8030\n",
      "Epoch 2652/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.6563 - val_loss: 115.5190\n",
      "Epoch 2653/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.7264 - val_loss: 111.3895\n",
      "Epoch 2654/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.1317 - val_loss: 113.9362\n",
      "Epoch 2655/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.5246 - val_loss: 170.4468\n",
      "Epoch 2656/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.5752 - val_loss: 149.3493\n",
      "Epoch 2657/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 299.0808 - val_loss: 245.3932\n",
      "Epoch 2658/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 267.6189 - val_loss: 139.4639\n",
      "Epoch 2659/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 241.9003 - val_loss: 133.8446\n",
      "Epoch 2660/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.6545 - val_loss: 153.0344\n",
      "Epoch 2661/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 190.5375 - val_loss: 136.4360\n",
      "Epoch 2662/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.6800 - val_loss: 133.0150\n",
      "Epoch 2663/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.6827 - val_loss: 162.5690\n",
      "Epoch 2664/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.1837 - val_loss: 131.8053\n",
      "Epoch 2665/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 162.6954 - val_loss: 128.9360\n",
      "Epoch 2666/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 171.1030 - val_loss: 149.6346\n",
      "Epoch 2667/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 212.8713 - val_loss: 131.2660\n",
      "Epoch 2668/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.8814 - val_loss: 131.0821\n",
      "Epoch 2669/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.7249 - val_loss: 143.9092\n",
      "Epoch 2670/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 199.2309 - val_loss: 168.0215\n",
      "Epoch 2671/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.6852 - val_loss: 121.8898\n",
      "Epoch 2672/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.4083 - val_loss: 127.9750\n",
      "Epoch 2673/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.8059 - val_loss: 130.6815\n",
      "Epoch 2674/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.4752 - val_loss: 133.1594\n",
      "Epoch 2675/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 192.4628 - val_loss: 118.3533\n",
      "Epoch 2676/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 173.9105 - val_loss: 122.7329\n",
      "Epoch 2677/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 149.9130 - val_loss: 127.6921\n",
      "Epoch 2678/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 151.3518 - val_loss: 157.2607\n",
      "Epoch 2679/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 145.4754 - val_loss: 118.5146\n",
      "Epoch 2680/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.5438 - val_loss: 206.9506\n",
      "Epoch 2681/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.9832 - val_loss: 278.0272\n",
      "Epoch 2682/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.7794 - val_loss: 135.3619\n",
      "Epoch 2683/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 238.2325 - val_loss: 144.8580\n",
      "Epoch 2684/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.8959 - val_loss: 127.8449\n",
      "Epoch 2685/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.3506 - val_loss: 117.2049\n",
      "Epoch 2686/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 259.3163 - val_loss: 127.9188\n",
      "Epoch 2687/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.5180 - val_loss: 143.4699\n",
      "Epoch 2688/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.4794 - val_loss: 122.7759\n",
      "Epoch 2689/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.1980 - val_loss: 119.4522\n",
      "Epoch 2690/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.0779 - val_loss: 122.9667\n",
      "Epoch 2691/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.3914 - val_loss: 117.4401\n",
      "Epoch 2692/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.9511 - val_loss: 152.1862\n",
      "Epoch 2693/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.3595 - val_loss: 118.0996\n",
      "Epoch 2694/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 201.0946 - val_loss: 180.0006\n",
      "Epoch 2695/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.0871 - val_loss: 163.5504\n",
      "Epoch 2696/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.6101 - val_loss: 128.8557\n",
      "Epoch 2697/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.6394 - val_loss: 207.8735\n",
      "Epoch 2698/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.0153 - val_loss: 129.8402\n",
      "Epoch 2699/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.5461 - val_loss: 114.7170\n",
      "Epoch 2700/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.4164 - val_loss: 139.3097\n",
      "Epoch 2701/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 194.1045 - val_loss: 177.1684\n",
      "Epoch 2702/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.1237 - val_loss: 165.5626\n",
      "Epoch 2703/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.5301 - val_loss: 118.3845\n",
      "Epoch 2704/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.9455 - val_loss: 134.8518\n",
      "Epoch 2705/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 157.2203 - val_loss: 141.2120\n",
      "Epoch 2706/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.1362 - val_loss: 121.2309\n",
      "Epoch 2707/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.8876 - val_loss: 156.6533\n",
      "Epoch 2708/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.8652 - val_loss: 114.3835\n",
      "Epoch 2709/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.8083 - val_loss: 129.0394\n",
      "Epoch 2710/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.4960 - val_loss: 173.5820\n",
      "Epoch 2711/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.8075 - val_loss: 117.6402\n",
      "Epoch 2712/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.2976 - val_loss: 177.9968\n",
      "Epoch 2713/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.3706 - val_loss: 134.8540\n",
      "Epoch 2714/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.6170 - val_loss: 143.0192\n",
      "Epoch 2715/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.7779 - val_loss: 115.6973\n",
      "Epoch 2716/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.6545 - val_loss: 151.9066\n",
      "Epoch 2717/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.8555 - val_loss: 111.1099\n",
      "Epoch 2718/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.9950 - val_loss: 174.7320\n",
      "Epoch 2719/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.9784 - val_loss: 123.1555\n",
      "Epoch 2720/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 267.2739 - val_loss: 182.0821\n",
      "Epoch 2721/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 169.9665 - val_loss: 120.6950\n",
      "Epoch 2722/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.8137 - val_loss: 145.7845\n",
      "Epoch 2723/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.4047 - val_loss: 112.4561\n",
      "Epoch 2724/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.5046 - val_loss: 153.3844\n",
      "Epoch 2725/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.9198 - val_loss: 125.0605\n",
      "Epoch 2726/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.0085 - val_loss: 131.2497\n",
      "Epoch 2727/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.4518 - val_loss: 115.7648\n",
      "Epoch 2728/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.6568 - val_loss: 113.9712\n",
      "Epoch 2729/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.2627 - val_loss: 141.7168\n",
      "Epoch 2730/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.8423 - val_loss: 116.5306\n",
      "Epoch 2731/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.0869 - val_loss: 114.9558\n",
      "Epoch 2732/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 154.0284 - val_loss: 117.7018\n",
      "Epoch 2733/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 140.6162 - val_loss: 115.0613\n",
      "Epoch 2734/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 142.2529 - val_loss: 142.0601\n",
      "Epoch 2735/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.8764 - val_loss: 114.0089\n",
      "Epoch 2736/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 139.3566 - val_loss: 122.2541\n",
      "Epoch 2737/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 138.9402 - val_loss: 133.4142\n",
      "Epoch 2738/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 146.5344 - val_loss: 156.9501\n",
      "Epoch 2739/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.0442 - val_loss: 116.5369\n",
      "Epoch 2740/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.1840 - val_loss: 121.8995\n",
      "Epoch 2741/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.4123 - val_loss: 138.2880\n",
      "Epoch 2742/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.7877 - val_loss: 119.2262\n",
      "Epoch 2743/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.1349 - val_loss: 147.4931\n",
      "Epoch 2744/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 201.0168 - val_loss: 119.2133\n",
      "Epoch 2745/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.2115 - val_loss: 150.3793\n",
      "Epoch 2746/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.7000 - val_loss: 122.0891\n",
      "Epoch 2747/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.5227 - val_loss: 112.3939\n",
      "Epoch 2748/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.5707 - val_loss: 139.0337\n",
      "Epoch 2749/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.7528 - val_loss: 126.9994\n",
      "Epoch 2750/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 137.0783 - val_loss: 146.4193\n",
      "Epoch 2751/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 136.4223 - val_loss: 131.4930\n",
      "Epoch 2752/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 131.2801 - val_loss: 120.9964\n",
      "Epoch 2753/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.2475 - val_loss: 123.2974\n",
      "Epoch 2754/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.0454 - val_loss: 132.9288\n",
      "Epoch 2755/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 139.7249 - val_loss: 130.4051\n",
      "Epoch 2756/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.2501 - val_loss: 130.6624\n",
      "Epoch 2757/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.7488 - val_loss: 118.3774\n",
      "Epoch 2758/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 213.0840 - val_loss: 118.6393\n",
      "Epoch 2759/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 150.4208 - val_loss: 129.0319\n",
      "Epoch 2760/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.9372 - val_loss: 147.3516\n",
      "Epoch 2761/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.4001 - val_loss: 191.2223\n",
      "Epoch 2762/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.4561 - val_loss: 121.3658\n",
      "Epoch 2763/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.1742 - val_loss: 120.4113\n",
      "Epoch 2764/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 138.3127 - val_loss: 117.4853\n",
      "Epoch 2765/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.9778 - val_loss: 145.5424\n",
      "Epoch 2766/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.5241 - val_loss: 118.1387\n",
      "Epoch 2767/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.4414 - val_loss: 198.6813\n",
      "Epoch 2768/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.4472 - val_loss: 158.9861\n",
      "Epoch 2769/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.4064 - val_loss: 133.6409\n",
      "Epoch 2770/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.1938 - val_loss: 178.4343\n",
      "Epoch 2771/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.5109 - val_loss: 128.0197\n",
      "Epoch 2772/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.5145 - val_loss: 117.8951\n",
      "Epoch 2773/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 139.7557 - val_loss: 112.3854\n",
      "Epoch 2774/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 148.7548 - val_loss: 134.9853\n",
      "Epoch 2775/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.8066 - val_loss: 116.4708\n",
      "Epoch 2776/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.0680 - val_loss: 206.0409\n",
      "Epoch 2777/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.7814 - val_loss: 115.1758\n",
      "Epoch 2778/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.3959 - val_loss: 110.5283\n",
      "Epoch 2779/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 133.8265 - val_loss: 118.8182\n",
      "Epoch 2780/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.9917 - val_loss: 124.7256\n",
      "Epoch 2781/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 221.3949 - val_loss: 127.8628\n",
      "Epoch 2782/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.9913 - val_loss: 121.7135\n",
      "Epoch 2783/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.7099 - val_loss: 128.1353\n",
      "Epoch 2784/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.1866 - val_loss: 122.3963\n",
      "Epoch 2785/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 195.1451 - val_loss: 120.5875\n",
      "Epoch 2786/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.2655 - val_loss: 122.7921\n",
      "Epoch 2787/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.3978 - val_loss: 113.5063\n",
      "Epoch 2788/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.0231 - val_loss: 131.3253\n",
      "Epoch 2789/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.5016 - val_loss: 117.3503\n",
      "Epoch 2790/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.9844 - val_loss: 115.8644\n",
      "Epoch 2791/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.7272 - val_loss: 169.0436\n",
      "Epoch 2792/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.4804 - val_loss: 153.1367\n",
      "Epoch 2793/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.6857 - val_loss: 114.1354\n",
      "Epoch 2794/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.2100 - val_loss: 127.7866\n",
      "Epoch 2795/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.4482 - val_loss: 123.7008\n",
      "Epoch 2796/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 143.0118 - val_loss: 117.1282\n",
      "Epoch 2797/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 147.8215 - val_loss: 123.4213\n",
      "Epoch 2798/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 140.1298 - val_loss: 137.2432\n",
      "Epoch 2799/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.9724 - val_loss: 116.2501\n",
      "Epoch 2800/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.4930 - val_loss: 125.0354\n",
      "Epoch 2801/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.8023 - val_loss: 111.1169\n",
      "Epoch 2802/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.2642 - val_loss: 132.5231\n",
      "Epoch 2803/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.1794 - val_loss: 124.9433\n",
      "Epoch 2804/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.9637 - val_loss: 117.8893\n",
      "Epoch 2805/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.9823 - val_loss: 157.1536\n",
      "Epoch 2806/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.2530 - val_loss: 117.6890\n",
      "Epoch 2807/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.3542 - val_loss: 166.3286\n",
      "Epoch 2808/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.7061 - val_loss: 112.3142\n",
      "Epoch 2809/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 152.1207 - val_loss: 130.0570\n",
      "Epoch 2810/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.6305 - val_loss: 117.1942\n",
      "Epoch 2811/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.9909 - val_loss: 129.6908\n",
      "Epoch 2812/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 358.5912 - val_loss: 473.9928\n",
      "Epoch 2813/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 270.5399 - val_loss: 133.5752\n",
      "Epoch 2814/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 211.3446 - val_loss: 133.5847\n",
      "Epoch 2815/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 195.2090 - val_loss: 179.1502\n",
      "Epoch 2816/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.1358 - val_loss: 117.4437\n",
      "Epoch 2817/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.8249 - val_loss: 142.6833\n",
      "Epoch 2818/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.6173 - val_loss: 131.6573\n",
      "Epoch 2819/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.3882 - val_loss: 119.8016\n",
      "Epoch 2820/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.1899 - val_loss: 149.6249\n",
      "Epoch 2821/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.1184 - val_loss: 126.1909\n",
      "Epoch 2822/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.6064 - val_loss: 172.1605\n",
      "Epoch 2823/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.0725 - val_loss: 114.2339\n",
      "Epoch 2824/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.0779 - val_loss: 120.1945\n",
      "Epoch 2825/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.1200 - val_loss: 167.1820\n",
      "Epoch 2826/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.0889 - val_loss: 126.2699\n",
      "Epoch 2827/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.1955 - val_loss: 122.6480\n",
      "Epoch 2828/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.4698 - val_loss: 136.1126\n",
      "Epoch 2829/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 169.9922 - val_loss: 411.8235\n",
      "Epoch 2830/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.3923 - val_loss: 150.1098\n",
      "Epoch 2831/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.1369 - val_loss: 200.5668\n",
      "Epoch 2832/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.2727 - val_loss: 117.9039\n",
      "Epoch 2833/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.8497 - val_loss: 141.6232\n",
      "Epoch 2834/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.9261 - val_loss: 154.3380\n",
      "Epoch 2835/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.6887 - val_loss: 133.7158\n",
      "Epoch 2836/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.9694 - val_loss: 118.4609\n",
      "Epoch 2837/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.7028 - val_loss: 139.0674\n",
      "Epoch 2838/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.7326 - val_loss: 114.5099\n",
      "Epoch 2839/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.2642 - val_loss: 147.2588\n",
      "Epoch 2840/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.8769 - val_loss: 116.3320\n",
      "Epoch 2841/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.4981 - val_loss: 133.3492\n",
      "Epoch 2842/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.6952 - val_loss: 116.6248\n",
      "Epoch 2843/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.3017 - val_loss: 120.5366\n",
      "Epoch 2844/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.4957 - val_loss: 129.6927\n",
      "Epoch 2845/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.4875 - val_loss: 211.6338\n",
      "Epoch 2846/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 178.1351 - val_loss: 113.8387\n",
      "Epoch 2847/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.8182 - val_loss: 115.2447\n",
      "Epoch 2848/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.0717 - val_loss: 123.6109\n",
      "Epoch 2849/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.1025 - val_loss: 123.2821\n",
      "Epoch 2850/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.9446 - val_loss: 117.9762\n",
      "Epoch 2851/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.0757 - val_loss: 114.8470\n",
      "Epoch 2852/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.7146 - val_loss: 116.9840\n",
      "Epoch 2853/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.6006 - val_loss: 130.4491\n",
      "Epoch 2854/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.0194 - val_loss: 130.6854\n",
      "Epoch 2855/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 166.9503 - val_loss: 125.1937\n",
      "Epoch 2856/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 150.1686 - val_loss: 121.3485\n",
      "Epoch 2857/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 146.6491 - val_loss: 121.8042\n",
      "Epoch 2858/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 134.8810 - val_loss: 121.9904\n",
      "Epoch 2859/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.3370 - val_loss: 145.5847\n",
      "Epoch 2860/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.8552 - val_loss: 123.8637\n",
      "Epoch 2861/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.9485 - val_loss: 114.9470\n",
      "Epoch 2862/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.3663 - val_loss: 241.9386\n",
      "Epoch 2863/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.0511 - val_loss: 119.1903\n",
      "Epoch 2864/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.0170 - val_loss: 131.1479\n",
      "Epoch 2865/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.4486 - val_loss: 130.5443\n",
      "Epoch 2866/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.6589 - val_loss: 119.9328\n",
      "Epoch 2867/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.8395 - val_loss: 116.6287\n",
      "Epoch 2868/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 257.8151 - val_loss: 125.9215\n",
      "Epoch 2869/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.5601 - val_loss: 123.9146\n",
      "Epoch 2870/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.7697 - val_loss: 126.9973\n",
      "Epoch 2871/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.5047 - val_loss: 166.0331\n",
      "Epoch 2872/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.8457 - val_loss: 156.0820\n",
      "Epoch 2873/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.9693 - val_loss: 114.5228\n",
      "Epoch 2874/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.8588 - val_loss: 116.5195\n",
      "Epoch 2875/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.2406 - val_loss: 122.3392\n",
      "Epoch 2876/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.8994 - val_loss: 120.8202\n",
      "Epoch 2877/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.2998 - val_loss: 131.3754\n",
      "Epoch 2878/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.2323 - val_loss: 121.8618\n",
      "Epoch 2879/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.2639 - val_loss: 137.4610\n",
      "Epoch 2880/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 167.6918 - val_loss: 152.0297\n",
      "Epoch 2881/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.7460 - val_loss: 147.0211\n",
      "Epoch 2882/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.7912 - val_loss: 115.4467\n",
      "Epoch 2883/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 138.1339 - val_loss: 111.8535\n",
      "Epoch 2884/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.3651 - val_loss: 126.9999\n",
      "Epoch 2885/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.0635 - val_loss: 117.1553\n",
      "Epoch 2886/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.8551 - val_loss: 128.6941\n",
      "Epoch 2887/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.4590 - val_loss: 127.3079\n",
      "Epoch 2888/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.5976 - val_loss: 121.5186\n",
      "Epoch 2889/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.3762 - val_loss: 121.7029\n",
      "Epoch 2890/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.2479 - val_loss: 134.0998\n",
      "Epoch 2891/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.6791 - val_loss: 118.0288\n",
      "Epoch 2892/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.6692 - val_loss: 119.1007\n",
      "Epoch 2893/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 135.2858 - val_loss: 130.5578\n",
      "Epoch 2894/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 143.8127 - val_loss: 123.4252\n",
      "Epoch 2895/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.2634 - val_loss: 117.6666\n",
      "Epoch 2896/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 164.5610 - val_loss: 138.8891\n",
      "Epoch 2897/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 140.7590 - val_loss: 127.7638\n",
      "Epoch 2898/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.5851 - val_loss: 128.0800\n",
      "Epoch 2899/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.9827 - val_loss: 112.7430\n",
      "Epoch 2900/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.0282 - val_loss: 113.1897\n",
      "Epoch 2901/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.8360 - val_loss: 143.7444\n",
      "Epoch 2902/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.5477 - val_loss: 137.1488\n",
      "Epoch 2903/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 221.1029 - val_loss: 114.7529\n",
      "Epoch 2904/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.7092 - val_loss: 119.6691\n",
      "Epoch 2905/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.0845 - val_loss: 124.2153\n",
      "Epoch 2906/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 131.1429 - val_loss: 136.7033\n",
      "Epoch 2907/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.5754 - val_loss: 117.3182\n",
      "Epoch 2908/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.3537 - val_loss: 119.0275\n",
      "Epoch 2909/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.1704 - val_loss: 122.6545\n",
      "Epoch 2910/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.8361 - val_loss: 155.5422\n",
      "Epoch 2911/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 131.8960 - val_loss: 135.9579\n",
      "Epoch 2912/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 135.7780 - val_loss: 174.7765\n",
      "Epoch 2913/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.9000 - val_loss: 123.5648\n",
      "Epoch 2914/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.7038 - val_loss: 132.5712\n",
      "Epoch 2915/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 137.4354 - val_loss: 121.2198\n",
      "Epoch 2916/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 146.2026 - val_loss: 144.1202\n",
      "Epoch 2917/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 134.5361 - val_loss: 115.4308\n",
      "Epoch 2918/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 131.5334 - val_loss: 186.6337\n",
      "Epoch 2919/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.7315 - val_loss: 140.7193\n",
      "Epoch 2920/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.8672 - val_loss: 119.0743\n",
      "Epoch 2921/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.0303 - val_loss: 126.9357\n",
      "Epoch 2922/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.5757 - val_loss: 115.8069\n",
      "Epoch 2923/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.1579 - val_loss: 130.3377\n",
      "Epoch 2924/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.1027 - val_loss: 153.4506\n",
      "Epoch 2925/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.1428 - val_loss: 127.7274\n",
      "Epoch 2926/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.9831 - val_loss: 119.1610\n",
      "Epoch 2927/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.4087 - val_loss: 270.0887\n",
      "Epoch 2928/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.4621 - val_loss: 135.8691\n",
      "Epoch 2929/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.0204 - val_loss: 125.5549\n",
      "Epoch 2930/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.0344 - val_loss: 120.0102\n",
      "Epoch 2931/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.5780 - val_loss: 241.3391\n",
      "Epoch 2932/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.4713 - val_loss: 128.4815\n",
      "Epoch 2933/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.0657 - val_loss: 126.4947\n",
      "Epoch 2934/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 133.442 - 1s 76us/step - loss: 138.1076 - val_loss: 134.9885\n",
      "Epoch 2935/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 144.9334 - val_loss: 192.9167\n",
      "Epoch 2936/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.8644 - val_loss: 132.0231\n",
      "Epoch 2937/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.7782 - val_loss: 156.5316\n",
      "Epoch 2938/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.6314 - val_loss: 113.9773\n",
      "Epoch 2939/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.9982 - val_loss: 113.2415\n",
      "Epoch 2940/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.8429 - val_loss: 115.4211\n",
      "Epoch 2941/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.8457 - val_loss: 147.5900\n",
      "Epoch 2942/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 139.6219 - val_loss: 117.0500\n",
      "Epoch 2943/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.5273 - val_loss: 113.3790\n",
      "Epoch 2944/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.6088 - val_loss: 165.0240\n",
      "Epoch 2945/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.0413 - val_loss: 126.8684\n",
      "Epoch 2946/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.7164 - val_loss: 123.2723\n",
      "Epoch 2947/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.0137 - val_loss: 181.3612\n",
      "Epoch 2948/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.6908 - val_loss: 117.6752\n",
      "Epoch 2949/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.8578 - val_loss: 149.2361\n",
      "Epoch 2950/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 181.7261 - val_loss: 137.9172\n",
      "Epoch 2951/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.1019 - val_loss: 126.0118\n",
      "Epoch 2952/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.8108 - val_loss: 124.1813\n",
      "Epoch 2953/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.5149 - val_loss: 114.4283\n",
      "Epoch 2954/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.6664 - val_loss: 117.2128\n",
      "Epoch 2955/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 259.5854 - val_loss: 135.0271\n",
      "Epoch 2956/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.7095 - val_loss: 116.7409\n",
      "Epoch 2957/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.3483 - val_loss: 141.3486\n",
      "Epoch 2958/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.2069 - val_loss: 123.5502\n",
      "Epoch 2959/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.0930 - val_loss: 122.1121\n",
      "Epoch 2960/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.7499 - val_loss: 174.2904\n",
      "Epoch 2961/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.5302 - val_loss: 146.4090\n",
      "Epoch 2962/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.3527 - val_loss: 118.8628\n",
      "Epoch 2963/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.5662 - val_loss: 136.6887\n",
      "Epoch 2964/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.2899 - val_loss: 119.3343\n",
      "Epoch 2965/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.3327 - val_loss: 124.7860\n",
      "Epoch 2966/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.5952 - val_loss: 119.4619\n",
      "Epoch 2967/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.5920 - val_loss: 112.4678\n",
      "Epoch 2968/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.7602 - val_loss: 202.1564\n",
      "Epoch 2969/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.0528 - val_loss: 127.0085\n",
      "Epoch 2970/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 136.6093 - val_loss: 113.2506\n",
      "Epoch 2971/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.3428 - val_loss: 121.4632\n",
      "Epoch 2972/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 140.7924 - val_loss: 125.2540\n",
      "Epoch 2973/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.2820 - val_loss: 171.7079\n",
      "Epoch 2974/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 165.2889 - val_loss: 121.3522\n",
      "Epoch 2975/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 144.2980 - val_loss: 132.1123\n",
      "Epoch 2976/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.6454 - val_loss: 112.3729\n",
      "Epoch 2977/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.1627 - val_loss: 120.8287\n",
      "Epoch 2978/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.3441 - val_loss: 120.5662\n",
      "Epoch 2979/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 129.9368 - val_loss: 119.6301\n",
      "Epoch 2980/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.4722 - val_loss: 140.1440\n",
      "Epoch 2981/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.3082 - val_loss: 169.3076\n",
      "Epoch 2982/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 171.5979 - val_loss: 249.4369\n",
      "Epoch 2983/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 151.0705 - val_loss: 120.6299\n",
      "Epoch 2984/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 175.8080 - val_loss: 125.7562\n",
      "Epoch 2985/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 133.4916 - val_loss: 120.5954\n",
      "Epoch 2986/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.7195 - val_loss: 136.1628\n",
      "Epoch 2987/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.0601 - val_loss: 115.5096\n",
      "Epoch 2988/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.8577 - val_loss: 156.1055\n",
      "Epoch 2989/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.9169 - val_loss: 119.3639\n",
      "Epoch 2990/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 160.6319 - val_loss: 124.1206\n",
      "Epoch 2991/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.1466 - val_loss: 123.9723\n",
      "Epoch 2992/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.3122 - val_loss: 120.1804\n",
      "Epoch 2993/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.9564 - val_loss: 113.8620\n",
      "Epoch 2994/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 136.0231 - val_loss: 115.8753\n",
      "Epoch 2995/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.6134 - val_loss: 111.4162\n",
      "Epoch 2996/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 139.8311 - val_loss: 170.3740\n",
      "Epoch 2997/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 135.0122 - val_loss: 122.9402\n",
      "Epoch 2998/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 137.5891 - val_loss: 115.5207\n",
      "Epoch 2999/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 138.8664 - val_loss: 140.8418\n",
      "Epoch 3000/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 164.117 - 1s 76us/step - loss: 161.9321 - val_loss: 126.2404\n",
      "Epoch 3001/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 144.3601 - val_loss: 141.2064\n",
      "Epoch 3002/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 153.3564 - val_loss: 115.6283\n",
      "Epoch 3003/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 137.4197 - val_loss: 131.4037\n",
      "Epoch 3004/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 139.1766 - val_loss: 130.9895\n",
      "Epoch 3005/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 145.3911 - val_loss: 117.8385\n",
      "Epoch 3006/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 137.6939 - val_loss: 135.1273\n",
      "Epoch 3007/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 152.9013 - val_loss: 149.8663\n",
      "Epoch 3008/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 192.0603 - val_loss: 130.1081\n",
      "Epoch 3009/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 178.3522 - val_loss: 228.4181\n",
      "Epoch 3010/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 151.7568 - val_loss: 127.8832\n",
      "Epoch 3011/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 136.6627 - val_loss: 178.2375\n",
      "Epoch 3012/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 136.1636 - val_loss: 111.6135\n",
      "Epoch 3013/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 152.7218 - val_loss: 137.5938\n",
      "Epoch 3014/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 135.1006 - val_loss: 208.8266\n",
      "Epoch 3015/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 153.6848 - val_loss: 115.2511\n",
      "Epoch 3016/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 136.9597 - val_loss: 112.3454\n",
      "Epoch 3017/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 151.0781 - val_loss: 128.3483\n",
      "Epoch 3018/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 138.6931 - val_loss: 122.9800\n",
      "Epoch 3019/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 141.4174 - val_loss: 119.8320\n",
      "Epoch 3020/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 135.7833 - val_loss: 121.2259\n",
      "Epoch 3021/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 140.4848 - val_loss: 217.8562\n",
      "Epoch 3022/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 173.2061 - val_loss: 117.7454\n",
      "Epoch 3023/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 137.6936 - val_loss: 117.0554\n",
      "Epoch 3024/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 155.2902 - val_loss: 113.3879\n",
      "Epoch 3025/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.4595 - val_loss: 113.0125\n",
      "Epoch 3026/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 186.1711 - val_loss: 127.1094\n",
      "Epoch 3027/10000\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 172.6476 - val_loss: 123.2553\n",
      "Epoch 3028/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 153.6795 - val_loss: 136.9706\n",
      "Epoch 3029/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 144.3905 - val_loss: 130.7779\n",
      "Epoch 3030/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 162.8817 - val_loss: 113.7878\n",
      "Epoch 3031/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 152.3526 - val_loss: 120.0653\n",
      "Epoch 3032/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 134.6184 - val_loss: 119.1892\n",
      "Epoch 3033/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 135.5290 - val_loss: 129.1367\n",
      "Epoch 3034/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 133.7550 - val_loss: 183.1838\n",
      "Epoch 3035/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 135.8311 - val_loss: 125.9242\n",
      "Epoch 3036/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 149.2427 - val_loss: 123.0656\n",
      "Epoch 3037/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.9362 - val_loss: 157.9516\n",
      "Epoch 3038/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 199.8673 - val_loss: 135.8172\n",
      "Epoch 3039/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 179.8223 - val_loss: 122.3006\n",
      "Epoch 3040/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.6639 - val_loss: 124.6399\n",
      "Epoch 3041/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.5121 - val_loss: 119.3519\n",
      "Epoch 3042/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.2515 - val_loss: 112.8422\n",
      "Epoch 3043/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 133.9745 - val_loss: 109.4204\n",
      "Epoch 3044/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.5670 - val_loss: 122.4008\n",
      "Epoch 3045/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.3896 - val_loss: 127.5269\n",
      "Epoch 3046/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 131.5537 - val_loss: 140.5691\n",
      "Epoch 3047/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.2341 - val_loss: 125.8075\n",
      "Epoch 3048/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.6703 - val_loss: 146.4003\n",
      "Epoch 3049/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.9478 - val_loss: 120.6093\n",
      "Epoch 3050/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.8343 - val_loss: 111.1111\n",
      "Epoch 3051/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.9135 - val_loss: 132.1034\n",
      "Epoch 3052/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.1260 - val_loss: 147.6452\n",
      "Epoch 3053/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.6539 - val_loss: 110.9168\n",
      "Epoch 3054/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.4174 - val_loss: 127.3321\n",
      "Epoch 3055/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.4388 - val_loss: 141.1894\n",
      "Epoch 3056/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.1606 - val_loss: 113.6612\n",
      "Epoch 3057/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.5131 - val_loss: 130.0788\n",
      "Epoch 3058/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.8732 - val_loss: 135.5348\n",
      "Epoch 3059/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.9216 - val_loss: 123.5948\n",
      "Epoch 3060/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.8360 - val_loss: 109.4073\n",
      "Epoch 3061/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.5324 - val_loss: 131.9048\n",
      "Epoch 3062/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.4967 - val_loss: 128.2417\n",
      "Epoch 3063/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.2864 - val_loss: 114.3098\n",
      "Epoch 3064/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.1421 - val_loss: 165.6895\n",
      "Epoch 3065/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.4647 - val_loss: 142.0193\n",
      "Epoch 3066/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.9486 - val_loss: 173.5518\n",
      "Epoch 3067/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.8722 - val_loss: 128.4206\n",
      "Epoch 3068/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.2188 - val_loss: 130.0354\n",
      "Epoch 3069/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.8791 - val_loss: 150.6371\n",
      "Epoch 3070/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.0676 - val_loss: 274.2742\n",
      "Epoch 3071/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.1684 - val_loss: 131.0408\n",
      "Epoch 3072/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.5045 - val_loss: 115.5868\n",
      "Epoch 3073/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.7804 - val_loss: 124.5012\n",
      "Epoch 3074/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 129.6406 - val_loss: 168.0542ETA: 0s - loss: 1\n",
      "Epoch 3075/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.2721 - val_loss: 146.5212\n",
      "Epoch 3076/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.7038 - val_loss: 111.7007\n",
      "Epoch 3077/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.4785 - val_loss: 127.6717\n",
      "Epoch 3078/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.5692 - val_loss: 138.8647\n",
      "Epoch 3079/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 147.0126 - val_loss: 148.5183\n",
      "Epoch 3080/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.7742 - val_loss: 112.4489\n",
      "Epoch 3081/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.6003 - val_loss: 120.7658\n",
      "Epoch 3082/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.7681 - val_loss: 134.1003\n",
      "Epoch 3083/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 137.5849 - val_loss: 158.2105\n",
      "Epoch 3084/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 139.7561 - val_loss: 129.3532\n",
      "Epoch 3085/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 145.0729 - val_loss: 114.8266\n",
      "Epoch 3086/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.6619 - val_loss: 123.5092\n",
      "Epoch 3087/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.9453 - val_loss: 127.3882\n",
      "Epoch 3088/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.1309 - val_loss: 182.5119\n",
      "Epoch 3089/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.4673 - val_loss: 112.2022\n",
      "Epoch 3090/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.5549 - val_loss: 122.5653\n",
      "Epoch 3091/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.7208 - val_loss: 126.7164\n",
      "Epoch 3092/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.4140 - val_loss: 125.5045\n",
      "Epoch 03092: early stopping\n",
      "Fold score (RMSE): 10.928815841674805\n",
      "Fold #5\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 5612.0685 - val_loss: 4398.3538\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 5032.2556 - val_loss: 4432.1119\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 4750.9994 - val_loss: 3752.7534\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 4416.2344 - val_loss: 3776.5913\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 4487.2667 - val_loss: 3626.8866\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 4387.6048 - val_loss: 4294.5557\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 4205.1202 - val_loss: 3398.2852\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 4105.0204 - val_loss: 3849.7137\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 4035.7112 - val_loss: 3885.4490\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 3923.9649 - val_loss: 3078.3838\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 3717.9496 - val_loss: 2874.9853\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 3422.8634 - val_loss: 2519.1749\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 3246.1932 - val_loss: 2590.1347\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 2829.0031 - val_loss: 3024.3408\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 2599.9722 - val_loss: 2058.4132\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 2143.2500 - val_loss: 1343.7985\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 1681.4336 - val_loss: 1017.1888\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 1433.9194 - val_loss: 833.7198\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 1475.9618 - val_loss: 803.9017\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 955.7652 - val_loss: 576.4279\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 920.4609 - val_loss: 755.3209\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 866.4158 - val_loss: 626.0334\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 957.2089 - val_loss: 1189.1678\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 745.5860 - val_loss: 499.2652\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 811.9552 - val_loss: 536.7093\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 698.1593 - val_loss: 613.7863\n",
      "Epoch 27/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 71us/step - loss: 655.8009 - val_loss: 718.8077\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 695.4132 - val_loss: 538.2538\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 535.7269 - val_loss: 693.2932\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 684.0765 - val_loss: 555.5382\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 551.9738 - val_loss: 537.8671\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 531.3319 - val_loss: 372.5998\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 628.7539 - val_loss: 2117.8936\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 634.5116 - val_loss: 430.6203\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 555.2816 - val_loss: 304.7212\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 541.3608 - val_loss: 323.4387\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 498.7722 - val_loss: 320.7057\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 514.6178 - val_loss: 651.1558\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 535.7936 - val_loss: 370.7059\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 466.0164 - val_loss: 315.6318\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 613.2066 - val_loss: 352.1178\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 564.4038 - val_loss: 288.3072\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 404.5333 - val_loss: 277.9113\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 479.2622 - val_loss: 347.2386\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 438.0939 - val_loss: 325.1822\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 404.6503 - val_loss: 319.1005\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 396.4478 - val_loss: 374.1788\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 418.9301 - val_loss: 282.5171\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 384.0780 - val_loss: 383.6075\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 488.8446 - val_loss: 585.1206\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 442.3794 - val_loss: 296.4754\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 396.6945 - val_loss: 244.5709\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 374.6946 - val_loss: 278.4474\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 344.6417 - val_loss: 294.4945\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 410.2724 - val_loss: 738.4529\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 436.3286 - val_loss: 258.5108\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 416.7160 - val_loss: 316.9001\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 424.9132 - val_loss: 255.3919\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 443.7864 - val_loss: 267.1690\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 546.2421 - val_loss: 328.2864\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 415.8633 - val_loss: 318.2561\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 390.2543 - val_loss: 349.7018\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 400.0147 - val_loss: 227.7044\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 359.4954 - val_loss: 286.9236\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 396.8336 - val_loss: 491.7590\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 501.2464 - val_loss: 242.1923\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 463.9797 - val_loss: 266.1685\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 355.1343 - val_loss: 266.9061\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 366.5099 - val_loss: 248.5501\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 346.1080 - val_loss: 319.9329\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 412.9342 - val_loss: 248.7013\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 361.8244 - val_loss: 233.0621\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 370.3521 - val_loss: 505.7341\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 318.8291 - val_loss: 283.4837\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 364.5628 - val_loss: 274.0597\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 448.2624 - val_loss: 228.4325\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 328.6653 - val_loss: 324.5902\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 350.9956 - val_loss: 254.5310\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 336.1550 - val_loss: 263.3358\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 401.7679 - val_loss: 228.7808\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 350.3698 - val_loss: 296.9145\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 321.2862 - val_loss: 413.1647\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 324.5537 - val_loss: 361.3683\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 321.3578 - val_loss: 216.2609\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 358.7645 - val_loss: 239.9646\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 344.7864 - val_loss: 212.1634\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 320.5544 - val_loss: 249.4905\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 426.1606 - val_loss: 214.1575\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 318.2114 - val_loss: 360.0827\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 302.0326 - val_loss: 446.7857\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 292.0882 - val_loss: 228.7749\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 357.9811 - val_loss: 281.8500\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 311.6765 - val_loss: 765.4428\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 354.5320 - val_loss: 345.8812\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 331.8668 - val_loss: 236.4015\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 304.5084 - val_loss: 196.5966\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 328.0294 - val_loss: 274.6772\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 344.5377 - val_loss: 282.1166\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 343.2661 - val_loss: 202.2287\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 334.9645 - val_loss: 270.4970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 272.5097 - val_loss: 188.1106\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 428.4299 - val_loss: 298.4386\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 302.8104 - val_loss: 307.9145\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 285.7782 - val_loss: 360.7705\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 310.0961 - val_loss: 189.8363\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 315.1405 - val_loss: 214.8054\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 266.6558 - val_loss: 224.0979\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 267.7924 - val_loss: 198.8530\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 317.4446 - val_loss: 428.1034\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 386.1137 - val_loss: 253.4057\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 341.2110 - val_loss: 328.6108\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 290.8053 - val_loss: 242.9913\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 335.5776 - val_loss: 199.9741\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 292.2956 - val_loss: 294.1988\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 307.8247 - val_loss: 205.6207\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 344.1609 - val_loss: 253.3608\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 329.7111 - val_loss: 399.2041\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 280.2716 - val_loss: 186.4572\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 301.4204 - val_loss: 365.8630\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 278.2066 - val_loss: 578.0328\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 394.9712 - val_loss: 192.1804\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 349.0780 - val_loss: 233.6007\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 275.3380 - val_loss: 619.7919\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 317.7915 - val_loss: 526.7741\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 303.1812 - val_loss: 212.1751\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 265.4142 - val_loss: 399.4066\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 266.3627 - val_loss: 174.5987\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 283.4829 - val_loss: 220.9091\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 269.6106 - val_loss: 612.2545\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 354.9983 - val_loss: 193.4482\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 323.9242 - val_loss: 199.7450\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 293.4513 - val_loss: 194.8057\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 285.1209 - val_loss: 174.6761\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 291.8643 - val_loss: 202.6029\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 290.6209 - val_loss: 174.9743\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 335.0002 - val_loss: 598.4387\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 268.8983 - val_loss: 175.7442\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 276.6037 - val_loss: 187.9327\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 320.9783 - val_loss: 497.5831\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 374.6365 - val_loss: 245.5976\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 235.5339 - val_loss: 170.8957\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 299.8797 - val_loss: 198.2588\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 350.2573 - val_loss: 211.9219\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 229.5293 - val_loss: 196.2615\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 286.8270 - val_loss: 175.4802\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 263.2295 - val_loss: 160.7131\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 217.8783 - val_loss: 167.3312\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 312.6627 - val_loss: 337.5851\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 223.6134 - val_loss: 235.1466\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 250.7113 - val_loss: 410.2061\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 257.0755 - val_loss: 166.6490\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 270.0410 - val_loss: 241.6393\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 270.0886 - val_loss: 181.9127\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 269.9906 - val_loss: 216.6060\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 238.8680 - val_loss: 215.0524\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 271.0921 - val_loss: 316.8561\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 339.4773 - val_loss: 184.9607\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 241.3353 - val_loss: 189.5778\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 263.7976 - val_loss: 241.0259\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 282.7635 - val_loss: 166.6821\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 245.7027 - val_loss: 193.4343\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 295.6406 - val_loss: 190.1637\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 229.8097 - val_loss: 173.5252\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 248.8433 - val_loss: 198.3852\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 264.0450 - val_loss: 182.2917\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 253.4456 - val_loss: 183.8877\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 221.6817 - val_loss: 171.8826\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 277.2961 - val_loss: 162.3216\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 272.9294 - val_loss: 186.3845\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 313.8941 - val_loss: 169.8012\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 246.1410 - val_loss: 533.1032\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 302.8702 - val_loss: 158.0663\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 214.8394 - val_loss: 186.6207\n",
      "Epoch 174/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 263.3427 - val_loss: 250.4641\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 238.4097 - val_loss: 208.6089\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 294.4948 - val_loss: 154.1764\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 297.2963 - val_loss: 160.8658\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 262.2784 - val_loss: 187.2488\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 261.9434 - val_loss: 229.2034\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 258.2721 - val_loss: 525.5360\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 260.9842 - val_loss: 150.2913\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 285.7523 - val_loss: 177.8578\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 223.0216 - val_loss: 399.6778\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 243.0485 - val_loss: 148.2893\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 210.5000 - val_loss: 222.2727\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 297.6249 - val_loss: 162.0857\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 244.0634 - val_loss: 154.4060\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 231.9667 - val_loss: 158.4537\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 231.4552 - val_loss: 419.5628\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 276.5169 - val_loss: 284.3690\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 253.0241 - val_loss: 157.6483\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 349.0343 - val_loss: 226.8041\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 290.1817 - val_loss: 181.0023\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 280.0365 - val_loss: 489.3719\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 250.0719 - val_loss: 269.5006\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 266.8759 - val_loss: 160.6882\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 272.7006 - val_loss: 175.3186\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 213.5559 - val_loss: 155.6611\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 279.1826 - val_loss: 918.7160\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 398.8888 - val_loss: 315.7891\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 354.1259 - val_loss: 212.0302\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 274.2291 - val_loss: 227.4281\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 223.4238 - val_loss: 149.6727\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 262.9355 - val_loss: 239.1723\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 278.6865 - val_loss: 162.2502\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 251.4811 - val_loss: 174.7985\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 235.6209 - val_loss: 140.9338\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 246.9389 - val_loss: 157.3932\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 255.7442 - val_loss: 151.0350\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 212.2347 - val_loss: 156.4991\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 203.6915 - val_loss: 145.3202\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 276.9690 - val_loss: 151.1201\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 212.6138 - val_loss: 148.2008\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 203.0792 - val_loss: 204.4620\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 320.2304 - val_loss: 174.1864\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 223.8332 - val_loss: 180.4285\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 220.9911 - val_loss: 178.1826\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 259.2006 - val_loss: 277.7782\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 200.1327 - val_loss: 260.5200\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 236.3383 - val_loss: 296.4084\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 225.2991 - val_loss: 172.0525\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 1s 124us/step - loss: 226.2742 - val_loss: 148.0662\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 213.2984 - val_loss: 149.3282\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 228.9253 - val_loss: 148.0898\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 360.4293 - val_loss: 165.9814\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 304.6532 - val_loss: 211.0841\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 346.7461 - val_loss: 318.5625\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 236.8351 - val_loss: 155.5783\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 221.5488 - val_loss: 163.4417\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 216.6426 - val_loss: 191.6676\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 254.1605 - val_loss: 146.8375\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 214.4096 - val_loss: 149.1483\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 224.1216 - val_loss: 183.9080\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 207.9954 - val_loss: 207.8232\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 226.8199 - val_loss: 336.6349\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 245.2076 - val_loss: 165.2992\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 295.3307 - val_loss: 2333.7480\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 568.5131 - val_loss: 275.0275\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 378.2103 - val_loss: 313.4615\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 330.6494 - val_loss: 166.5076\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 239.3204 - val_loss: 157.1351\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 253.5858 - val_loss: 171.0651\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 228.4834 - val_loss: 186.0852\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 323.3579 - val_loss: 185.2094\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 242.6488 - val_loss: 426.5702\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 229.7777 - val_loss: 148.8241\n",
      "Epoch 247/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 78us/step - loss: 222.7676 - val_loss: 210.8992\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 220.1289 - val_loss: 175.7923\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 241.3845 - val_loss: 267.3586\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 260.8961 - val_loss: 291.3908\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 259.8291 - val_loss: 211.0427\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 255.9810 - val_loss: 194.4772\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 1s 129us/step - loss: 245.0635 - val_loss: 179.7882\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 200.6937 - val_loss: 137.8229\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 231.5402 - val_loss: 143.8483\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 229.0913 - val_loss: 205.7553\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 195.3433 - val_loss: 168.1758\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 276.5302 - val_loss: 157.4771\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 227.9309 - val_loss: 157.1624\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 211.8568 - val_loss: 252.5840\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 267.2599 - val_loss: 149.3693\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 219.8707 - val_loss: 162.9020\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 265.1215 - val_loss: 379.9775\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 219.1550 - val_loss: 138.3174\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 204.0690 - val_loss: 136.0645\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 266.2981 - val_loss: 819.6709\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 340.4523 - val_loss: 197.9926\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 214.3315 - val_loss: 143.5180\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 336.5207 - val_loss: 149.6404\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 222.0258 - val_loss: 145.3483\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 209.6056 - val_loss: 282.1924\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 188.2440 - val_loss: 146.5925\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 218.0935 - val_loss: 209.4532\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 193.4180 - val_loss: 148.7624\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 212.9270 - val_loss: 138.2923\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 207.4055 - val_loss: 163.1737\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 242.6374 - val_loss: 138.8142\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 205.8953 - val_loss: 178.2207\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 211.9821 - val_loss: 160.4033\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 268.6557 - val_loss: 156.3036\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 1s 124us/step - loss: 227.4724 - val_loss: 147.0554\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 195.3106 - val_loss: 175.6586\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 202.9877 - val_loss: 276.1402\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 206.6560 - val_loss: 164.3766\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 250.0946 - val_loss: 134.7282\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 184.1779 - val_loss: 144.4542\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 194.1881 - val_loss: 142.7680\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 277.6793 - val_loss: 150.6856\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 198.1421 - val_loss: 133.5786\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 274.5311 - val_loss: 317.4991\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 216.6922 - val_loss: 146.7607\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 201.6385 - val_loss: 151.4095\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 202.1084 - val_loss: 177.6284\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 198.0611 - val_loss: 142.9025\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 195.6495 - val_loss: 208.7841\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 220.5506 - val_loss: 152.3123\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 308.0867 - val_loss: 174.4159\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 220.4313 - val_loss: 262.9183\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 245.0179 - val_loss: 155.6770\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 201.8201 - val_loss: 146.8015\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 269.2158 - val_loss: 150.4039\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 191.5446 - val_loss: 140.1735\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 189.3808 - val_loss: 152.1803\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 256.0949 - val_loss: 142.2109\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 199.1813 - val_loss: 216.8104\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 206.6710 - val_loss: 143.5397\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 209.7230 - val_loss: 139.8973\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 202.1296 - val_loss: 178.6572\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 190.6576 - val_loss: 161.2841\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 253.3937 - val_loss: 227.2997\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 301.7082 - val_loss: 340.3574\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 282.5654 - val_loss: 183.7915\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 359.8135 - val_loss: 164.4565\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 190.0518 - val_loss: 136.5576\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 189.6147 - val_loss: 148.1175\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 172.7999 - val_loss: 131.1529\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 176.1710 - val_loss: 147.5584\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 190.5591 - val_loss: 308.8037\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 474.2299 - val_loss: 227.3416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 242.5343 - val_loss: 153.5268\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 259.2507 - val_loss: 201.3297\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 224.5908 - val_loss: 145.4395\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 216.0729 - val_loss: 138.8438\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 197.2765 - val_loss: 166.5549\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 217.4476 - val_loss: 150.6472\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 244.1973 - val_loss: 151.5928\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 271.7616 - val_loss: 202.1889\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 243.5820 - val_loss: 243.8116\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 227.013 - 1s 63us/step - loss: 238.4714 - val_loss: 352.9271\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 225.1272 - val_loss: 162.4425\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 200.1857 - val_loss: 140.8223\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 221.5318 - val_loss: 163.8609\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 204.5540 - val_loss: 161.9494\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 276.1084 - val_loss: 160.8831\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 201.0219 - val_loss: 146.1259\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 241.3343 - val_loss: 248.3941\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 205.7320 - val_loss: 266.6682\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 230.3490 - val_loss: 283.8753\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 215.6957 - val_loss: 164.8229\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 201.8897 - val_loss: 141.3511\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 220.2981 - val_loss: 170.0971\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 214.1938 - val_loss: 288.7946\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 228.2519 - val_loss: 154.3794\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 239.0125 - val_loss: 151.3829\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 494.6414 - val_loss: 165.0907\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 212.3171 - val_loss: 164.8326\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 197.8402 - val_loss: 158.8676\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 190.2069 - val_loss: 184.9462\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 212.5846 - val_loss: 151.6745\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 384.0445 - val_loss: 303.8606\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 290.5249 - val_loss: 166.4375\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 222.7830 - val_loss: 193.0010\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 211.6112 - val_loss: 153.4443\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 239.6861 - val_loss: 202.6778\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 241.6480 - val_loss: 232.4128\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 242.9536 - val_loss: 174.7811\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 295.5215 - val_loss: 180.1648\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 189.8541 - val_loss: 153.8008\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 229.2281 - val_loss: 137.1569\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 217.2143 - val_loss: 231.5875\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 216.1743 - val_loss: 140.4833\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 230.6965 - val_loss: 142.2317\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 228.4480 - val_loss: 175.3150\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 225.2959 - val_loss: 217.1756\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 240.2523 - val_loss: 164.8694\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 214.6845 - val_loss: 188.7335\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 243.3093 - val_loss: 137.2203\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 190.6618 - val_loss: 276.5805\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 207.5043 - val_loss: 152.3067\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 192.7211 - val_loss: 136.1036\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 239.4740 - val_loss: 196.2956\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 232.2959 - val_loss: 345.9724\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 204.1527 - val_loss: 136.9996\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 191.5251 - val_loss: 181.3889\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 218.8192 - val_loss: 152.2392\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 268.2639 - val_loss: 156.0744\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 204.6680 - val_loss: 333.9813\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 222.7773 - val_loss: 192.3164\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 219.0815 - val_loss: 148.5099\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 172.7298 - val_loss: 140.0756\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 193.7200 - val_loss: 149.2536\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 211.8211 - val_loss: 153.6388\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 286.9749 - val_loss: 185.3737\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 226.6145 - val_loss: 149.0806\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 192.5285 - val_loss: 147.9685\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 178.0094 - val_loss: 186.8330\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 204.3138 - val_loss: 143.0378\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 189.8246 - val_loss: 160.6550\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 219.6743 - val_loss: 138.9502\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 209.6787 - val_loss: 178.7739\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 196.8532 - val_loss: 167.1656\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 238.0900 - val_loss: 257.6257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 207.5993 - val_loss: 191.1088\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 202.2966 - val_loss: 134.1771\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 190.3018 - val_loss: 164.0827\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 219.7765 - val_loss: 136.7257\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 181.1931 - val_loss: 130.4327\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 260.4821 - val_loss: 245.7173\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 204.7503 - val_loss: 144.8610\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 234.2625 - val_loss: 146.3907\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 179.1131 - val_loss: 300.6856\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 218.4496 - val_loss: 129.2423\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 205.8921 - val_loss: 139.7259\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 190.0761 - val_loss: 299.6034\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 203.2922 - val_loss: 330.2141\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 226.6677 - val_loss: 328.4632\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 175.1258 - val_loss: 131.7336\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 274.5351 - val_loss: 224.6441\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 185.6179 - val_loss: 143.2472\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 178.0452 - val_loss: 147.9423\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 209.9594 - val_loss: 132.1711\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 185.3460 - val_loss: 142.8645\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 245.3024 - val_loss: 172.3299\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 177.1404 - val_loss: 139.4013\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 192.6939 - val_loss: 128.2448\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 169.9270 - val_loss: 161.5572\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 240.5112 - val_loss: 247.5145\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 181.9266 - val_loss: 135.6903\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 219.9675 - val_loss: 150.3508\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 227.5645 - val_loss: 132.3960\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 177.5962 - val_loss: 136.3991\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 191.9545 - val_loss: 134.9159\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 629.7084 - val_loss: 203.5025\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 276.2198 - val_loss: 232.2882\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 198.5299 - val_loss: 141.4939\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 222.8447 - val_loss: 187.4883\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 216.0506 - val_loss: 150.3396\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 191.7702 - val_loss: 228.2651\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 203.8012 - val_loss: 141.8799\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 218.9687 - val_loss: 166.0321\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 189.0423 - val_loss: 165.3377\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 181.9071 - val_loss: 150.2619\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 179.8905 - val_loss: 262.3699\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 247.4275 - val_loss: 235.1029\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 199.1165 - val_loss: 215.8649\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 191.8483 - val_loss: 197.0873\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 209.6039 - val_loss: 133.6997\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 183.1054 - val_loss: 154.1373\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 191.5681 - val_loss: 174.0861\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 187.0055 - val_loss: 141.4468\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 218.4385 - val_loss: 146.8801\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.6207 - val_loss: 153.5913\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 194.5891 - val_loss: 163.5987\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 220.4235 - val_loss: 154.6746\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 203.7049 - val_loss: 178.1602\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 230.5224 - val_loss: 145.4019\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.1061 - val_loss: 129.6604\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 200.9800 - val_loss: 127.8072\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 196.9389 - val_loss: 244.1650\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 211.8486 - val_loss: 178.8374\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 205.1400 - val_loss: 127.3709\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 207.6487 - val_loss: 241.8107\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 189.7230 - val_loss: 141.5715\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 164.6640 - val_loss: 132.3472\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 194.1034 - val_loss: 127.8443\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 208.8470 - val_loss: 154.1011\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 179.3462 - val_loss: 150.7559\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.1542 - val_loss: 132.6411\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 200.7911 - val_loss: 137.7141\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 201.4090 - val_loss: 144.6202\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 203.2695 - val_loss: 148.9868\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 191.4531 - val_loss: 289.8231\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 210.0850 - val_loss: 135.3154\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 187.7740 - val_loss: 167.9293\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 194.8330 - val_loss: 130.6210\n",
      "Epoch 466/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 65us/step - loss: 174.4596 - val_loss: 178.6360\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.8073 - val_loss: 139.7132\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 190.0119 - val_loss: 137.4342\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 178.9147 - val_loss: 233.8700\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 193.4210 - val_loss: 161.6955\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.8215 - val_loss: 151.8506\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 194.2563 - val_loss: 176.9516\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 225.5740 - val_loss: 146.6746\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.1860 - val_loss: 168.8131\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.9608 - val_loss: 134.3433\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 201.7765 - val_loss: 172.6134\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 183.8016 - val_loss: 179.3378\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.9106 - val_loss: 126.7766\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 223.7044 - val_loss: 164.4223\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 206.6141 - val_loss: 154.9528\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 214.0148 - val_loss: 213.5329\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 178.5434 - val_loss: 125.1889\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.5400 - val_loss: 141.0044\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.4532 - val_loss: 125.3909\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 182.2434 - val_loss: 141.4087\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.9922 - val_loss: 132.6281\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.1811 - val_loss: 144.6663\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.5604 - val_loss: 135.6559\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 216.3037 - val_loss: 127.2118\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 191.2543 - val_loss: 164.5828\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 189.6928 - val_loss: 211.0082\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 225.7143 - val_loss: 147.0391\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 170.719 - 1s 64us/step - loss: 172.1046 - val_loss: 180.6950\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.8142 - val_loss: 214.3265\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.4620 - val_loss: 138.4546\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.8954 - val_loss: 120.1527\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.0449 - val_loss: 239.2859\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 187.9087 - val_loss: 128.6902\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 228.3017 - val_loss: 188.9043\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 199.3608 - val_loss: 125.3066\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 167.4612 - val_loss: 175.0352\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 184.2643 - val_loss: 206.6676\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 176.1662 - val_loss: 127.9062\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.7573 - val_loss: 173.6793\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 247.6301 - val_loss: 135.3342\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 188.9744 - val_loss: 136.9572\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.3972 - val_loss: 198.4808\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 171.3262 - val_loss: 148.8778\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 174.2268 - val_loss: 167.2690\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 180.3247 - val_loss: 137.6569\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 152.0728 - val_loss: 145.0383\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 232.9321 - val_loss: 135.7973\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.6744 - val_loss: 169.8227\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 205.4962 - val_loss: 166.3890\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.0219 - val_loss: 164.2250\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 181.7140 - val_loss: 141.1165\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 201.2738 - val_loss: 224.9371\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 183.6626 - val_loss: 196.7700\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 202.7074 - val_loss: 148.0494\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.4323 - val_loss: 190.7026\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 176.1811 - val_loss: 143.9498\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 182.8718 - val_loss: 124.3821\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 175.6823 - val_loss: 124.1460\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 221.7190 - val_loss: 412.8909\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 201.2826 - val_loss: 254.6546\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.0310 - val_loss: 140.2714\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 157.4562 - val_loss: 130.9989\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.1279 - val_loss: 218.0531\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 170.3846 - val_loss: 121.0934\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 192.4332 - val_loss: 149.2888\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 177.9456 - val_loss: 161.9859\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 203.7508 - val_loss: 628.0168\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 243.7040 - val_loss: 146.5775\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 185.1154 - val_loss: 157.4718\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 233.3133 - val_loss: 315.5710\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 200.2683 - val_loss: 127.2899\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 206.0661 - val_loss: 132.1104\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 176.4986 - val_loss: 135.9925\n",
      "Epoch 539/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 176.3289 - val_loss: 121.2845\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 178.8011 - val_loss: 139.2432\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 253.4994 - val_loss: 254.7746\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 230.5428 - val_loss: 198.1067\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 197.1277 - val_loss: 128.5176\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 186.2620 - val_loss: 122.4183\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 181.5901 - val_loss: 130.0229\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 192.5958 - val_loss: 253.1955\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 281.6758 - val_loss: 165.2686\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 183.7503 - val_loss: 125.4816\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.5045 - val_loss: 120.5046\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 186.5706 - val_loss: 119.5557\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 232.5199 - val_loss: 176.4338\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 186.1318 - val_loss: 170.5444\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 188.9021 - val_loss: 126.7529\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.5392 - val_loss: 216.6726\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 228.2872 - val_loss: 151.0463\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 204.9516 - val_loss: 197.1953\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 205.7879 - val_loss: 314.5164\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 215.0202 - val_loss: 137.7656\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 164.8048 - val_loss: 140.5087\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.8505 - val_loss: 175.5122\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 178.0520 - val_loss: 131.1068\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 164.7939 - val_loss: 118.1532\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.8826 - val_loss: 130.1846\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 212.9415 - val_loss: 161.2113\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 234.2407 - val_loss: 176.3601\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 195.8114 - val_loss: 143.4433\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 277.3493 - val_loss: 238.4262\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 224.9787 - val_loss: 130.1299\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 186.3653 - val_loss: 128.1115\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 183.3027 - val_loss: 178.7253\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 189.3472 - val_loss: 144.5272\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 184.5029 - val_loss: 146.7571\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 246.5908 - val_loss: 195.5218\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 185.4646 - val_loss: 128.9517\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 215.0563 - val_loss: 293.0878\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 331.4926 - val_loss: 130.3578\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.1534 - val_loss: 230.0686\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 206.3273 - val_loss: 126.3072\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 183.2854 - val_loss: 150.5220\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 210.4974 - val_loss: 133.7324\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.9438 - val_loss: 127.3022\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 218.7513 - val_loss: 186.5021\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 289.6414 - val_loss: 217.2153\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 261.3849 - val_loss: 153.0565\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 189.0204 - val_loss: 148.8383\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 209.3706 - val_loss: 148.4516\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 177.7072 - val_loss: 123.2606\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 201.1487 - val_loss: 128.5224\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 216.7779 - val_loss: 170.4528\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 274.5896 - val_loss: 151.3898\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 185.1162 - val_loss: 145.0388\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 461.6464 - val_loss: 142.7087\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 184.9282 - val_loss: 131.3186\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 192.9657 - val_loss: 129.6244\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.0143 - val_loss: 133.2407\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.7204 - val_loss: 126.1475\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.3894 - val_loss: 150.9250\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 285.1494 - val_loss: 136.1053\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 182.0795 - val_loss: 118.4552\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 182.5821 - val_loss: 123.7451\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.7535 - val_loss: 253.4709\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.5493 - val_loss: 137.0218\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 272.2149 - val_loss: 136.5733\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 233.9205 - val_loss: 150.2868\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 170.8399 - val_loss: 145.3785\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 194.7287 - val_loss: 129.6591\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 233.2579 - val_loss: 127.1998\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 165.4840 - val_loss: 120.8528\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 167.2540 - val_loss: 138.4373\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 165.1203 - val_loss: 122.7606\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 175.0368 - val_loss: 127.0815\n",
      "Epoch 612/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 65us/step - loss: 286.3277 - val_loss: 231.1375\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 274.3879 - val_loss: 130.0183\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 179.1059 - val_loss: 185.9869\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 180.5798 - val_loss: 165.0105\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.2893 - val_loss: 133.1290\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 264.5991 - val_loss: 145.6186\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.4145 - val_loss: 131.3679\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 201.6463 - val_loss: 138.9636\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 198.8544 - val_loss: 126.3241\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.2883 - val_loss: 133.4273\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.6905 - val_loss: 149.6666\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.6748 - val_loss: 167.8937\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 236.8696 - val_loss: 171.0485\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 229.7810 - val_loss: 944.5209\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 238.5499 - val_loss: 210.1401\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 188.8119 - val_loss: 128.2312\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 163.5475 - val_loss: 134.6000\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 206.5402 - val_loss: 126.0810\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.2354 - val_loss: 124.9326\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.5911 - val_loss: 179.7504\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.5039 - val_loss: 140.4516\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.8946 - val_loss: 126.7848\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 216.3924 - val_loss: 119.6286\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 316.9571 - val_loss: 157.3904\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.9833 - val_loss: 123.6981\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 160.5915 - val_loss: 142.4302\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.0463 - val_loss: 123.4202\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 175.7960 - val_loss: 293.8377\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 220.2798 - val_loss: 174.4777\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 208.1563 - val_loss: 136.4653\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.2671 - val_loss: 144.4836\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 177.1672 - val_loss: 127.5701\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 185.8554 - val_loss: 128.7562\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 199.3428 - val_loss: 143.2862\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 178.4946 - val_loss: 169.5394\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 184.9605 - val_loss: 164.3208\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.4915 - val_loss: 144.8813\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 167.4588 - val_loss: 151.4860\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 193.7814 - val_loss: 130.6596\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.8364 - val_loss: 189.2618\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 340.6418 - val_loss: 179.4242\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 184.1702 - val_loss: 140.9132\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 189.6524 - val_loss: 120.0558\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.5407 - val_loss: 155.5832\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.6046 - val_loss: 133.3789\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.5301 - val_loss: 128.2653\n",
      "Epoch 658/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 168.0079 - val_loss: 126.4657\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 175.9835 - val_loss: 123.1528\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 178.2797 - val_loss: 132.3710\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.7036 - val_loss: 183.7819\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 259.1601 - val_loss: 185.9856\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 248.0343 - val_loss: 135.8857\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 176.3297 - val_loss: 242.6437\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.0392 - val_loss: 132.4287\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 200.1404 - val_loss: 184.2834\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 334.1246 - val_loss: 182.4924\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 177.7968 - val_loss: 126.4577\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.9046 - val_loss: 128.5120\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.0815 - val_loss: 145.7730\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.6737 - val_loss: 124.2382\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.9445 - val_loss: 121.3579\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.9415 - val_loss: 158.2306\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 185.3893 - val_loss: 213.2522\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 198.3716 - val_loss: 117.9078\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.8178 - val_loss: 132.3164\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 167.2218 - val_loss: 128.9176\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 187.0655 - val_loss: 135.6753\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 226.4382 - val_loss: 151.6330\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 168.8021 - val_loss: 129.4115\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.9324 - val_loss: 156.6244\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 158.8999 - val_loss: 126.4847\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.3927 - val_loss: 146.0154\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 182.2211 - val_loss: 130.3948\n",
      "Epoch 685/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.5800 - val_loss: 140.3382\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 323.7154 - val_loss: 140.1561\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 171.3822 - val_loss: 148.2701\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 172.3316 - val_loss: 168.9637\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 179.5761 - val_loss: 119.0382\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 247.8843 - val_loss: 165.2363\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 260.8703 - val_loss: 134.1627\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 192.0838 - val_loss: 139.0475\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 203.3667 - val_loss: 184.5951\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.4343 - val_loss: 125.9349\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 216.5747 - val_loss: 163.3870\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 228.6617 - val_loss: 128.6814\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.4439 - val_loss: 132.2639\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 230.6464 - val_loss: 126.9452\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 186.6282 - val_loss: 178.8699\n",
      "Epoch 700/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.9650 - val_loss: 135.7232\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 177.4977 - val_loss: 165.4312\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 507.4304 - val_loss: 164.0554\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 198.9623 - val_loss: 149.5685\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.5044 - val_loss: 149.9445\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 178.3236 - val_loss: 131.6992\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 168.3243 - val_loss: 143.6522\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.8440 - val_loss: 188.1786\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 185.0139 - val_loss: 122.9269\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.2949 - val_loss: 122.1534\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 163.1350 - val_loss: 139.7312\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 294.0506 - val_loss: 152.9888\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 186.8180 - val_loss: 188.9448\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 175.1874 - val_loss: 139.0343\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 180.5325 - val_loss: 206.6243\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 210.6079 - val_loss: 124.3847\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 159.2570 - val_loss: 134.8085\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.7020 - val_loss: 119.0051\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 225.3426 - val_loss: 142.3639\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 172.7122 - val_loss: 126.0814\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.6009 - val_loss: 157.4235\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.5134 - val_loss: 291.4521\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 170.3276 - val_loss: 187.2176\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 191.8202 - val_loss: 122.1971\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.4103 - val_loss: 126.0596\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 201.6031 - val_loss: 151.7383\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 178.1750 - val_loss: 152.6062\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 170.4342 - val_loss: 159.0024\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 209.1923 - val_loss: 246.4930\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 163.1745 - val_loss: 223.0229\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.4253 - val_loss: 121.0662\n",
      "Epoch 731/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 159.8525 - val_loss: 147.0435\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 157.2179 - val_loss: 114.9430\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 196.2040 - val_loss: 256.1633\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 288.3118 - val_loss: 474.8009\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 208.3425 - val_loss: 118.5329\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.4805 - val_loss: 136.0689\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.4666 - val_loss: 121.4599\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.2114 - val_loss: 116.1706\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 221.9829 - val_loss: 130.3971\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.4269 - val_loss: 121.9698\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 290.8623 - val_loss: 159.0034\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 232.1097 - val_loss: 185.0807\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 197.9842 - val_loss: 127.4178\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 177.1684 - val_loss: 131.0457\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 181.4321 - val_loss: 120.1950\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 166.9660 - val_loss: 157.1904\n",
      "Epoch 747/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 176.1659 - val_loss: 139.7768\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 208.9225 - val_loss: 167.3374\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.8169 - val_loss: 134.5396\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.8766 - val_loss: 125.4776\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.9763 - val_loss: 163.3748\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 227.6393 - val_loss: 177.0468\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 439.9623 - val_loss: 380.0532\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 345.8525 - val_loss: 214.7078\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 253.6830 - val_loss: 212.3393\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 215.4332 - val_loss: 173.0331\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 278.9090 - val_loss: 175.8689\n",
      "Epoch 758/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 216.9377 - val_loss: 340.8068\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 255.0611 - val_loss: 160.0866\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 225.8963 - val_loss: 144.2161\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 227.4666 - val_loss: 153.7961\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 254.8281 - val_loss: 148.8117\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 224.7422 - val_loss: 140.0916\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 223.0451 - val_loss: 146.8324\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 196.2280 - val_loss: 144.3741\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 225.3015 - val_loss: 306.8704\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 322.5918 - val_loss: 209.1810\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 209.6163 - val_loss: 165.0643\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 222.9769 - val_loss: 145.1964\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 188.6982 - val_loss: 135.5917\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 232.7419 - val_loss: 141.6835\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 217.7840 - val_loss: 132.0319\n",
      "Epoch 773/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 231.3445 - val_loss: 203.1900\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 224.4501 - val_loss: 147.9927\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 199.8775 - val_loss: 143.6470\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 231.8358 - val_loss: 139.9099\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.9128 - val_loss: 162.1615\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 229.8058 - val_loss: 348.3255\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 256.5949 - val_loss: 313.8700\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 201.3939 - val_loss: 143.7864\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 212.0399 - val_loss: 145.1418\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.5226 - val_loss: 141.7716\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 199.9986 - val_loss: 408.7255\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 209.7034 - val_loss: 164.9989\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 230.6313 - val_loss: 144.0900\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 210.3601 - val_loss: 137.7013\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 202.5260 - val_loss: 135.2479\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 191.3185 - val_loss: 247.4025\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 215.9213 - val_loss: 253.0659\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 194.4113 - val_loss: 140.8132\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 178.9536 - val_loss: 180.2181\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 282.6524 - val_loss: 164.6363\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 203.6930 - val_loss: 180.5487\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 188.2405 - val_loss: 125.7856\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 213.7215 - val_loss: 177.4281\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 194.9743 - val_loss: 156.9451\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 199.4252 - val_loss: 166.6875\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 212.6602 - val_loss: 202.5300\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.3151 - val_loss: 134.8259\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 196.7736 - val_loss: 171.4128\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 220.8720 - val_loss: 200.3311\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 234.1409 - val_loss: 158.9999\n",
      "Epoch 803/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 191.7449 - val_loss: 134.6985\n",
      "Epoch 804/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 186.6178 - val_loss: 136.8438\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 213.5078 - val_loss: 146.8970\n",
      "Epoch 806/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 187.7182 - val_loss: 181.1615\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 215.8590 - val_loss: 147.6502\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 206.1180 - val_loss: 232.5846\n",
      "Epoch 809/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 221.5594 - val_loss: 139.6989\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.1112 - val_loss: 156.8875\n",
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 182.1607 - val_loss: 161.9982\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 207.1624 - val_loss: 214.1661\n",
      "Epoch 813/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.0079 - val_loss: 126.7630\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 187.2427 - val_loss: 135.8042\n",
      "Epoch 815/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 196.5716 - val_loss: 147.5495\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 181.1031 - val_loss: 210.0037\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 225.8341 - val_loss: 125.9623\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 229.2893 - val_loss: 136.9408\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 174.1208 - val_loss: 137.9092\n",
      "Epoch 820/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 191.7817 - val_loss: 169.6597\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 242.3959 - val_loss: 143.8144\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 179.7967 - val_loss: 125.1981\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 193.5320 - val_loss: 149.1074\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 233.7813 - val_loss: 171.6571\n",
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 185.9262 - val_loss: 152.8024\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 171.5067 - val_loss: 384.7916\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 205.1917 - val_loss: 141.7180\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 212.2204 - val_loss: 183.0486\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 187.1843 - val_loss: 145.0426\n",
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 202.7053 - val_loss: 134.7800\n",
      "Epoch 831/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.5019 - val_loss: 212.6072\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 186.1005 - val_loss: 148.9483\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.4602 - val_loss: 248.4776\n",
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 184.8676 - val_loss: 219.7679\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.6392 - val_loss: 148.8544\n",
      "Epoch 836/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 197.4304 - val_loss: 136.4257\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 187.3826 - val_loss: 152.7470\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 202.7878 - val_loss: 154.8718\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 226.1599 - val_loss: 163.3669\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 208.5574 - val_loss: 142.3344\n",
      "Epoch 841/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 189.6509 - val_loss: 164.0523\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 191.1622 - val_loss: 146.3173\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.5622 - val_loss: 124.6575\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 188.3616 - val_loss: 128.7092\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 168.0676 - val_loss: 133.6822\n",
      "Epoch 846/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 216.7391 - val_loss: 186.4458\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 196.2330 - val_loss: 164.0916\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 239.8162 - val_loss: 185.9232\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 187.7744 - val_loss: 129.0488\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.9449 - val_loss: 151.3884\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 188.3313 - val_loss: 167.0124\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 208.1417 - val_loss: 181.6346\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 233.7200 - val_loss: 472.1177\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 220.2666 - val_loss: 129.3727\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 187.9515 - val_loss: 147.1389\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 191.7514 - val_loss: 135.1644\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 200.5195 - val_loss: 151.8579\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 205.3166 - val_loss: 143.2882\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 185.4158 - val_loss: 131.0625\n",
      "Epoch 860/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 249.4396 - val_loss: 263.1550\n",
      "Epoch 861/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.0079 - val_loss: 216.1652\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.6097 - val_loss: 133.2004\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 184.8800 - val_loss: 168.7465\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 208.7310 - val_loss: 139.7204\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 233.2020 - val_loss: 142.5889\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 195.7507 - val_loss: 152.6887\n",
      "Epoch 867/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 186.1115 - val_loss: 211.1171\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 184.5459 - val_loss: 133.1544\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 189.0065 - val_loss: 131.6650\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 176.6534 - val_loss: 136.8762\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 177.8507 - val_loss: 127.7000\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.3489 - val_loss: 120.0690\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 220.8362 - val_loss: 155.8773\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.7095 - val_loss: 124.2374\n",
      "Epoch 875/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 256.5529 - val_loss: 605.6588\n",
      "Epoch 876/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 316.3302 - val_loss: 149.8566\n",
      "Epoch 877/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 232.0951 - val_loss: 260.2319\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 256.8524 - val_loss: 150.4534\n",
      "Epoch 879/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 201.6844 - val_loss: 146.8874\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 192.3623 - val_loss: 143.6841\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.0428 - val_loss: 145.0509\n",
      "Epoch 882/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 213.4501 - val_loss: 161.2661\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 209.9549 - val_loss: 338.3611\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 224.9402 - val_loss: 140.0523\n",
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 184.2132 - val_loss: 162.5216\n",
      "Epoch 886/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 224.2900 - val_loss: 156.8104\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 213.6523 - val_loss: 177.3926\n",
      "Epoch 888/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.1812 - val_loss: 160.1155\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 211.3483 - val_loss: 141.0047\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 192.1869 - val_loss: 127.7221\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 191.8298 - val_loss: 135.7321\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 200.4579 - val_loss: 132.3597\n",
      "Epoch 893/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 202.1994 - val_loss: 197.0445\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 200.6619 - val_loss: 129.9420\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 207.6793 - val_loss: 177.8675\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 196.8728 - val_loss: 133.2506\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 191.5939 - val_loss: 144.8833\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 232.6555 - val_loss: 180.3893\n",
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 196.5280 - val_loss: 140.9586\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 200.9723 - val_loss: 140.3000\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 193.2286 - val_loss: 196.4611\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 204.5473 - val_loss: 155.4286\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 246.5783 - val_loss: 164.5476\n",
      "Epoch 904/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.6244 - val_loss: 130.2070\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 188.6412 - val_loss: 163.5212\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 206.3997 - val_loss: 214.2184\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 243.0872 - val_loss: 183.4959\n",
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 195.3977 - val_loss: 131.1591\n",
      "Epoch 909/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 204.0577 - val_loss: 185.2581\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 182.6732 - val_loss: 142.7027\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 198.3849 - val_loss: 134.4087\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 185.5448 - val_loss: 146.7785\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 255.5227 - val_loss: 216.7091\n",
      "Epoch 914/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 195.6793 - val_loss: 134.1167\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 191.0583 - val_loss: 126.2005\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 187.8119 - val_loss: 131.3901\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 176.0594 - val_loss: 148.7507\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 211.6918 - val_loss: 129.4598\n",
      "Epoch 919/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 180.1070 - val_loss: 207.4191\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 214.6625 - val_loss: 244.2628\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 244.9280 - val_loss: 131.1773\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 177.0653 - val_loss: 132.7044\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 169.0029 - val_loss: 138.5642\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 189.5315 - val_loss: 219.0640\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 188.5522 - val_loss: 130.8459\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.8180 - val_loss: 149.2152\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 173.8579 - val_loss: 228.6347\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 186.2611 - val_loss: 319.8973\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 188.0743 - val_loss: 215.2721\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 184.8417 - val_loss: 159.2953\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 209.2274 - val_loss: 141.9051\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 206.2998 - val_loss: 151.0118\n",
      "Epoch 933/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.3616 - val_loss: 129.7287\n",
      "Epoch 934/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 193.0836 - val_loss: 323.7601\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 201.6568 - val_loss: 147.1998\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.9751 - val_loss: 155.0293\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 225.6753 - val_loss: 175.0478\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 200.4162 - val_loss: 159.9249\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 185.8639 - val_loss: 159.8839\n",
      "Epoch 940/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 202.6635 - val_loss: 132.7824\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 175.0987 - val_loss: 125.5720\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 178.0604 - val_loss: 134.0461\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 190.5753 - val_loss: 178.4535\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 202.2851 - val_loss: 148.6683\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 203.7835 - val_loss: 142.7441\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 199.7865 - val_loss: 131.5098\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 169.5020 - val_loss: 124.8167\n",
      "Epoch 948/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 182.7118 - val_loss: 198.2987\n",
      "Epoch 949/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 211.1452 - val_loss: 125.8897\n",
      "Epoch 950/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 178.9784 - val_loss: 158.2928\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 198.0904 - val_loss: 233.9683\n",
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 217.9924 - val_loss: 143.0880\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 192.0337 - val_loss: 229.3014\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 192.0955 - val_loss: 134.3162\n",
      "Epoch 955/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.4201 - val_loss: 133.1412\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 185.4295 - val_loss: 125.0659\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 191.8742 - val_loss: 125.0013\n",
      "Epoch 958/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 184.6704 - val_loss: 183.8078\n",
      "Epoch 959/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 182.2742 - val_loss: 130.7051\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 204.3236 - val_loss: 144.4991\n",
      "Epoch 961/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.6662 - val_loss: 129.6354\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 216.6350 - val_loss: 126.1052\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 171.5910 - val_loss: 142.6114\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 188.7637 - val_loss: 155.0513\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 169.7425 - val_loss: 127.1837\n",
      "Epoch 966/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 175.7440 - val_loss: 145.8715\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 186.4766 - val_loss: 129.3384\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 179.7893 - val_loss: 181.5232\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.6556 - val_loss: 185.7378\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 192.7409 - val_loss: 130.0089\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 183.5817 - val_loss: 131.2501\n",
      "Epoch 972/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.2069 - val_loss: 191.5966\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.6048 - val_loss: 127.0938\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 222.7713 - val_loss: 206.7603\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.9279 - val_loss: 136.7197\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 177.9057 - val_loss: 135.0195\n",
      "Epoch 977/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.5753 - val_loss: 150.5620\n",
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 209.3495 - val_loss: 274.4773\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 212.0599 - val_loss: 133.4924\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 187.2021 - val_loss: 135.6261\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 191.3812 - val_loss: 141.3622\n",
      "Epoch 982/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 167.7478 - val_loss: 161.0098\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.3488 - val_loss: 178.5428\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 193.4639 - val_loss: 136.8172\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 203.7704 - val_loss: 143.8848\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 217.7298 - val_loss: 148.5131\n",
      "Epoch 987/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 179.7191 - val_loss: 149.8929\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 168.8337 - val_loss: 268.5469\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 176.4778 - val_loss: 125.6886\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 164.1162 - val_loss: 134.3584\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 162.2247 - val_loss: 133.3884\n",
      "Epoch 992/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.7583 - val_loss: 156.7324\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 255.0806 - val_loss: 128.7189\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 184.9329 - val_loss: 134.0427\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.6443 - val_loss: 142.0410\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.4768 - val_loss: 123.7474\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 184.0460 - val_loss: 265.6091\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 185.3621 - val_loss: 147.9460\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.3914 - val_loss: 122.6374\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 178.1202 - val_loss: 135.5747\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 170.2519 - val_loss: 129.6553\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 201.8877 - val_loss: 166.5797\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 187.7996 - val_loss: 123.9461\n",
      "Epoch 1004/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.2311 - val_loss: 132.6994\n",
      "Epoch 1005/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 164.5952 - val_loss: 141.2081\n",
      "Epoch 1006/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 180.6384 - val_loss: 193.8436\n",
      "Epoch 1007/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 200.6819 - val_loss: 135.8446\n",
      "Epoch 1008/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.6712 - val_loss: 130.4502\n",
      "Epoch 1009/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 168.5819 - val_loss: 142.5272\n",
      "Epoch 1010/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 188.1419 - val_loss: 211.3158\n",
      "Epoch 1011/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 178.9588 - val_loss: 128.4623\n",
      "Epoch 1012/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 171.4646 - val_loss: 133.9582\n",
      "Epoch 1013/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 179.5203 - val_loss: 133.0283\n",
      "Epoch 1014/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 196.8172 - val_loss: 149.0976\n",
      "Epoch 1015/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 192.0341 - val_loss: 127.3803\n",
      "Epoch 1016/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.7324 - val_loss: 155.9556\n",
      "Epoch 1017/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.3812 - val_loss: 159.4399\n",
      "Epoch 1018/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 178.2402 - val_loss: 132.1404\n",
      "Epoch 1019/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 186.4036 - val_loss: 123.5384\n",
      "Epoch 1020/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.2827 - val_loss: 152.1225\n",
      "Epoch 1021/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 237.5109 - val_loss: 128.8696\n",
      "Epoch 1022/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.3644 - val_loss: 162.2073\n",
      "Epoch 1023/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 174.2737 - val_loss: 125.8259\n",
      "Epoch 1024/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.4801 - val_loss: 129.3340\n",
      "Epoch 1025/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.1835 - val_loss: 125.6229\n",
      "Epoch 1026/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 175.4909 - val_loss: 168.9979\n",
      "Epoch 1027/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 171.7961 - val_loss: 195.3649\n",
      "Epoch 1028/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.2724 - val_loss: 136.6896\n",
      "Epoch 1029/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 178.4015 - val_loss: 164.4387\n",
      "Epoch 1030/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 169.3440 - val_loss: 315.4483\n",
      "Epoch 1031/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.8719 - val_loss: 134.8445\n",
      "Epoch 1032/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 188.9450 - val_loss: 144.6879\n",
      "Epoch 1033/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.8704 - val_loss: 144.6326\n",
      "Epoch 1034/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.7411 - val_loss: 165.0592\n",
      "Epoch 1035/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 217.9510 - val_loss: 188.3546\n",
      "Epoch 1036/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.1704 - val_loss: 128.7771\n",
      "Epoch 1037/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.7398 - val_loss: 145.0584\n",
      "Epoch 1038/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.8362 - val_loss: 132.9182\n",
      "Epoch 1039/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 197.9135 - val_loss: 254.5812\n",
      "Epoch 1040/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 191.6507 - val_loss: 130.4606\n",
      "Epoch 1041/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 170.9155 - val_loss: 126.5874\n",
      "Epoch 1042/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 176.4348 - val_loss: 128.3656\n",
      "Epoch 1043/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 200.8813 - val_loss: 253.4375\n",
      "Epoch 1044/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 180.4140 - val_loss: 143.1959\n",
      "Epoch 1045/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.3171 - val_loss: 120.8516\n",
      "Epoch 1046/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 167.5158 - val_loss: 183.6104\n",
      "Epoch 1047/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 433.4902 - val_loss: 154.6099\n",
      "Epoch 1048/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 196.3023 - val_loss: 210.5918\n",
      "Epoch 1049/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 211.5513 - val_loss: 143.8307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1050/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 196.0704 - val_loss: 147.6103\n",
      "Epoch 1051/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 190.2320 - val_loss: 122.9533\n",
      "Epoch 1052/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.0788 - val_loss: 198.9157\n",
      "Epoch 1053/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 187.9806 - val_loss: 134.5109\n",
      "Epoch 1054/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 218.4631 - val_loss: 188.7763\n",
      "Epoch 1055/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 184.8604 - val_loss: 133.7666\n",
      "Epoch 1056/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 199.6080 - val_loss: 138.4143\n",
      "Epoch 1057/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 179.3397 - val_loss: 135.2879\n",
      "Epoch 1058/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 176.8155 - val_loss: 132.0763\n",
      "Epoch 1059/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 192.2729 - val_loss: 127.3512\n",
      "Epoch 1060/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.1039 - val_loss: 186.8082\n",
      "Epoch 1061/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 193.9865 - val_loss: 165.2581\n",
      "Epoch 1062/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 189.3788 - val_loss: 131.2441\n",
      "Epoch 1063/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 172.1467 - val_loss: 156.0746\n",
      "Epoch 1064/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 179.2286 - val_loss: 132.7177\n",
      "Epoch 1065/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.5666 - val_loss: 114.8353\n",
      "Epoch 1066/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 184.3287 - val_loss: 179.8763\n",
      "Epoch 1067/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 184.9040 - val_loss: 134.5638\n",
      "Epoch 1068/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 193.5941 - val_loss: 126.3203\n",
      "Epoch 1069/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 193.1591 - val_loss: 155.4002\n",
      "Epoch 1070/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 179.7508 - val_loss: 146.8344\n",
      "Epoch 1071/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 201.6680 - val_loss: 125.1417\n",
      "Epoch 1072/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 206.0173 - val_loss: 120.3270\n",
      "Epoch 1073/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 174.3195 - val_loss: 171.2583\n",
      "Epoch 1074/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 164.3366 - val_loss: 134.8574\n",
      "Epoch 1075/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.9251 - val_loss: 119.3912\n",
      "Epoch 1076/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 190.3750 - val_loss: 133.8279\n",
      "Epoch 1077/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.6220 - val_loss: 124.8726\n",
      "Epoch 1078/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 220.9301 - val_loss: 124.2108\n",
      "Epoch 1079/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.5711 - val_loss: 126.1096\n",
      "Epoch 1080/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 176.2507 - val_loss: 127.8408\n",
      "Epoch 1081/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 182.0905 - val_loss: 119.6217\n",
      "Epoch 1082/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.8830 - val_loss: 139.2535\n",
      "Epoch 1083/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 231.7456 - val_loss: 203.5539\n",
      "Epoch 1084/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.4067 - val_loss: 137.5144\n",
      "Epoch 1085/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.1412 - val_loss: 145.0958\n",
      "Epoch 1086/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 179.2753 - val_loss: 201.5136\n",
      "Epoch 1087/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.4530 - val_loss: 166.8328\n",
      "Epoch 1088/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 170.7072 - val_loss: 140.0977\n",
      "Epoch 1089/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.1960 - val_loss: 189.5247\n",
      "Epoch 1090/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 184.0634 - val_loss: 120.6468\n",
      "Epoch 1091/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 184.0416 - val_loss: 148.8615\n",
      "Epoch 1092/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 196.0561 - val_loss: 121.1469\n",
      "Epoch 1093/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.8779 - val_loss: 171.0687\n",
      "Epoch 1094/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 179.4949 - val_loss: 122.3188\n",
      "Epoch 1095/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.2200 - val_loss: 139.0244\n",
      "Epoch 1096/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.1379 - val_loss: 188.2471\n",
      "Epoch 1097/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 219.9215 - val_loss: 280.3702\n",
      "Epoch 1098/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.7795 - val_loss: 118.8411\n",
      "Epoch 1099/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 179.5109 - val_loss: 121.0428\n",
      "Epoch 1100/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 171.4986 - val_loss: 138.3074\n",
      "Epoch 1101/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 194.7632 - val_loss: 143.8101\n",
      "Epoch 1102/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 176.4243 - val_loss: 224.5666\n",
      "Epoch 1103/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 179.1702 - val_loss: 121.4416\n",
      "Epoch 1104/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.7481 - val_loss: 114.9103\n",
      "Epoch 1105/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 178.5683 - val_loss: 115.9874\n",
      "Epoch 1106/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 171.7535 - val_loss: 125.7436\n",
      "Epoch 1107/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 194.1261 - val_loss: 122.9315\n",
      "Epoch 1108/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.4924 - val_loss: 153.4473\n",
      "Epoch 1109/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 201.5459 - val_loss: 123.1451\n",
      "Epoch 1110/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 178.1225 - val_loss: 134.3202\n",
      "Epoch 1111/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.8284 - val_loss: 151.4362\n",
      "Epoch 1112/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 182.3394 - val_loss: 167.0631\n",
      "Epoch 1113/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.3743 - val_loss: 181.7886\n",
      "Epoch 1114/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 176.1112 - val_loss: 128.1291\n",
      "Epoch 1115/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 179.7819 - val_loss: 127.7331\n",
      "Epoch 1116/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 177.0874 - val_loss: 131.8074\n",
      "Epoch 1117/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 172.4961 - val_loss: 122.2005\n",
      "Epoch 1118/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 171.4753 - val_loss: 133.9949\n",
      "Epoch 1119/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 235.9453 - val_loss: 141.6485\n",
      "Epoch 1120/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.8721 - val_loss: 131.7704\n",
      "Epoch 1121/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.6495 - val_loss: 116.9352\n",
      "Epoch 1122/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 207.9992 - val_loss: 381.6074\n",
      "Epoch 1123/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 193.0395 - val_loss: 122.4674\n",
      "Epoch 1124/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.8471 - val_loss: 133.8991\n",
      "Epoch 1125/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.7234 - val_loss: 124.3726\n",
      "Epoch 1126/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.0991 - val_loss: 116.2061\n",
      "Epoch 1127/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 186.3954 - val_loss: 159.7349\n",
      "Epoch 1128/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 175.5857 - val_loss: 128.3486\n",
      "Epoch 1129/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.6344 - val_loss: 133.2605\n",
      "Epoch 1130/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.6569 - val_loss: 133.0949\n",
      "Epoch 1131/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 189.9196 - val_loss: 179.9700\n",
      "Epoch 1132/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 177.1962 - val_loss: 120.8134\n",
      "Epoch 1133/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 190.8984 - val_loss: 245.4845\n",
      "Epoch 1134/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 214.8235 - val_loss: 165.8663\n",
      "Epoch 1135/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.8630 - val_loss: 129.5126\n",
      "Epoch 1136/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.6522 - val_loss: 130.8975\n",
      "Epoch 1137/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.0593 - val_loss: 126.5236\n",
      "Epoch 1138/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 167.8066 - val_loss: 183.1584\n",
      "Epoch 1139/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.6023 - val_loss: 156.3388\n",
      "Epoch 1140/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 157.3501 - val_loss: 117.3429\n",
      "Epoch 1141/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 195.7729 - val_loss: 132.3009\n",
      "Epoch 1142/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 160.9699 - val_loss: 121.4805\n",
      "Epoch 1143/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 174.9984 - val_loss: 150.1079\n",
      "Epoch 1144/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 159.7591 - val_loss: 171.3503\n",
      "Epoch 1145/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 170.8114 - val_loss: 125.9146\n",
      "Epoch 1146/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.0058 - val_loss: 118.9017\n",
      "Epoch 1147/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 162.3530 - val_loss: 129.7868\n",
      "Epoch 1148/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 192.1532 - val_loss: 130.1819\n",
      "Epoch 1149/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 161.4870 - val_loss: 137.0989\n",
      "Epoch 1150/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 180.9199 - val_loss: 122.2339\n",
      "Epoch 1151/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 177.5745 - val_loss: 140.0711\n",
      "Epoch 1152/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 177.8089 - val_loss: 124.2252\n",
      "Epoch 1153/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 182.3883 - val_loss: 124.4817\n",
      "Epoch 1154/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.0052 - val_loss: 130.5567\n",
      "Epoch 1155/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.4716 - val_loss: 184.9529\n",
      "Epoch 1156/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 209.8302 - val_loss: 221.4634\n",
      "Epoch 1157/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.7536 - val_loss: 125.3515\n",
      "Epoch 1158/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 152.5814 - val_loss: 131.2453\n",
      "Epoch 1159/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 160.4453 - val_loss: 119.5140\n",
      "Epoch 1160/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 203.8082 - val_loss: 121.1347\n",
      "Epoch 1161/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.1024 - val_loss: 198.9246\n",
      "Epoch 1162/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 163.1313 - val_loss: 130.3998\n",
      "Epoch 1163/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 197.9426 - val_loss: 196.1999\n",
      "Epoch 1164/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.0580 - val_loss: 129.1323\n",
      "Epoch 1165/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.5376 - val_loss: 120.2657\n",
      "Epoch 1166/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.2804 - val_loss: 180.6206\n",
      "Epoch 1167/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.6655 - val_loss: 222.1218\n",
      "Epoch 1168/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 178.5319 - val_loss: 117.0231\n",
      "Epoch 1169/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 194.5687 - val_loss: 136.7664\n",
      "Epoch 1170/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.6421 - val_loss: 139.0463\n",
      "Epoch 1171/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.2719 - val_loss: 157.9868\n",
      "Epoch 1172/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 185.9687 - val_loss: 118.9235\n",
      "Epoch 1173/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.4223 - val_loss: 123.9816\n",
      "Epoch 1174/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 180.5958 - val_loss: 158.3798\n",
      "Epoch 1175/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.5926 - val_loss: 129.6080\n",
      "Epoch 1176/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 174.6056 - val_loss: 171.6673\n",
      "Epoch 1177/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 169.7289 - val_loss: 126.7699\n",
      "Epoch 1178/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 205.7929 - val_loss: 145.8271\n",
      "Epoch 1179/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.8285 - val_loss: 125.7208\n",
      "Epoch 1180/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 195.3188 - val_loss: 126.9086\n",
      "Epoch 1181/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 189.3458 - val_loss: 164.2528\n",
      "Epoch 1182/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 196.1371 - val_loss: 122.0713\n",
      "Epoch 1183/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 162.8817 - val_loss: 121.7349\n",
      "Epoch 1184/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.7318 - val_loss: 116.0559\n",
      "Epoch 1185/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.7436 - val_loss: 137.4116\n",
      "Epoch 1186/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.3010 - val_loss: 149.9584\n",
      "Epoch 1187/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 171.4670 - val_loss: 132.4934\n",
      "Epoch 1188/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 168.3170 - val_loss: 129.3146\n",
      "Epoch 1189/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 195.9946 - val_loss: 124.2372\n",
      "Epoch 1190/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.6872 - val_loss: 263.2769\n",
      "Epoch 1191/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 169.5951 - val_loss: 153.5864\n",
      "Epoch 1192/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.9038 - val_loss: 163.0301\n",
      "Epoch 1193/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 157.1853 - val_loss: 135.2188\n",
      "Epoch 1194/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 206.2398 - val_loss: 132.5559\n",
      "Epoch 1195/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.0674 - val_loss: 126.9868\n",
      "Epoch 1196/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 174.2744 - val_loss: 153.6008\n",
      "Epoch 1197/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.4277 - val_loss: 129.2230\n",
      "Epoch 1198/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.9021 - val_loss: 115.6808\n",
      "Epoch 1199/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.8514 - val_loss: 131.1166\n",
      "Epoch 1200/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.3429 - val_loss: 259.7639\n",
      "Epoch 1201/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.6561 - val_loss: 116.5221\n",
      "Epoch 1202/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.9058 - val_loss: 118.4145\n",
      "Epoch 1203/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 248.7152 - val_loss: 122.2241\n",
      "Epoch 1204/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.2026 - val_loss: 156.7587\n",
      "Epoch 1205/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.6364 - val_loss: 219.0440\n",
      "Epoch 1206/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 165.0645 - val_loss: 142.8792\n",
      "Epoch 1207/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.2751 - val_loss: 133.8643\n",
      "Epoch 1208/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 192.2504 - val_loss: 134.8248\n",
      "Epoch 1209/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.5680 - val_loss: 135.3529\n",
      "Epoch 1210/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.5585 - val_loss: 129.1099\n",
      "Epoch 1211/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.8165 - val_loss: 130.2452\n",
      "Epoch 1212/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 174.2758 - val_loss: 115.6668\n",
      "Epoch 1213/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 175.5553 - val_loss: 123.9044\n",
      "Epoch 1214/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.2902 - val_loss: 120.0641\n",
      "Epoch 1215/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 179.2104 - val_loss: 135.5664\n",
      "Epoch 1216/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.3333 - val_loss: 131.8268\n",
      "Epoch 1217/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 150.1280 - val_loss: 114.9700\n",
      "Epoch 1218/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 156.6574 - val_loss: 128.2393\n",
      "Epoch 1219/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 210.7643 - val_loss: 187.3478\n",
      "Epoch 1220/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 152.9544 - val_loss: 122.3875\n",
      "Epoch 1221/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 157.8246 - val_loss: 135.3185\n",
      "Epoch 1222/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 166.8151 - val_loss: 271.3886\n",
      "Epoch 1223/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.3153 - val_loss: 139.2099\n",
      "Epoch 1224/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.0554 - val_loss: 144.4392\n",
      "Epoch 1225/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 156.0173 - val_loss: 119.9704\n",
      "Epoch 1226/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.4628 - val_loss: 329.9526\n",
      "Epoch 1227/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.1799 - val_loss: 118.5099\n",
      "Epoch 1228/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 186.4687 - val_loss: 133.6704\n",
      "Epoch 1229/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.2345 - val_loss: 120.4646\n",
      "Epoch 1230/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.0538 - val_loss: 122.4920\n",
      "Epoch 1231/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.3463 - val_loss: 122.8002\n",
      "Epoch 1232/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.4084 - val_loss: 121.2107\n",
      "Epoch 1233/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.2642 - val_loss: 126.0877\n",
      "Epoch 1234/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.6853 - val_loss: 121.9159\n",
      "Epoch 1235/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 155.3391 - val_loss: 187.9286\n",
      "Epoch 1236/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.0601 - val_loss: 125.0538\n",
      "Epoch 1237/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.0994 - val_loss: 131.5716\n",
      "Epoch 1238/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.9684 - val_loss: 133.4163\n",
      "Epoch 1239/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.2709 - val_loss: 127.7435\n",
      "Epoch 1240/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 158.9394 - val_loss: 125.8267\n",
      "Epoch 1241/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.4743 - val_loss: 112.8866\n",
      "Epoch 1242/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.2493 - val_loss: 139.2671\n",
      "Epoch 1243/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 163.4453 - val_loss: 147.0421\n",
      "Epoch 1244/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 161.1708 - val_loss: 153.5660\n",
      "Epoch 1245/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.1377 - val_loss: 152.5478\n",
      "Epoch 1246/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 180.1697 - val_loss: 119.0295\n",
      "Epoch 1247/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 164.5372 - val_loss: 128.7911\n",
      "Epoch 1248/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 169.0828 - val_loss: 137.0304\n",
      "Epoch 1249/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.9838 - val_loss: 123.7646\n",
      "Epoch 1250/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 151.6221 - val_loss: 120.0134\n",
      "Epoch 1251/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.5179 - val_loss: 114.3213\n",
      "Epoch 1252/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.5495 - val_loss: 132.6937\n",
      "Epoch 1253/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.3881 - val_loss: 135.2003\n",
      "Epoch 1254/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 188.8231 - val_loss: 121.5676\n",
      "Epoch 1255/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.8026 - val_loss: 125.2213\n",
      "Epoch 1256/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.5519 - val_loss: 115.5504\n",
      "Epoch 1257/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.6764 - val_loss: 127.9633\n",
      "Epoch 1258/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.8151 - val_loss: 125.3406\n",
      "Epoch 1259/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 175.5059 - val_loss: 163.0425\n",
      "Epoch 1260/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.7090 - val_loss: 161.8034\n",
      "Epoch 1261/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 228.8987 - val_loss: 209.9273\n",
      "Epoch 1262/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 169.8988 - val_loss: 119.9825\n",
      "Epoch 1263/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.9644 - val_loss: 128.5722\n",
      "Epoch 1264/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.6521 - val_loss: 127.1215\n",
      "Epoch 1265/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 163.7645 - val_loss: 130.6465\n",
      "Epoch 1266/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 176.4055 - val_loss: 137.5893\n",
      "Epoch 1267/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 155.0900 - val_loss: 137.6384\n",
      "Epoch 1268/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.1675 - val_loss: 114.3840\n",
      "Epoch 1269/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 173.0481 - val_loss: 149.8293\n",
      "Epoch 1270/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.4265 - val_loss: 139.2252\n",
      "Epoch 1271/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 204.1275 - val_loss: 232.7693\n",
      "Epoch 1272/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 183.8255 - val_loss: 370.0532\n",
      "Epoch 1273/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 187.8648 - val_loss: 205.1059\n",
      "Epoch 1274/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.1492 - val_loss: 120.2725\n",
      "Epoch 1275/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.6210 - val_loss: 134.9005\n",
      "Epoch 1276/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 156.2403 - val_loss: 115.9160\n",
      "Epoch 1277/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 158.1670 - val_loss: 125.2266\n",
      "Epoch 1278/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 155.9247 - val_loss: 137.0235\n",
      "Epoch 1279/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 158.6764 - val_loss: 168.3477\n",
      "Epoch 1280/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.8035 - val_loss: 140.7320\n",
      "Epoch 1281/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 192.6698 - val_loss: 123.8714\n",
      "Epoch 1282/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.5061 - val_loss: 188.0825\n",
      "Epoch 1283/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.5478 - val_loss: 146.1376\n",
      "Epoch 1284/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.6729 - val_loss: 122.3768\n",
      "Epoch 1285/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.6988 - val_loss: 134.4505\n",
      "Epoch 1286/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.1089 - val_loss: 166.8432\n",
      "Epoch 1287/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.9405 - val_loss: 121.3736\n",
      "Epoch 1288/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 202.2075 - val_loss: 117.7253\n",
      "Epoch 1289/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.3565 - val_loss: 116.1855\n",
      "Epoch 1290/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.9777 - val_loss: 127.6985\n",
      "Epoch 1291/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.7030 - val_loss: 249.5277\n",
      "Epoch 1292/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 252.7048 - val_loss: 172.6609\n",
      "Epoch 1293/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.4760 - val_loss: 131.5725\n",
      "Epoch 1294/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 166.6389 - val_loss: 142.2142\n",
      "Epoch 1295/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.5283 - val_loss: 118.5702\n",
      "Epoch 1296/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.0208 - val_loss: 128.5561\n",
      "Epoch 1297/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 299.7102 - val_loss: 132.4632\n",
      "Epoch 1298/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.6220 - val_loss: 126.0277\n",
      "Epoch 1299/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.8803 - val_loss: 127.7973\n",
      "Epoch 1300/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.0869 - val_loss: 135.1630\n",
      "Epoch 1301/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 158.4680 - val_loss: 120.5967\n",
      "Epoch 1302/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 193.2872 - val_loss: 121.6805\n",
      "Epoch 1303/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 188.2584 - val_loss: 133.1515\n",
      "Epoch 1304/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.5193 - val_loss: 128.3752\n",
      "Epoch 1305/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.3644 - val_loss: 127.4193\n",
      "Epoch 1306/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.2947 - val_loss: 139.1284\n",
      "Epoch 1307/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 183.7899 - val_loss: 122.1348\n",
      "Epoch 1308/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 163.0898 - val_loss: 132.0881\n",
      "Epoch 1309/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 286.1497 - val_loss: 195.5603\n",
      "Epoch 1310/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 188.9686 - val_loss: 128.9987\n",
      "Epoch 1311/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.3627 - val_loss: 115.0604\n",
      "Epoch 1312/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.3164 - val_loss: 118.2562\n",
      "Epoch 1313/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.8232 - val_loss: 114.3824\n",
      "Epoch 1314/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.8961 - val_loss: 120.1570\n",
      "Epoch 1315/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 186.9354 - val_loss: 126.7535\n",
      "Epoch 1316/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.6134 - val_loss: 123.6686\n",
      "Epoch 1317/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 162.4739 - val_loss: 120.5808\n",
      "Epoch 1318/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.4994 - val_loss: 128.8842\n",
      "Epoch 1319/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.9914 - val_loss: 188.7345\n",
      "Epoch 1320/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 168.2853 - val_loss: 136.8158\n",
      "Epoch 1321/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.8067 - val_loss: 128.8510\n",
      "Epoch 1322/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 264.1759 - val_loss: 137.8242\n",
      "Epoch 1323/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 219.4007 - val_loss: 119.7433\n",
      "Epoch 1324/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 183.6827 - val_loss: 178.4731\n",
      "Epoch 1325/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 173.0664 - val_loss: 154.4622\n",
      "Epoch 1326/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.3475 - val_loss: 126.8842\n",
      "Epoch 1327/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 170.0211 - val_loss: 125.9291\n",
      "Epoch 1328/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 166.9821 - val_loss: 126.8899\n",
      "Epoch 1329/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.3008 - val_loss: 140.7216\n",
      "Epoch 1330/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 175.4967 - val_loss: 125.7879\n",
      "Epoch 1331/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 178.6948 - val_loss: 138.0047\n",
      "Epoch 1332/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.3727 - val_loss: 127.8110\n",
      "Epoch 1333/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.2630 - val_loss: 143.1878\n",
      "Epoch 1334/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.6517 - val_loss: 135.8587\n",
      "Epoch 1335/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 175.3516 - val_loss: 120.4700\n",
      "Epoch 1336/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 147.9353 - val_loss: 120.3694\n",
      "Epoch 1337/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 185.7706 - val_loss: 518.3393\n",
      "Epoch 1338/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 68us/step - loss: 192.4461 - val_loss: 139.6075\n",
      "Epoch 1339/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 186.7556 - val_loss: 156.3222\n",
      "Epoch 1340/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 186.2611 - val_loss: 118.7487\n",
      "Epoch 1341/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 170.1032 - val_loss: 125.1122\n",
      "Epoch 1342/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 180.5807 - val_loss: 149.9890\n",
      "Epoch 1343/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 157.6912 - val_loss: 123.4224\n",
      "Epoch 1344/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 178.0231 - val_loss: 141.0406\n",
      "Epoch 1345/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 167.6182 - val_loss: 137.4776\n",
      "Epoch 1346/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.7942 - val_loss: 119.4846\n",
      "Epoch 1347/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.0002 - val_loss: 148.1481\n",
      "Epoch 1348/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 183.9529 - val_loss: 148.4267\n",
      "Epoch 1349/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 177.8833 - val_loss: 124.2226\n",
      "Epoch 1350/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.1172 - val_loss: 123.5126\n",
      "Epoch 1351/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.0576 - val_loss: 114.6644\n",
      "Epoch 1352/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 189.4440 - val_loss: 121.4184\n",
      "Epoch 1353/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 202.2842 - val_loss: 146.5430\n",
      "Epoch 1354/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 147.8137 - val_loss: 160.9661\n",
      "Epoch 1355/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.8670 - val_loss: 116.5085\n",
      "Epoch 1356/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.5305 - val_loss: 123.5616\n",
      "Epoch 1357/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.8655 - val_loss: 135.4267\n",
      "Epoch 1358/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 170.2686 - val_loss: 131.7404\n",
      "Epoch 1359/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 187.1652 - val_loss: 120.3761\n",
      "Epoch 1360/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.6552 - val_loss: 117.1801\n",
      "Epoch 1361/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 187.3504 - val_loss: 170.3266\n",
      "Epoch 1362/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 167.3764 - val_loss: 120.0910\n",
      "Epoch 1363/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.0870 - val_loss: 179.3267\n",
      "Epoch 1364/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.0839 - val_loss: 127.4888\n",
      "Epoch 1365/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.6201 - val_loss: 115.3536\n",
      "Epoch 1366/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 186.8232 - val_loss: 146.2222\n",
      "Epoch 1367/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 166.8592 - val_loss: 127.0873\n",
      "Epoch 1368/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 188.2439 - val_loss: 119.9339\n",
      "Epoch 1369/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 179.5496 - val_loss: 140.0981\n",
      "Epoch 1370/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.9791 - val_loss: 141.2713\n",
      "Epoch 1371/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 191.9184 - val_loss: 152.9853\n",
      "Epoch 1372/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 170.7801 - val_loss: 120.3738\n",
      "Epoch 1373/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.0511 - val_loss: 127.7267\n",
      "Epoch 1374/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 180.3925 - val_loss: 117.6137\n",
      "Epoch 1375/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.6629 - val_loss: 137.3418\n",
      "Epoch 1376/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 157.8248 - val_loss: 115.9416\n",
      "Epoch 1377/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.4930 - val_loss: 137.8716\n",
      "Epoch 1378/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.1673 - val_loss: 160.4158\n",
      "Epoch 1379/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.9044 - val_loss: 151.8389\n",
      "Epoch 1380/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 192.9636 - val_loss: 155.2724\n",
      "Epoch 1381/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 179.1098 - val_loss: 211.2527\n",
      "Epoch 1382/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 180.0177 - val_loss: 303.4801\n",
      "Epoch 1383/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.6386 - val_loss: 114.1890\n",
      "Epoch 1384/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 168.8873 - val_loss: 123.0018\n",
      "Epoch 1385/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 200.4686 - val_loss: 140.4495\n",
      "Epoch 1386/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 167.7309 - val_loss: 147.5159\n",
      "Epoch 1387/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 156.6053 - val_loss: 116.8664\n",
      "Epoch 1388/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.2763 - val_loss: 116.0355\n",
      "Epoch 1389/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.4822 - val_loss: 122.3719\n",
      "Epoch 1390/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 174.0719 - val_loss: 146.9849\n",
      "Epoch 1391/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 181.5207 - val_loss: 158.2985\n",
      "Epoch 1392/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.8832 - val_loss: 178.8536\n",
      "Epoch 1393/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 184.0196 - val_loss: 161.2633\n",
      "Epoch 1394/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 161.5672 - val_loss: 141.7295\n",
      "Epoch 1395/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 180.3554 - val_loss: 126.8383\n",
      "Epoch 1396/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 160.0351 - val_loss: 190.8761\n",
      "Epoch 1397/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 161.5666 - val_loss: 156.7641\n",
      "Epoch 1398/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 189.9588 - val_loss: 124.9465\n",
      "Epoch 1399/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.7162 - val_loss: 117.0701\n",
      "Epoch 1400/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 159.5986 - val_loss: 120.8025\n",
      "Epoch 1401/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 165.9249 - val_loss: 136.1502\n",
      "Epoch 1402/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.1680 - val_loss: 117.6011\n",
      "Epoch 1403/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.0563 - val_loss: 144.0982\n",
      "Epoch 1404/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.8238 - val_loss: 113.5754\n",
      "Epoch 1405/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 187.0733 - val_loss: 131.1424\n",
      "Epoch 1406/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 215.5235 - val_loss: 149.8782\n",
      "Epoch 1407/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.3023 - val_loss: 132.2016\n",
      "Epoch 1408/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.6497 - val_loss: 123.5275\n",
      "Epoch 1409/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.0076 - val_loss: 138.3074\n",
      "Epoch 1410/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.2258 - val_loss: 156.5817\n",
      "Epoch 1411/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.6690 - val_loss: 155.5502\n",
      "Epoch 1412/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 156.9306 - val_loss: 112.8942\n",
      "Epoch 1413/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 188.6662 - val_loss: 121.9928\n",
      "Epoch 1414/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.8430 - val_loss: 128.1226\n",
      "Epoch 1415/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.7044 - val_loss: 124.9647\n",
      "Epoch 1416/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.1448 - val_loss: 120.5972\n",
      "Epoch 1417/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 197.9360 - val_loss: 123.0855\n",
      "Epoch 1418/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.5201 - val_loss: 120.5908\n",
      "Epoch 1419/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 165.0862 - val_loss: 120.1727\n",
      "Epoch 1420/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 170.2812 - val_loss: 130.3274\n",
      "Epoch 1421/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.3982 - val_loss: 124.4738\n",
      "Epoch 1422/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.4467 - val_loss: 130.1418\n",
      "Epoch 1423/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 176.3488 - val_loss: 203.0991\n",
      "Epoch 1424/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.4697 - val_loss: 142.9420\n",
      "Epoch 1425/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 165.4702 - val_loss: 153.5047\n",
      "Epoch 1426/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 148.1588 - val_loss: 118.3091\n",
      "Epoch 1427/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.9515 - val_loss: 118.7434\n",
      "Epoch 1428/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.0065 - val_loss: 122.2166\n",
      "Epoch 1429/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.4316 - val_loss: 154.1455\n",
      "Epoch 1430/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.8760 - val_loss: 159.7663\n",
      "Epoch 1431/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 188.5789 - val_loss: 149.2442\n",
      "Epoch 1432/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 217.3023 - val_loss: 146.7468\n",
      "Epoch 1433/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.9865 - val_loss: 127.0644\n",
      "Epoch 1434/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.8165 - val_loss: 130.9086\n",
      "Epoch 1435/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 179.5636 - val_loss: 152.8747\n",
      "Epoch 1436/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.0994 - val_loss: 121.6631\n",
      "Epoch 1437/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 175.7458 - val_loss: 126.5222\n",
      "Epoch 1438/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.5039 - val_loss: 122.4987\n",
      "Epoch 1439/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.6491 - val_loss: 132.3290\n",
      "Epoch 1440/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.5184 - val_loss: 127.7538\n",
      "Epoch 1441/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 157.2629 - val_loss: 115.3307\n",
      "Epoch 1442/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 158.3589 - val_loss: 153.8483\n",
      "Epoch 1443/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 154.4916 - val_loss: 118.1151\n",
      "Epoch 1444/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.5508 - val_loss: 116.8118\n",
      "Epoch 1445/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.5325 - val_loss: 121.0564\n",
      "Epoch 1446/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.0384 - val_loss: 123.4080\n",
      "Epoch 1447/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.0467 - val_loss: 118.1390\n",
      "Epoch 1448/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.1065 - val_loss: 121.9752\n",
      "Epoch 1449/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 343.9502 - val_loss: 182.7671\n",
      "Epoch 1450/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 242.7215 - val_loss: 138.5740\n",
      "Epoch 1451/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 235.2873 - val_loss: 187.1348\n",
      "Epoch 1452/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 207.9138 - val_loss: 166.7453\n",
      "Epoch 1453/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 241.5793 - val_loss: 171.5007\n",
      "Epoch 1454/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 185.7251 - val_loss: 181.2388\n",
      "Epoch 1455/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 195.0279 - val_loss: 138.9221\n",
      "Epoch 1456/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 186.9545 - val_loss: 293.4532\n",
      "Epoch 1457/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 175.8203 - val_loss: 132.8944\n",
      "Epoch 1458/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 219.0984 - val_loss: 142.0748\n",
      "Epoch 1459/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.2057 - val_loss: 139.1584\n",
      "Epoch 1460/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 184.8382 - val_loss: 177.4604\n",
      "Epoch 1461/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 191.3771 - val_loss: 140.1997\n",
      "Epoch 1462/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.0921 - val_loss: 121.0784\n",
      "Epoch 1463/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.1262 - val_loss: 156.2674\n",
      "Epoch 1464/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 174.2804 - val_loss: 224.2696\n",
      "Epoch 1465/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 198.1469 - val_loss: 213.0191\n",
      "Epoch 1466/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 178.7931 - val_loss: 140.8535\n",
      "Epoch 1467/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.6344 - val_loss: 130.0161\n",
      "Epoch 1468/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.3778 - val_loss: 118.6456\n",
      "Epoch 1469/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.4692 - val_loss: 146.2362\n",
      "Epoch 1470/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.2504 - val_loss: 121.9156\n",
      "Epoch 1471/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 165.1914 - val_loss: 116.2906\n",
      "Epoch 1472/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 182.1438 - val_loss: 164.2343\n",
      "Epoch 1473/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 177.0918 - val_loss: 145.7847\n",
      "Epoch 1474/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.1034 - val_loss: 129.5328\n",
      "Epoch 1475/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.7364 - val_loss: 117.0256\n",
      "Epoch 1476/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 157.7882 - val_loss: 297.0722\n",
      "Epoch 1477/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 237.2428 - val_loss: 148.8116\n",
      "Epoch 1478/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 155.8624 - val_loss: 129.0165\n",
      "Epoch 1479/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 183.6233 - val_loss: 132.3492\n",
      "Epoch 1480/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.5086 - val_loss: 124.2308\n",
      "Epoch 1481/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 173.3045 - val_loss: 142.1889\n",
      "Epoch 1482/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 68us/step - loss: 158.2157 - val_loss: 120.0863\n",
      "Epoch 1483/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.8863 - val_loss: 149.1743\n",
      "Epoch 1484/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 192.4592 - val_loss: 127.0112\n",
      "Epoch 1485/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 189.4332 - val_loss: 133.5534\n",
      "Epoch 1486/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.2911 - val_loss: 139.8746\n",
      "Epoch 1487/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.6078 - val_loss: 117.5052\n",
      "Epoch 1488/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 178.0190 - val_loss: 119.8439\n",
      "Epoch 1489/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.3246 - val_loss: 133.8630\n",
      "Epoch 1490/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.0498 - val_loss: 120.5535\n",
      "Epoch 1491/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 178.5634 - val_loss: 125.7520\n",
      "Epoch 1492/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 178.4110 - val_loss: 133.0800\n",
      "Epoch 1493/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 228.8158 - val_loss: 121.3067\n",
      "Epoch 1494/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 188.1592 - val_loss: 134.5078\n",
      "Epoch 1495/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 169.1821 - val_loss: 125.9506\n",
      "Epoch 1496/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.7399 - val_loss: 137.5446\n",
      "Epoch 1497/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.7939 - val_loss: 112.4067\n",
      "Epoch 1498/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.7931 - val_loss: 270.0376\n",
      "Epoch 1499/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.1755 - val_loss: 111.8512\n",
      "Epoch 1500/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 159.6597 - val_loss: 132.6053\n",
      "Epoch 1501/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.2004 - val_loss: 166.9901\n",
      "Epoch 1502/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 174.1698 - val_loss: 121.8785\n",
      "Epoch 1503/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.0934 - val_loss: 192.3008\n",
      "Epoch 1504/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 174.0860 - val_loss: 142.0990\n",
      "Epoch 1505/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.1862 - val_loss: 135.2704\n",
      "Epoch 1506/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.3900 - val_loss: 269.8063\n",
      "Epoch 1507/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.3403 - val_loss: 114.7399\n",
      "Epoch 1508/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.8303 - val_loss: 116.0482\n",
      "Epoch 1509/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.0347 - val_loss: 120.0842\n",
      "Epoch 1510/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.9620 - val_loss: 125.0373\n",
      "Epoch 1511/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.9590 - val_loss: 128.4284\n",
      "Epoch 1512/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.4347 - val_loss: 118.8271\n",
      "Epoch 1513/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 172.2327 - val_loss: 124.4836\n",
      "Epoch 1514/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 229.5466 - val_loss: 167.7028\n",
      "Epoch 1515/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 169.0435 - val_loss: 177.2496\n",
      "Epoch 1516/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.5044 - val_loss: 110.7097\n",
      "Epoch 1517/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 183.6947 - val_loss: 134.1142\n",
      "Epoch 1518/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 179.9395 - val_loss: 152.4806\n",
      "Epoch 1519/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 170.1429 - val_loss: 137.7307\n",
      "Epoch 1520/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.9720 - val_loss: 126.8451\n",
      "Epoch 1521/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.7132 - val_loss: 182.3073\n",
      "Epoch 1522/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.7581 - val_loss: 163.3680\n",
      "Epoch 1523/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.7099 - val_loss: 113.9357\n",
      "Epoch 1524/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.1807 - val_loss: 127.7960\n",
      "Epoch 1525/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.9356 - val_loss: 152.1641\n",
      "Epoch 1526/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.5337 - val_loss: 160.7503\n",
      "Epoch 1527/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.8156 - val_loss: 145.5375\n",
      "Epoch 1528/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.8930 - val_loss: 127.0297\n",
      "Epoch 1529/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 197.7137 - val_loss: 128.0613\n",
      "Epoch 1530/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.6581 - val_loss: 127.5818\n",
      "Epoch 1531/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.7150 - val_loss: 122.0432\n",
      "Epoch 1532/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 161.8052 - val_loss: 223.5148\n",
      "Epoch 1533/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.7554 - val_loss: 150.4733\n",
      "Epoch 1534/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 168.8376 - val_loss: 158.9417\n",
      "Epoch 1535/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.5313 - val_loss: 117.6313\n",
      "Epoch 1536/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.5514 - val_loss: 128.3913\n",
      "Epoch 1537/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 175.5837 - val_loss: 120.8664\n",
      "Epoch 1538/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 177.1498 - val_loss: 120.8622\n",
      "Epoch 1539/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.1867 - val_loss: 140.9223\n",
      "Epoch 1540/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 161.0971 - val_loss: 178.3911\n",
      "Epoch 1541/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 169.8004 - val_loss: 117.2705\n",
      "Epoch 1542/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 156.4342 - val_loss: 123.4155\n",
      "Epoch 1543/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 156.9083 - val_loss: 154.1568\n",
      "Epoch 1544/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.5768 - val_loss: 124.5872\n",
      "Epoch 1545/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.0585 - val_loss: 171.1795\n",
      "Epoch 1546/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.1716 - val_loss: 137.1121\n",
      "Epoch 1547/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 212.3805 - val_loss: 152.8247\n",
      "Epoch 1548/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.5308 - val_loss: 123.1480\n",
      "Epoch 1549/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.1732 - val_loss: 128.4712\n",
      "Epoch 1550/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.2269 - val_loss: 129.5274\n",
      "Epoch 1551/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.9031 - val_loss: 139.5777\n",
      "Epoch 1552/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.5613 - val_loss: 180.5835\n",
      "Epoch 1553/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.5230 - val_loss: 128.6816\n",
      "Epoch 1554/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.1551 - val_loss: 122.3428\n",
      "Epoch 1555/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.3956 - val_loss: 118.2290\n",
      "Epoch 1556/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.3202 - val_loss: 163.3413\n",
      "Epoch 1557/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.7686 - val_loss: 123.1161\n",
      "Epoch 1558/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.1225 - val_loss: 114.6303\n",
      "Epoch 1559/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 169.8466 - val_loss: 192.3898\n",
      "Epoch 1560/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 177.8534 - val_loss: 324.7339\n",
      "Epoch 1561/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 161.4621 - val_loss: 121.3553\n",
      "Epoch 1562/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.9583 - val_loss: 136.1008\n",
      "Epoch 1563/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 174.9620 - val_loss: 120.7314\n",
      "Epoch 1564/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.2236 - val_loss: 119.3820\n",
      "Epoch 1565/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.6475 - val_loss: 110.7828\n",
      "Epoch 1566/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 174.4250 - val_loss: 149.6359\n",
      "Epoch 1567/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.3796 - val_loss: 128.8439\n",
      "Epoch 1568/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.0906 - val_loss: 115.8676\n",
      "Epoch 1569/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.5258 - val_loss: 114.2086\n",
      "Epoch 1570/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.3989 - val_loss: 125.1476\n",
      "Epoch 1571/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.3594 - val_loss: 111.5565\n",
      "Epoch 1572/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.0214 - val_loss: 124.7398\n",
      "Epoch 1573/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 154.3474 - val_loss: 114.6534\n",
      "Epoch 1574/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 153.3695 - val_loss: 119.0192\n",
      "Epoch 1575/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.3304 - val_loss: 184.1830\n",
      "Epoch 1576/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.4788 - val_loss: 116.8715\n",
      "Epoch 1577/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.1269 - val_loss: 141.3951\n",
      "Epoch 1578/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.8781 - val_loss: 135.5071\n",
      "Epoch 1579/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.1254 - val_loss: 128.1020\n",
      "Epoch 1580/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.2630 - val_loss: 115.1331\n",
      "Epoch 1581/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 146.4782 - val_loss: 117.6294\n",
      "Epoch 1582/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 187.3101 - val_loss: 144.7743\n",
      "Epoch 1583/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 152.0215 - val_loss: 115.5449\n",
      "Epoch 1584/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.2637 - val_loss: 112.0643\n",
      "Epoch 1585/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.5694 - val_loss: 175.2872\n",
      "Epoch 1586/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 143.0650 - val_loss: 136.9877\n",
      "Epoch 1587/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.9877 - val_loss: 146.7449\n",
      "Epoch 1588/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.1330 - val_loss: 185.7742\n",
      "Epoch 1589/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 162.9044 - val_loss: 129.8884\n",
      "Epoch 1590/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.6511 - val_loss: 130.9607\n",
      "Epoch 1591/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.0901 - val_loss: 160.6160\n",
      "Epoch 1592/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.3899 - val_loss: 175.8498\n",
      "Epoch 1593/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.1244 - val_loss: 119.6746\n",
      "Epoch 1594/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.8045 - val_loss: 126.4500\n",
      "Epoch 1595/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.2712 - val_loss: 130.3184\n",
      "Epoch 1596/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.1746 - val_loss: 126.8661\n",
      "Epoch 1597/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 185.6624 - val_loss: 121.0911\n",
      "Epoch 1598/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.1644 - val_loss: 118.0201\n",
      "Epoch 1599/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.5479 - val_loss: 138.8909\n",
      "Epoch 1600/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.6021 - val_loss: 116.4245\n",
      "Epoch 1601/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.0764 - val_loss: 125.8975\n",
      "Epoch 1602/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.6691 - val_loss: 165.5140\n",
      "Epoch 1603/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 167.2572 - val_loss: 148.9238\n",
      "Epoch 1604/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.1967 - val_loss: 177.4651\n",
      "Epoch 1605/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.8372 - val_loss: 164.5914\n",
      "Epoch 1606/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 174.8664 - val_loss: 137.6198\n",
      "Epoch 1607/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.5408 - val_loss: 129.0331\n",
      "Epoch 1608/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 171.4994 - val_loss: 124.6982\n",
      "Epoch 1609/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.5075 - val_loss: 114.3584\n",
      "Epoch 1610/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.0855 - val_loss: 124.3387\n",
      "Epoch 1611/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.4732 - val_loss: 115.7642\n",
      "Epoch 1612/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.9006 - val_loss: 134.5719\n",
      "Epoch 1613/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.9430 - val_loss: 117.1513\n",
      "Epoch 1614/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 174.2456 - val_loss: 195.2931\n",
      "Epoch 1615/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 176.8561 - val_loss: 127.0508\n",
      "Epoch 1616/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.4350 - val_loss: 126.6440\n",
      "Epoch 1617/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.9093 - val_loss: 138.6626\n",
      "Epoch 1618/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.3440 - val_loss: 114.2685\n",
      "Epoch 1619/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 190.7996 - val_loss: 117.0062\n",
      "Epoch 1620/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.3226 - val_loss: 122.3022\n",
      "Epoch 1621/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.3974 - val_loss: 118.4568\n",
      "Epoch 1622/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 148.4534 - val_loss: 115.1529\n",
      "Epoch 1623/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 148.0357 - val_loss: 115.4868\n",
      "Epoch 1624/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 148.8702 - val_loss: 115.0921\n",
      "Epoch 1625/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 169.8112 - val_loss: 130.3034\n",
      "Epoch 1626/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 64us/step - loss: 174.1411 - val_loss: 122.5364\n",
      "Epoch 1627/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.0233 - val_loss: 316.5190\n",
      "Epoch 1628/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 161.8330 - val_loss: 118.8994\n",
      "Epoch 1629/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 166.3354 - val_loss: 146.3468\n",
      "Epoch 1630/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 159.6614 - val_loss: 267.7761\n",
      "Epoch 1631/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 159.5916 - val_loss: 142.6168\n",
      "Epoch 1632/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 159.0633 - val_loss: 125.9702\n",
      "Epoch 1633/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 142.4698 - val_loss: 127.5239\n",
      "Epoch 1634/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 157.3559 - val_loss: 111.2955\n",
      "Epoch 1635/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.8915 - val_loss: 181.3688\n",
      "Epoch 1636/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 154.6555 - val_loss: 125.1065\n",
      "Epoch 1637/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 165.9451 - val_loss: 115.1746\n",
      "Epoch 1638/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 165.7379 - val_loss: 149.8976\n",
      "Epoch 1639/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 180.7763 - val_loss: 117.7397\n",
      "Epoch 1640/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 158.8132 - val_loss: 291.1828\n",
      "Epoch 1641/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 156.9189 - val_loss: 144.6623\n",
      "Epoch 1642/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 159.7122 - val_loss: 121.1722\n",
      "Epoch 1643/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 157.5948 - val_loss: 111.2846\n",
      "Epoch 1644/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.4965 - val_loss: 114.9055\n",
      "Epoch 1645/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.8777 - val_loss: 123.9685\n",
      "Epoch 1646/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 154.9100 - val_loss: 167.3669\n",
      "Epoch 1647/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 150.3129 - val_loss: 163.1332\n",
      "Epoch 1648/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 169.9211 - val_loss: 219.0742\n",
      "Epoch 1649/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 175.9868 - val_loss: 192.5253\n",
      "Epoch 1650/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 153.4922 - val_loss: 117.4575\n",
      "Epoch 1651/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 168.5684 - val_loss: 125.8528\n",
      "Epoch 1652/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.1261 - val_loss: 111.4573\n",
      "Epoch 1653/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.7225 - val_loss: 120.1475\n",
      "Epoch 1654/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 188.1529 - val_loss: 166.2848\n",
      "Epoch 1655/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.7125 - val_loss: 193.9872\n",
      "Epoch 1656/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.4235 - val_loss: 137.7468\n",
      "Epoch 1657/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 146.8839 - val_loss: 152.0797\n",
      "Epoch 1658/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.5971 - val_loss: 119.3115\n",
      "Epoch 1659/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 154.8696 - val_loss: 124.0538\n",
      "Epoch 1660/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 178.2023 - val_loss: 122.0346\n",
      "Epoch 1661/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.6951 - val_loss: 124.6354\n",
      "Epoch 1662/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.4813 - val_loss: 127.8735\n",
      "Epoch 1663/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 194.4167 - val_loss: 128.7148\n",
      "Epoch 1664/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.5107 - val_loss: 111.6758\n",
      "Epoch 1665/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 165.8436 - val_loss: 119.1267\n",
      "Epoch 1666/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 164.2919 - val_loss: 130.7782\n",
      "Epoch 1667/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 157.4851 - val_loss: 135.0228\n",
      "Epoch 1668/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.9712 - val_loss: 161.2585\n",
      "Epoch 1669/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.5392 - val_loss: 117.5967\n",
      "Epoch 1670/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.1035 - val_loss: 115.7180\n",
      "Epoch 1671/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.4056 - val_loss: 130.7408\n",
      "Epoch 1672/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.5732 - val_loss: 147.2693\n",
      "Epoch 1673/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.1592 - val_loss: 188.7420\n",
      "Epoch 1674/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 160.5566 - val_loss: 121.4622\n",
      "Epoch 1675/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.0183 - val_loss: 136.5163\n",
      "Epoch 1676/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 152.9586 - val_loss: 135.8136\n",
      "Epoch 1677/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.5601 - val_loss: 184.1891\n",
      "Epoch 1678/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.9927 - val_loss: 249.6459\n",
      "Epoch 1679/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 162.7410 - val_loss: 120.3545\n",
      "Epoch 1680/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.4681 - val_loss: 114.8557\n",
      "Epoch 1681/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.9227 - val_loss: 131.5808\n",
      "Epoch 1682/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.5783 - val_loss: 121.3990\n",
      "Epoch 1683/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.1528 - val_loss: 183.8618\n",
      "Epoch 1684/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.5305 - val_loss: 123.2564\n",
      "Epoch 1685/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.4863 - val_loss: 118.4114\n",
      "Epoch 1686/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.7083 - val_loss: 123.1067\n",
      "Epoch 1687/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.7058 - val_loss: 119.6849\n",
      "Epoch 1688/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.3340 - val_loss: 168.4160\n",
      "Epoch 1689/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 146.5368 - val_loss: 111.9860\n",
      "Epoch 1690/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 174.7538 - val_loss: 120.0447\n",
      "Epoch 1691/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 149.2644 - val_loss: 114.1362\n",
      "Epoch 1692/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 156.2417 - val_loss: 130.5489\n",
      "Epoch 1693/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.3507 - val_loss: 142.3687\n",
      "Epoch 1694/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.2937 - val_loss: 139.3988\n",
      "Epoch 1695/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.6958 - val_loss: 159.0891\n",
      "Epoch 1696/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 146.9054 - val_loss: 202.0144\n",
      "Epoch 1697/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.3373 - val_loss: 115.5216\n",
      "Epoch 1698/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.7785 - val_loss: 119.7889\n",
      "Epoch 1699/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.2119 - val_loss: 117.3339\n",
      "Epoch 1700/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.5973 - val_loss: 133.1534\n",
      "Epoch 1701/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.8500 - val_loss: 110.0480\n",
      "Epoch 1702/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.9613 - val_loss: 115.7540\n",
      "Epoch 1703/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 153.8332 - val_loss: 114.9839\n",
      "Epoch 1704/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.8208 - val_loss: 228.3107\n",
      "Epoch 1705/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.5751 - val_loss: 134.2242\n",
      "Epoch 1706/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.2260 - val_loss: 118.0260\n",
      "Epoch 1707/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 175.2513 - val_loss: 126.1679\n",
      "Epoch 1708/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.0204 - val_loss: 187.3962\n",
      "Epoch 1709/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 156.3796 - val_loss: 152.9074\n",
      "Epoch 1710/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.0033 - val_loss: 117.0293\n",
      "Epoch 1711/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.6373 - val_loss: 279.0320\n",
      "Epoch 1712/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 160.9587 - val_loss: 181.5841\n",
      "Epoch 1713/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 170.7775 - val_loss: 122.1522\n",
      "Epoch 1714/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 159.6407 - val_loss: 133.7395\n",
      "Epoch 1715/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 152.5622 - val_loss: 207.9814\n",
      "Epoch 1716/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.5745 - val_loss: 117.0798\n",
      "Epoch 1717/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.0993 - val_loss: 124.4617\n",
      "Epoch 1718/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 157.4160 - val_loss: 176.1544\n",
      "Epoch 1719/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 174.0677 - val_loss: 110.4701\n",
      "Epoch 1720/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 155.3275 - val_loss: 126.4495\n",
      "Epoch 1721/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 154.0645 - val_loss: 116.5687\n",
      "Epoch 1722/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 157.4648 - val_loss: 122.5948\n",
      "Epoch 1723/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.3868 - val_loss: 136.4098\n",
      "Epoch 1724/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.3902 - val_loss: 139.9792\n",
      "Epoch 1725/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.2034 - val_loss: 143.1968\n",
      "Epoch 1726/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.8754 - val_loss: 199.0116\n",
      "Epoch 1727/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 175.7522 - val_loss: 129.0445\n",
      "Epoch 1728/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.1052 - val_loss: 118.8692\n",
      "Epoch 1729/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.6155 - val_loss: 165.1054\n",
      "Epoch 1730/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.9078 - val_loss: 171.6978\n",
      "Epoch 1731/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.0546 - val_loss: 125.3510\n",
      "Epoch 1732/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.4432 - val_loss: 117.5877\n",
      "Epoch 1733/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 142.9687 - val_loss: 135.0064\n",
      "Epoch 1734/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 168.0388 - val_loss: 137.3657\n",
      "Epoch 1735/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.5991 - val_loss: 119.2357\n",
      "Epoch 1736/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 162.3657 - val_loss: 123.6154\n",
      "Epoch 1737/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.3634 - val_loss: 122.5131\n",
      "Epoch 1738/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.5165 - val_loss: 211.3882\n",
      "Epoch 1739/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.4576 - val_loss: 124.8236\n",
      "Epoch 1740/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 140.8882 - val_loss: 115.3368\n",
      "Epoch 1741/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.3948 - val_loss: 140.4546\n",
      "Epoch 1742/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.6587 - val_loss: 121.5883\n",
      "Epoch 1743/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.0417 - val_loss: 183.2040\n",
      "Epoch 1744/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.2091 - val_loss: 145.5151\n",
      "Epoch 1745/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.3116 - val_loss: 117.4337\n",
      "Epoch 1746/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 175.6362 - val_loss: 121.3134\n",
      "Epoch 1747/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 181.0566 - val_loss: 130.9436\n",
      "Epoch 1748/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 152.6359 - val_loss: 128.7769\n",
      "Epoch 1749/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 146.1460 - val_loss: 128.1783\n",
      "Epoch 1750/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 139.8968 - val_loss: 116.6816\n",
      "Epoch 1751/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 147.6886 - val_loss: 115.1154\n",
      "Epoch 1752/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.9198 - val_loss: 123.4822\n",
      "Epoch 1753/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.2327 - val_loss: 150.0054\n",
      "Epoch 1754/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.6564 - val_loss: 143.6990\n",
      "Epoch 1755/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.9237 - val_loss: 113.1190\n",
      "Epoch 1756/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.9235 - val_loss: 115.4145\n",
      "Epoch 1757/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.8616 - val_loss: 121.7871\n",
      "Epoch 1758/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 150.7808 - val_loss: 114.2378\n",
      "Epoch 1759/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.9140 - val_loss: 161.3686\n",
      "Epoch 1760/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 214.6755 - val_loss: 122.4389\n",
      "Epoch 1761/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.8041 - val_loss: 135.1358\n",
      "Epoch 1762/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.1021 - val_loss: 115.5259\n",
      "Epoch 1763/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.1998 - val_loss: 115.4927\n",
      "Epoch 1764/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.2868 - val_loss: 113.7023\n",
      "Epoch 1765/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.3124 - val_loss: 190.2591\n",
      "Epoch 1766/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.5147 - val_loss: 161.2288\n",
      "Epoch 1767/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.8962 - val_loss: 118.6516\n",
      "Epoch 1768/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.2090 - val_loss: 113.9853\n",
      "Epoch 1769/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 163.0512 - val_loss: 133.6696\n",
      "Epoch 1770/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 74us/step - loss: 147.6657 - val_loss: 119.7193\n",
      "Epoch 1771/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.3303 - val_loss: 115.6349\n",
      "Epoch 1772/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 158.8285 - val_loss: 116.4983\n",
      "Epoch 1773/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.7251 - val_loss: 113.7605\n",
      "Epoch 1774/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.8767 - val_loss: 123.5165\n",
      "Epoch 1775/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.3336 - val_loss: 124.5749\n",
      "Epoch 1776/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.3771 - val_loss: 128.4981\n",
      "Epoch 1777/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.4442 - val_loss: 116.9419\n",
      "Epoch 1778/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.4819 - val_loss: 157.3494\n",
      "Epoch 1779/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.8828 - val_loss: 113.1105\n",
      "Epoch 1780/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.5640 - val_loss: 119.2617\n",
      "Epoch 1781/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.7840 - val_loss: 150.1392\n",
      "Epoch 1782/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.9881 - val_loss: 120.0178\n",
      "Epoch 1783/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.0763 - val_loss: 119.1003\n",
      "Epoch 1784/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.4794 - val_loss: 162.2951\n",
      "Epoch 1785/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.5264 - val_loss: 125.3738\n",
      "Epoch 1786/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.3384 - val_loss: 120.2257\n",
      "Epoch 1787/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.1005 - val_loss: 123.8275\n",
      "Epoch 1788/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.9478 - val_loss: 116.4713\n",
      "Epoch 1789/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.5000 - val_loss: 142.8964\n",
      "Epoch 1790/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.8285 - val_loss: 160.2322\n",
      "Epoch 1791/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 157.8357 - val_loss: 124.7287\n",
      "Epoch 1792/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.7973 - val_loss: 256.0033\n",
      "Epoch 1793/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.5136 - val_loss: 128.9847\n",
      "Epoch 1794/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.3246 - val_loss: 126.9683\n",
      "Epoch 1795/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.3642 - val_loss: 123.2938\n",
      "Epoch 1796/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.1595 - val_loss: 154.3454\n",
      "Epoch 1797/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.7675 - val_loss: 111.8674\n",
      "Epoch 1798/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.8797 - val_loss: 305.4319\n",
      "Epoch 1799/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.5475 - val_loss: 195.0197\n",
      "Epoch 1800/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.8995 - val_loss: 121.2644\n",
      "Epoch 1801/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.1084 - val_loss: 116.4934\n",
      "Epoch 1802/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.7390 - val_loss: 139.0968\n",
      "Epoch 1803/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.2502 - val_loss: 117.0404\n",
      "Epoch 1804/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.0377 - val_loss: 121.0478\n",
      "Epoch 1805/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.4506 - val_loss: 138.7839\n",
      "Epoch 1806/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 165.0099 - val_loss: 121.6367\n",
      "Epoch 1807/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.6670 - val_loss: 155.3217\n",
      "Epoch 1808/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 150.7529 - val_loss: 131.3494\n",
      "Epoch 1809/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 154.4593 - val_loss: 151.8285\n",
      "Epoch 1810/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 146.7371 - val_loss: 133.3195\n",
      "Epoch 1811/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.8129 - val_loss: 114.8591\n",
      "Epoch 1812/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.5749 - val_loss: 118.3055\n",
      "Epoch 1813/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.7925 - val_loss: 119.5627\n",
      "Epoch 1814/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.7455 - val_loss: 122.8309\n",
      "Epoch 1815/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.5534 - val_loss: 164.6445\n",
      "Epoch 1816/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.6527 - val_loss: 112.5498\n",
      "Epoch 1817/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 162.1099 - val_loss: 113.3905\n",
      "Epoch 1818/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.5961 - val_loss: 132.7252\n",
      "Epoch 1819/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.9815 - val_loss: 114.5616\n",
      "Epoch 1820/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 154.2968 - val_loss: 119.3209\n",
      "Epoch 1821/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.2462 - val_loss: 127.1068\n",
      "Epoch 1822/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.0880 - val_loss: 159.9589\n",
      "Epoch 1823/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.0183 - val_loss: 118.2065\n",
      "Epoch 1824/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.0751 - val_loss: 241.8084\n",
      "Epoch 1825/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.0128 - val_loss: 119.6958\n",
      "Epoch 1826/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.0750 - val_loss: 128.8608\n",
      "Epoch 1827/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.6160 - val_loss: 156.9931\n",
      "Epoch 1828/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.5645 - val_loss: 132.0975\n",
      "Epoch 1829/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.9394 - val_loss: 118.1471\n",
      "Epoch 1830/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.5044 - val_loss: 128.2417\n",
      "Epoch 1831/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 195.3080 - val_loss: 135.3249\n",
      "Epoch 1832/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.9645 - val_loss: 123.6505\n",
      "Epoch 1833/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.7968 - val_loss: 146.9949\n",
      "Epoch 1834/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 166.3905 - val_loss: 178.5757\n",
      "Epoch 1835/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.1525 - val_loss: 138.4174\n",
      "Epoch 1836/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.3151 - val_loss: 138.9028\n",
      "Epoch 1837/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.4902 - val_loss: 121.8173\n",
      "Epoch 1838/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.6939 - val_loss: 146.4193\n",
      "Epoch 1839/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.2825 - val_loss: 115.1824\n",
      "Epoch 1840/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.7116 - val_loss: 128.1421\n",
      "Epoch 1841/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.6599 - val_loss: 117.6073\n",
      "Epoch 1842/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.1876 - val_loss: 121.3073\n",
      "Epoch 1843/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.3317 - val_loss: 122.8764\n",
      "Epoch 1844/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.5722 - val_loss: 120.8524\n",
      "Epoch 1845/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.8450 - val_loss: 263.2746\n",
      "Epoch 1846/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 176.3890 - val_loss: 114.9419\n",
      "Epoch 1847/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.2633 - val_loss: 112.1852\n",
      "Epoch 1848/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.2185 - val_loss: 155.8220\n",
      "Epoch 1849/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.8546 - val_loss: 126.1193\n",
      "Epoch 1850/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.7867 - val_loss: 113.8338\n",
      "Epoch 1851/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 153.0139 - val_loss: 146.0133\n",
      "Epoch 1852/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 175.8960 - val_loss: 126.1036\n",
      "Epoch 1853/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.1896 - val_loss: 138.2394\n",
      "Epoch 1854/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.1406 - val_loss: 134.6294\n",
      "Epoch 1855/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.3898 - val_loss: 128.1329\n",
      "Epoch 1856/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.9132 - val_loss: 114.9541\n",
      "Epoch 1857/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.0557 - val_loss: 126.7930\n",
      "Epoch 1858/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 147.8031 - val_loss: 136.8855\n",
      "Epoch 1859/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.3556 - val_loss: 111.8941\n",
      "Epoch 1860/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.9607 - val_loss: 120.1844\n",
      "Epoch 1861/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.4178 - val_loss: 115.7495\n",
      "Epoch 1862/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.3130 - val_loss: 132.4988\n",
      "Epoch 1863/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.6936 - val_loss: 221.0579\n",
      "Epoch 1864/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.1541 - val_loss: 144.8676\n",
      "Epoch 1865/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.5337 - val_loss: 112.7623\n",
      "Epoch 1866/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 222.7659 - val_loss: 116.2548\n",
      "Epoch 1867/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 161.4228 - val_loss: 116.6451\n",
      "Epoch 1868/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 145.2123 - val_loss: 126.6754\n",
      "Epoch 1869/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 168.3344 - val_loss: 133.6373\n",
      "Epoch 1870/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 155.6829 - val_loss: 130.4649\n",
      "Epoch 1871/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.7280 - val_loss: 116.0610\n",
      "Epoch 1872/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.2654 - val_loss: 115.0940\n",
      "Epoch 1873/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.1636 - val_loss: 114.8345\n",
      "Epoch 1874/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.3837 - val_loss: 116.9708\n",
      "Epoch 1875/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.7321 - val_loss: 113.2836\n",
      "Epoch 1876/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.4210 - val_loss: 119.4977\n",
      "Epoch 1877/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.9720 - val_loss: 151.3899\n",
      "Epoch 1878/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 158.3936 - val_loss: 113.5364\n",
      "Epoch 1879/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.4422 - val_loss: 134.3494\n",
      "Epoch 1880/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.0769 - val_loss: 165.0257\n",
      "Epoch 1881/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.9411 - val_loss: 118.9116\n",
      "Epoch 1882/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.8966 - val_loss: 115.0473\n",
      "Epoch 1883/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.9391 - val_loss: 133.6641\n",
      "Epoch 1884/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.8041 - val_loss: 152.6316\n",
      "Epoch 1885/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.9333 - val_loss: 140.4784\n",
      "Epoch 1886/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.5182 - val_loss: 124.9052\n",
      "Epoch 1887/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.8587 - val_loss: 116.6922\n",
      "Epoch 1888/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.5806 - val_loss: 118.6073\n",
      "Epoch 1889/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.9387 - val_loss: 175.0797\n",
      "Epoch 1890/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.6226 - val_loss: 149.2647\n",
      "Epoch 1891/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.2341 - val_loss: 130.2615\n",
      "Epoch 1892/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.7697 - val_loss: 141.3437\n",
      "Epoch 1893/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.6387 - val_loss: 240.7289\n",
      "Epoch 1894/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.5844 - val_loss: 118.5593\n",
      "Epoch 1895/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 150.8350 - val_loss: 119.0006\n",
      "Epoch 1896/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.5079 - val_loss: 138.2169\n",
      "Epoch 1897/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.1050 - val_loss: 116.9636\n",
      "Epoch 1898/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 163.0896 - val_loss: 135.6340\n",
      "Epoch 1899/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 151.4110 - val_loss: 131.0248\n",
      "Epoch 1900/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.2257 - val_loss: 188.8788\n",
      "Epoch 1901/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.0566 - val_loss: 112.4861\n",
      "Epoch 1902/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.6838 - val_loss: 152.8664\n",
      "Epoch 1903/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.9251 - val_loss: 157.7796\n",
      "Epoch 1904/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.2975 - val_loss: 146.7109\n",
      "Epoch 1905/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.9768 - val_loss: 126.7741\n",
      "Epoch 1906/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.8300 - val_loss: 118.2459\n",
      "Epoch 1907/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.5866 - val_loss: 116.5665\n",
      "Epoch 1908/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.1164 - val_loss: 116.7451\n",
      "Epoch 1909/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.2186 - val_loss: 134.1648\n",
      "Epoch 1910/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.4607 - val_loss: 113.9413\n",
      "Epoch 1911/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.0868 - val_loss: 203.4058\n",
      "Epoch 1912/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 175.7018 - val_loss: 159.6354\n",
      "Epoch 1913/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.1452 - val_loss: 122.2900\n",
      "Epoch 1914/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.2245 - val_loss: 114.6096\n",
      "Epoch 1915/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.7040 - val_loss: 114.6508\n",
      "Epoch 1916/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 161.1424 - val_loss: 170.2254\n",
      "Epoch 1917/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.2738 - val_loss: 116.1454\n",
      "Epoch 1918/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.9348 - val_loss: 124.1512\n",
      "Epoch 1919/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.0136 - val_loss: 147.4720\n",
      "Epoch 1920/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.2935 - val_loss: 114.0378\n",
      "Epoch 1921/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.9800 - val_loss: 137.6989\n",
      "Epoch 1922/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.6519 - val_loss: 117.2915\n",
      "Epoch 1923/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.1202 - val_loss: 124.1715\n",
      "Epoch 1924/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.7504 - val_loss: 125.4966\n",
      "Epoch 1925/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.7950 - val_loss: 118.4473\n",
      "Epoch 1926/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.1182 - val_loss: 185.2678\n",
      "Epoch 1927/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 151.7205 - val_loss: 111.9904\n",
      "Epoch 1928/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 146.3358 - val_loss: 245.0760\n",
      "Epoch 1929/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 155.9319 - val_loss: 182.3081\n",
      "Epoch 1930/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.2722 - val_loss: 149.8221\n",
      "Epoch 1931/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.4446 - val_loss: 114.4072\n",
      "Epoch 1932/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.9466 - val_loss: 122.6167\n",
      "Epoch 1933/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.7865 - val_loss: 163.8689\n",
      "Epoch 1934/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 158.1154 - val_loss: 153.5179\n",
      "Epoch 1935/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.7853 - val_loss: 125.5355\n",
      "Epoch 1936/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.8573 - val_loss: 119.5257\n",
      "Epoch 1937/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.8956 - val_loss: 137.2019\n",
      "Epoch 1938/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 144.3021 - val_loss: 118.0564\n",
      "Epoch 1939/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 158.0778 - val_loss: 115.1011\n",
      "Epoch 1940/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.9365 - val_loss: 121.3928\n",
      "Epoch 1941/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.7893 - val_loss: 115.0678\n",
      "Epoch 1942/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.6500 - val_loss: 116.4821\n",
      "Epoch 1943/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.1827 - val_loss: 121.1049\n",
      "Epoch 1944/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.0726 - val_loss: 130.6163\n",
      "Epoch 1945/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.4696 - val_loss: 114.4158\n",
      "Epoch 1946/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.3055 - val_loss: 114.0962\n",
      "Epoch 1947/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.3615 - val_loss: 143.6426\n",
      "Epoch 1948/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.3552 - val_loss: 179.6709\n",
      "Epoch 1949/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 160.4453 - val_loss: 121.0784\n",
      "Epoch 1950/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.4313 - val_loss: 150.6329\n",
      "Epoch 1951/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.1546 - val_loss: 132.4290\n",
      "Epoch 1952/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.9405 - val_loss: 134.0854\n",
      "Epoch 1953/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.1308 - val_loss: 114.2865\n",
      "Epoch 1954/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 189.3088 - val_loss: 182.7810\n",
      "Epoch 1955/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.9038 - val_loss: 117.7293\n",
      "Epoch 1956/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.6916 - val_loss: 129.8912\n",
      "Epoch 1957/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 141.9158 - val_loss: 122.4925\n",
      "Epoch 1958/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 148.7078 - val_loss: 123.9613\n",
      "Epoch 1959/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.6872 - val_loss: 113.9757\n",
      "Epoch 1960/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.2017 - val_loss: 112.9032\n",
      "Epoch 1961/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 153.7412 - val_loss: 160.3130\n",
      "Epoch 1962/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.9610 - val_loss: 124.6281\n",
      "Epoch 1963/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.5457 - val_loss: 117.6776\n",
      "Epoch 1964/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 189.5995 - val_loss: 124.3862\n",
      "Epoch 1965/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.1064 - val_loss: 121.0335\n",
      "Epoch 1966/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.8885 - val_loss: 189.5137\n",
      "Epoch 1967/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.7071 - val_loss: 171.9829\n",
      "Epoch 1968/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 156.5521 - val_loss: 128.9805\n",
      "Epoch 1969/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.6059 - val_loss: 117.1702\n",
      "Epoch 1970/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.2699 - val_loss: 116.8245\n",
      "Epoch 1971/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.3637 - val_loss: 129.2685\n",
      "Epoch 1972/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.8293 - val_loss: 116.3071\n",
      "Epoch 1973/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 162.1270 - val_loss: 138.3306\n",
      "Epoch 1974/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.3707 - val_loss: 123.3275\n",
      "Epoch 1975/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.1360 - val_loss: 120.5853\n",
      "Epoch 1976/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.8039 - val_loss: 129.0416\n",
      "Epoch 1977/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 152.9778 - val_loss: 117.1133\n",
      "Epoch 1978/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 158.2727 - val_loss: 117.5475\n",
      "Epoch 1979/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.4649 - val_loss: 113.7856\n",
      "Epoch 1980/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.0965 - val_loss: 130.2345\n",
      "Epoch 1981/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.5895 - val_loss: 127.6752\n",
      "Epoch 1982/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.2941 - val_loss: 139.6414\n",
      "Epoch 1983/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 154.4258 - val_loss: 151.3630\n",
      "Epoch 1984/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 184.8212 - val_loss: 123.9434\n",
      "Epoch 1985/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.8875 - val_loss: 129.7551\n",
      "Epoch 1986/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 75us/step - loss: 145.1259 - val_loss: 160.8189\n",
      "Epoch 1987/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 157.8860 - val_loss: 169.4618\n",
      "Epoch 1988/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 149.2145 - val_loss: 138.8811\n",
      "Epoch 1989/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.9792 - val_loss: 114.2502\n",
      "Epoch 1990/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.6555 - val_loss: 184.7391\n",
      "Epoch 1991/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.6423 - val_loss: 214.7798\n",
      "Epoch 1992/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.8584 - val_loss: 114.7107\n",
      "Epoch 1993/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.2230 - val_loss: 144.8101\n",
      "Epoch 1994/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.4978 - val_loss: 118.9119\n",
      "Epoch 1995/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.2567 - val_loss: 175.6877\n",
      "Epoch 1996/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.4551 - val_loss: 117.4338\n",
      "Epoch 1997/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.4448 - val_loss: 117.2678\n",
      "Epoch 1998/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.6229 - val_loss: 113.6760\n",
      "Epoch 1999/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.2816 - val_loss: 133.6088\n",
      "Epoch 2000/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.3263 - val_loss: 128.1316\n",
      "Epoch 2001/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.5056 - val_loss: 120.4934\n",
      "Epoch 2002/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.2312 - val_loss: 118.7410\n",
      "Epoch 2003/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.1487 - val_loss: 114.8535\n",
      "Epoch 2004/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.9878 - val_loss: 184.1121\n",
      "Epoch 2005/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.5285 - val_loss: 126.1969\n",
      "Epoch 2006/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.9715 - val_loss: 128.5637\n",
      "Epoch 2007/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.9586 - val_loss: 148.5183\n",
      "Epoch 2008/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 163.0693 - val_loss: 139.3631\n",
      "Epoch 2009/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.4196 - val_loss: 129.3857\n",
      "Epoch 2010/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.6344 - val_loss: 112.8492\n",
      "Epoch 2011/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.9918 - val_loss: 132.1787\n",
      "Epoch 2012/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 159.3264 - val_loss: 149.6993\n",
      "Epoch 2013/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.4896 - val_loss: 132.6116\n",
      "Epoch 2014/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.0325 - val_loss: 115.4720\n",
      "Epoch 2015/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.6556 - val_loss: 116.1705\n",
      "Epoch 2016/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.3463 - val_loss: 127.3134\n",
      "Epoch 2017/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 150.097 - 1s 64us/step - loss: 152.6258 - val_loss: 117.9025\n",
      "Epoch 2018/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 160.8349 - val_loss: 129.9560\n",
      "Epoch 2019/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.8215 - val_loss: 182.0420\n",
      "Epoch 2020/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.6742 - val_loss: 168.5593\n",
      "Epoch 2021/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.7076 - val_loss: 188.3988\n",
      "Epoch 2022/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.7622 - val_loss: 164.8974\n",
      "Epoch 2023/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.0688 - val_loss: 150.3084\n",
      "Epoch 2024/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.0922 - val_loss: 206.1994\n",
      "Epoch 2025/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.5911 - val_loss: 192.8997\n",
      "Epoch 2026/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.9944 - val_loss: 143.1920\n",
      "Epoch 2027/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 154.8988 - val_loss: 126.8410\n",
      "Epoch 2028/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 169.8413 - val_loss: 138.9210\n",
      "Epoch 2029/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.5218 - val_loss: 146.5229\n",
      "Epoch 2030/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.0345 - val_loss: 183.0144\n",
      "Epoch 2031/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.7737 - val_loss: 134.9767\n",
      "Epoch 2032/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.8934 - val_loss: 126.9392\n",
      "Epoch 2033/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.9948 - val_loss: 115.3816\n",
      "Epoch 2034/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.3046 - val_loss: 117.8328\n",
      "Epoch 2035/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.6050 - val_loss: 146.7178\n",
      "Epoch 2036/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.8617 - val_loss: 117.8878\n",
      "Epoch 2037/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 183.2344 - val_loss: 120.1207\n",
      "Epoch 2038/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.1459 - val_loss: 149.6975\n",
      "Epoch 2039/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.9196 - val_loss: 126.2378\n",
      "Epoch 2040/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.6305 - val_loss: 120.7928\n",
      "Epoch 2041/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.1647 - val_loss: 116.4901\n",
      "Epoch 2042/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.9593 - val_loss: 116.4857\n",
      "Epoch 2043/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.6439 - val_loss: 115.3893\n",
      "Epoch 2044/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.3283 - val_loss: 116.1971\n",
      "Epoch 2045/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 139.8224 - val_loss: 221.3154\n",
      "Epoch 2046/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 147.3658 - val_loss: 123.0802\n",
      "Epoch 2047/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 147.2092 - val_loss: 119.5817\n",
      "Epoch 2048/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 148.1144 - val_loss: 120.8334\n",
      "Epoch 2049/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.1726 - val_loss: 133.5005\n",
      "Epoch 2050/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.7002 - val_loss: 121.6179\n",
      "Epoch 2051/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.6538 - val_loss: 138.5344\n",
      "Epoch 2052/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.8804 - val_loss: 127.3840\n",
      "Epoch 2053/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.0763 - val_loss: 146.5272\n",
      "Epoch 2054/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.6832 - val_loss: 156.5237\n",
      "Epoch 2055/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.7629 - val_loss: 134.4317\n",
      "Epoch 2056/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.5981 - val_loss: 115.3006\n",
      "Epoch 2057/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 135.2854 - val_loss: 151.8866\n",
      "Epoch 2058/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.3314 - val_loss: 112.7195\n",
      "Epoch 2059/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 169.7723 - val_loss: 124.2154\n",
      "Epoch 2060/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.4073 - val_loss: 157.4673\n",
      "Epoch 2061/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.4107 - val_loss: 134.4480\n",
      "Epoch 2062/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.7440 - val_loss: 128.3786\n",
      "Epoch 2063/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.7154 - val_loss: 130.4576\n",
      "Epoch 2064/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.3400 - val_loss: 134.4117\n",
      "Epoch 2065/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.9741 - val_loss: 114.4108\n",
      "Epoch 2066/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.4486 - val_loss: 160.3731\n",
      "Epoch 2067/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.2186 - val_loss: 152.0521\n",
      "Epoch 2068/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.5155 - val_loss: 155.7080\n",
      "Epoch 2069/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 152.3418 - val_loss: 185.2794\n",
      "Epoch 2070/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.5149 - val_loss: 119.9173\n",
      "Epoch 2071/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.0319 - val_loss: 134.2303\n",
      "Epoch 2072/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.5386 - val_loss: 117.9605\n",
      "Epoch 2073/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.7804 - val_loss: 151.6275\n",
      "Epoch 2074/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.4145 - val_loss: 180.9644\n",
      "Epoch 2075/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.3149 - val_loss: 123.5745\n",
      "Epoch 2076/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.9507 - val_loss: 114.5596\n",
      "Epoch 2077/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.8874 - val_loss: 120.0487\n",
      "Epoch 2078/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.1086 - val_loss: 197.2964\n",
      "Epoch 2079/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.0135 - val_loss: 139.4846\n",
      "Epoch 2080/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.3495 - val_loss: 184.2460\n",
      "Epoch 2081/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.6006 - val_loss: 115.5656\n",
      "Epoch 2082/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.3612 - val_loss: 119.8978\n",
      "Epoch 2083/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.5446 - val_loss: 170.8399\n",
      "Epoch 2084/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.7988 - val_loss: 156.8119\n",
      "Epoch 2085/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.3931 - val_loss: 115.6822\n",
      "Epoch 2086/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 155.6068 - val_loss: 183.9438\n",
      "Epoch 2087/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.7230 - val_loss: 142.2026\n",
      "Epoch 2088/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.7286 - val_loss: 116.7856\n",
      "Epoch 2089/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.6297 - val_loss: 132.5980\n",
      "Epoch 2090/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 175.2342 - val_loss: 116.5125\n",
      "Epoch 2091/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.9589 - val_loss: 136.5265\n",
      "Epoch 2092/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.2214 - val_loss: 120.5916\n",
      "Epoch 2093/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.2084 - val_loss: 119.3720\n",
      "Epoch 2094/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.6472 - val_loss: 167.0721\n",
      "Epoch 2095/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.6568 - val_loss: 120.6058\n",
      "Epoch 2096/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 165.6449 - val_loss: 146.4628\n",
      "Epoch 2097/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.3666 - val_loss: 119.9045\n",
      "Epoch 2098/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.2645 - val_loss: 119.8604\n",
      "Epoch 2099/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.5040 - val_loss: 123.4625\n",
      "Epoch 2100/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.8205 - val_loss: 113.4404\n",
      "Epoch 2101/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 132.8080 - val_loss: 115.8675\n",
      "Epoch 2102/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.8450 - val_loss: 113.0888\n",
      "Epoch 2103/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.8515 - val_loss: 114.1629\n",
      "Epoch 2104/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.8108 - val_loss: 175.5125\n",
      "Epoch 2105/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 155.0356 - val_loss: 125.9091\n",
      "Epoch 2106/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 151.9795 - val_loss: 128.3817\n",
      "Epoch 2107/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 146.4224 - val_loss: 124.8497\n",
      "Epoch 2108/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.7771 - val_loss: 125.1178\n",
      "Epoch 2109/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 171.1732 - val_loss: 116.6227\n",
      "Epoch 2110/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.9562 - val_loss: 160.6456\n",
      "Epoch 2111/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.7813 - val_loss: 140.2688\n",
      "Epoch 2112/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.0657 - val_loss: 125.7048\n",
      "Epoch 2113/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.8725 - val_loss: 131.4303\n",
      "Epoch 2114/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.2421 - val_loss: 112.4993\n",
      "Epoch 2115/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.4539 - val_loss: 115.9402\n",
      "Epoch 2116/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 142.4047 - val_loss: 226.7388\n",
      "Epoch 2117/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 151.0035 - val_loss: 134.0860\n",
      "Epoch 2118/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.8775 - val_loss: 140.3982\n",
      "Epoch 2119/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.8411 - val_loss: 129.7020\n",
      "Epoch 2120/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.4580 - val_loss: 200.5392\n",
      "Epoch 2121/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.8971 - val_loss: 121.3991\n",
      "Epoch 2122/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.7092 - val_loss: 111.7541\n",
      "Epoch 2123/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.9282 - val_loss: 128.6953\n",
      "Epoch 2124/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.1015 - val_loss: 111.5191\n",
      "Epoch 2125/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.2461 - val_loss: 118.0949\n",
      "Epoch 2126/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.7756 - val_loss: 135.7608\n",
      "Epoch 2127/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.4315 - val_loss: 126.0276\n",
      "Epoch 2128/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.1682 - val_loss: 118.5329\n",
      "Epoch 2129/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.6361 - val_loss: 114.8410\n",
      "Epoch 2130/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.2165 - val_loss: 138.5904\n",
      "Epoch 2131/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.0718 - val_loss: 113.8831\n",
      "Epoch 2132/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.8986 - val_loss: 146.6258\n",
      "Epoch 2133/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.8945 - val_loss: 116.2806\n",
      "Epoch 2134/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.2980 - val_loss: 125.1881\n",
      "Epoch 2135/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.8332 - val_loss: 116.3436\n",
      "Epoch 2136/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.9430 - val_loss: 128.5666\n",
      "Epoch 2137/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.6503 - val_loss: 116.8169\n",
      "Epoch 2138/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.5559 - val_loss: 144.1831\n",
      "Epoch 2139/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 137.6737 - val_loss: 113.5692\n",
      "Epoch 2140/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 147.7378 - val_loss: 116.5161\n",
      "Epoch 2141/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.3287 - val_loss: 114.9301\n",
      "Epoch 2142/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.5913 - val_loss: 113.7814\n",
      "Epoch 2143/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.7347 - val_loss: 141.7335\n",
      "Epoch 2144/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.3616 - val_loss: 117.1031\n",
      "Epoch 2145/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 182.3374 - val_loss: 113.4027\n",
      "Epoch 2146/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 159.4669 - val_loss: 141.4834\n",
      "Epoch 2147/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.2199 - val_loss: 111.2586\n",
      "Epoch 2148/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.3025 - val_loss: 120.6695\n",
      "Epoch 2149/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.6600 - val_loss: 119.8409\n",
      "Epoch 2150/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.4237 - val_loss: 117.7134\n",
      "Epoch 2151/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.4977 - val_loss: 141.0903\n",
      "Epoch 2152/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.9856 - val_loss: 140.2737\n",
      "Epoch 2153/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.3878 - val_loss: 112.5235\n",
      "Epoch 2154/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.7468 - val_loss: 113.4464\n",
      "Epoch 2155/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.5167 - val_loss: 118.7968\n",
      "Epoch 2156/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.8955 - val_loss: 124.0581\n",
      "Epoch 2157/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.7226 - val_loss: 201.5623\n",
      "Epoch 2158/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.8766 - val_loss: 128.5855\n",
      "Epoch 2159/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.8850 - val_loss: 111.0482\n",
      "Epoch 2160/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 142.5532 - val_loss: 154.9736\n",
      "Epoch 2161/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.1262 - val_loss: 117.9872\n",
      "Epoch 2162/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.7273 - val_loss: 113.0141\n",
      "Epoch 2163/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.5274 - val_loss: 157.1557\n",
      "Epoch 2164/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.5675 - val_loss: 120.7583\n",
      "Epoch 2165/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.0293 - val_loss: 123.3342\n",
      "Epoch 2166/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.0766 - val_loss: 121.0358\n",
      "Epoch 2167/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.8281 - val_loss: 125.6913\n",
      "Epoch 2168/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.6461 - val_loss: 146.3916\n",
      "Epoch 2169/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.1640 - val_loss: 120.7756\n",
      "Epoch 2170/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.8916 - val_loss: 132.6278\n",
      "Epoch 2171/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 186.3139 - val_loss: 319.0769\n",
      "Epoch 2172/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 253.8174 - val_loss: 202.3841\n",
      "Epoch 2173/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 211.6749 - val_loss: 162.2286\n",
      "Epoch 2174/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 192.9843 - val_loss: 131.0259\n",
      "Epoch 2175/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 181.1222 - val_loss: 156.6856\n",
      "Epoch 2176/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.9707 - val_loss: 146.4872\n",
      "Epoch 2177/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 179.3989 - val_loss: 143.6953\n",
      "Epoch 2178/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.9139 - val_loss: 128.0698\n",
      "Epoch 2179/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 172.7035 - val_loss: 126.6577\n",
      "Epoch 2180/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 169.4081 - val_loss: 125.9335\n",
      "Epoch 2181/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 159.5107 - val_loss: 130.8543\n",
      "Epoch 2182/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.8757 - val_loss: 153.4458\n",
      "Epoch 2183/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 170.5363 - val_loss: 160.1664\n",
      "Epoch 2184/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.7667 - val_loss: 134.6697\n",
      "Epoch 2185/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.5211 - val_loss: 132.5880\n",
      "Epoch 2186/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 173.6424 - val_loss: 129.5342\n",
      "Epoch 2187/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 173.2531 - val_loss: 208.1863\n",
      "Epoch 2188/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 191.1808 - val_loss: 130.9233\n",
      "Epoch 2189/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 162.2429 - val_loss: 122.6614\n",
      "Epoch 2190/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.8317 - val_loss: 139.6036\n",
      "Epoch 2191/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.0119 - val_loss: 181.5943\n",
      "Epoch 2192/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.6339 - val_loss: 150.6471\n",
      "Epoch 2193/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 183.1336 - val_loss: 125.7650\n",
      "Epoch 2194/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.8474 - val_loss: 163.3413\n",
      "Epoch 2195/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 164.3919 - val_loss: 137.2743\n",
      "Epoch 2196/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 164.3821 - val_loss: 144.1734\n",
      "Epoch 2197/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.4819 - val_loss: 213.9640\n",
      "Epoch 2198/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.6030 - val_loss: 197.8810\n",
      "Epoch 2199/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 172.7199 - val_loss: 277.3721\n",
      "Epoch 2200/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 150.3119 - val_loss: 125.6332\n",
      "Epoch 2201/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.1883 - val_loss: 130.9557\n",
      "Epoch 2202/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.1287 - val_loss: 158.0429\n",
      "Epoch 2203/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.3906 - val_loss: 119.6493\n",
      "Epoch 2204/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 158.5730 - val_loss: 128.0592\n",
      "Epoch 2205/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 158.7714 - val_loss: 118.7725\n",
      "Epoch 2206/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.4338 - val_loss: 131.1666\n",
      "Epoch 2207/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.3143 - val_loss: 135.4515\n",
      "Epoch 2208/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.1644 - val_loss: 123.7517\n",
      "Epoch 2209/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 171.7241 - val_loss: 122.1211\n",
      "Epoch 2210/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.7044 - val_loss: 119.8723\n",
      "Epoch 2211/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.1094 - val_loss: 129.7666\n",
      "Epoch 2212/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.3864 - val_loss: 153.5515\n",
      "Epoch 2213/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.6783 - val_loss: 131.5759\n",
      "Epoch 2214/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.2585 - val_loss: 142.8427\n",
      "Epoch 2215/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.6540 - val_loss: 156.3469\n",
      "Epoch 2216/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.0648 - val_loss: 133.3447\n",
      "Epoch 2217/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.3446 - val_loss: 117.9206\n",
      "Epoch 2218/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 161.9509 - val_loss: 155.1484\n",
      "Epoch 2219/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 157.2389 - val_loss: 139.3215\n",
      "Epoch 2220/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.0265 - val_loss: 151.3183\n",
      "Epoch 2221/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.7137 - val_loss: 208.0206\n",
      "Epoch 2222/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 176.6758 - val_loss: 221.7247\n",
      "Epoch 2223/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 181.9834 - val_loss: 117.8409\n",
      "Epoch 2224/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 161.6437 - val_loss: 117.1126\n",
      "Epoch 2225/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 170.7481 - val_loss: 165.7818\n",
      "Epoch 2226/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.5771 - val_loss: 127.0414\n",
      "Epoch 2227/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.3734 - val_loss: 159.4200\n",
      "Epoch 2228/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 208.6149 - val_loss: 156.7918\n",
      "Epoch 2229/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 179.9369 - val_loss: 144.3768\n",
      "Epoch 2230/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 185.3484 - val_loss: 123.6301\n",
      "Epoch 2231/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 187.3679 - val_loss: 126.7424\n",
      "Epoch 2232/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 185.6344 - val_loss: 125.7267\n",
      "Epoch 2233/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 162.3850 - val_loss: 120.4540\n",
      "Epoch 2234/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.0961 - val_loss: 147.0951\n",
      "Epoch 2235/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 178.5850 - val_loss: 275.1407\n",
      "Epoch 2236/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 193.4198 - val_loss: 174.1975\n",
      "Epoch 2237/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 190.1262 - val_loss: 132.0103\n",
      "Epoch 2238/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 186.3127 - val_loss: 133.0824\n",
      "Epoch 2239/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 184.2800 - val_loss: 159.5121\n",
      "Epoch 2240/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.8796 - val_loss: 130.0427\n",
      "Epoch 2241/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 195.6603 - val_loss: 137.4922\n",
      "Epoch 2242/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 191.8029 - val_loss: 143.4963\n",
      "Epoch 2243/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.0379 - val_loss: 181.4720\n",
      "Epoch 2244/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.2805 - val_loss: 118.1251\n",
      "Epoch 2245/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.4875 - val_loss: 146.4280\n",
      "Epoch 2246/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 159.8522 - val_loss: 114.8238\n",
      "Epoch 2247/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 152.0684 - val_loss: 122.8197\n",
      "Epoch 2248/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 156.1581 - val_loss: 136.6642\n",
      "Epoch 2249/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.4625 - val_loss: 130.0087\n",
      "Epoch 2250/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.5437 - val_loss: 126.2838\n",
      "Epoch 2251/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.8659 - val_loss: 156.7090\n",
      "Epoch 2252/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.6734 - val_loss: 140.3698\n",
      "Epoch 2253/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.4249 - val_loss: 125.5613\n",
      "Epoch 2254/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.4026 - val_loss: 121.8105\n",
      "Epoch 2255/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 192.8370 - val_loss: 126.0398\n",
      "Epoch 2256/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.5141 - val_loss: 144.2011\n",
      "Epoch 2257/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.9358 - val_loss: 127.3451\n",
      "Epoch 2258/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 175.3537 - val_loss: 126.2736\n",
      "Epoch 2259/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 152.8824 - val_loss: 160.6465\n",
      "Epoch 2260/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.1923 - val_loss: 113.5674\n",
      "Epoch 2261/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.1486 - val_loss: 137.4200\n",
      "Epoch 2262/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.2226 - val_loss: 115.3180\n",
      "Epoch 2263/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 154.6518 - val_loss: 117.7583\n",
      "Epoch 2264/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.8428 - val_loss: 151.5328\n",
      "Epoch 2265/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.9772 - val_loss: 139.1575\n",
      "Epoch 2266/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.1157 - val_loss: 126.5813\n",
      "Epoch 2267/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.2192 - val_loss: 173.4723\n",
      "Epoch 2268/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.8792 - val_loss: 117.4359\n",
      "Epoch 2269/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 174.1903 - val_loss: 120.7043\n",
      "Epoch 2270/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.9515 - val_loss: 115.0826\n",
      "Epoch 2271/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.0573 - val_loss: 121.7088\n",
      "Epoch 2272/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.0020 - val_loss: 121.8313\n",
      "Epoch 2273/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.9900 - val_loss: 117.3943\n",
      "Epoch 2274/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 63us/step - loss: 177.0713 - val_loss: 123.2403\n",
      "Epoch 2275/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.2389 - val_loss: 130.8694\n",
      "Epoch 2276/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 170.2454 - val_loss: 113.1034\n",
      "Epoch 2277/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.8649 - val_loss: 131.7458\n",
      "Epoch 2278/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.6686 - val_loss: 123.2748\n",
      "Epoch 2279/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.4508 - val_loss: 151.1103\n",
      "Epoch 2280/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.6888 - val_loss: 260.6282\n",
      "Epoch 2281/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.8217 - val_loss: 129.8418\n",
      "Epoch 2282/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 161.7444 - val_loss: 113.6559\n",
      "Epoch 2283/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 152.3499 - val_loss: 179.9285\n",
      "Epoch 2284/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 160.1208 - val_loss: 130.8820\n",
      "Epoch 2285/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.5013 - val_loss: 136.5751\n",
      "Epoch 2286/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.5743 - val_loss: 115.8282\n",
      "Epoch 2287/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.8792 - val_loss: 144.0382\n",
      "Epoch 2288/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.7840 - val_loss: 116.6744\n",
      "Epoch 2289/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 162.5585 - val_loss: 122.9246\n",
      "Epoch 2290/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.6362 - val_loss: 147.7942\n",
      "Epoch 2291/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.2695 - val_loss: 119.3205\n",
      "Epoch 2292/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.7941 - val_loss: 116.1002\n",
      "Epoch 2293/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.6392 - val_loss: 162.0778\n",
      "Epoch 2294/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 162.5589 - val_loss: 322.7318\n",
      "Epoch 2295/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 167.5490 - val_loss: 122.6795\n",
      "Epoch 2296/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.0273 - val_loss: 134.3326\n",
      "Epoch 2297/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.6147 - val_loss: 148.7585\n",
      "Epoch 2298/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 164.8324 - val_loss: 195.3558\n",
      "Epoch 2299/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.6296 - val_loss: 123.4925\n",
      "Epoch 2300/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.4005 - val_loss: 120.4216\n",
      "Epoch 2301/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 157.8557 - val_loss: 169.0286\n",
      "Epoch 2302/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.3221 - val_loss: 119.1265\n",
      "Epoch 2303/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.0551 - val_loss: 117.8451\n",
      "Epoch 2304/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.8842 - val_loss: 160.7228\n",
      "Epoch 2305/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.3990 - val_loss: 123.0886\n",
      "Epoch 2306/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 147.1626 - val_loss: 170.3910\n",
      "Epoch 2307/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 173.8019 - val_loss: 122.4037\n",
      "Epoch 2308/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.5696 - val_loss: 130.8609\n",
      "Epoch 2309/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.2580 - val_loss: 120.7336\n",
      "Epoch 2310/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.1991 - val_loss: 124.5446\n",
      "Epoch 2311/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.0532 - val_loss: 141.4723\n",
      "Epoch 2312/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.9016 - val_loss: 135.0690\n",
      "Epoch 2313/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.3148 - val_loss: 117.7843\n",
      "Epoch 2314/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.3362 - val_loss: 120.9807\n",
      "Epoch 2315/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.0184 - val_loss: 132.3980\n",
      "Epoch 2316/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.7479 - val_loss: 150.3903\n",
      "Epoch 2317/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.0943 - val_loss: 123.9687\n",
      "Epoch 2318/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.5667 - val_loss: 134.9330\n",
      "Epoch 2319/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.1323 - val_loss: 163.3157\n",
      "Epoch 2320/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 164.1739 - val_loss: 150.3395\n",
      "Epoch 2321/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 187.5643 - val_loss: 135.9203\n",
      "Epoch 2322/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 150.9166 - val_loss: 121.1019\n",
      "Epoch 2323/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.6947 - val_loss: 156.2075\n",
      "Epoch 2324/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.0045 - val_loss: 115.5637\n",
      "Epoch 2325/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.4182 - val_loss: 129.7351\n",
      "Epoch 2326/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.0266 - val_loss: 114.1256\n",
      "Epoch 2327/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.2420 - val_loss: 136.2797\n",
      "Epoch 2328/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.3905 - val_loss: 127.4980\n",
      "Epoch 2329/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.4917 - val_loss: 147.8992\n",
      "Epoch 2330/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.6731 - val_loss: 124.8435\n",
      "Epoch 2331/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.3869 - val_loss: 117.3236\n",
      "Epoch 2332/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.6116 - val_loss: 137.9925\n",
      "Epoch 2333/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.5311 - val_loss: 202.4376\n",
      "Epoch 2334/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 178.8937 - val_loss: 145.0707\n",
      "Epoch 2335/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.4028 - val_loss: 153.3100\n",
      "Epoch 2336/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.1475 - val_loss: 179.1101\n",
      "Epoch 2337/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 163.5442 - val_loss: 231.8090\n",
      "Epoch 2338/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 144.6879 - val_loss: 145.8649\n",
      "Epoch 2339/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.9083 - val_loss: 163.6803\n",
      "Epoch 2340/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.1392 - val_loss: 138.9211\n",
      "Epoch 2341/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.7060 - val_loss: 143.1513\n",
      "Epoch 2342/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 148.924 - 1s 76us/step - loss: 148.9655 - val_loss: 130.7851\n",
      "Epoch 2343/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 166.2260 - val_loss: 160.1793\n",
      "Epoch 2344/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 157.0486 - val_loss: 132.2655\n",
      "Epoch 2345/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.9640 - val_loss: 130.3245\n",
      "Epoch 2346/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 66us/step - loss: 148.0424 - val_loss: 209.9798\n",
      "Epoch 2347/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.9881 - val_loss: 125.7003\n",
      "Epoch 2348/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.4286 - val_loss: 116.4017\n",
      "Epoch 2349/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.3348 - val_loss: 174.4380\n",
      "Epoch 2350/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 162.2767 - val_loss: 129.2694\n",
      "Epoch 2351/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 163.6083 - val_loss: 135.6393\n",
      "Epoch 2352/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.8588 - val_loss: 181.4385\n",
      "Epoch 2353/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 150.3474 - val_loss: 120.1889\n",
      "Epoch 2354/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.6798 - val_loss: 121.1355\n",
      "Epoch 2355/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 158.8809 - val_loss: 113.3587\n",
      "Epoch 2356/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.2035 - val_loss: 120.8615\n",
      "Epoch 2357/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.5074 - val_loss: 156.6617\n",
      "Epoch 2358/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.0624 - val_loss: 141.2978\n",
      "Epoch 2359/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.0229 - val_loss: 111.7486\n",
      "Epoch 2360/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.5822 - val_loss: 133.1788\n",
      "Epoch 2361/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.5651 - val_loss: 177.5474\n",
      "Epoch 2362/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 164.5756 - val_loss: 135.0724\n",
      "Epoch 2363/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.6121 - val_loss: 123.1636\n",
      "Epoch 2364/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.8437 - val_loss: 123.2999\n",
      "Epoch 2365/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.9636 - val_loss: 125.7791\n",
      "Epoch 2366/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 148.7362 - val_loss: 199.1124\n",
      "Epoch 2367/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.9422 - val_loss: 115.2165\n",
      "Epoch 2368/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.4001 - val_loss: 138.3675\n",
      "Epoch 2369/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.3942 - val_loss: 142.9514\n",
      "Epoch 2370/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.5943 - val_loss: 117.6429\n",
      "Epoch 2371/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.4984 - val_loss: 117.3554\n",
      "Epoch 2372/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.9351 - val_loss: 115.4111\n",
      "Epoch 2373/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.3625 - val_loss: 120.5507\n",
      "Epoch 2374/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.1181 - val_loss: 117.4314\n",
      "Epoch 2375/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 157.4101 - val_loss: 122.4011\n",
      "Epoch 2376/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.7543 - val_loss: 113.4668\n",
      "Epoch 2377/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 181.8920 - val_loss: 130.0664\n",
      "Epoch 2378/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.1862 - val_loss: 127.9762\n",
      "Epoch 2379/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 183.7489 - val_loss: 116.5367\n",
      "Epoch 2380/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.3885 - val_loss: 121.9371\n",
      "Epoch 2381/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 158.7581 - val_loss: 114.5747\n",
      "Epoch 2382/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.3981 - val_loss: 136.0886\n",
      "Epoch 2383/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.4479 - val_loss: 126.1925\n",
      "Epoch 2384/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.5592 - val_loss: 132.1958\n",
      "Epoch 2385/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.5558 - val_loss: 120.5526\n",
      "Epoch 2386/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.5074 - val_loss: 133.6824\n",
      "Epoch 2387/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.9320 - val_loss: 119.3087\n",
      "Epoch 2388/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.2127 - val_loss: 126.5512\n",
      "Epoch 2389/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.6113 - val_loss: 137.0663\n",
      "Epoch 2390/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.6982 - val_loss: 134.4196\n",
      "Epoch 2391/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 161.6904 - val_loss: 118.2807\n",
      "Epoch 2392/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.4622 - val_loss: 122.8547\n",
      "Epoch 2393/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.8070 - val_loss: 257.9093\n",
      "Epoch 2394/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.5599 - val_loss: 119.3441\n",
      "Epoch 2395/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 161.3946 - val_loss: 114.7618\n",
      "Epoch 2396/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 149.1924 - val_loss: 116.1927\n",
      "Epoch 2397/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.7892 - val_loss: 162.5367\n",
      "Epoch 2398/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.7577 - val_loss: 134.4903\n",
      "Epoch 2399/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.3406 - val_loss: 191.3557\n",
      "Epoch 2400/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.1521 - val_loss: 129.5175\n",
      "Epoch 2401/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 156.6603 - val_loss: 131.8340\n",
      "Epoch 2402/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 145.8320 - val_loss: 123.0586\n",
      "Epoch 2403/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 146.8730 - val_loss: 150.5417\n",
      "Epoch 2404/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.2100 - val_loss: 117.7691\n",
      "Epoch 2405/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 168.2771 - val_loss: 227.9259\n",
      "Epoch 2406/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 187.1606 - val_loss: 121.8520\n",
      "Epoch 2407/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.4588 - val_loss: 133.1829\n",
      "Epoch 2408/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.6532 - val_loss: 195.6118\n",
      "Epoch 2409/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.1144 - val_loss: 118.5964\n",
      "Epoch 2410/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 155.0468 - val_loss: 115.9578\n",
      "Epoch 2411/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.2854 - val_loss: 119.8084\n",
      "Epoch 2412/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.4987 - val_loss: 120.8835\n",
      "Epoch 2413/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.3808 - val_loss: 147.2944\n",
      "Epoch 2414/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.4694 - val_loss: 170.2194\n",
      "Epoch 2415/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.2367 - val_loss: 163.8156\n",
      "Epoch 2416/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 158.1160 - val_loss: 134.1966\n",
      "Epoch 2417/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.3340 - val_loss: 144.2207\n",
      "Epoch 2418/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.5056 - val_loss: 120.6433\n",
      "Epoch 2419/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.8853 - val_loss: 117.8783\n",
      "Epoch 2420/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 150.7628 - val_loss: 134.4491\n",
      "Epoch 2421/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.8921 - val_loss: 196.4586\n",
      "Epoch 2422/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.1956 - val_loss: 137.0667\n",
      "Epoch 2423/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 160.6552 - val_loss: 170.6292\n",
      "Epoch 2424/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.4955 - val_loss: 122.0482\n",
      "Epoch 2425/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.6704 - val_loss: 119.8539\n",
      "Epoch 2426/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.8641 - val_loss: 124.5912\n",
      "Epoch 2427/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.9145 - val_loss: 145.6723\n",
      "Epoch 2428/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.0392 - val_loss: 136.3889\n",
      "Epoch 2429/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.6138 - val_loss: 123.3865\n",
      "Epoch 2430/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.2259 - val_loss: 125.0189\n",
      "Epoch 2431/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.0042 - val_loss: 124.8274\n",
      "Epoch 2432/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.8807 - val_loss: 117.3921\n",
      "Epoch 2433/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 159.4909 - val_loss: 121.7787\n",
      "Epoch 2434/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 143.2717 - val_loss: 209.3838\n",
      "Epoch 2435/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.3757 - val_loss: 148.3803\n",
      "Epoch 2436/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.7443 - val_loss: 123.2416\n",
      "Epoch 2437/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.5724 - val_loss: 134.4224\n",
      "Epoch 2438/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.0076 - val_loss: 120.6861\n",
      "Epoch 2439/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 152.5209 - val_loss: 232.2796\n",
      "Epoch 2440/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.4689 - val_loss: 150.5310\n",
      "Epoch 2441/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.6590 - val_loss: 154.2338\n",
      "Epoch 2442/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.5632 - val_loss: 116.7188\n",
      "Epoch 2443/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.7843 - val_loss: 143.7129\n",
      "Epoch 2444/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.2090 - val_loss: 129.4118\n",
      "Epoch 2445/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.6105 - val_loss: 145.5835\n",
      "Epoch 2446/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.9178 - val_loss: 133.2460\n",
      "Epoch 2447/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.0753 - val_loss: 166.2658\n",
      "Epoch 2448/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 156.4739 - val_loss: 136.5544\n",
      "Epoch 2449/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.4064 - val_loss: 136.6576\n",
      "Epoch 2450/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.7791 - val_loss: 119.0336\n",
      "Epoch 2451/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 142.5050 - val_loss: 116.4684\n",
      "Epoch 2452/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.4268 - val_loss: 122.3959\n",
      "Epoch 2453/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.2858 - val_loss: 120.5959\n",
      "Epoch 2454/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.0359 - val_loss: 160.2725\n",
      "Epoch 2455/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.7843 - val_loss: 127.1903\n",
      "Epoch 2456/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.8358 - val_loss: 129.3873\n",
      "Epoch 2457/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 189.2096 - val_loss: 116.8407\n",
      "Epoch 2458/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 162.3440 - val_loss: 122.1846\n",
      "Epoch 2459/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.8059 - val_loss: 139.2478\n",
      "Epoch 2460/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 151.4016 - val_loss: 146.8160\n",
      "Epoch 2461/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 142.9183 - val_loss: 118.2391\n",
      "Epoch 2462/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 147.1504 - val_loss: 146.8865\n",
      "Epoch 2463/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.8694 - val_loss: 131.1641\n",
      "Epoch 2464/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.3345 - val_loss: 118.3436\n",
      "Epoch 2465/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.7555 - val_loss: 128.6707\n",
      "Epoch 2466/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 182.0740 - val_loss: 129.7574\n",
      "Epoch 2467/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.4436 - val_loss: 121.2314\n",
      "Epoch 2468/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.8581 - val_loss: 127.0605\n",
      "Epoch 2469/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.0708 - val_loss: 153.4518\n",
      "Epoch 2470/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.4202 - val_loss: 139.9877\n",
      "Epoch 2471/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.6278 - val_loss: 117.5917\n",
      "Epoch 2472/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.0134 - val_loss: 114.7131\n",
      "Epoch 2473/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.0303 - val_loss: 126.3395\n",
      "Epoch 2474/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.9274 - val_loss: 229.9868\n",
      "Epoch 2475/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.5757 - val_loss: 142.6097\n",
      "Epoch 2476/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.3028 - val_loss: 117.4259\n",
      "Epoch 2477/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 163.5721 - val_loss: 119.2220\n",
      "Epoch 2478/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 174.2088 - val_loss: 135.5891\n",
      "Epoch 2479/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.5758 - val_loss: 161.5776\n",
      "Epoch 2480/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.0181 - val_loss: 135.6160\n",
      "Epoch 2481/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.4637 - val_loss: 121.4107\n",
      "Epoch 2482/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.4342 - val_loss: 132.2854\n",
      "Epoch 2483/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.3345 - val_loss: 121.9838\n",
      "Epoch 2484/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 164.5992 - val_loss: 148.2773\n",
      "Epoch 2485/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.5617 - val_loss: 196.0119\n",
      "Epoch 2486/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.9552 - val_loss: 120.3168\n",
      "Epoch 2487/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 166.1424 - val_loss: 121.6834\n",
      "Epoch 2488/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.6577 - val_loss: 144.9626\n",
      "Epoch 2489/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.3561 - val_loss: 113.4913\n",
      "Epoch 2490/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.6654 - val_loss: 187.4869\n",
      "Epoch 2491/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 167.1936 - val_loss: 133.9561\n",
      "Epoch 2492/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.6847 - val_loss: 151.0364\n",
      "Epoch 2493/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 149.1756 - val_loss: 126.9353\n",
      "Epoch 2494/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.8978 - val_loss: 121.3821\n",
      "Epoch 2495/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.0031 - val_loss: 126.1512\n",
      "Epoch 2496/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.6675 - val_loss: 125.6279\n",
      "Epoch 2497/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 166.4013 - val_loss: 155.6370\n",
      "Epoch 2498/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.5463 - val_loss: 121.1932\n",
      "Epoch 2499/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.1647 - val_loss: 117.1696\n",
      "Epoch 2500/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.3861 - val_loss: 127.3586\n",
      "Epoch 2501/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.7172 - val_loss: 132.3075\n",
      "Epoch 2502/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.9785 - val_loss: 126.5836\n",
      "Epoch 2503/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 157.0529 - val_loss: 119.7022\n",
      "Epoch 2504/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.6695 - val_loss: 118.0139\n",
      "Epoch 2505/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.1035 - val_loss: 193.5084\n",
      "Epoch 2506/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 174.3341 - val_loss: 133.5631\n",
      "Epoch 2507/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.1573 - val_loss: 178.1343\n",
      "Epoch 2508/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.4950 - val_loss: 116.9663\n",
      "Epoch 2509/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 157.6824 - val_loss: 115.8748\n",
      "Epoch 2510/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.9296 - val_loss: 155.6447\n",
      "Epoch 2511/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.9850 - val_loss: 120.9396\n",
      "Epoch 2512/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 149.3096 - val_loss: 137.7773\n",
      "Epoch 2513/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.3601 - val_loss: 120.2372\n",
      "Epoch 2514/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.8158 - val_loss: 111.3284\n",
      "Epoch 2515/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.8543 - val_loss: 121.7445\n",
      "Epoch 2516/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.8884 - val_loss: 161.6761\n",
      "Epoch 2517/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.4721 - val_loss: 117.7237\n",
      "Epoch 2518/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.9050 - val_loss: 115.3840\n",
      "Epoch 2519/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 148.7721 - val_loss: 150.6884\n",
      "Epoch 2520/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 155.7982 - val_loss: 124.3897\n",
      "Epoch 2521/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 152.5760 - val_loss: 116.2510\n",
      "Epoch 2522/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.7320 - val_loss: 114.1699\n",
      "Epoch 2523/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.1657 - val_loss: 130.7863\n",
      "Epoch 2524/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.9967 - val_loss: 180.0937\n",
      "Epoch 2525/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 151.4794 - val_loss: 169.0813\n",
      "Epoch 2526/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.6174 - val_loss: 125.4498\n",
      "Epoch 2527/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.0509 - val_loss: 124.2664\n",
      "Epoch 2528/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.9295 - val_loss: 138.7947\n",
      "Epoch 2529/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.7125 - val_loss: 162.3182\n",
      "Epoch 2530/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.2461 - val_loss: 150.2881\n",
      "Epoch 2531/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.2905 - val_loss: 123.0020\n",
      "Epoch 2532/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.4387 - val_loss: 134.9134\n",
      "Epoch 2533/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 175.2984 - val_loss: 207.4569\n",
      "Epoch 2534/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 168.0647 - val_loss: 201.4141\n",
      "Epoch 2535/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 181.8789 - val_loss: 122.1824\n",
      "Epoch 2536/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.0327 - val_loss: 115.0091\n",
      "Epoch 2537/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 156.4722 - val_loss: 128.8956\n",
      "Epoch 2538/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 171.7916 - val_loss: 115.4296\n",
      "Epoch 2539/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.9825 - val_loss: 126.8643\n",
      "Epoch 2540/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.1330 - val_loss: 159.2427\n",
      "Epoch 2541/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.3614 - val_loss: 118.3782\n",
      "Epoch 2542/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.3631 - val_loss: 124.0737\n",
      "Epoch 2543/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 150.0530 - val_loss: 142.7761\n",
      "Epoch 2544/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.5223 - val_loss: 120.0757\n",
      "Epoch 2545/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.7295 - val_loss: 117.6137\n",
      "Epoch 2546/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 177.9433 - val_loss: 118.1355\n",
      "Epoch 2547/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.8384 - val_loss: 119.2115\n",
      "Epoch 2548/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.7977 - val_loss: 161.1769\n",
      "Epoch 2549/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 153.9884 - val_loss: 125.2398\n",
      "Epoch 2550/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.7476 - val_loss: 119.3063\n",
      "Epoch 2551/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.9524 - val_loss: 170.2015\n",
      "Epoch 2552/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.1544 - val_loss: 114.0299\n",
      "Epoch 2553/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.7872 - val_loss: 116.3894\n",
      "Epoch 2554/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.8642 - val_loss: 162.3209\n",
      "Epoch 2555/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.9683 - val_loss: 114.1952\n",
      "Epoch 2556/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 155.3306 - val_loss: 120.9058\n",
      "Epoch 2557/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.9508 - val_loss: 115.4175\n",
      "Epoch 2558/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.2905 - val_loss: 157.2643\n",
      "Epoch 2559/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.0697 - val_loss: 124.9545\n",
      "Epoch 2560/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.7912 - val_loss: 117.5096\n",
      "Epoch 2561/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.5892 - val_loss: 119.7001\n",
      "Epoch 2562/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.5311 - val_loss: 119.6362\n",
      "Epoch 2563/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.3814 - val_loss: 129.7720\n",
      "Epoch 2564/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.4372 - val_loss: 129.0127\n",
      "Epoch 2565/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 154.3660 - val_loss: 114.2160\n",
      "Epoch 2566/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.4361 - val_loss: 150.3228\n",
      "Epoch 2567/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.9340 - val_loss: 180.5038\n",
      "Epoch 2568/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.0751 - val_loss: 114.2947\n",
      "Epoch 2569/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.6173 - val_loss: 192.3516\n",
      "Epoch 2570/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 148.7203 - val_loss: 122.8294\n",
      "Epoch 2571/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 158.1981 - val_loss: 127.6103\n",
      "Epoch 2572/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.9228 - val_loss: 127.8953\n",
      "Epoch 2573/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.0016 - val_loss: 151.0537\n",
      "Epoch 2574/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.2079 - val_loss: 152.5787\n",
      "Epoch 2575/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 158.3030 - val_loss: 134.2366\n",
      "Epoch 2576/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.5102 - val_loss: 117.9396\n",
      "Epoch 2577/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 174.5553 - val_loss: 114.5331\n",
      "Epoch 2578/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 142.8313 - val_loss: 147.5715\n",
      "Epoch 2579/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 144.1873 - val_loss: 124.1310\n",
      "Epoch 2580/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 162.4047 - val_loss: 123.3121\n",
      "Epoch 2581/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.3723 - val_loss: 195.0568\n",
      "Epoch 2582/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.8137 - val_loss: 118.6214\n",
      "Epoch 2583/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.9131 - val_loss: 171.9504\n",
      "Epoch 2584/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 173.5680 - val_loss: 128.4118\n",
      "Epoch 2585/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 190.4863 - val_loss: 197.2878\n",
      "Epoch 2586/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.9981 - val_loss: 235.2395\n",
      "Epoch 2587/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.0449 - val_loss: 125.4168\n",
      "Epoch 2588/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.6655 - val_loss: 147.0942\n",
      "Epoch 2589/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.0640 - val_loss: 123.9373\n",
      "Epoch 2590/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.7672 - val_loss: 124.1398\n",
      "Epoch 2591/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.0653 - val_loss: 114.1278\n",
      "Epoch 2592/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.2140 - val_loss: 137.1473\n",
      "Epoch 2593/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.7996 - val_loss: 117.1391\n",
      "Epoch 2594/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.1081 - val_loss: 118.1932\n",
      "Epoch 2595/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.5830 - val_loss: 149.4213\n",
      "Epoch 2596/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.5457 - val_loss: 176.8740\n",
      "Epoch 2597/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.8957 - val_loss: 138.5453\n",
      "Epoch 2598/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.9866 - val_loss: 121.9365\n",
      "Epoch 2599/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.8506 - val_loss: 175.5865\n",
      "Epoch 2600/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.7839 - val_loss: 114.8594\n",
      "Epoch 2601/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.5199 - val_loss: 133.8166\n",
      "Epoch 2602/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.2745 - val_loss: 165.6915\n",
      "Epoch 2603/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.2729 - val_loss: 116.6688\n",
      "Epoch 2604/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.4163 - val_loss: 124.5164\n",
      "Epoch 2605/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.4476 - val_loss: 154.8199\n",
      "Epoch 2606/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.9655 - val_loss: 141.3535\n",
      "Epoch 2607/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.3722 - val_loss: 113.3484\n",
      "Epoch 2608/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.2469 - val_loss: 114.0451\n",
      "Epoch 2609/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.5923 - val_loss: 120.9221\n",
      "Epoch 2610/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.8137 - val_loss: 119.8278\n",
      "Epoch 2611/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.7352 - val_loss: 130.8072\n",
      "Epoch 2612/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.9602 - val_loss: 202.7719\n",
      "Epoch 2613/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.4156 - val_loss: 150.1632\n",
      "Epoch 2614/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.4900 - val_loss: 121.3401\n",
      "Epoch 2615/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.7880 - val_loss: 120.8864\n",
      "Epoch 2616/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.2936 - val_loss: 123.5635\n",
      "Epoch 2617/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 149.9305 - val_loss: 118.8956\n",
      "Epoch 2618/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.8735 - val_loss: 128.6003\n",
      "Epoch 2619/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 163.6818 - val_loss: 135.0600\n",
      "Epoch 2620/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.4563 - val_loss: 155.3240\n",
      "Epoch 2621/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.7565 - val_loss: 126.2898\n",
      "Epoch 2622/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.0455 - val_loss: 115.4396\n",
      "Epoch 2623/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.7089 - val_loss: 116.6463\n",
      "Epoch 2624/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.2883 - val_loss: 120.7074\n",
      "Epoch 2625/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.4590 - val_loss: 131.3645\n",
      "Epoch 2626/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.8977 - val_loss: 124.2709\n",
      "Epoch 2627/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.7981 - val_loss: 115.4634\n",
      "Epoch 2628/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.1641 - val_loss: 119.3166\n",
      "Epoch 2629/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.5765 - val_loss: 114.4398\n",
      "Epoch 2630/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.3991 - val_loss: 143.7147\n",
      "Epoch 2631/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.3092 - val_loss: 120.0815\n",
      "Epoch 2632/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.4738 - val_loss: 168.9774\n",
      "Epoch 2633/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.3065 - val_loss: 154.1922\n",
      "Epoch 2634/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.5554 - val_loss: 190.7262\n",
      "Epoch 2635/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.6330 - val_loss: 128.5937\n",
      "Epoch 2636/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 145.115 - 1s 63us/step - loss: 147.8099 - val_loss: 120.7694\n",
      "Epoch 2637/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 138.9694 - val_loss: 132.3393\n",
      "Epoch 2638/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 161.8296 - val_loss: 149.4026\n",
      "Epoch 2639/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 153.0523 - val_loss: 151.4276\n",
      "Epoch 2640/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.0833 - val_loss: 119.9897\n",
      "Epoch 2641/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.4757 - val_loss: 115.9359\n",
      "Epoch 2642/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.3380 - val_loss: 149.9505\n",
      "Epoch 2643/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 168.2242 - val_loss: 136.3492\n",
      "Epoch 2644/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.8730 - val_loss: 134.6170\n",
      "Epoch 2645/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.1284 - val_loss: 139.2312\n",
      "Epoch 2646/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.1276 - val_loss: 115.2564\n",
      "Epoch 2647/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.7993 - val_loss: 174.6351\n",
      "Epoch 2648/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 157.6374 - val_loss: 122.7137\n",
      "Epoch 2649/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.8391 - val_loss: 119.7790\n",
      "Epoch 2650/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.7336 - val_loss: 148.2791\n",
      "Epoch 2651/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.0510 - val_loss: 115.6532\n",
      "Epoch 2652/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.3410 - val_loss: 123.3545\n",
      "Epoch 2653/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.7458 - val_loss: 139.0709\n",
      "Epoch 2654/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 196.3956 - val_loss: 239.2115\n",
      "Epoch 2655/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.1766 - val_loss: 119.5689\n",
      "Epoch 2656/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.3439 - val_loss: 118.7642\n",
      "Epoch 2657/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.1536 - val_loss: 136.2660\n",
      "Epoch 2658/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 159.1231 - val_loss: 121.5213\n",
      "Epoch 2659/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 149.7367 - val_loss: 119.1061\n",
      "Epoch 2660/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.1435 - val_loss: 206.0587\n",
      "Epoch 2661/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 153.0883 - val_loss: 162.3559\n",
      "Epoch 2662/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.4871 - val_loss: 167.2722\n",
      "Epoch 2663/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.9493 - val_loss: 154.5318\n",
      "Epoch 2664/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.7748 - val_loss: 176.6791\n",
      "Epoch 2665/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.8892 - val_loss: 127.8533\n",
      "Epoch 2666/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.8207 - val_loss: 120.7381\n",
      "Epoch 2667/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.2761 - val_loss: 120.0463\n",
      "Epoch 2668/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.9810 - val_loss: 148.7217\n",
      "Epoch 2669/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 143.8619 - val_loss: 119.7377\n",
      "Epoch 2670/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 147.5202 - val_loss: 116.1600\n",
      "Epoch 2671/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.7288 - val_loss: 127.0703\n",
      "Epoch 2672/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 148.7984 - val_loss: 114.9040\n",
      "Epoch 2673/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 142.1178 - val_loss: 148.5077\n",
      "Epoch 2674/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.8044 - val_loss: 122.8365\n",
      "Epoch 2675/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 165.1241 - val_loss: 157.3020\n",
      "Epoch 2676/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 171.9099 - val_loss: 172.5479\n",
      "Epoch 2677/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.5785 - val_loss: 122.6396\n",
      "Epoch 2678/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.8794 - val_loss: 189.5208\n",
      "Epoch 2679/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.6027 - val_loss: 126.7973\n",
      "Epoch 2680/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.1702 - val_loss: 115.6190\n",
      "Epoch 2681/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.6906 - val_loss: 278.9700\n",
      "Epoch 2682/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.1228 - val_loss: 172.0122\n",
      "Epoch 2683/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.5755 - val_loss: 145.0907\n",
      "Epoch 2684/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.9660 - val_loss: 141.4187\n",
      "Epoch 2685/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 151.4125 - val_loss: 160.3499\n",
      "Epoch 2686/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.6427 - val_loss: 127.7375\n",
      "Epoch 2687/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.0741 - val_loss: 125.6518\n",
      "Epoch 2688/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.5420 - val_loss: 131.8918\n",
      "Epoch 2689/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.8948 - val_loss: 115.3805\n",
      "Epoch 2690/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.3935 - val_loss: 117.3133\n",
      "Epoch 2691/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.7557 - val_loss: 134.6125\n",
      "Epoch 2692/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.7687 - val_loss: 126.4336\n",
      "Epoch 2693/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.7183 - val_loss: 113.8074\n",
      "Epoch 2694/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.1042 - val_loss: 150.0782\n",
      "Epoch 2695/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 145.1589 - val_loss: 119.4053\n",
      "Epoch 2696/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 163.6659 - val_loss: 112.9853\n",
      "Epoch 2697/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 149.9034 - val_loss: 192.2062\n",
      "Epoch 2698/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 142.2147 - val_loss: 128.9640\n",
      "Epoch 2699/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.6891 - val_loss: 114.7559\n",
      "Epoch 2700/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 138.6432 - val_loss: 116.4435\n",
      "Epoch 2701/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.8724 - val_loss: 121.6203\n",
      "Epoch 02701: early stopping\n",
      "Fold score (RMSE): 10.789312362670898\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=filename_checkpoint, verbose=0, save_best_only=True)\n",
    "\n",
    "# Turn off KFold\n",
    "#if (0):\n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "    \n",
    "for train, test in kf.split(x):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "    model.add(Dropout(0.01)) # Dropout Layer\n",
    "    model.add(Dense(50, activation='relu')) # Hidden 2\n",
    "    model.add(Dense(25, \n",
    "                kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01),activation='relu')) # Hidden 3 w/regularization\n",
    "    model.add(Dense(10, activation='relu')) # Hidden 4\n",
    "    model.add(Dense(1)) # Output\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=1000, verbose=1, mode='auto')\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpoint],verbose=1,epochs=10000)\n",
    "    \n",
    "    pred = model.predict(x_test)\n",
    "    \n",
    "    oos_y.append(y_test)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure this fold's RMSE\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "    print(\"Fold score (RMSE): {}\".format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final, out of sample score (RMSE): 12.342395782470703\n"
     ]
    }
   ],
   "source": [
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))\n",
    "print(\"Final, out of sample score (RMSE): {}\".format(score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#pred = model.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "#score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "#print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "#chart_regression(pred.flatten(),y_test)\n",
    "#chart_regression(pred.flatten(),y_test,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(filename_checkpoint)\n",
    "\n",
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "extract_and_encode_features(df_test)\n",
    "\n",
    "ids_test = df_test['id']\n",
    "df_test.drop('id',1,inplace=True)\n",
    "\n",
    "names_test = df_test['name']\n",
    "df_test.drop('name',1,inplace=True)\n",
    "\n",
    "x_submit = df_test.as_matrix().astype(np.float32)\n",
    "pred_submit = model.predict(x_submit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = [n if n > 0 else n * -1 for n in pred_submit[:,0]]\n",
    "df_submit = pd.DataFrame({'id': ids_test,'cost': cost})\n",
    "df_submit = df_submit[['id', 'cost']]\n",
    "df_submit.to_csv(filename_submit, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
