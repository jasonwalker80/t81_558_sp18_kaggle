{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n",
    "\n",
    "**Kaggle Assignment: **\n",
    "\n",
    "**Student Name: Jason Walker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## Description\n",
    "This is one of the projects from the course T81-855: Applications of Deep Learning at Washington University in St. Louis. All students must create a Kaggle account and submit a solution. Once you have submitted your solution entry log into Blackboard (at WUSTL) and submit a single file telling me your Kaggle name on the leaderboard (you do not need to register to Kaggle with your real name). This competition will be visible to the public, so there may be non-student submissions as well as student.\n",
    "\n",
    "The data set for this competition consists of a number of input columns that should be used to predict a stores sales. This is a regression problem. The inputs are a mixture of discrete and category values. The data set is from a simulation.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The evaluation pages describes how submissions will be scored and how students should format their submissions. The scores are in RMSE.\n",
    "Submission Format\n",
    "\n",
    "For every store in the dataset, submission files should contain a sales volume.\n",
    "\n",
    "The file should contain a header and have the following format:\n",
    "\n",
    "```\n",
    "100000,1.23\n",
    "100001,1.123\n",
    "100002,3.332\n",
    "100003,1.53\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The data contains data and costs for various office supplies. The data came from a simulation and do not directly correspond to any real-world items. See how well you can predict the cost of an item using the provided data. Feature engineering will likely help you. The *name* column may seem useless at first glance; however, it contains information that you can parse to help your predictions.\n",
    "File descriptions\n",
    "```\n",
    "    id - The identifier/primary key.\n",
    "    name - The name of this item.\n",
    "    manufacturer - The manufacturer.\n",
    "    pack - The number of items in this pack.\n",
    "    weight - The weight of a pack of these items.\n",
    "    height - The height of a pack of these items.\n",
    "    width - The width of a pack of these items.\n",
    "    length - The length of a pack of these items.\n",
    "    cost - The cost for this item pack. This is what you are to predict (the target). \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions\n",
    "\n",
    "You will see these at the top of every module and assignment.  These are simply a set of reusable functions that we will make use of.  Each of them will be explained as the semester progresses.  They are explained in greater detail as the course progresses.  Class 4 contains a complete overview of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n",
    "        \n",
    "# This function submits an assignment.  You can submit an assignment as much as you like, only the final\n",
    "# submission counts.  The paramaters are as follows:\n",
    "# data - Pandas dataframe output.\n",
    "# key - Your student key that was emailed to you.\n",
    "# no - The assignment class number, should be 1 through 1.\n",
    "# source_file - The full path to your Python or IPYNB file.  This must have \"_class1\" as part of its name.  \n",
    "# .             The number must match your assignment number.  For example \"_class2\" for class assignment #2.\n",
    "def submit(data,key,no,source_file=None):\n",
    "    if source_file is None and '__file__' not in globals(): raise Exception('Must specify a filename when a Jupyter notebook.')\n",
    "    if source_file is None: source_file = __file__\n",
    "    suffix = '_class{}'.format(no)\n",
    "    if suffix not in source_file: raise Exception('{} must be part of the filename.'.format(suffix))\n",
    "    with open(source_file, \"rb\") as image_file:\n",
    "        encoded_python = base64.b64encode(image_file.read()).decode('ascii')\n",
    "    ext = os.path.splitext(source_file)[-1].lower()\n",
    "    if ext not in ['.ipynb','.py']: raise Exception(\"Source file is {} must be .py or .ipynb\".format(ext))\n",
    "    r = requests.post(\"https://api.heatonresearch.com/assignment-submit\",\n",
    "        headers={'x-api-key':key}, json={'csv':base64.b64encode(data.to_csv(index=False).encode('ascii')).decode(\"ascii\"),\n",
    "        'assignment': no, 'ext':ext, 'py':encoded_python})\n",
    "    if r.status_code == 200:\n",
    "        print(\"Success: {}\".format(r.text))\n",
    "    else: print(\"Failure: {}\".format(r.text))\n",
    "        \n",
    "        \n",
    "def perturbation_rank(model, x, y, names, regression):\n",
    "    errors = []\n",
    "\n",
    "    for i in range(x.shape[1]):\n",
    "        hold = np.array(x[:, i])\n",
    "        np.random.shuffle(x[:, i])\n",
    "        \n",
    "        if regression:\n",
    "            pred = model.predict(x)\n",
    "            error = metrics.mean_squared_error(y, pred)\n",
    "        else:\n",
    "            pred = model.predict_proba(x)\n",
    "            error = metrics.log_loss(y, pred)\n",
    "            \n",
    "        errors.append(error)\n",
    "        x[:, i] = hold\n",
    "        \n",
    "    max_error = np.max(errors)\n",
    "    importance = [e/max_error for e in errors]\n",
    "\n",
    "    data = {'name':names,'error':errors,'importance':importance}\n",
    "    result = pd.DataFrame(data, columns = ['name','error','importance'])\n",
    "    result.sort_values(by=['importance'], ascending=[0], inplace=True)\n",
    "    result.reset_index(inplace=True, drop=True)\n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kaggle Code\n",
    "\n",
    "## Load Data and Encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwalker/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers\n",
    "\n",
    "path = './data'\n",
    "\n",
    "filename_test = os.path.join(path,\"test.csv\")\n",
    "filename_train = os.path.join(path,\"train.csv\")\n",
    "filename_sample = os.path.join(path,\"sample.csv\")\n",
    "filename_submit = os.path.join(path,\"submit.csv\")\n",
    "filename_checkpoint = os.path.join(path,\"checkpoint.hdf5\")\n",
    "\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "\n",
    "np.random.seed(42) # Uncomment this line to get the same shuffle each time\n",
    "df_train = df_train.reindex(np.random.permutation(df_train.index))\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Encode Features\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def extract_and_encode_features(df):\n",
    "    color_regex='(?P<color>red|blue|green|yellow|orange|pink|black|brown|white)'\n",
    "    df['color'] = df.name.str.extract(color_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    quality_regex='(?P<quality>generic|high\\squality)'\n",
    "    df['quality'] = df.name.str.extract(quality_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    size_regex='(?P<size>tiny|small|medium|large)'\n",
    "    df['size'] = df.name.str.extract(size_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    item_regex='(?P<item>paperclips|paperweights|ink\\spens|pencils|stapler|tablets|thumbtacks|post\\sit\\snotes)'\n",
    "    df['item'] = df.name.str.extract(item_regex, flags=re.IGNORECASE, expand=False)\n",
    "    \n",
    "    for column in ['pack','weight','height','width','length']:\n",
    "        missing_median(df,column)\n",
    "    \n",
    "    df.insert(1,'surface_area',(df['height']*df['width']*df['length']).astype(np.float32))\n",
    "    \n",
    "    for column in ['height','width','length']:\n",
    "        df.drop(column,1,inplace=True)\n",
    "    \n",
    "    ## encode numeric features\n",
    "    #for column in ['pack','weight','surface_area']:\n",
    "    #    encode_numeric_zscore(df,column)\n",
    "\n",
    "    # encode text/categorical features\n",
    "    for column in ['manufacturer','color','quality','size','item']:\n",
    "        encode_text_dummy(df,column)\n",
    "  \n",
    "extract_and_encode_features(df_train)\n",
    "\n",
    "ids_train = df_train['id']\n",
    "df_train.drop('id',1,inplace=True)\n",
    "\n",
    "names_train = df_train['name']\n",
    "df_train.drop('name',1,inplace=True)\n",
    "\n",
    "x,y = to_xy(df_train,'cost')\n",
    "\n",
    "# Used before KFold\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression (Coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 32.62861251831055\n",
      "['surface_area', 'pack', 'weight', 'manufacturer-6% Solution', 'manufacturer-Deep Office Supplies', 'manufacturer-Duck Lake', 'manufacturer-Offices-R-Us', 'manufacturer-WizBang', 'color-Black', 'color-Blue', 'color-Brown', 'color-Green', 'color-Pink', 'color-Red', 'color-White', 'quality-Generic', 'quality-High Quality', 'size-Large', 'size-Medium', 'size-Small', 'size-Tiny', 'item-Ink Pens', 'item-Paperclips', 'item-Paperweights', 'item-Pencils', 'item-Post It Notes', 'item-Stapler', 'item-Tablets', 'item-Thumbtacks']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item-Pencils</th>\n",
       "      <td>-40.993912</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Thumbtacks</th>\n",
       "      <td>-37.697784</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Red</th>\n",
       "      <td>-37.003139</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Green</th>\n",
       "      <td>-24.856337</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperweights</th>\n",
       "      <td>-24.808296</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Post It Notes</th>\n",
       "      <td>-20.775148</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperclips</th>\n",
       "      <td>-14.083073</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Large</th>\n",
       "      <td>-12.790818</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Blue</th>\n",
       "      <td>-10.408613</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Ink Pens</th>\n",
       "      <td>-7.070616</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Generic</th>\n",
       "      <td>-3.121280</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Medium</th>\n",
       "      <td>-2.668133</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Deep Office Supplies</th>\n",
       "      <td>-1.170921</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Offices-R-Us</th>\n",
       "      <td>-0.313105</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-6% Solution</th>\n",
       "      <td>-0.140253</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack</th>\n",
       "      <td>0.026290</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>0.034018</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Brown</th>\n",
       "      <td>0.198042</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surface_area</th>\n",
       "      <td>0.471036</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-WizBang</th>\n",
       "      <td>0.662018</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Duck Lake</th>\n",
       "      <td>0.962900</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Stapler</th>\n",
       "      <td>1.517555</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Small</th>\n",
       "      <td>6.224488</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-High Quality</th>\n",
       "      <td>9.074377</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Tiny</th>\n",
       "      <td>14.480067</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Black</th>\n",
       "      <td>14.724663</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-White</th>\n",
       "      <td>28.208469</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Pink</th>\n",
       "      <td>38.893696</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Tablets</th>\n",
       "      <td>143.909576</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         coef  positive\n",
       "item-Pencils                       -40.993912     False\n",
       "item-Thumbtacks                    -37.697784     False\n",
       "color-Red                          -37.003139     False\n",
       "color-Green                        -24.856337     False\n",
       "item-Paperweights                  -24.808296     False\n",
       "item-Post It Notes                 -20.775148     False\n",
       "item-Paperclips                    -14.083073     False\n",
       "size-Large                         -12.790818     False\n",
       "color-Blue                         -10.408613     False\n",
       "item-Ink Pens                       -7.070616     False\n",
       "quality-Generic                     -3.121280     False\n",
       "size-Medium                         -2.668133     False\n",
       "manufacturer-Deep Office Supplies   -1.170921     False\n",
       "manufacturer-Offices-R-Us           -0.313105     False\n",
       "manufacturer-6% Solution            -0.140253     False\n",
       "pack                                 0.026290      True\n",
       "weight                               0.034018      True\n",
       "color-Brown                          0.198042      True\n",
       "surface_area                         0.471036      True\n",
       "manufacturer-WizBang                 0.662018      True\n",
       "manufacturer-Duck Lake               0.962900      True\n",
       "item-Stapler                         1.517555      True\n",
       "size-Small                           6.224488      True\n",
       "quality-High Quality                 9.074377      True\n",
       "size-Tiny                           14.480067      True\n",
       "color-Black                         14.724663      True\n",
       "color-White                         28.208469      True\n",
       "color-Pink                          38.893696      True\n",
       "item-Tablets                       143.909576      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [ 23.33177185]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhgAAAD8CAYAAAA1zt2tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1080a9400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple function to evaluate the coefficients of a regression\n",
    "%matplotlib inline    \n",
    "from IPython.display import display, HTML    \n",
    "\n",
    "def report_coef(names,coef,intercept):\n",
    "    r = pd.DataFrame( { 'coef': coef, 'positive': coef>=0  }, index = names )\n",
    "    r = r.sort_values(by=['coef'])\n",
    "    display(r)\n",
    "    print(\"Intercept: {}\".format(intercept))\n",
    "    r['coef'].plot(kind='barh', color=r['positive'].map({True: 'b', False: 'r'}))\n",
    "    \n",
    "# Create linear regression\n",
    "regressor = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# Fit/train linear regression\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "print(names)\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_[0,:],\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization (Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 32.604698181152344\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>color-Red</th>\n",
       "      <td>-38.013115</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Pencils</th>\n",
       "      <td>-26.922997</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Green</th>\n",
       "      <td>-25.852888</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Thumbtacks</th>\n",
       "      <td>-22.862341</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Large</th>\n",
       "      <td>-12.912994</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Blue</th>\n",
       "      <td>-11.693470</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperweights</th>\n",
       "      <td>-10.087886</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Post It Notes</th>\n",
       "      <td>-6.140296</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Generic</th>\n",
       "      <td>-4.635633</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Medium</th>\n",
       "      <td>-2.939024</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Offices-R-Us</th>\n",
       "      <td>-1.074063</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Deep Office Supplies</th>\n",
       "      <td>-0.895616</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Brown</th>\n",
       "      <td>-0.250410</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Duck Lake</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperclips</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-6% Solution</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack</th>\n",
       "      <td>0.026343</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>0.034064</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surface_area</th>\n",
       "      <td>0.473913</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-WizBang</th>\n",
       "      <td>1.121873</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Small</th>\n",
       "      <td>6.089336</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Ink Pens</th>\n",
       "      <td>7.178909</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-High Quality</th>\n",
       "      <td>7.511516</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Black</th>\n",
       "      <td>13.975151</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Tiny</th>\n",
       "      <td>14.368844</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Stapler</th>\n",
       "      <td>15.797664</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-White</th>\n",
       "      <td>26.880367</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Pink</th>\n",
       "      <td>38.646488</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Tablets</th>\n",
       "      <td>157.879654</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         coef  positive\n",
       "color-Red                          -38.013115     False\n",
       "item-Pencils                       -26.922997     False\n",
       "color-Green                        -25.852888     False\n",
       "item-Thumbtacks                    -22.862341     False\n",
       "size-Large                         -12.912994     False\n",
       "color-Blue                         -11.693470     False\n",
       "item-Paperweights                  -10.087886     False\n",
       "item-Post It Notes                  -6.140296     False\n",
       "quality-Generic                     -4.635633     False\n",
       "size-Medium                         -2.939024     False\n",
       "manufacturer-Offices-R-Us           -1.074063     False\n",
       "manufacturer-Deep Office Supplies   -0.895616     False\n",
       "color-Brown                         -0.250410     False\n",
       "manufacturer-Duck Lake               0.000000      True\n",
       "item-Paperclips                      0.000000      True\n",
       "manufacturer-6% Solution             0.000000      True\n",
       "pack                                 0.026343      True\n",
       "weight                               0.034064      True\n",
       "surface_area                         0.473913      True\n",
       "manufacturer-WizBang                 1.121873      True\n",
       "size-Small                           6.089336      True\n",
       "item-Ink Pens                        7.178909      True\n",
       "quality-High Quality                 7.511516      True\n",
       "color-Black                         13.975151      True\n",
       "size-Tiny                           14.368844      True\n",
       "item-Stapler                        15.797664      True\n",
       "color-White                         26.880367      True\n",
       "color-Pink                          38.646488      True\n",
       "item-Tablets                       157.879654      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [ 11.28668213]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAD8CAYAAADExYYgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZmZmLeXBhZmZmbXU/wNqesUVSIiofQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f1fd358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Create linear regression\n",
    "regressor = Lasso(random_state=0,alpha=0.01)\n",
    "\n",
    "# Fit/train LASSO\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_,\n",
    "  regressor.intercept_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization (Plot LassoCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1e-08, 100000000.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAF6CAYAAAAeZ/GvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ed00f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lasso = Lasso(random_state=42)\n",
    "alphas = np.logspace(-8, 8, 10)\n",
    "\n",
    "scores = list()\n",
    "scores_std = list()\n",
    "\n",
    "n_folds = 3\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso.alpha = alpha\n",
    "    this_scores = cross_val_score(lasso, x, y, cv=n_folds, n_jobs=1)\n",
    "    scores.append(np.mean(this_scores))\n",
    "    scores_std.append(np.std(this_scores))\n",
    "\n",
    "scores, scores_std = np.array(scores), np.array(scores_std)\n",
    "\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "plt.semilogx(alphas, scores)\n",
    "\n",
    "# plot error lines showing +/- std. errors of the scores\n",
    "std_error = scores_std / np.sqrt(n_folds)\n",
    "\n",
    "plt.semilogx(alphas, scores + std_error, 'b--')\n",
    "plt.semilogx(alphas, scores - std_error, 'b--')\n",
    "\n",
    "# alpha=0.2 controls the translucency of the fill color\n",
    "plt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2)\n",
    "\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(scores), linestyle='--', color='.5')\n",
    "plt.xlim([alphas[0], alphas[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization (Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 32.602691650390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number/precision: 1.5615852361205995e-11 / 5.960464477539063e-08\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item-Pencils</th>\n",
       "      <td>-41.255924</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Red</th>\n",
       "      <td>-38.167084</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Thumbtacks</th>\n",
       "      <td>-37.115662</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Green</th>\n",
       "      <td>-26.019794</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperweights</th>\n",
       "      <td>-24.572489</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Post It Notes</th>\n",
       "      <td>-20.477089</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperclips</th>\n",
       "      <td>-14.039446</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Large</th>\n",
       "      <td>-13.092483</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Blue</th>\n",
       "      <td>-11.873223</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Ink Pens</th>\n",
       "      <td>-7.117135</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Generic</th>\n",
       "      <td>-4.662254</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Medium</th>\n",
       "      <td>-3.035687</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Offices-R-Us</th>\n",
       "      <td>-0.953065</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Deep Office Supplies</th>\n",
       "      <td>-0.780119</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Brown</th>\n",
       "      <td>-0.436413</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack</th>\n",
       "      <td>0.025762</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>0.034138</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Duck Lake</th>\n",
       "      <td>0.171321</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-6% Solution</th>\n",
       "      <td>0.210985</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surface_area</th>\n",
       "      <td>0.479207</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-WizBang</th>\n",
       "      <td>1.335120</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Stapler</th>\n",
       "      <td>1.382184</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Small</th>\n",
       "      <td>6.146697</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-High Quality</th>\n",
       "      <td>7.520899</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Black</th>\n",
       "      <td>13.927071</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Tiny</th>\n",
       "      <td>14.474496</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-White</th>\n",
       "      <td>26.804325</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Pink</th>\n",
       "      <td>38.568123</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Tablets</th>\n",
       "      <td>143.324081</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         coef  positive\n",
       "item-Pencils                       -41.255924     False\n",
       "color-Red                          -38.167084     False\n",
       "item-Thumbtacks                    -37.115662     False\n",
       "color-Green                        -26.019794     False\n",
       "item-Paperweights                  -24.572489     False\n",
       "item-Post It Notes                 -20.477089     False\n",
       "item-Paperclips                    -14.039446     False\n",
       "size-Large                         -13.092483     False\n",
       "color-Blue                         -11.873223     False\n",
       "item-Ink Pens                       -7.117135     False\n",
       "quality-Generic                     -4.662254     False\n",
       "size-Medium                         -3.035687     False\n",
       "manufacturer-Offices-R-Us           -0.953065     False\n",
       "manufacturer-Deep Office Supplies   -0.780119     False\n",
       "color-Brown                         -0.436413     False\n",
       "pack                                 0.025762      True\n",
       "weight                               0.034138      True\n",
       "manufacturer-Duck Lake               0.171321      True\n",
       "manufacturer-6% Solution             0.210985      True\n",
       "surface_area                         0.479207      True\n",
       "manufacturer-WizBang                 1.335120      True\n",
       "item-Stapler                         1.382184      True\n",
       "size-Small                           6.146697      True\n",
       "quality-High Quality                 7.520899      True\n",
       "color-Black                         13.927071      True\n",
       "size-Tiny                           14.474496      True\n",
       "color-White                         26.804325      True\n",
       "color-Pink                          38.568123      True\n",
       "item-Tablets                       143.324081      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [ 25.55213165]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhkAAAD8CAYAAADaDLaTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wHDU0cMmI3RcQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ed29b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Create linear regression\n",
    "regressor = Ridge(alpha=1)\n",
    "\n",
    "# Fit/train Ridge\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_[0,:],\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 32.98986053466797\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item-Pencils</th>\n",
       "      <td>-37.654243</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Red</th>\n",
       "      <td>-35.690369</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Thumbtacks</th>\n",
       "      <td>-30.097212</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperweights</th>\n",
       "      <td>-25.119093</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Green</th>\n",
       "      <td>-24.735006</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Large</th>\n",
       "      <td>-18.559225</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Post It Notes</th>\n",
       "      <td>-13.660245</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Blue</th>\n",
       "      <td>-11.465649</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Ink Pens</th>\n",
       "      <td>-9.740009</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperclips</th>\n",
       "      <td>-4.943350</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Medium</th>\n",
       "      <td>-4.767353</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Generic</th>\n",
       "      <td>-4.598038</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Stapler</th>\n",
       "      <td>-4.164591</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Offices-R-Us</th>\n",
       "      <td>-1.082188</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Deep Office Supplies</th>\n",
       "      <td>-0.795328</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Brown</th>\n",
       "      <td>-0.226433</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack</th>\n",
       "      <td>0.006219</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>0.037553</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Duck Lake</th>\n",
       "      <td>0.219731</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-6% Solution</th>\n",
       "      <td>0.293640</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surface_area</th>\n",
       "      <td>0.775406</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-WizBang</th>\n",
       "      <td>1.253029</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-High Quality</th>\n",
       "      <td>7.368784</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Small</th>\n",
       "      <td>7.378531</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Black</th>\n",
       "      <td>12.904881</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Tiny</th>\n",
       "      <td>18.263298</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-White</th>\n",
       "      <td>25.021986</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Pink</th>\n",
       "      <td>35.909603</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Tablets</th>\n",
       "      <td>126.045387</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         coef  positive\n",
       "item-Pencils                       -37.654243     False\n",
       "color-Red                          -35.690369     False\n",
       "item-Thumbtacks                    -30.097212     False\n",
       "item-Paperweights                  -25.119093     False\n",
       "color-Green                        -24.735006     False\n",
       "size-Large                         -18.559225     False\n",
       "item-Post It Notes                 -13.660245     False\n",
       "color-Blue                         -11.465649     False\n",
       "item-Ink Pens                       -9.740009     False\n",
       "item-Paperclips                     -4.943350     False\n",
       "size-Medium                         -4.767353     False\n",
       "quality-Generic                     -4.598038     False\n",
       "item-Stapler                        -4.164591     False\n",
       "manufacturer-Offices-R-Us           -1.082188     False\n",
       "manufacturer-Deep Office Supplies   -0.795328     False\n",
       "color-Brown                         -0.226433     False\n",
       "pack                                 0.006219      True\n",
       "weight                               0.037553      True\n",
       "manufacturer-Duck Lake               0.219731      True\n",
       "manufacturer-6% Solution             0.293640      True\n",
       "surface_area                         0.775406      True\n",
       "manufacturer-WizBang                 1.253029      True\n",
       "quality-High Quality                 7.368784      True\n",
       "size-Small                           7.378531      True\n",
       "color-Black                         12.904881      True\n",
       "size-Tiny                           18.263298      True\n",
       "color-White                         25.021986      True\n",
       "color-Pink                          35.909603      True\n",
       "item-Tablets                       126.045387      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [ 20.59030914]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAD8CAYAAADExYYgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1080c79b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Create linear regression\n",
    "regressor = ElasticNet(alpha=0.01, l1_ratio=0.1)\n",
    "\n",
    "# Fit/train LASSO\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_,\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 126us/step - loss: 6041.9172 - val_loss: 4553.7658\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 4890.7620 - val_loss: 4333.5768\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 4634.6241 - val_loss: 3771.0887\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 4442.6107 - val_loss: 3713.9194\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 4377.7952 - val_loss: 3646.7101\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 4368.1724 - val_loss: 3621.9104\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 4252.0115 - val_loss: 3736.0521\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 4187.6623 - val_loss: 3591.6761\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 4155.1800 - val_loss: 3449.8745\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 3970.5430 - val_loss: 3458.6643\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 4072.7902 - val_loss: 3188.4583\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 3942.5209 - val_loss: 3168.8786\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 3586.4410 - val_loss: 3253.5555\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 3515.7240 - val_loss: 3117.3187\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 3172.1672 - val_loss: 2347.5916\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 2844.9870 - val_loss: 2167.4774\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 2484.1898 - val_loss: 1776.6752\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 2226.2724 - val_loss: 1384.8121\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 1961.8820 - val_loss: 1162.1805\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 1428.8014 - val_loss: 935.4672\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 1609.9155 - val_loss: 2106.7098\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 1224.8258 - val_loss: 665.8376\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 1s 151us/step - loss: 966.8571 - val_loss: 948.8051\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 1s 146us/step - loss: 828.3350 - val_loss: 728.8144\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 993.6786 - val_loss: 634.6908\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 640.3174 - val_loss: 436.7852\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 1s 127us/step - loss: 719.5685 - val_loss: 575.0610\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 630.0902 - val_loss: 466.4677\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 679.3222 - val_loss: 529.8222\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 652.1892 - val_loss: 729.9453\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 601.6004 - val_loss: 399.8202\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 634.8899 - val_loss: 1951.1871\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 598.1337 - val_loss: 338.9937\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 616.1767 - val_loss: 380.9774\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 510.6773 - val_loss: 534.4219\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 497.0447 - val_loss: 428.6276\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 448.7134 - val_loss: 666.2249\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 531.8416 - val_loss: 388.6934\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 488.2341 - val_loss: 382.2969\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 481.8558 - val_loss: 320.6169\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 574.1851 - val_loss: 332.9758\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 585.6790 - val_loss: 574.8399\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 526.3570 - val_loss: 319.7524\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 458.4744 - val_loss: 363.7875\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 428.7520 - val_loss: 450.6185\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 441.9569 - val_loss: 319.2795\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 517.8918 - val_loss: 620.3758\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 387.7827 - val_loss: 419.0976\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 447.1367 - val_loss: 385.4804\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 447.2968 - val_loss: 562.9615\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 522.2211 - val_loss: 381.7798\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 404.2488 - val_loss: 627.4276\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 523.0131 - val_loss: 375.4372\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 462.1068 - val_loss: 298.1348\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 418.1571 - val_loss: 271.3046\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 378.0502 - val_loss: 843.6917\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 389.6551 - val_loss: 658.7726\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 403.7665 - val_loss: 391.7860\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 403.6330 - val_loss: 296.3438\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 463.4591 - val_loss: 363.4111\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 425.8041 - val_loss: 256.3189\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 328.8966 - val_loss: 269.8546\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 1s 146us/step - loss: 381.1496 - val_loss: 276.9323\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 364.4475 - val_loss: 265.2839\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 375.9804 - val_loss: 615.2275\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 422.8696 - val_loss: 347.4879\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 446.2751 - val_loss: 265.7934\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 368.4953 - val_loss: 273.5614\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 357.4975 - val_loss: 416.9335\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 596.7403 - val_loss: 362.5952\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 409.2619 - val_loss: 301.9161\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 375.3839 - val_loss: 298.3490\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 345.4647 - val_loss: 335.5756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 306.1088 - val_loss: 272.4220\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 344.3191 - val_loss: 324.1982\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 442.7260 - val_loss: 595.2649\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 362.0991 - val_loss: 242.4523\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 408.6907 - val_loss: 359.1419\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 400.0809 - val_loss: 317.7552\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 306.7548 - val_loss: 595.5576\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 324.2731 - val_loss: 292.5844\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 395.3265 - val_loss: 231.5367\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 286.6784 - val_loss: 260.9866\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 299.6290 - val_loss: 237.3331\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 364.2910 - val_loss: 261.5779\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 323.4087 - val_loss: 231.4211\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 619.4107 - val_loss: 265.6239\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 460.8848 - val_loss: 298.4797\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 329.2232 - val_loss: 328.5891\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 342.2711 - val_loss: 227.1947\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 292.2614 - val_loss: 629.3875\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 357.2412 - val_loss: 596.3550\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 331.1013 - val_loss: 235.9897\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 362.7148 - val_loss: 223.8168\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 282.8328 - val_loss: 308.6707\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 345.0547 - val_loss: 255.8086\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 301.7158 - val_loss: 230.1678\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 338.4032 - val_loss: 216.3077\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 313.6448 - val_loss: 229.1236\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 311.5440 - val_loss: 313.1371\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 271.5537 - val_loss: 252.8449\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 297.8543 - val_loss: 228.6461\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 279.6584 - val_loss: 243.2022\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 314.3363 - val_loss: 263.3249\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 349.9321 - val_loss: 229.1218\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 308.7407 - val_loss: 331.0282\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 339.2025 - val_loss: 423.8340\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 269.8652 - val_loss: 235.5249\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 277.9600 - val_loss: 205.1932\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 284.9881 - val_loss: 248.0036\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 252.5430 - val_loss: 218.8079\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 270.7479 - val_loss: 273.0134\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 313.6290 - val_loss: 211.1272\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 372.1866 - val_loss: 270.6206\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 298.0864 - val_loss: 245.5364\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 305.4392 - val_loss: 204.8958\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 349.1355 - val_loss: 213.2395\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 289.0885 - val_loss: 244.6437\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 292.2474 - val_loss: 377.7546\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 264.2703 - val_loss: 299.3042\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 269.7581 - val_loss: 213.9670\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 291.2451 - val_loss: 208.2027\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 290.0919 - val_loss: 238.4318\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 292.2231 - val_loss: 229.8954\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 286.8210 - val_loss: 208.1048\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 297.9879 - val_loss: 220.1318\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 269.4188 - val_loss: 193.1478\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 248.3008 - val_loss: 217.6276\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 268.3776 - val_loss: 306.0808\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 330.2573 - val_loss: 286.5163\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 301.7185 - val_loss: 235.1778\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 250.9169 - val_loss: 288.7251\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 244.4097 - val_loss: 306.1035\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 269.6376 - val_loss: 237.3030\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 282.1213 - val_loss: 302.1273\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 276.3194 - val_loss: 225.6990\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 294.8363 - val_loss: 204.3996\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 266.7685 - val_loss: 217.9788\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 312.0879 - val_loss: 233.3116\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 233.9127 - val_loss: 200.9516\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 232.9259 - val_loss: 383.3481\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 244.5302 - val_loss: 965.1533\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 298.6207 - val_loss: 512.4533\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 333.5733 - val_loss: 183.9908\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 229.0365 - val_loss: 192.8200\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 261.8911 - val_loss: 231.2055\n",
      "Epoch 147/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 235.2635 - val_loss: 205.7141\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 224.2041 - val_loss: 194.3339\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 260.0902 - val_loss: 223.5703\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 265.1966 - val_loss: 201.5501\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 239.6523 - val_loss: 391.8299\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 380.7469 - val_loss: 199.6278\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 303.2612 - val_loss: 222.8623\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 305.9623 - val_loss: 218.6218\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 230.6914 - val_loss: 184.3119\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 298.5098 - val_loss: 218.7476\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 301.4290 - val_loss: 198.1467\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 251.2180 - val_loss: 188.9538\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 260.3708 - val_loss: 239.1724\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 334.9894 - val_loss: 1044.9847\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 293.0442 - val_loss: 202.2896\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 276.6600 - val_loss: 225.5666\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 228.6632 - val_loss: 1131.7874\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 265.3113 - val_loss: 191.8524\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 265.1448 - val_loss: 270.3595\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 223.5174 - val_loss: 229.1005\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 233.4264 - val_loss: 207.2210\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 212.0866 - val_loss: 204.7784\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 240.3589 - val_loss: 347.0176\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 267.5892 - val_loss: 221.1291\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 203.9260 - val_loss: 235.7383\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 273.4081 - val_loss: 261.0487\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 263.7566 - val_loss: 192.1394\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 225.6021 - val_loss: 204.0740\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 207.7591 - val_loss: 325.5210\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 251.4033 - val_loss: 184.7842\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 329.2734 - val_loss: 268.4746\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 232.2172 - val_loss: 174.8273\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 191.4240 - val_loss: 199.8567\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 203.6498 - val_loss: 185.2436\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 307.1548 - val_loss: 211.1043\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 280.0422 - val_loss: 252.4287\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 233.8294 - val_loss: 190.7110\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 205.1692 - val_loss: 179.1856\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 368.1197 - val_loss: 296.2733\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 232.1405 - val_loss: 343.7739\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 207.8206 - val_loss: 217.9708\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 214.5994 - val_loss: 265.2504\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 467.6648 - val_loss: 278.5851\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 220.9524 - val_loss: 233.1930\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 197.2981 - val_loss: 170.9901\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 235.8179 - val_loss: 248.9351\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 202.5761 - val_loss: 196.9214\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 214.5750 - val_loss: 208.0082\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 217.4173 - val_loss: 219.0208\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 230.0401 - val_loss: 298.0705\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 277.6365 - val_loss: 244.6246\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 239.0696 - val_loss: 173.9737\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 222.6768 - val_loss: 207.0434\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 259.9871 - val_loss: 241.6934\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 196.8864 - val_loss: 352.8386\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 198.0825 - val_loss: 256.5436\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 296.7385 - val_loss: 333.1169\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 201.1536 - val_loss: 174.0697\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 275.3910 - val_loss: 169.6412\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 207.8562 - val_loss: 172.7476\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 212.7413 - val_loss: 186.7199\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 281.7813 - val_loss: 208.9110\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 195.8961 - val_loss: 221.9772\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 223.9295 - val_loss: 227.0960\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 228.7680 - val_loss: 211.1661\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 390.0789 - val_loss: 207.3082\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 254.2825 - val_loss: 252.4836\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 283.5078 - val_loss: 257.7681\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 201.5184 - val_loss: 227.8593\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 203.3545 - val_loss: 179.9309\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 206.6875 - val_loss: 192.0560\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 212.5976 - val_loss: 209.4812\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 233.7864 - val_loss: 550.2279\n",
      "Epoch 220/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 83us/step - loss: 551.4890 - val_loss: 398.7224\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 273.3253 - val_loss: 194.7622\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 1s 148us/step - loss: 198.9135 - val_loss: 222.4452\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 258.5510 - val_loss: 166.8095\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 264.5242 - val_loss: 246.8424\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 252.9449 - val_loss: 191.6216\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 214.6567 - val_loss: 185.4944\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 228.2689 - val_loss: 655.6725\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 246.9492 - val_loss: 211.3896\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 218.8985 - val_loss: 190.9146\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 188.0059 - val_loss: 357.9604\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 209.2324 - val_loss: 258.6596\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 194.7358 - val_loss: 184.4271\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 204.1555 - val_loss: 241.3345\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 249.6990 - val_loss: 183.5270\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 204.3708 - val_loss: 360.2280\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 221.2048 - val_loss: 168.8563\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 185.7849 - val_loss: 185.0664\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 229.3052 - val_loss: 226.6488\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 206.8406 - val_loss: 169.2471\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 188.1200 - val_loss: 217.9752\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 248.6684 - val_loss: 1381.9934\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 303.7194 - val_loss: 251.3143\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 303.1652 - val_loss: 462.0320\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 227.6998 - val_loss: 410.5171\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 219.1162 - val_loss: 311.8116\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 189.0007 - val_loss: 266.1517\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 203.3988 - val_loss: 176.6724\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 291.1027 - val_loss: 165.9254\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 206.2396 - val_loss: 167.7785\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 189.3286 - val_loss: 218.9678\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 196.9557 - val_loss: 167.7721\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 185.0234 - val_loss: 179.7798\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 193.0534 - val_loss: 185.0429\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 193.4244 - val_loss: 236.9773\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 254.6481 - val_loss: 200.5867\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 212.5280 - val_loss: 200.8035\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 377.4395 - val_loss: 190.2353\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 226.2328 - val_loss: 169.6041\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 196.4794 - val_loss: 173.5502\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 236.2365 - val_loss: 194.4319\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 213.8372 - val_loss: 196.4281\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 203.3887 - val_loss: 315.2049\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 214.2520 - val_loss: 498.5042\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 249.7930 - val_loss: 403.4345\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 222.2104 - val_loss: 176.7787\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 227.7385 - val_loss: 252.6387\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 221.1241 - val_loss: 405.7880\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 192.4701 - val_loss: 157.8979\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 202.8595 - val_loss: 166.3053\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 246.5226 - val_loss: 184.8482\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 201.2223 - val_loss: 174.6992\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 217.7412 - val_loss: 495.0411\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 235.6942 - val_loss: 205.4986\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 185.2166 - val_loss: 180.9177\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 216.4865 - val_loss: 207.4591\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 248.2056 - val_loss: 224.4120\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 238.4928 - val_loss: 309.7096\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 196.6753 - val_loss: 249.0809\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 346.7089 - val_loss: 251.2222\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 311.8949 - val_loss: 232.2470\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 239.8048 - val_loss: 204.5194\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 234.0341 - val_loss: 206.2030\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 230.5326 - val_loss: 161.0588\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 198.0948 - val_loss: 247.8379\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 211.1698 - val_loss: 271.0716\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 198.4915 - val_loss: 200.7633\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 193.9446 - val_loss: 195.7971\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 226.8548 - val_loss: 235.0172\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 205.9262 - val_loss: 211.2524\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 215.3012 - val_loss: 202.6033\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 195.7418 - val_loss: 167.3479\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 192.4919 - val_loss: 180.9975\n",
      "Epoch 293/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 345.2738 - val_loss: 291.4638\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 259.4477 - val_loss: 176.5078\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.1367 - val_loss: 170.8535\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.3519 - val_loss: 196.3092\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 233.6596 - val_loss: 648.2276\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 194.8185 - val_loss: 162.4419\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 178.8550 - val_loss: 160.1965\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 183.2055 - val_loss: 184.4607\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 194.1926 - val_loss: 314.5765\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 210.4219 - val_loss: 167.8591\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.1982 - val_loss: 204.1777\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 220.0094 - val_loss: 189.0294\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 271.4790 - val_loss: 157.6747\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 206.2473 - val_loss: 182.6575\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 267.4523 - val_loss: 346.3338\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 293.0879 - val_loss: 173.9038\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 181.6756 - val_loss: 159.5262\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 243.8851 - val_loss: 283.0467\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 256.5173 - val_loss: 246.1642\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 205.6108 - val_loss: 251.0610\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 187.3722 - val_loss: 171.6020\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 210.3605 - val_loss: 160.1322\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 237.2347 - val_loss: 214.3255\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 184.8610 - val_loss: 160.2570\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 196.4218 - val_loss: 206.5463\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.8652 - val_loss: 163.6243\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 211.2292 - val_loss: 172.0192\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 241.9567 - val_loss: 271.1689\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 225.6714 - val_loss: 208.4531\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 220.1854 - val_loss: 467.4800\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 231.9072 - val_loss: 164.9282\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 237.7752 - val_loss: 268.8621\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 198.1937 - val_loss: 183.1565\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 179.4745 - val_loss: 156.5150\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 188.6242 - val_loss: 157.4954\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 247.4342 - val_loss: 154.1136\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 218.0675 - val_loss: 167.9961\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 215.0468 - val_loss: 152.9612\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 238.0008 - val_loss: 209.7933\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 220.1139 - val_loss: 194.7591\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 208.5982 - val_loss: 159.8897\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 198.2849 - val_loss: 261.2897\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 195.3639 - val_loss: 153.2261\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 187.1494 - val_loss: 204.1432\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 193.7534 - val_loss: 165.9971\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 225.3772 - val_loss: 155.2567\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 219.9827 - val_loss: 186.2771\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 194.1548 - val_loss: 215.7882\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 171.8071 - val_loss: 164.9547\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 213.2632 - val_loss: 182.9923\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 269.3533 - val_loss: 165.9200\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 193.8936 - val_loss: 160.2810\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 173.6275 - val_loss: 222.4637\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 174.3114 - val_loss: 151.4214\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 220.7365 - val_loss: 512.9569\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 216.6073 - val_loss: 153.4209\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 181.5098 - val_loss: 224.0619\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 195.8998 - val_loss: 161.6963\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 244.7522 - val_loss: 170.8359\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 419.9258 - val_loss: 228.2559\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 267.4490 - val_loss: 209.7141\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 245.2693 - val_loss: 221.9833\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 215.1006 - val_loss: 183.4403\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 218.7611 - val_loss: 185.8042\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 222.0383 - val_loss: 234.7750\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 218.9689 - val_loss: 337.9535\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 208.5263 - val_loss: 187.3521\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 201.3065 - val_loss: 172.9687\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 213.5091 - val_loss: 437.9929\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 246.9425 - val_loss: 220.0833\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 197.7621 - val_loss: 305.2553\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 211.5646 - val_loss: 210.0640\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 202.0297 - val_loss: 208.8099\n",
      "Epoch 366/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 196.7764 - val_loss: 161.1755\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 193.1724 - val_loss: 258.2696\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 221.6520 - val_loss: 294.0042\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 180.5164 - val_loss: 186.1843\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 174.2310 - val_loss: 220.6639\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 183.5668 - val_loss: 173.8996\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 181.8489 - val_loss: 159.8559\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 223.1604 - val_loss: 226.2929\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.6977 - val_loss: 202.8136\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 182.0195 - val_loss: 216.6554\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 173.5373 - val_loss: 174.0659\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 196.7743 - val_loss: 269.1581\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 255.6393 - val_loss: 190.8684\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 178.4252 - val_loss: 178.0541\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 230.5964 - val_loss: 174.1616\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 158.2362 - val_loss: 165.4075\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 194.1723 - val_loss: 160.0426\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 171.3489 - val_loss: 150.3769\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 268.7822 - val_loss: 241.5900\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 191.7687 - val_loss: 207.0227\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 240.5493 - val_loss: 172.6116\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 206.5027 - val_loss: 160.4941\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 160.1883 - val_loss: 161.4904\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 261.0341 - val_loss: 297.1563\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 235.5947 - val_loss: 385.7886\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 194.0635 - val_loss: 182.2108\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 187.8179 - val_loss: 157.3326\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 198.1850 - val_loss: 165.7236\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 191.2359 - val_loss: 152.2549\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 169.3192 - val_loss: 154.5431\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 180.7271 - val_loss: 156.9475\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 233.2135 - val_loss: 179.2097\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 222.6084 - val_loss: 175.2009\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 229.5507 - val_loss: 162.2961\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 344.8130 - val_loss: 208.0944\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.9485 - val_loss: 154.2714\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 179.3323 - val_loss: 202.9917\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 170.6821 - val_loss: 150.2518\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 171.9065 - val_loss: 249.3566\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 195.0694 - val_loss: 186.7694\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 169.1730 - val_loss: 159.7693\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 175.2882 - val_loss: 168.2175\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 188.2810 - val_loss: 165.9283\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 167.9505 - val_loss: 169.7545\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 188.9453 - val_loss: 166.6812\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 187.1392 - val_loss: 152.8970\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 388.3720 - val_loss: 202.6570\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 174.2761 - val_loss: 163.7742\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 185.4799 - val_loss: 163.4282\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 162.5372 - val_loss: 158.3213\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 208.9662 - val_loss: 217.0854\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 175.7969 - val_loss: 150.5969\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 209.4433 - val_loss: 267.3736\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 194.8210 - val_loss: 180.5465\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 192.7208 - val_loss: 264.8672\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 184.7533 - val_loss: 156.1545\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 283.4606 - val_loss: 147.7643\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 165.4960 - val_loss: 148.5903\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 172.8437 - val_loss: 189.4682\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 220.4697 - val_loss: 244.2038\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 167.1373 - val_loss: 221.3208\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 557.5344 - val_loss: 244.1899\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 241.9586 - val_loss: 194.2428\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 217.9898 - val_loss: 187.4011\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 260.6353 - val_loss: 224.9908\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 203.9418 - val_loss: 268.9995\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 175.0918 - val_loss: 167.3421\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 181.4828 - val_loss: 161.5827\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 199.7963 - val_loss: 227.1941\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.8916 - val_loss: 158.3744\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 175.5535 - val_loss: 154.8648\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 175.6850 - val_loss: 163.6796\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 173.8964 - val_loss: 223.9146\n",
      "Epoch 439/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 194.2772 - val_loss: 188.2971\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.4018 - val_loss: 238.3362\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 192.2316 - val_loss: 240.4871\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 236.5966 - val_loss: 211.3831\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 201.6291 - val_loss: 284.9658\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 197.8150 - val_loss: 311.6597\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 174.4067 - val_loss: 287.6337\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 229.3715 - val_loss: 158.2598\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 182.7473 - val_loss: 198.1069\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 287.2165 - val_loss: 189.1942\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 264.3577 - val_loss: 160.4133\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 193.4703 - val_loss: 163.7191\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 178.5490 - val_loss: 183.5666\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 208.7557 - val_loss: 291.6956\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 264.3438 - val_loss: 161.2388\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 219.9967 - val_loss: 450.2921\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 207.0658 - val_loss: 156.3014\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 184.2212 - val_loss: 157.2803\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 187.7878 - val_loss: 196.0217\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 165.4763 - val_loss: 179.0315\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 183.9241 - val_loss: 357.6166\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 181.8426 - val_loss: 288.2672\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 305.6941 - val_loss: 157.7811\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 338.5327 - val_loss: 222.8263\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 173.0280 - val_loss: 235.8147\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 176.5415 - val_loss: 148.8588\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 176.9912 - val_loss: 214.7062\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 200.1674 - val_loss: 143.2384\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 235.3258 - val_loss: 215.3382\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 164.0496 - val_loss: 153.9598\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 179.4000 - val_loss: 158.3173\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.0615 - val_loss: 196.4511\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 248.8761 - val_loss: 153.4776\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 271.5956 - val_loss: 206.2729\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 219.9234 - val_loss: 709.2841\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 268.9015 - val_loss: 219.9228\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 325.8870 - val_loss: 216.1737\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 220.4169 - val_loss: 162.4710\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 233.5201 - val_loss: 175.4306\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 215.9242 - val_loss: 167.2064\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 156.3771 - val_loss: 180.0851\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 172.5289 - val_loss: 167.2912\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 154.2033 - val_loss: 188.4314\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 286.9651 - val_loss: 369.8288\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 178.4184 - val_loss: 179.5583\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 160.6451 - val_loss: 209.0868\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 189.8796 - val_loss: 151.4339\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 163.8285 - val_loss: 160.8138\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 286.5321 - val_loss: 167.8556\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 178.0598 - val_loss: 212.2113\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 376.9063 - val_loss: 303.2024\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 174.7162 - val_loss: 154.1737\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 213.8513 - val_loss: 159.2175\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 372.5278 - val_loss: 238.3580\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 298.9696 - val_loss: 164.3742\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 242.2717 - val_loss: 155.1938\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 205.7329 - val_loss: 167.3044\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 192.3551 - val_loss: 185.2312\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 189.9468 - val_loss: 162.9168\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 194.1144 - val_loss: 157.5219\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 167.8058 - val_loss: 240.9916\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 152.4485 - val_loss: 164.7958\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 333.4144 - val_loss: 208.7388\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 214.9414 - val_loss: 173.1067\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 198.2616 - val_loss: 147.9232\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 175.6974 - val_loss: 171.9846\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 253.5457 - val_loss: 187.9832\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 327.9333 - val_loss: 163.1766\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 223.9993 - val_loss: 208.4487\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 237.5250 - val_loss: 174.2573\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 215.5545 - val_loss: 151.2981\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 166.7575 - val_loss: 171.5792\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 195.4537 - val_loss: 151.4998\n",
      "Epoch 512/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 55us/step - loss: 201.6001 - val_loss: 158.0503\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.4068 - val_loss: 155.4689\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 193.2909 - val_loss: 159.8630\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 214.8981 - val_loss: 384.1682\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 202.0017 - val_loss: 507.8693\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 185.5150 - val_loss: 166.8944\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 210.7232 - val_loss: 153.0958\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 265.0620 - val_loss: 267.5100\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 238.7617 - val_loss: 172.2325\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.7426 - val_loss: 154.0400\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 162.3784 - val_loss: 148.0025\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 184.5858 - val_loss: 165.6519\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 251.5561 - val_loss: 149.9825\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 359.3235 - val_loss: 196.8062\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 181.5544 - val_loss: 270.7517\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 186.4688 - val_loss: 162.5118\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 175.1651 - val_loss: 192.5376\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 159.4507 - val_loss: 182.0126\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.3193 - val_loss: 148.3339\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 177.9799 - val_loss: 201.2881\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 161.2554 - val_loss: 187.7954\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 420.6357 - val_loss: 278.1077\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 197.5784 - val_loss: 307.2914\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 191.8260 - val_loss: 257.8619\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 176.2129 - val_loss: 157.7205\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.8740 - val_loss: 187.5951\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.7347 - val_loss: 189.6996\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 155.7778 - val_loss: 156.2979\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 171.0289 - val_loss: 286.7428\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 259.8521 - val_loss: 323.5744\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 229.8876 - val_loss: 151.7816\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.2791 - val_loss: 188.5562\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 158.2623 - val_loss: 220.2498\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 237.0165 - val_loss: 202.1091\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 233.6491 - val_loss: 200.2963\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 328.2527 - val_loss: 635.7065\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 263.4325 - val_loss: 154.7895\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 200.7475 - val_loss: 180.2498\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 163.9026 - val_loss: 159.1969\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.5933 - val_loss: 165.4264\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 157.1289 - val_loss: 222.6930\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 407.3304 - val_loss: 184.9920\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 189.3502 - val_loss: 145.5396\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 159.3616 - val_loss: 225.4417\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 224.8572 - val_loss: 273.2933\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.4049 - val_loss: 151.3819\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 209.9873 - val_loss: 144.5148\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 156.1888 - val_loss: 199.4960\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 176.1859 - val_loss: 258.4390\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 178.0125 - val_loss: 275.3606\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 269.9046 - val_loss: 171.4881\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 225.6430 - val_loss: 158.7915\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.8112 - val_loss: 164.5764\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.4025 - val_loss: 179.0104\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 181.5800 - val_loss: 155.0554\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.3336 - val_loss: 155.4553\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 156.1472 - val_loss: 238.3225\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 179.5853 - val_loss: 150.8705\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 169.8587 - val_loss: 172.2543\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 172.3196 - val_loss: 147.3380\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 166.6842 - val_loss: 165.6540\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 401.1428 - val_loss: 282.6840\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 259.5742 - val_loss: 157.6283\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 198.9845 - val_loss: 156.1892\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 196.9707 - val_loss: 220.1651\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 183.8879 - val_loss: 212.8346\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 159.3853 - val_loss: 192.6589\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 208.2799 - val_loss: 154.3715\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 223.8620 - val_loss: 151.6030\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 226.7019 - val_loss: 210.5872\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 212.4339 - val_loss: 177.8998\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 234.9824 - val_loss: 156.9885\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 198.9231 - val_loss: 146.2252\n",
      "Epoch 585/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 155.7178 - val_loss: 181.3738\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 176.0513 - val_loss: 151.3008\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 173.1184 - val_loss: 141.7180\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 176.4079 - val_loss: 295.3463\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 195.3453 - val_loss: 205.5686\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 159.959 - 0s 51us/step - loss: 158.3953 - val_loss: 205.5899\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 165.7009 - val_loss: 143.7344\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 259.9020 - val_loss: 264.0071\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 216.6417 - val_loss: 198.4326\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 177.3715 - val_loss: 169.6898\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 195.8586 - val_loss: 163.4308\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 262.1262 - val_loss: 149.5227\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 176.6321 - val_loss: 147.9096\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 209.3320 - val_loss: 264.1997\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 287.5409 - val_loss: 198.3013\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 210.2229 - val_loss: 242.7494\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 177.0553 - val_loss: 167.5260\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.9786 - val_loss: 147.2341\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.6496 - val_loss: 147.8366\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 281.9789 - val_loss: 173.7410\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 172.8162 - val_loss: 148.2435\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 174.9430 - val_loss: 174.0971\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 190.0816 - val_loss: 224.3694\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 174.3993 - val_loss: 155.8996\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 176.4282 - val_loss: 152.0966\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 145.3883 - val_loss: 164.7468\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 192.0977 - val_loss: 142.0970\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 196.1806 - val_loss: 160.9428\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 172.3108 - val_loss: 147.2508\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 179.3787 - val_loss: 220.6651\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 180.1314 - val_loss: 162.4950\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 207.6054 - val_loss: 459.7503\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 201.7537 - val_loss: 161.2276\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 168.8314 - val_loss: 144.0141\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 179.2770 - val_loss: 180.0431\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 202.4218 - val_loss: 321.4444\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 243.6173 - val_loss: 214.8233\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 186.2426 - val_loss: 160.4295\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 157.8469 - val_loss: 182.9297\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 196.0546 - val_loss: 153.4693\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.3077 - val_loss: 298.3231\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 207.4586 - val_loss: 166.8258\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 219.4872 - val_loss: 238.4374\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 199.8768 - val_loss: 214.9667\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 197.1876 - val_loss: 220.8799\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 173.5394 - val_loss: 145.2088\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 177.7417 - val_loss: 168.9006\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 197.4060 - val_loss: 167.9673\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 251.2262 - val_loss: 178.7315\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 225.8962 - val_loss: 148.1387\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 220.7760 - val_loss: 151.7797\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 170.9460 - val_loss: 177.7021\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 178.9380 - val_loss: 147.8357\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 194.1702 - val_loss: 278.8940\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 179.5344 - val_loss: 192.1224\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 202.6788 - val_loss: 149.5940\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 154.8707 - val_loss: 155.3713\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 199.4415 - val_loss: 227.4510\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 247.1584 - val_loss: 248.9517\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 170.2997 - val_loss: 151.0355\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 189.7655 - val_loss: 156.8625\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 202.2964 - val_loss: 366.6672\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 200.7258 - val_loss: 172.5750\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.8875 - val_loss: 147.8977\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 158.1588 - val_loss: 169.5945\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.2844 - val_loss: 151.8273\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 439.4481 - val_loss: 161.4408\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 171.2341 - val_loss: 205.4307\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 206.6198 - val_loss: 164.7039\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 165.3796 - val_loss: 236.0395\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.0759 - val_loss: 144.7270\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.6967 - val_loss: 208.3417\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 246.9498 - val_loss: 376.6667\n",
      "Epoch 658/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 181.1134 - val_loss: 156.9619\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 162.6746 - val_loss: 149.9592\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 177.5921 - val_loss: 291.9674\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 286.1611 - val_loss: 152.7066\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 166.5803 - val_loss: 147.4010\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 170.1117 - val_loss: 198.9313\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 168.2908 - val_loss: 162.5989\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 159.9849 - val_loss: 167.0246\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 163.4505 - val_loss: 174.3013\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 169.9106 - val_loss: 151.7032\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 155.1441 - val_loss: 155.4349\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.1481 - val_loss: 146.3064\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.3570 - val_loss: 181.4253\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 184.7604 - val_loss: 169.1724\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 162.7492 - val_loss: 157.3895\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 178.1578 - val_loss: 145.6403\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 167.4194 - val_loss: 194.0507\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 253.5356 - val_loss: 235.1681\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 211.0133 - val_loss: 161.8671\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 175.6497 - val_loss: 148.1440\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 175.9884 - val_loss: 184.9142\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.4935 - val_loss: 159.0299\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 230.2409 - val_loss: 161.4816\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 225.9250 - val_loss: 163.9351\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 218.7658 - val_loss: 157.6842\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 227.8501 - val_loss: 205.2939\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 242.2072 - val_loss: 297.4769\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 198.6259 - val_loss: 171.7151\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 208.9532 - val_loss: 169.1253\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 180.8928 - val_loss: 177.9599\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 193.7223 - val_loss: 145.7912\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 306.4278 - val_loss: 218.3224\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 193.7964 - val_loss: 161.1244\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 173.8147 - val_loss: 173.9251\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 220.6727 - val_loss: 147.0817\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 208.9307 - val_loss: 187.4391\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 199.9096 - val_loss: 167.8365\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 185.5543 - val_loss: 234.8834\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 197.9285 - val_loss: 172.9177\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 206.5665 - val_loss: 166.4031\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 170.8513 - val_loss: 292.0391\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 180.7435 - val_loss: 169.6288\n",
      "Epoch 700/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 202.1670 - val_loss: 669.4320\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 203.4331 - val_loss: 183.0292\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 180.2797 - val_loss: 153.7404\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 172.4014 - val_loss: 178.0391\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 302.9834 - val_loss: 221.7406\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 172.3074 - val_loss: 183.5582\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 177.6197 - val_loss: 203.8534\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 186.5035 - val_loss: 167.3925\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.6814 - val_loss: 142.9001\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.8792 - val_loss: 146.4392\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 180.6035 - val_loss: 157.7088\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 182.1480 - val_loss: 488.7568\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 213.7927 - val_loss: 285.9936\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 191.3104 - val_loss: 158.0727\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 171.8987 - val_loss: 228.9821\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 304.2069 - val_loss: 2329.3407\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 307.0349 - val_loss: 187.8915\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 172.6314 - val_loss: 151.8276\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.6286 - val_loss: 183.3723\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 196.1722 - val_loss: 160.0215\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 163.3504 - val_loss: 174.5185\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 169.6732 - val_loss: 156.5411\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 204.3539 - val_loss: 150.1179\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 178.3744 - val_loss: 191.9929\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 178.8074 - val_loss: 148.2445\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 163.5944 - val_loss: 151.8783\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 215.4433 - val_loss: 230.6686\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 177.8630 - val_loss: 150.9954\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 186.3144 - val_loss: 157.7983\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 207.8180 - val_loss: 282.0130\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 182.1608 - val_loss: 273.1727\n",
      "Epoch 731/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 63us/step - loss: 205.8020 - val_loss: 164.6992\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 190.7431 - val_loss: 156.4024\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 153.5022 - val_loss: 163.3599\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.0305 - val_loss: 174.2422\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 205.4197 - val_loss: 166.3990\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 243.6302 - val_loss: 156.9869\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 223.1002 - val_loss: 214.1892\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 285.1786 - val_loss: 170.6745\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 187.3716 - val_loss: 180.6428\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 652.3268 - val_loss: 260.7205\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 324.4633 - val_loss: 205.7255\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 245.8180 - val_loss: 204.5979\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 218.8743 - val_loss: 173.9819\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 184.3799 - val_loss: 163.9333\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 235.2502 - val_loss: 196.9426\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 177.9459 - val_loss: 173.7400\n",
      "Epoch 747/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 191.0038 - val_loss: 292.6170\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 180.2089 - val_loss: 165.6473\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 255.8856 - val_loss: 150.4765\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 170.1380 - val_loss: 173.4559\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 183.5890 - val_loss: 191.4773\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 200.4647 - val_loss: 179.6310\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 185.4802 - val_loss: 213.5014\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 194.7668 - val_loss: 205.9455\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 177.0291 - val_loss: 175.0809\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 178.9257 - val_loss: 156.9991\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 223.2079 - val_loss: 202.5810\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 201.3240 - val_loss: 160.7705\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 160.8162 - val_loss: 144.0586\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 185.6721 - val_loss: 150.0103\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 173.8547 - val_loss: 156.1714\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 189.1092 - val_loss: 150.2961\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 163.6998 - val_loss: 189.6140\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 220.2662 - val_loss: 236.4783\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 205.9238 - val_loss: 180.8609\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 184.7212 - val_loss: 178.2862\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 188.6355 - val_loss: 160.4265\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 234.6690 - val_loss: 265.6213\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 185.3056 - val_loss: 145.3884\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 172.5648 - val_loss: 162.4411\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 221.6779 - val_loss: 166.2759\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 237.9254 - val_loss: 417.2327\n",
      "Epoch 773/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 219.1816 - val_loss: 245.6397\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 178.7009 - val_loss: 152.0368\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 178.8788 - val_loss: 148.3841\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 184.4356 - val_loss: 164.0974\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 162.4655 - val_loss: 168.8216\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 191.7700 - val_loss: 172.9745\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 189.4494 - val_loss: 202.1445\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 188.1266 - val_loss: 211.1985\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 182.6437 - val_loss: 155.3394\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 171.8002 - val_loss: 149.6829\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 166.6851 - val_loss: 167.1512\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 285.2924 - val_loss: 163.0527\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 192.7646 - val_loss: 155.0079\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 155.1746 - val_loss: 157.9615\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 155.4272 - val_loss: 154.6658\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 181.3904 - val_loss: 160.1855\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 157.4148 - val_loss: 148.7302\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 155.2002 - val_loss: 151.2049\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 174.8511 - val_loss: 142.0749\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 170.5661 - val_loss: 171.0850\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 313.6508 - val_loss: 152.0542\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 192.3830 - val_loss: 174.0366\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 191.9781 - val_loss: 191.6382\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.6928 - val_loss: 169.1143\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 201.6084 - val_loss: 162.7569\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 172.3213 - val_loss: 282.6773\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 249.4756 - val_loss: 299.1077\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 195.6200 - val_loss: 169.3695\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 199.8832 - val_loss: 295.7189\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 251.4660 - val_loss: 205.3415\n",
      "Epoch 803/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 208.0340 - val_loss: 180.8053\n",
      "Epoch 804/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 57us/step - loss: 204.8345 - val_loss: 195.2139\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 185.1204 - val_loss: 159.3513\n",
      "Epoch 806/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 186.3132 - val_loss: 156.0781\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 184.0899 - val_loss: 170.3404\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 188.8499 - val_loss: 155.6924\n",
      "Epoch 809/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 184.6319 - val_loss: 158.3262\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 214.8304 - val_loss: 188.8670\n",
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.5714 - val_loss: 203.1760\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.7494 - val_loss: 149.8084\n",
      "Epoch 813/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 191.0744 - val_loss: 159.9273\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 197.4495 - val_loss: 177.4245\n",
      "Epoch 815/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 200.4329 - val_loss: 170.8251\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 212.0007 - val_loss: 163.7272\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 196.8825 - val_loss: 227.0030\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 208.4999 - val_loss: 177.3853\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 171.8977 - val_loss: 238.2828\n",
      "Epoch 820/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 186.8544 - val_loss: 175.5423\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 206.3574 - val_loss: 166.4757\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 169.6181 - val_loss: 193.3837\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.0472 - val_loss: 180.6287\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 167.4796 - val_loss: 198.5726\n",
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 174.6047 - val_loss: 185.0404\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 209.9143 - val_loss: 171.2569\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 191.7983 - val_loss: 162.6302\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 189.0144 - val_loss: 148.6380\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 160.1888 - val_loss: 151.3360\n",
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 598.8894 - val_loss: 294.0504\n",
      "Epoch 831/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 261.6942 - val_loss: 176.3701\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 199.9654 - val_loss: 184.1050\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 194.5851 - val_loss: 208.1559\n",
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 193.5859 - val_loss: 181.5814\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 170.1159 - val_loss: 154.5298\n",
      "Epoch 836/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 169.8800 - val_loss: 986.1900\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 216.7130 - val_loss: 152.4911\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.9269 - val_loss: 215.8448\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 167.2365 - val_loss: 158.8141\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 184.5381 - val_loss: 181.8764\n",
      "Epoch 841/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 185.2900 - val_loss: 157.3081\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 182.869 - 1s 64us/step - loss: 180.3703 - val_loss: 159.6538\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 225.4349 - val_loss: 194.8232\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 167.0557 - val_loss: 168.9460\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 179.9638 - val_loss: 163.5179\n",
      "Epoch 846/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 242.0750 - val_loss: 392.3685\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.6417 - val_loss: 265.0684\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.4580 - val_loss: 165.6465\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 163.4506 - val_loss: 170.6766\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 300.9080 - val_loss: 154.9169\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 189.3521 - val_loss: 176.9078\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 177.2668 - val_loss: 434.9090\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 177.3745 - val_loss: 152.6253\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 179.5378 - val_loss: 224.0882\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 178.6932 - val_loss: 147.7690\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.3180 - val_loss: 178.8353\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 198.1109 - val_loss: 213.5600\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 220.6283 - val_loss: 264.4062\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 178.2960 - val_loss: 145.2305\n",
      "Epoch 860/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 197.4548 - val_loss: 193.0185\n",
      "Epoch 861/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 168.3924 - val_loss: 156.3266\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 171.8300 - val_loss: 188.6729\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 210.5017 - val_loss: 154.4743\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 159.9218 - val_loss: 171.4973\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 166.7140 - val_loss: 176.1405\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 154.5362 - val_loss: 206.7779\n",
      "Epoch 867/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 169.8718 - val_loss: 254.1772\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 364.4174 - val_loss: 183.5435\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 165.3619 - val_loss: 222.7304\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.5823 - val_loss: 150.2147\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 243.5038 - val_loss: 154.5720\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 292.2499 - val_loss: 342.0642\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 261.8397 - val_loss: 156.5928\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 167.3858 - val_loss: 180.4844\n",
      "Epoch 875/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.7705 - val_loss: 152.7091\n",
      "Epoch 876/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 168.0939 - val_loss: 150.1543\n",
      "Epoch 877/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 161.5082 - val_loss: 200.8370\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 178.1124 - val_loss: 165.0263\n",
      "Epoch 879/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 189.0669 - val_loss: 167.0528\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 249.0546 - val_loss: 909.7674\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 211.1866 - val_loss: 174.1290\n",
      "Epoch 882/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 183.7082 - val_loss: 149.0427\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 175.2939 - val_loss: 150.7827\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 163.2018 - val_loss: 149.3421\n",
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 153.6405 - val_loss: 161.4135\n",
      "Epoch 886/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 164.8759 - val_loss: 201.5958\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.4206 - val_loss: 145.1062\n",
      "Epoch 888/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 282.0174 - val_loss: 443.2225\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 185.3944 - val_loss: 153.6255\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.3135 - val_loss: 197.9372\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 182.8327 - val_loss: 149.7741\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 189.0974 - val_loss: 237.8171\n",
      "Epoch 893/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 388.7066 - val_loss: 204.7787\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 216.3883 - val_loss: 218.7150\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 188.8716 - val_loss: 164.7261\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 190.7448 - val_loss: 192.8699\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 174.4310 - val_loss: 160.5142\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 179.8340 - val_loss: 293.5232\n",
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 175.7756 - val_loss: 163.6984\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 189.7713 - val_loss: 158.0182\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 251.2420 - val_loss: 189.6637\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.0957 - val_loss: 170.5857\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 186.9758 - val_loss: 157.7338\n",
      "Epoch 904/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 163.6596 - val_loss: 286.8843\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 180.4842 - val_loss: 148.5055\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 180.0919 - val_loss: 286.2922\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 170.7138 - val_loss: 148.2360\n",
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 180.6071 - val_loss: 226.9094\n",
      "Epoch 909/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 183.3027 - val_loss: 160.5346\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 181.4284 - val_loss: 184.9755\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 201.467 - 0s 57us/step - loss: 202.5235 - val_loss: 176.4672\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 182.1003 - val_loss: 176.8717\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.0937 - val_loss: 179.2834\n",
      "Epoch 914/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 167.3578 - val_loss: 164.7598\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 165.2419 - val_loss: 163.3049\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 290.1054 - val_loss: 179.6678\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 250.5340 - val_loss: 179.8165\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 181.6262 - val_loss: 200.6278\n",
      "Epoch 919/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 178.9749 - val_loss: 158.9849\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 185.4846 - val_loss: 154.1009\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 178.5290 - val_loss: 144.8170\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 161.1134 - val_loss: 141.9592\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 156.6917 - val_loss: 165.1047\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 160.1073 - val_loss: 163.3371\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 154.2674 - val_loss: 169.2500\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 150.1500 - val_loss: 222.3837\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 231.1394 - val_loss: 157.7609\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 192.3424 - val_loss: 190.6096\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 160.9952 - val_loss: 145.3387\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 152.8609 - val_loss: 180.7234\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 149.5043 - val_loss: 149.1115\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 156.3034 - val_loss: 149.3359\n",
      "Epoch 933/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 179.6956 - val_loss: 152.6800\n",
      "Epoch 934/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 165.7977 - val_loss: 172.9539\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 304.8455 - val_loss: 149.5709\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 168.3027 - val_loss: 159.6599\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 257.9591 - val_loss: 159.0445\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 170.8703 - val_loss: 145.0969\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 151.5033 - val_loss: 144.7381\n",
      "Epoch 940/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 175.6667 - val_loss: 151.9450\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 164.1055 - val_loss: 154.3502\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 160.9801 - val_loss: 155.8837\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 173.1731 - val_loss: 146.1362\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 169.6815 - val_loss: 148.0467\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 191.0912 - val_loss: 157.9900\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 231.9948 - val_loss: 150.9036\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 146.8056 - val_loss: 157.4556\n",
      "Epoch 948/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 152.6063 - val_loss: 163.1400\n",
      "Epoch 949/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 204.3978 - val_loss: 178.8695\n",
      "Epoch 950/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 203.8534 - val_loss: 283.3715\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 176.1706 - val_loss: 145.7434\n",
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 161.0150 - val_loss: 144.7704\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 167.9937 - val_loss: 157.1005\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 294.7577 - val_loss: 238.1374\n",
      "Epoch 955/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 245.8241 - val_loss: 164.2404\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 187.0584 - val_loss: 185.7860\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.8972 - val_loss: 165.0203\n",
      "Epoch 958/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 166.7590 - val_loss: 146.1312\n",
      "Epoch 959/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 173.4310 - val_loss: 155.6328\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 158.6072 - val_loss: 157.2896\n",
      "Epoch 961/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 163.5372 - val_loss: 181.0319\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 282.1562 - val_loss: 218.0264\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.6629 - val_loss: 154.6493\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 162.0686 - val_loss: 164.1502\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 149.3672 - val_loss: 176.2408\n",
      "Epoch 966/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 181.2688 - val_loss: 189.3150\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.8838 - val_loss: 159.1754\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 198.6345 - val_loss: 167.8676\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.5390 - val_loss: 163.5541\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 169.5364 - val_loss: 143.1187\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 162.1810 - val_loss: 147.1401\n",
      "Epoch 972/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 175.1373 - val_loss: 152.2638\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 168.2640 - val_loss: 148.5816\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 175.6953 - val_loss: 163.4515\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 285.0432 - val_loss: 168.4817\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 169.1168 - val_loss: 267.1022\n",
      "Epoch 977/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.2544 - val_loss: 147.6277\n",
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.8718 - val_loss: 201.1790\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.8960 - val_loss: 148.8227\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 170.8047 - val_loss: 163.7307\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 255.8134 - val_loss: 156.0337\n",
      "Epoch 982/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 186.7635 - val_loss: 170.5394\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 161.4655 - val_loss: 164.0041\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.6700 - val_loss: 165.5731\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 161.0426 - val_loss: 158.6328\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 179.7750 - val_loss: 170.6275\n",
      "Epoch 987/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.8607 - val_loss: 147.0564\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 183.4258 - val_loss: 184.4317\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 168.9803 - val_loss: 284.1802\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.2501 - val_loss: 145.7768\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 238.5519 - val_loss: 149.9412\n",
      "Epoch 992/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 180.6544 - val_loss: 143.8819\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 159.7686 - val_loss: 159.1125\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 157.1926 - val_loss: 163.4348\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 171.6773 - val_loss: 147.0787\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 224.7848 - val_loss: 551.1702\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 191.2639 - val_loss: 143.0720\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 177.2741 - val_loss: 154.4427\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 147.0218 - val_loss: 158.0474\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 153.5008 - val_loss: 234.3552\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 168.7023 - val_loss: 165.8426\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 163.9903 - val_loss: 171.5575\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 177.0555 - val_loss: 170.8132\n",
      "Epoch 1004/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 149.6045 - val_loss: 165.7354\n",
      "Epoch 1005/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 176.2077 - val_loss: 164.5643\n",
      "Epoch 1006/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 207.7911 - val_loss: 2538.4085\n",
      "Epoch 1007/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 451.4222 - val_loss: 178.9066\n",
      "Epoch 1008/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 211.1344 - val_loss: 298.4757\n",
      "Epoch 1009/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 223.9364 - val_loss: 168.6049\n",
      "Epoch 1010/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 231.1802 - val_loss: 419.0863\n",
      "Epoch 1011/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 190.2603 - val_loss: 171.1740\n",
      "Epoch 1012/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 205.6288 - val_loss: 204.1017\n",
      "Epoch 1013/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 207.0004 - val_loss: 154.3481\n",
      "Epoch 1014/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 178.3056 - val_loss: 147.9058\n",
      "Epoch 1015/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 197.4830 - val_loss: 250.2989\n",
      "Epoch 1016/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 222.8360 - val_loss: 166.7587\n",
      "Epoch 1017/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 197.2228 - val_loss: 153.5323\n",
      "Epoch 1018/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 179.7191 - val_loss: 149.9344\n",
      "Epoch 1019/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 183.8870 - val_loss: 170.7882\n",
      "Epoch 1020/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 392.7114 - val_loss: 191.1387\n",
      "Epoch 1021/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 267.6427 - val_loss: 197.3411\n",
      "Epoch 1022/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 188.2537 - val_loss: 158.0282\n",
      "Epoch 1023/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 177.0482 - val_loss: 158.7280\n",
      "Epoch 1024/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 197.9555 - val_loss: 171.1811\n",
      "Epoch 1025/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 215.3612 - val_loss: 408.7514\n",
      "Epoch 1026/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 177.4720 - val_loss: 157.1155\n",
      "Epoch 1027/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 186.4397 - val_loss: 158.6302\n",
      "Epoch 1028/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 200.7690 - val_loss: 193.1669\n",
      "Epoch 1029/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 193.7199 - val_loss: 186.7895\n",
      "Epoch 1030/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 190.3535 - val_loss: 192.1218\n",
      "Epoch 1031/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 164.7127 - val_loss: 260.6392\n",
      "Epoch 1032/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 165.7478 - val_loss: 183.1612\n",
      "Epoch 1033/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 162.6871 - val_loss: 152.3889\n",
      "Epoch 1034/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 161.3191 - val_loss: 187.5206\n",
      "Epoch 1035/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 169.5612 - val_loss: 157.5326\n",
      "Epoch 1036/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 158.4911 - val_loss: 168.9165\n",
      "Epoch 1037/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 512.8364 - val_loss: 689.8541\n",
      "Epoch 1038/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 285.9272 - val_loss: 334.9284\n",
      "Epoch 1039/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 232.0750 - val_loss: 177.2825\n",
      "Epoch 1040/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 224.5370 - val_loss: 153.1383\n",
      "Epoch 1041/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 195.2653 - val_loss: 319.8754\n",
      "Epoch 1042/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 214.7508 - val_loss: 204.9703\n",
      "Epoch 1043/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 222.9810 - val_loss: 152.5356\n",
      "Epoch 1044/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 230.6123 - val_loss: 155.6726\n",
      "Epoch 1045/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 201.4371 - val_loss: 189.4035\n",
      "Epoch 1046/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 219.9933 - val_loss: 154.7973\n",
      "Epoch 1047/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 185.1510 - val_loss: 164.7995\n",
      "Epoch 1048/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 203.3282 - val_loss: 174.4773\n",
      "Epoch 1049/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 195.5500 - val_loss: 172.1157\n",
      "Epoch 1050/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 243.3803 - val_loss: 154.7504\n",
      "Epoch 1051/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 210.6844 - val_loss: 198.9450\n",
      "Epoch 1052/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 178.5990 - val_loss: 150.5102\n",
      "Epoch 1053/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 216.1349 - val_loss: 188.0260\n",
      "Epoch 1054/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 175.3463 - val_loss: 158.7926\n",
      "Epoch 1055/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 186.2471 - val_loss: 207.8384\n",
      "Epoch 1056/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 209.1150 - val_loss: 174.6993\n",
      "Epoch 1057/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 247.1038 - val_loss: 241.7870\n",
      "Epoch 1058/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 204.7795 - val_loss: 184.6680\n",
      "Epoch 1059/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 198.7617 - val_loss: 171.7703\n",
      "Epoch 1060/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 213.9651 - val_loss: 198.7619\n",
      "Epoch 1061/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 192.6448 - val_loss: 155.6207\n",
      "Epoch 1062/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 241.4166 - val_loss: 163.9288\n",
      "Epoch 1063/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 229.5851 - val_loss: 190.2596\n",
      "Epoch 1064/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 175.7481 - val_loss: 157.8389\n",
      "Epoch 1065/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 179.6439 - val_loss: 170.0210\n",
      "Epoch 1066/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 209.5758 - val_loss: 228.3635\n",
      "Epoch 1067/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 209.6460 - val_loss: 147.2101\n",
      "Epoch 1068/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 186.2705 - val_loss: 150.2735\n",
      "Epoch 1069/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 233.8338 - val_loss: 164.3114\n",
      "Epoch 1070/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 186.8647 - val_loss: 150.9182\n",
      "Epoch 1071/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 198.1009 - val_loss: 159.1519\n",
      "Epoch 1072/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 185.5208 - val_loss: 158.2071\n",
      "Epoch 1073/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 176.5450 - val_loss: 150.8915\n",
      "Epoch 1074/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 202.0044 - val_loss: 152.9778\n",
      "Epoch 1075/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 204.6540 - val_loss: 209.9901\n",
      "Epoch 1076/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 193.4205 - val_loss: 148.5473\n",
      "Epoch 1077/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 191.1536 - val_loss: 159.4485\n",
      "Epoch 1078/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 222.7213 - val_loss: 184.5835\n",
      "Epoch 1079/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 188.7010 - val_loss: 185.6507\n",
      "Epoch 1080/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 191.7367 - val_loss: 191.8188\n",
      "Epoch 1081/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 191.0918 - val_loss: 222.1234\n",
      "Epoch 1082/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 199.9484 - val_loss: 157.7712\n",
      "Epoch 1083/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 179.2702 - val_loss: 211.8170\n",
      "Epoch 1084/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 176.1973 - val_loss: 225.5284\n",
      "Epoch 1085/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 180.8069 - val_loss: 184.4909\n",
      "Epoch 1086/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 189.8867 - val_loss: 173.7828\n",
      "Epoch 1087/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.7996 - val_loss: 148.6278\n",
      "Epoch 1088/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 207.0925- ETA: 0s - loss: 215.10 - 0s 57us/step - loss: 205.8983 - val_loss: 175.4591\n",
      "Epoch 1089/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 171.4578 - val_loss: 161.5509\n",
      "Epoch 1090/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 192.1256 - val_loss: 175.1677\n",
      "Epoch 1091/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 187.5766 - val_loss: 150.3287\n",
      "Epoch 1092/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 170.0212 - val_loss: 186.1278\n",
      "Epoch 1093/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 195.8128 - val_loss: 157.7019\n",
      "Epoch 1094/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 210.8411 - val_loss: 186.9872\n",
      "Epoch 1095/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 171.1337 - val_loss: 173.7403\n",
      "Epoch 1096/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 190.1557 - val_loss: 197.1916\n",
      "Epoch 1097/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 186.1197 - val_loss: 200.3255\n",
      "Epoch 1098/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 165.1539 - val_loss: 149.1180\n",
      "Epoch 1099/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 169.9398 - val_loss: 219.5992\n",
      "Epoch 1100/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.7446 - val_loss: 152.0378\n",
      "Epoch 1101/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 172.5393 - val_loss: 175.6994\n",
      "Epoch 1102/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 180.3812 - val_loss: 148.2599\n",
      "Epoch 1103/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 168.0468 - val_loss: 166.9992\n",
      "Epoch 1104/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 244.9874 - val_loss: 170.8359\n",
      "Epoch 1105/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 162.9302 - val_loss: 165.2616\n",
      "Epoch 1106/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 188.7604 - val_loss: 219.5505\n",
      "Epoch 1107/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 224.9680 - val_loss: 195.2545\n",
      "Epoch 1108/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 181.3807 - val_loss: 161.0584\n",
      "Epoch 1109/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 191.6627 - val_loss: 176.0288\n",
      "Epoch 1110/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 172.6736 - val_loss: 223.0536\n",
      "Epoch 1111/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 197.7401 - val_loss: 178.4692- ETA: 0s - loss: 177\n",
      "Epoch 1112/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 185.4307 - val_loss: 154.2569\n",
      "Epoch 1113/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.3792 - val_loss: 173.7518\n",
      "Epoch 1114/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 163.6563 - val_loss: 200.5256\n",
      "Epoch 1115/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 162.8415 - val_loss: 192.1302\n",
      "Epoch 1116/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 210.8852 - val_loss: 162.8586\n",
      "Epoch 1117/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 171.4724 - val_loss: 171.1521\n",
      "Epoch 1118/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 168.8798 - val_loss: 212.2063\n",
      "Epoch 1119/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 164.5758 - val_loss: 156.6404\n",
      "Epoch 1120/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 171.1205 - val_loss: 190.6993\n",
      "Epoch 1121/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 173.7553 - val_loss: 153.3916\n",
      "Epoch 1122/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 183.9697 - val_loss: 202.9142\n",
      "Epoch 1123/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 171.5540 - val_loss: 362.5179\n",
      "Epoch 1124/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.2291 - val_loss: 149.8619\n",
      "Epoch 1125/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 180.6232 - val_loss: 304.7190\n",
      "Epoch 1126/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 174.0257 - val_loss: 176.7362\n",
      "Epoch 1127/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.7381 - val_loss: 164.9619\n",
      "Epoch 1128/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.2400 - val_loss: 248.6867\n",
      "Epoch 1129/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 203.1717 - val_loss: 277.7183\n",
      "Epoch 1130/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 165.7935 - val_loss: 203.3671\n",
      "Epoch 1131/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 178.7692 - val_loss: 240.9390\n",
      "Epoch 1132/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 314.6721 - val_loss: 177.6065\n",
      "Epoch 1133/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 160.5979 - val_loss: 248.9413\n",
      "Epoch 1134/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 184.4795 - val_loss: 167.8794\n",
      "Epoch 1135/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 168.8335 - val_loss: 168.7780\n",
      "Epoch 1136/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.7625 - val_loss: 172.1779\n",
      "Epoch 1137/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 190.5408 - val_loss: 156.9158\n",
      "Epoch 1138/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 158.3934 - val_loss: 160.2129\n",
      "Epoch 1139/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 155.5483 - val_loss: 158.1717\n",
      "Epoch 1140/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 168.0305 - val_loss: 187.0514\n",
      "Epoch 1141/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 163.2462 - val_loss: 213.5593\n",
      "Epoch 1142/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 167.4935 - val_loss: 160.6334\n",
      "Epoch 1143/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 153.3450 - val_loss: 165.8707\n",
      "Epoch 1144/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 241.6740 - val_loss: 197.4134\n",
      "Epoch 1145/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 162.4271 - val_loss: 160.3557\n",
      "Epoch 1146/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 147.1868 - val_loss: 156.4037\n",
      "Epoch 1147/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.4181 - val_loss: 169.8091\n",
      "Epoch 1148/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 155.0012 - val_loss: 159.1156\n",
      "Epoch 1149/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 207.5171 - val_loss: 161.6250\n",
      "Epoch 1150/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 164.0039 - val_loss: 186.7491\n",
      "Epoch 1151/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 167.1099 - val_loss: 226.6287\n",
      "Epoch 1152/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 357.9083 - val_loss: 558.4063\n",
      "Epoch 1153/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 502.8974 - val_loss: 424.0950\n",
      "Epoch 1154/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 390.7110 - val_loss: 345.2464\n",
      "Epoch 1155/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 364.6277 - val_loss: 266.3758\n",
      "Epoch 1156/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 337.0702 - val_loss: 279.9825\n",
      "Epoch 1157/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 211.9030 - val_loss: 198.5789\n",
      "Epoch 1158/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 168.7974 - val_loss: 159.5145\n",
      "Epoch 1159/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 164.9195 - val_loss: 162.5022\n",
      "Epoch 1160/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 166.7760 - val_loss: 183.0590\n",
      "Epoch 1161/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 160.7573 - val_loss: 164.8515\n",
      "Epoch 1162/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 268.6972 - val_loss: 238.0737\n",
      "Epoch 1163/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 311.6709 - val_loss: 208.7744\n",
      "Epoch 1164/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 245.7171 - val_loss: 234.5205\n",
      "Epoch 1165/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 257.0784 - val_loss: 232.1431\n",
      "Epoch 1166/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 223.7636 - val_loss: 192.5645\n",
      "Epoch 1167/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 57us/step - loss: 249.2426 - val_loss: 179.8305\n",
      "Epoch 1168/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 195.6693 - val_loss: 232.4981\n",
      "Epoch 1169/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 252.9597 - val_loss: 257.4139\n",
      "Epoch 1170/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 232.0228 - val_loss: 172.9768\n",
      "Epoch 1171/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 209.5829 - val_loss: 171.7955\n",
      "Epoch 1172/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 197.4363 - val_loss: 179.4115\n",
      "Epoch 1173/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 214.7219 - val_loss: 218.2527\n",
      "Epoch 1174/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 267.4806 - val_loss: 204.4091\n",
      "Epoch 1175/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 205.9719 - val_loss: 176.9216\n",
      "Epoch 1176/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 199.1147 - val_loss: 171.1025\n",
      "Epoch 1177/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 206.4808 - val_loss: 202.4257\n",
      "Epoch 1178/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 218.0824 - val_loss: 240.2845\n",
      "Epoch 1179/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 202.6614 - val_loss: 162.0628\n",
      "Epoch 1180/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 174.8021 - val_loss: 252.6111\n",
      "Epoch 1181/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 192.4410 - val_loss: 157.9212\n",
      "Epoch 1182/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 207.2375 - val_loss: 164.5435\n",
      "Epoch 1183/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 212.8579 - val_loss: 200.8614\n",
      "Epoch 1184/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 196.9995 - val_loss: 152.3440\n",
      "Epoch 1185/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 187.6265 - val_loss: 178.4545\n",
      "Epoch 1186/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 193.3624 - val_loss: 182.8599\n",
      "Epoch 1187/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 226.5342 - val_loss: 277.5304\n",
      "Epoch 1188/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 182.3338 - val_loss: 207.7757\n",
      "Epoch 1189/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 187.6344 - val_loss: 170.2325\n",
      "Epoch 1190/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 193.2984 - val_loss: 157.5407\n",
      "Epoch 1191/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 167.3073 - val_loss: 158.7549\n",
      "Epoch 1192/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 280.1856 - val_loss: 370.8643\n",
      "Epoch 1193/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 209.7823 - val_loss: 194.6654\n",
      "Epoch 1194/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 202.4516 - val_loss: 182.5937\n",
      "Epoch 1195/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 184.9003 - val_loss: 165.5281\n",
      "Epoch 1196/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 208.4897 - val_loss: 159.7815\n",
      "Epoch 1197/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 258.1879 - val_loss: 161.8283\n",
      "Epoch 1198/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 210.1979 - val_loss: 271.1508\n",
      "Epoch 1199/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 225.1702 - val_loss: 198.9990\n",
      "Epoch 1200/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 232.4332 - val_loss: 216.6616\n",
      "Epoch 1201/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 198.7390 - val_loss: 191.8456\n",
      "Epoch 1202/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 212.8421 - val_loss: 163.0156\n",
      "Epoch 1203/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 211.3443 - val_loss: 173.5607\n",
      "Epoch 1204/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 183.7444 - val_loss: 154.4905\n",
      "Epoch 1205/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 200.2368 - val_loss: 193.1485\n",
      "Epoch 1206/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 194.0239 - val_loss: 166.3570\n",
      "Epoch 1207/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 210.7216 - val_loss: 159.7302\n",
      "Epoch 1208/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 179.4281 - val_loss: 174.3655\n",
      "Epoch 1209/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 225.6610 - val_loss: 238.3970\n",
      "Epoch 1210/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 266.3147 - val_loss: 240.8139\n",
      "Epoch 1211/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 207.2833 - val_loss: 268.5774\n",
      "Epoch 1212/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 181.4506 - val_loss: 178.7195\n",
      "Epoch 1213/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 215.5773 - val_loss: 177.0420\n",
      "Epoch 1214/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 173.3859 - val_loss: 169.5930\n",
      "Epoch 1215/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 186.7053 - val_loss: 165.5663\n",
      "Epoch 1216/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.3461 - val_loss: 179.7581\n",
      "Epoch 1217/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 195.5812 - val_loss: 204.0091\n",
      "Epoch 1218/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 209.7816 - val_loss: 184.0564\n",
      "Epoch 1219/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 185.2420 - val_loss: 147.8664\n",
      "Epoch 1220/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 179.3576 - val_loss: 159.4438\n",
      "Epoch 1221/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 200.4775 - val_loss: 194.7099\n",
      "Epoch 1222/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 186.0016 - val_loss: 239.3551\n",
      "Epoch 1223/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 172.8268 - val_loss: 146.3327\n",
      "Epoch 1224/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 236.1254 - val_loss: 208.7832\n",
      "Epoch 1225/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 169.5892 - val_loss: 150.6412\n",
      "Epoch 1226/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 183.2827 - val_loss: 171.0392\n",
      "Epoch 1227/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 210.8726 - val_loss: 167.5133\n",
      "Epoch 1228/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 157.7562 - val_loss: 158.3909\n",
      "Epoch 1229/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.2647 - val_loss: 373.5994\n",
      "Epoch 1230/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 220.1553 - val_loss: 157.1632\n",
      "Epoch 1231/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.6548 - val_loss: 162.2608\n",
      "Epoch 1232/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 168.1556 - val_loss: 174.2241\n",
      "Epoch 1233/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 163.2195 - val_loss: 162.0329\n",
      "Epoch 1234/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 176.5169 - val_loss: 156.5848\n",
      "Epoch 1235/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 174.6469 - val_loss: 190.2221\n",
      "Epoch 1236/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 192.6198 - val_loss: 181.1631\n",
      "Epoch 1237/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 262.8457 - val_loss: 165.7690\n",
      "Epoch 1238/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 172.6679 - val_loss: 161.7608\n",
      "Epoch 1239/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 184.4488 - val_loss: 151.4672\n",
      "Epoch 1240/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 162.3987 - val_loss: 192.4764\n",
      "Epoch 1241/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 185.5186 - val_loss: 163.3799\n",
      "Epoch 1242/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 181.7888 - val_loss: 154.3622\n",
      "Epoch 1243/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 161.5469 - val_loss: 197.8428\n",
      "Epoch 1244/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 171.5562 - val_loss: 147.3867\n",
      "Epoch 1245/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.5273 - val_loss: 159.2371\n",
      "Epoch 1246/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 176.4540 - val_loss: 163.3524\n",
      "Epoch 1247/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 160.3120 - val_loss: 173.0312\n",
      "Epoch 1248/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 202.9161 - val_loss: 161.8453\n",
      "Epoch 1249/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 229.0215 - val_loss: 186.6727\n",
      "Epoch 1250/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.3100 - val_loss: 180.7485\n",
      "Epoch 1251/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 187.6345 - val_loss: 190.7135\n",
      "Epoch 1252/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 190.2400 - val_loss: 162.0761\n",
      "Epoch 1253/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 162.2596 - val_loss: 157.7784\n",
      "Epoch 1254/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 204.7273 - val_loss: 148.8291\n",
      "Epoch 1255/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 171.7187 - val_loss: 194.3912\n",
      "Epoch 1256/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 160.9671 - val_loss: 165.5489\n",
      "Epoch 1257/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 188.0024 - val_loss: 194.4584\n",
      "Epoch 1258/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 167.7379 - val_loss: 178.5201\n",
      "Epoch 1259/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 172.5127 - val_loss: 161.1184\n",
      "Epoch 1260/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 227.7510 - val_loss: 150.1122\n",
      "Epoch 1261/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 170.5476 - val_loss: 158.6202\n",
      "Epoch 1262/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 162.2311 - val_loss: 154.7558\n",
      "Epoch 1263/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 159.2741 - val_loss: 217.6850\n",
      "Epoch 1264/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 193.6133 - val_loss: 172.3307\n",
      "Epoch 1265/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.2192 - val_loss: 160.0601\n",
      "Epoch 1266/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 152.3173 - val_loss: 153.8274\n",
      "Epoch 1267/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 184.6009 - val_loss: 163.0450\n",
      "Epoch 1268/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.6545 - val_loss: 274.3842\n",
      "Epoch 1269/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 196.8544 - val_loss: 149.5234\n",
      "Epoch 1270/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 156.3159 - val_loss: 168.8431\n",
      "Epoch 1271/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.3445 - val_loss: 179.6628\n",
      "Epoch 1272/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 205.1445 - val_loss: 163.5967\n",
      "Epoch 1273/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 169.0402 - val_loss: 153.5769\n",
      "Epoch 1274/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 240.5815 - val_loss: 469.1121\n",
      "Epoch 1275/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 179.4317 - val_loss: 162.8041\n",
      "Epoch 1276/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.1460 - val_loss: 267.4306\n",
      "Epoch 1277/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 157.3707 - val_loss: 158.4682\n",
      "Epoch 1278/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.2618 - val_loss: 148.9710\n",
      "Epoch 1279/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 163.9075 - val_loss: 270.3763\n",
      "Epoch 1280/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.5267 - val_loss: 310.5580\n",
      "Epoch 1281/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 171.4239 - val_loss: 154.9248\n",
      "Epoch 1282/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 174.2826 - val_loss: 166.6684\n",
      "Epoch 1283/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 162.2592 - val_loss: 146.4776\n",
      "Epoch 1284/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.3646 - val_loss: 176.2430\n",
      "Epoch 1285/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 164.5367 - val_loss: 149.5953\n",
      "Epoch 1286/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 156.2506 - val_loss: 173.8168\n",
      "Epoch 1287/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 172.0633 - val_loss: 150.4925\n",
      "Epoch 1288/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 166.6126 - val_loss: 217.4677\n",
      "Epoch 1289/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 191.9525 - val_loss: 273.9220\n",
      "Epoch 1290/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 180.5123 - val_loss: 145.0781\n",
      "Epoch 1291/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 157.4678 - val_loss: 144.9464\n",
      "Epoch 1292/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.1495 - val_loss: 153.4617\n",
      "Epoch 1293/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 187.2616 - val_loss: 152.1236\n",
      "Epoch 1294/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 164.4833 - val_loss: 147.9047\n",
      "Epoch 1295/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 209.3469 - val_loss: 163.2882\n",
      "Epoch 1296/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 169.6986 - val_loss: 188.1401\n",
      "Epoch 1297/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 187.1999 - val_loss: 142.7604\n",
      "Epoch 1298/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 208.7588 - val_loss: 427.1785\n",
      "Epoch 1299/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 159.3946 - val_loss: 159.5951\n",
      "Epoch 1300/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 167.0681 - val_loss: 202.5381\n",
      "Epoch 1301/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 171.1862 - val_loss: 156.7406\n",
      "Epoch 1302/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 175.3898 - val_loss: 191.4053\n",
      "Epoch 1303/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 159.2550 - val_loss: 207.6061\n",
      "Epoch 1304/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 184.8162 - val_loss: 162.2985\n",
      "Epoch 1305/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 214.3478 - val_loss: 192.2149\n",
      "Epoch 1306/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 167.7406 - val_loss: 241.8464\n",
      "Epoch 1307/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.0445 - val_loss: 154.7772\n",
      "Epoch 1308/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 156.6474 - val_loss: 183.5812\n",
      "Epoch 1309/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.1374 - val_loss: 144.0401\n",
      "Epoch 1310/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 162.4804 - val_loss: 194.9058\n",
      "Epoch 1311/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 79us/step - loss: 180.2430 - val_loss: 154.9709\n",
      "Epoch 1312/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 150.4468 - val_loss: 168.0759\n",
      "Epoch 1313/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 193.0789 - val_loss: 275.1222\n",
      "Epoch 1314/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 179.6598 - val_loss: 214.1135\n",
      "Epoch 1315/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 168.2423 - val_loss: 151.2736\n",
      "Epoch 1316/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 159.6997 - val_loss: 173.9817\n",
      "Epoch 1317/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.2466 - val_loss: 162.0038\n",
      "Epoch 1318/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.1598 - val_loss: 146.4879\n",
      "Epoch 1319/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.3792 - val_loss: 148.1033\n",
      "Epoch 1320/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 175.5817 - val_loss: 160.2083\n",
      "Epoch 1321/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.2503 - val_loss: 167.5249\n",
      "Epoch 1322/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 156.7203 - val_loss: 154.7101\n",
      "Epoch 1323/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 146.0988 - val_loss: 146.3423\n",
      "Epoch 1324/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 182.5350 - val_loss: 158.7329\n",
      "Epoch 1325/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 204.5337 - val_loss: 148.7748\n",
      "Epoch 1326/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 169.1539 - val_loss: 173.3486\n",
      "Epoch 1327/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 149.4504 - val_loss: 159.0587\n",
      "Epoch 1328/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.6292 - val_loss: 159.8582\n",
      "Epoch 1329/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 180.3103 - val_loss: 159.0435\n",
      "Epoch 1330/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.4366 - val_loss: 212.7645\n",
      "Epoch 1331/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 170.9078 - val_loss: 184.1650\n",
      "Epoch 1332/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 171.6386 - val_loss: 163.8657\n",
      "Epoch 1333/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 168.8070 - val_loss: 146.0657\n",
      "Epoch 1334/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 154.1839 - val_loss: 211.8871\n",
      "Epoch 1335/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 147.9803 - val_loss: 153.7261\n",
      "Epoch 1336/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 171.2548 - val_loss: 159.7805\n",
      "Epoch 1337/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 176.9028 - val_loss: 181.6436\n",
      "Epoch 1338/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 170.0121 - val_loss: 158.3869\n",
      "Epoch 1339/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 168.4199 - val_loss: 148.8165\n",
      "Epoch 1340/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 160.9306 - val_loss: 177.5937\n",
      "Epoch 1341/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 167.7494 - val_loss: 233.4677\n",
      "Epoch 1342/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 173.4093 - val_loss: 188.5886\n",
      "Epoch 1343/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 167.8209 - val_loss: 145.9217\n",
      "Epoch 1344/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 190.0271 - val_loss: 176.4691\n",
      "Epoch 1345/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 164.5806 - val_loss: 257.7516\n",
      "Epoch 1346/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.3813 - val_loss: 194.0292\n",
      "Epoch 1347/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.2935 - val_loss: 142.9700\n",
      "Epoch 1348/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.5052 - val_loss: 169.8712\n",
      "Epoch 1349/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 173.3750 - val_loss: 290.7302\n",
      "Epoch 1350/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.1153 - val_loss: 161.7957\n",
      "Epoch 1351/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.7973 - val_loss: 184.1429\n",
      "Epoch 1352/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.1900 - val_loss: 155.2056\n",
      "Epoch 1353/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 183.2134 - val_loss: 195.0831\n",
      "Epoch 1354/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 159.4534 - val_loss: 148.5119\n",
      "Epoch 1355/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.1147 - val_loss: 172.7905\n",
      "Epoch 1356/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 175.9549 - val_loss: 180.9483\n",
      "Epoch 1357/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 177.9031 - val_loss: 158.9342\n",
      "Epoch 1358/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 171.2065 - val_loss: 143.9301\n",
      "Epoch 1359/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 234.6696 - val_loss: 160.8828\n",
      "Epoch 1360/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.4944 - val_loss: 176.5066\n",
      "Epoch 1361/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.6800 - val_loss: 147.6833\n",
      "Epoch 1362/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.9892 - val_loss: 146.6398\n",
      "Epoch 1363/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 161.3537 - val_loss: 155.0339\n",
      "Epoch 1364/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 148.2136 - val_loss: 167.8712\n",
      "Epoch 1365/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.8313 - val_loss: 176.4423\n",
      "Epoch 1366/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 162.0552 - val_loss: 149.5516\n",
      "Epoch 1367/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 158.3965 - val_loss: 149.8716\n",
      "Epoch 1368/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 177.9036 - val_loss: 198.1314\n",
      "Epoch 1369/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 180.7501 - val_loss: 145.7654\n",
      "Epoch 1370/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.5182 - val_loss: 153.5270\n",
      "Epoch 1371/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 191.1054 - val_loss: 165.7894\n",
      "Epoch 1372/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 147.4596 - val_loss: 146.7592\n",
      "Epoch 1373/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 163.9098 - val_loss: 144.6732\n",
      "Epoch 1374/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.3069 - val_loss: 165.6219\n",
      "Epoch 1375/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 175.1287 - val_loss: 152.0179\n",
      "Epoch 1376/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 155.5226 - val_loss: 145.2840\n",
      "Epoch 1377/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 173.8659 - val_loss: 159.6405\n",
      "Epoch 1378/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.7366 - val_loss: 257.6796\n",
      "Epoch 1379/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 155.6897 - val_loss: 208.0129\n",
      "Epoch 1380/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 171.0607 - val_loss: 164.1903\n",
      "Epoch 1381/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 194.3884 - val_loss: 205.0431\n",
      "Epoch 1382/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.9821 - val_loss: 156.3676\n",
      "Epoch 1383/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step - loss: 150.1686 - val_loss: 172.8680\n",
      "Epoch 1384/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.2747 - val_loss: 145.0091\n",
      "Epoch 1385/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 156.6609 - val_loss: 162.7866\n",
      "Epoch 1386/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 186.1350 - val_loss: 167.4843\n",
      "Epoch 1387/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 155.2943 - val_loss: 145.4784\n",
      "Epoch 1388/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 168.1747 - val_loss: 167.4292\n",
      "Epoch 1389/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 159.3186 - val_loss: 169.6925\n",
      "Epoch 1390/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 252.9572 - val_loss: 177.8217\n",
      "Epoch 1391/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 206.4200 - val_loss: 210.3521\n",
      "Epoch 1392/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 205.4688 - val_loss: 201.3188\n",
      "Epoch 1393/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 191.0232 - val_loss: 186.6426\n",
      "Epoch 1394/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 177.8230 - val_loss: 162.4150\n",
      "Epoch 1395/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 170.2968 - val_loss: 165.3910\n",
      "Epoch 1396/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 175.7179 - val_loss: 267.4389\n",
      "Epoch 1397/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 178.3300 - val_loss: 181.6385\n",
      "Epoch 1398/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 183.1143 - val_loss: 211.0627\n",
      "Epoch 1399/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 168.2612 - val_loss: 175.4954\n",
      "Epoch 1400/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 182.1779 - val_loss: 185.0541\n",
      "Epoch 1401/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 173.6640 - val_loss: 250.9568\n",
      "Epoch 1402/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 188.3262 - val_loss: 154.8907\n",
      "Epoch 1403/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 157.7455 - val_loss: 147.2827\n",
      "Epoch 1404/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 172.0573 - val_loss: 171.9052\n",
      "Epoch 1405/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 155.8759 - val_loss: 148.4365\n",
      "Epoch 1406/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 179.6277 - val_loss: 179.0880\n",
      "Epoch 1407/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 278.6651 - val_loss: 227.7998\n",
      "Epoch 1408/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 224.5876 - val_loss: 164.5791\n",
      "Epoch 1409/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 196.6662 - val_loss: 154.8045\n",
      "Epoch 1410/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 166.1170 - val_loss: 164.5277\n",
      "Epoch 1411/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.5386 - val_loss: 176.8850\n",
      "Epoch 1412/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.9463 - val_loss: 157.8064\n",
      "Epoch 1413/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 170.7954 - val_loss: 194.3110\n",
      "Epoch 1414/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 160.4117 - val_loss: 161.6273\n",
      "Epoch 1415/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.1533 - val_loss: 159.9793\n",
      "Epoch 1416/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 160.3691 - val_loss: 161.7451\n",
      "Epoch 1417/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 180.7145 - val_loss: 182.0560\n",
      "Epoch 1418/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 157.7199 - val_loss: 285.7705\n",
      "Epoch 1419/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.1888 - val_loss: 177.7145\n",
      "Epoch 1420/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 186.1161 - val_loss: 169.6669\n",
      "Epoch 1421/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 169.9514 - val_loss: 158.3673\n",
      "Epoch 1422/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 167.1710 - val_loss: 240.2846\n",
      "Epoch 1423/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 166.5756 - val_loss: 148.2533\n",
      "Epoch 1424/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 162.2851 - val_loss: 155.5991\n",
      "Epoch 1425/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.5046 - val_loss: 152.1106\n",
      "Epoch 1426/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.7859 - val_loss: 172.7715\n",
      "Epoch 1427/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.2541 - val_loss: 147.2960\n",
      "Epoch 1428/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 151.3851 - val_loss: 148.7620\n",
      "Epoch 1429/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 155.5132 - val_loss: 176.6873\n",
      "Epoch 1430/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 155.1703 - val_loss: 150.0878\n",
      "Epoch 1431/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 158.0885 - val_loss: 241.3838\n",
      "Epoch 1432/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 150.5755 - val_loss: 163.1791\n",
      "Epoch 1433/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 162.4169 - val_loss: 151.1557\n",
      "Epoch 1434/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 155.2063 - val_loss: 148.4067\n",
      "Epoch 1435/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 147.8351 - val_loss: 176.1534\n",
      "Epoch 1436/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 158.7599 - val_loss: 208.9181\n",
      "Epoch 1437/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.4829 - val_loss: 144.6062\n",
      "Epoch 1438/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.9289 - val_loss: 155.3261\n",
      "Epoch 1439/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.2946 - val_loss: 171.5761\n",
      "Epoch 1440/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.5155 - val_loss: 146.2455\n",
      "Epoch 1441/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 165.2126 - val_loss: 172.6288\n",
      "Epoch 1442/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 144.3848 - val_loss: 172.2369\n",
      "Epoch 1443/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 155.2404 - val_loss: 206.3589\n",
      "Epoch 1444/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.8287 - val_loss: 141.3897\n",
      "Epoch 1445/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.9739 - val_loss: 204.2751\n",
      "Epoch 1446/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 153.4917 - val_loss: 147.4492\n",
      "Epoch 1447/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 140.5478 - val_loss: 196.6173\n",
      "Epoch 1448/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.3015 - val_loss: 155.3566\n",
      "Epoch 1449/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 155.1899 - val_loss: 154.1015\n",
      "Epoch 1450/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.7361 - val_loss: 147.8915\n",
      "Epoch 1451/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 165.2979 - val_loss: 203.4534\n",
      "Epoch 1452/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.2483 - val_loss: 159.5926\n",
      "Epoch 1453/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 158.1931 - val_loss: 213.1495\n",
      "Epoch 1454/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 160.4029 - val_loss: 147.1526\n",
      "Epoch 1455/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.3488 - val_loss: 157.5262\n",
      "Epoch 1456/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 151.5965 - val_loss: 142.1093\n",
      "Epoch 1457/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 146.9038 - val_loss: 157.6488\n",
      "Epoch 1458/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 155.3643 - val_loss: 164.1584\n",
      "Epoch 1459/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 143.3491 - val_loss: 142.8361\n",
      "Epoch 1460/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 148.4138 - val_loss: 187.6749\n",
      "Epoch 1461/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.6719 - val_loss: 165.5780\n",
      "Epoch 1462/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 245.4786 - val_loss: 153.7972\n",
      "Epoch 1463/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 143.7607 - val_loss: 162.3325\n",
      "Epoch 1464/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 147.3919 - val_loss: 176.7872\n",
      "Epoch 1465/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 152.2806 - val_loss: 150.4925\n",
      "Epoch 1466/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.5315 - val_loss: 156.1583\n",
      "Epoch 1467/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 171.8927 - val_loss: 220.9082\n",
      "Epoch 1468/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 160.1650 - val_loss: 165.3039\n",
      "Epoch 1469/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 142.2153 - val_loss: 142.1601\n",
      "Epoch 1470/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 140.3343 - val_loss: 176.3034\n",
      "Epoch 1471/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 150.0456 - val_loss: 166.9480\n",
      "Epoch 1472/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 174.2367 - val_loss: 263.3918\n",
      "Epoch 1473/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 144.4080 - val_loss: 145.3023\n",
      "Epoch 1474/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 141.9169 - val_loss: 147.4420\n",
      "Epoch 1475/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 146.1929 - val_loss: 199.4035\n",
      "Epoch 1476/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.2454 - val_loss: 146.7971\n",
      "Epoch 1477/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 169.7608 - val_loss: 173.1435\n",
      "Epoch 1478/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 143.1758 - val_loss: 147.9069\n",
      "Epoch 1479/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.6777 - val_loss: 226.7527\n",
      "Epoch 1480/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 160.3270 - val_loss: 189.5084\n",
      "Epoch 1481/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 152.2407 - val_loss: 177.7565\n",
      "Epoch 1482/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.3551 - val_loss: 209.6174\n",
      "Epoch 1483/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.3555 - val_loss: 169.1866\n",
      "Epoch 1484/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 147.3827 - val_loss: 148.9657\n",
      "Epoch 1485/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 155.4814 - val_loss: 150.1732\n",
      "Epoch 1486/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 148.5958 - val_loss: 149.3098\n",
      "Epoch 1487/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 159.9767 - val_loss: 236.5666\n",
      "Epoch 1488/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.1522 - val_loss: 164.0860\n",
      "Epoch 1489/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 172.0682 - val_loss: 180.4024\n",
      "Epoch 1490/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.1009 - val_loss: 156.9293\n",
      "Epoch 1491/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.3500 - val_loss: 143.9532\n",
      "Epoch 1492/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.0816 - val_loss: 149.1969\n",
      "Epoch 1493/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 154.8470 - val_loss: 144.0636\n",
      "Epoch 1494/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 154.2645 - val_loss: 173.4248\n",
      "Epoch 1495/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 142.2337 - val_loss: 149.1931\n",
      "Epoch 1496/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 145.6390 - val_loss: 143.8212\n",
      "Epoch 1497/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.4342 - val_loss: 167.7248\n",
      "Epoch 1498/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 164.7211 - val_loss: 174.0224\n",
      "Epoch 1499/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 151.8383 - val_loss: 150.2921\n",
      "Epoch 1500/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 147.7687 - val_loss: 177.0440\n",
      "Epoch 1501/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.0086 - val_loss: 143.1427\n",
      "Epoch 1502/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 149.7424 - val_loss: 149.3855\n",
      "Epoch 1503/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 146.8112 - val_loss: 162.1929\n",
      "Epoch 1504/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 160.4879 - val_loss: 141.4545\n",
      "Epoch 1505/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.6840 - val_loss: 249.9307\n",
      "Epoch 1506/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 156.9591 - val_loss: 153.2488\n",
      "Epoch 1507/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 153.5235 - val_loss: 153.1179\n",
      "Epoch 1508/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 141.5608 - val_loss: 184.2430\n",
      "Epoch 1509/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 138.0089 - val_loss: 175.9214\n",
      "Epoch 1510/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.8245 - val_loss: 160.2155\n",
      "Epoch 1511/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.2514 - val_loss: 273.7575\n",
      "Epoch 1512/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 154.4466 - val_loss: 144.0011\n",
      "Epoch 1513/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 177.0272 - val_loss: 156.3158\n",
      "Epoch 1514/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 162.1287 - val_loss: 159.1022\n",
      "Epoch 1515/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.1345 - val_loss: 179.7892\n",
      "Epoch 1516/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 185.5848 - val_loss: 177.7111\n",
      "Epoch 1517/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 234.9766 - val_loss: 165.2107\n",
      "Epoch 1518/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.1674 - val_loss: 152.6038\n",
      "Epoch 1519/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 169.9083 - val_loss: 154.2532\n",
      "Epoch 1520/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 158.1899 - val_loss: 169.8285\n",
      "Epoch 1521/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 179.3476 - val_loss: 207.0269\n",
      "Epoch 1522/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.7399 - val_loss: 216.6406\n",
      "Epoch 1523/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 173.1323 - val_loss: 164.9894\n",
      "Epoch 1524/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.0327 - val_loss: 248.3396\n",
      "Epoch 1525/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 164.9083 - val_loss: 139.6811\n",
      "Epoch 1526/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 188.4495 - val_loss: 152.9897\n",
      "Epoch 1527/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 148.3258 - val_loss: 140.1202\n",
      "Epoch 1528/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 184.4576 - val_loss: 147.9342\n",
      "Epoch 1529/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 146.2208 - val_loss: 138.2667\n",
      "Epoch 1530/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 186.2025 - val_loss: 144.2654\n",
      "Epoch 1531/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.9477 - val_loss: 150.0856\n",
      "Epoch 1532/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.5615 - val_loss: 147.2166\n",
      "Epoch 1533/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 143.8302 - val_loss: 149.7369\n",
      "Epoch 1534/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 172.9563 - val_loss: 144.2452\n",
      "Epoch 1535/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 152.9489 - val_loss: 143.5086\n",
      "Epoch 1536/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 144.4643 - val_loss: 185.7291\n",
      "Epoch 1537/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 141.2204 - val_loss: 190.3632\n",
      "Epoch 1538/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 151.5185 - val_loss: 171.0744\n",
      "Epoch 1539/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.7769 - val_loss: 152.3765\n",
      "Epoch 1540/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 150.7653 - val_loss: 165.5755\n",
      "Epoch 1541/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 166.6326 - val_loss: 223.9428\n",
      "Epoch 1542/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 146.4477 - val_loss: 262.2852\n",
      "Epoch 1543/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 167.4937 - val_loss: 155.3177\n",
      "Epoch 1544/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.3313 - val_loss: 171.3054\n",
      "Epoch 1545/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 164.5087 - val_loss: 145.4895\n",
      "Epoch 1546/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 168.2642 - val_loss: 143.8334\n",
      "Epoch 1547/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.2009 - val_loss: 148.8526\n",
      "Epoch 1548/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 140.9831 - val_loss: 172.3378\n",
      "Epoch 1549/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 190.8859 - val_loss: 176.5272\n",
      "Epoch 1550/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.9034 - val_loss: 168.7019\n",
      "Epoch 1551/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 191.7612 - val_loss: 185.5247\n",
      "Epoch 1552/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 216.2429 - val_loss: 306.1716\n",
      "Epoch 1553/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 155.9470 - val_loss: 150.1497\n",
      "Epoch 1554/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 148.1768 - val_loss: 146.9221\n",
      "Epoch 1555/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.1667 - val_loss: 207.4888\n",
      "Epoch 1556/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 167.7262 - val_loss: 220.5760\n",
      "Epoch 1557/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 163.6873 - val_loss: 167.2869\n",
      "Epoch 1558/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 160.7197 - val_loss: 142.5271\n",
      "Epoch 1559/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.9324 - val_loss: 156.5948\n",
      "Epoch 1560/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.5739 - val_loss: 149.4801\n",
      "Epoch 1561/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.7157 - val_loss: 149.0521\n",
      "Epoch 1562/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 140.0924 - val_loss: 147.7480\n",
      "Epoch 1563/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.3809 - val_loss: 151.5322\n",
      "Epoch 1564/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 177.6001 - val_loss: 146.9689\n",
      "Epoch 1565/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 142.3842 - val_loss: 159.6396\n",
      "Epoch 1566/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 168.6008 - val_loss: 174.4734\n",
      "Epoch 1567/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 158.0806 - val_loss: 178.7176\n",
      "Epoch 1568/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 162.6765 - val_loss: 172.4588\n",
      "Epoch 1569/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 162.7539 - val_loss: 158.1798\n",
      "Epoch 1570/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.0810 - val_loss: 165.4467\n",
      "Epoch 1571/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 156.9174 - val_loss: 156.1063\n",
      "Epoch 1572/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.8053 - val_loss: 143.2341\n",
      "Epoch 1573/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.0422 - val_loss: 168.9158\n",
      "Epoch 1574/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 152.3198 - val_loss: 143.0663\n",
      "Epoch 1575/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 139.1769 - val_loss: 139.5731\n",
      "Epoch 1576/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.3402 - val_loss: 161.2643\n",
      "Epoch 1577/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 160.4474 - val_loss: 187.2531\n",
      "Epoch 1578/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.7067 - val_loss: 148.5853\n",
      "Epoch 1579/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 147.9914 - val_loss: 160.6144\n",
      "Epoch 1580/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 222.8433 - val_loss: 186.4756\n",
      "Epoch 1581/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.1892 - val_loss: 160.5719\n",
      "Epoch 1582/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.2595 - val_loss: 149.6959\n",
      "Epoch 1583/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.2877 - val_loss: 142.9991\n",
      "Epoch 1584/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 157.5337 - val_loss: 209.3302\n",
      "Epoch 1585/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 165.7192 - val_loss: 162.6125\n",
      "Epoch 1586/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.1693 - val_loss: 141.6352\n",
      "Epoch 1587/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.4054 - val_loss: 155.1234\n",
      "Epoch 1588/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 159.3426 - val_loss: 224.3564\n",
      "Epoch 1589/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 167.3856 - val_loss: 144.9161\n",
      "Epoch 1590/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 145.6951 - val_loss: 155.0803\n",
      "Epoch 1591/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 186.1803 - val_loss: 156.4305\n",
      "Epoch 1592/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 174.5019 - val_loss: 231.9946\n",
      "Epoch 1593/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 169.9387 - val_loss: 265.2841\n",
      "Epoch 1594/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 170.5944 - val_loss: 152.8521\n",
      "Epoch 1595/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.2532 - val_loss: 149.1276\n",
      "Epoch 1596/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 165.1687 - val_loss: 156.3991\n",
      "Epoch 1597/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 154.3456 - val_loss: 260.1641\n",
      "Epoch 1598/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 165.3995 - val_loss: 193.9315\n",
      "Epoch 1599/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 84us/step - loss: 155.4740 - val_loss: 146.9101\n",
      "Epoch 1600/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.4291 - val_loss: 183.0324\n",
      "Epoch 1601/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.6243 - val_loss: 147.2197\n",
      "Epoch 1602/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 153.3358 - val_loss: 149.4006\n",
      "Epoch 1603/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 164.4277 - val_loss: 148.1940\n",
      "Epoch 1604/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 150.2539 - val_loss: 147.0955\n",
      "Epoch 1605/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.3042 - val_loss: 183.8513\n",
      "Epoch 1606/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 185.1325 - val_loss: 194.7702\n",
      "Epoch 1607/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.1693 - val_loss: 146.6077\n",
      "Epoch 1608/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 162.4469 - val_loss: 153.0719\n",
      "Epoch 1609/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 152.2323 - val_loss: 140.7690\n",
      "Epoch 1610/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 181.1890 - val_loss: 167.4878\n",
      "Epoch 1611/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 180.3126 - val_loss: 183.5821\n",
      "Epoch 1612/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 189.0474 - val_loss: 266.3844\n",
      "Epoch 1613/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 142.9736 - val_loss: 151.8692\n",
      "Epoch 1614/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 165.0792 - val_loss: 235.5199\n",
      "Epoch 1615/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 154.7808 - val_loss: 194.3414\n",
      "Epoch 1616/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.4544 - val_loss: 153.7228\n",
      "Epoch 1617/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 161.6688 - val_loss: 144.0651\n",
      "Epoch 1618/10000\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 153.9910 - val_loss: 143.8956\n",
      "Epoch 1619/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 142.5853 - val_loss: 215.0324\n",
      "Epoch 1620/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.0940 - val_loss: 197.4885\n",
      "Epoch 1621/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 166.8039 - val_loss: 181.7977\n",
      "Epoch 1622/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 164.1984 - val_loss: 173.5948\n",
      "Epoch 1623/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 143.6142 - val_loss: 141.1227\n",
      "Epoch 1624/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 153.6344 - val_loss: 168.8691\n",
      "Epoch 1625/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 169.6809 - val_loss: 202.4254\n",
      "Epoch 1626/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 155.5210 - val_loss: 158.6727\n",
      "Epoch 1627/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 155.8572 - val_loss: 143.9544\n",
      "Epoch 1628/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 139.9359 - val_loss: 143.3787\n",
      "Epoch 1629/10000\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 164.6279 - val_loss: 171.4728\n",
      "Epoch 1630/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 140.7590 - val_loss: 139.7159\n",
      "Epoch 1631/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.8170 - val_loss: 150.8930\n",
      "Epoch 1632/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.2847 - val_loss: 143.0961\n",
      "Epoch 1633/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 169.1548 - val_loss: 176.8730\n",
      "Epoch 1634/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 163.3195 - val_loss: 141.5455\n",
      "Epoch 1635/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.3034 - val_loss: 172.8970\n",
      "Epoch 1636/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 167.4821 - val_loss: 159.4312\n",
      "Epoch 1637/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.0492 - val_loss: 174.3533\n",
      "Epoch 1638/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 172.7787 - val_loss: 166.9612\n",
      "Epoch 1639/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 155.1557 - val_loss: 144.5717\n",
      "Epoch 1640/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 192.7121 - val_loss: 141.6649\n",
      "Epoch 1641/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.3356 - val_loss: 142.2409\n",
      "Epoch 1642/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.9576 - val_loss: 142.8201\n",
      "Epoch 1643/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 141.5762 - val_loss: 164.6537\n",
      "Epoch 1644/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 162.1573 - val_loss: 202.2877\n",
      "Epoch 1645/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 157.2605 - val_loss: 149.0045\n",
      "Epoch 1646/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.0539 - val_loss: 151.2987\n",
      "Epoch 1647/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 155.9870 - val_loss: 155.9427\n",
      "Epoch 1648/10000\n",
      "8000/8000 [==============================] - 1s 151us/step - loss: 162.0996 - val_loss: 142.8395\n",
      "Epoch 1649/10000\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 160.4356 - val_loss: 146.7398\n",
      "Epoch 1650/10000\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 163.0363 - val_loss: 149.4234\n",
      "Epoch 1651/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 155.9185 - val_loss: 148.7816\n",
      "Epoch 1652/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 163.5272 - val_loss: 174.2529\n",
      "Epoch 1653/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.5069 - val_loss: 199.0953\n",
      "Epoch 1654/10000\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 220.6092 - val_loss: 165.8015\n",
      "Epoch 1655/10000\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 146.2188 - val_loss: 151.6198\n",
      "Epoch 1656/10000\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 141.8594 - val_loss: 176.6766\n",
      "Epoch 1657/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 156.0728 - val_loss: 146.5362\n",
      "Epoch 1658/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 161.5980 - val_loss: 172.3856\n",
      "Epoch 1659/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 156.4521 - val_loss: 177.5624\n",
      "Epoch 1660/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 153.2793 - val_loss: 209.9284\n",
      "Epoch 1661/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.1354 - val_loss: 147.3850\n",
      "Epoch 1662/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.2811 - val_loss: 212.6959\n",
      "Epoch 1663/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 145.2509 - val_loss: 142.5596\n",
      "Epoch 1664/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.7706 - val_loss: 220.6404\n",
      "Epoch 1665/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.2861 - val_loss: 153.8700\n",
      "Epoch 1666/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 149.1837 - val_loss: 142.6261\n",
      "Epoch 1667/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 150.0036 - val_loss: 150.9210\n",
      "Epoch 1668/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 172.8893 - val_loss: 273.7194\n",
      "Epoch 1669/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.6010 - val_loss: 169.2891\n",
      "Epoch 1670/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.9619 - val_loss: 184.9520\n",
      "Epoch 1671/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.6414 - val_loss: 145.7617\n",
      "Epoch 1672/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 154.9528 - val_loss: 162.6055\n",
      "Epoch 1673/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 156.7961 - val_loss: 148.0122\n",
      "Epoch 1674/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 151.1967 - val_loss: 150.3104\n",
      "Epoch 1675/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 148.5922 - val_loss: 141.8605\n",
      "Epoch 1676/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 205.2657 - val_loss: 192.2537\n",
      "Epoch 1677/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 153.628 - 0s 57us/step - loss: 155.4736 - val_loss: 155.5087\n",
      "Epoch 1678/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.5536 - val_loss: 152.3576\n",
      "Epoch 1679/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 146.1828 - val_loss: 161.2794\n",
      "Epoch 1680/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 156.1323 - val_loss: 146.1039\n",
      "Epoch 1681/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.5089 - val_loss: 180.7532\n",
      "Epoch 1682/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 163.7967 - val_loss: 181.4131\n",
      "Epoch 1683/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.1207 - val_loss: 135.9629\n",
      "Epoch 1684/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.0867 - val_loss: 442.3698\n",
      "Epoch 1685/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.8913 - val_loss: 172.0902\n",
      "Epoch 1686/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.0102 - val_loss: 214.2995\n",
      "Epoch 1687/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 156.2567 - val_loss: 160.3458\n",
      "Epoch 1688/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.8485 - val_loss: 174.3054\n",
      "Epoch 1689/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.7416 - val_loss: 170.9581\n",
      "Epoch 1690/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 162.2706 - val_loss: 150.3779\n",
      "Epoch 1691/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 155.5064 - val_loss: 142.5577\n",
      "Epoch 1692/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.8370 - val_loss: 143.9485\n",
      "Epoch 1693/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.0568 - val_loss: 142.3073\n",
      "Epoch 1694/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 155.2038 - val_loss: 143.8040\n",
      "Epoch 1695/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 146.7018 - val_loss: 175.6620\n",
      "Epoch 1696/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 172.6539 - val_loss: 179.3508\n",
      "Epoch 1697/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.8778 - val_loss: 142.5705\n",
      "Epoch 1698/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.2606 - val_loss: 269.6291\n",
      "Epoch 1699/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 169.3154 - val_loss: 226.4209\n",
      "Epoch 1700/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.3156 - val_loss: 149.7727\n",
      "Epoch 1701/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 159.6604 - val_loss: 147.3466\n",
      "Epoch 1702/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 174.9080 - val_loss: 174.4337\n",
      "Epoch 1703/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 150.2177 - val_loss: 198.9400\n",
      "Epoch 1704/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 156.551 - 1s 77us/step - loss: 157.6580 - val_loss: 180.1208\n",
      "Epoch 1705/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 149.0565 - val_loss: 143.5951\n",
      "Epoch 1706/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.2579 - val_loss: 182.0827\n",
      "Epoch 1707/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.1336 - val_loss: 172.7747\n",
      "Epoch 1708/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.5743 - val_loss: 143.9892\n",
      "Epoch 1709/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 147.3493 - val_loss: 295.5712\n",
      "Epoch 1710/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.6943 - val_loss: 140.0066\n",
      "Epoch 1711/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.6676 - val_loss: 291.1618\n",
      "Epoch 1712/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.8580 - val_loss: 155.7035\n",
      "Epoch 1713/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.6198 - val_loss: 214.4702\n",
      "Epoch 1714/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.1435 - val_loss: 156.9777\n",
      "Epoch 1715/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.4839 - val_loss: 151.4745\n",
      "Epoch 1716/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 156.4651 - val_loss: 151.6323\n",
      "Epoch 1717/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 156.1344 - val_loss: 183.8253\n",
      "Epoch 1718/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 173.0852 - val_loss: 153.9734\n",
      "Epoch 1719/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 177.8449 - val_loss: 164.5636\n",
      "Epoch 1720/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.9168 - val_loss: 151.8854\n",
      "Epoch 1721/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.9931 - val_loss: 150.3657\n",
      "Epoch 1722/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 162.2005 - val_loss: 147.7862\n",
      "Epoch 1723/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 146.9999 - val_loss: 181.0199\n",
      "Epoch 1724/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 158.1246 - val_loss: 185.9186\n",
      "Epoch 1725/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.6743 - val_loss: 163.9512\n",
      "Epoch 1726/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 156.6043 - val_loss: 156.2226\n",
      "Epoch 1727/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.0501 - val_loss: 154.2855\n",
      "Epoch 1728/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 148.1329 - val_loss: 166.3262\n",
      "Epoch 1729/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 142.5516 - val_loss: 171.2477\n",
      "Epoch 1730/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.7819 - val_loss: 143.3592\n",
      "Epoch 1731/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.4742 - val_loss: 149.6593\n",
      "Epoch 1732/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.5115 - val_loss: 171.4040\n",
      "Epoch 1733/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 152.7896 - val_loss: 183.5483\n",
      "Epoch 1734/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 192.0290 - val_loss: 156.1770\n",
      "Epoch 1735/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 165.5789 - val_loss: 145.5726\n",
      "Epoch 1736/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 168.5495 - val_loss: 208.9927\n",
      "Epoch 1737/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 143.9053 - val_loss: 232.5572\n",
      "Epoch 1738/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.6530 - val_loss: 202.4421\n",
      "Epoch 1739/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 139.0344 - val_loss: 201.6912\n",
      "Epoch 1740/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 180.0521 - val_loss: 188.4040\n",
      "Epoch 1741/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 143.4401 - val_loss: 192.9196\n",
      "Epoch 1742/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.0802 - val_loss: 175.2336\n",
      "Epoch 1743/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 56us/step - loss: 167.4730 - val_loss: 203.9553\n",
      "Epoch 1744/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.3335 - val_loss: 150.6770\n",
      "Epoch 1745/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 138.7502 - val_loss: 141.9206\n",
      "Epoch 1746/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.9315 - val_loss: 182.6508\n",
      "Epoch 1747/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 164.3614 - val_loss: 248.1943\n",
      "Epoch 1748/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.7246 - val_loss: 172.1771\n",
      "Epoch 1749/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 176.6861 - val_loss: 137.1747\n",
      "Epoch 1750/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.2767 - val_loss: 161.1238\n",
      "Epoch 1751/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.2378 - val_loss: 145.0165\n",
      "Epoch 1752/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.5730 - val_loss: 142.7914\n",
      "Epoch 1753/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 157.1793 - val_loss: 155.5205\n",
      "Epoch 1754/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.9651 - val_loss: 268.2392\n",
      "Epoch 1755/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.9521 - val_loss: 159.1448\n",
      "Epoch 1756/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.2560 - val_loss: 198.3753\n",
      "Epoch 1757/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 167.2592 - val_loss: 150.8241\n",
      "Epoch 1758/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.3411 - val_loss: 150.4245\n",
      "Epoch 1759/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 198.9013 - val_loss: 151.7878\n",
      "Epoch 1760/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.1768 - val_loss: 153.8456\n",
      "Epoch 1761/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.5229 - val_loss: 146.8880\n",
      "Epoch 1762/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 155.4256 - val_loss: 165.7953\n",
      "Epoch 1763/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 141.7760 - val_loss: 143.1446\n",
      "Epoch 1764/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 149.3435 - val_loss: 180.2853\n",
      "Epoch 1765/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 170.3861 - val_loss: 144.6537\n",
      "Epoch 1766/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.2030 - val_loss: 144.1848\n",
      "Epoch 1767/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 152.1898 - val_loss: 145.6587\n",
      "Epoch 1768/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.7243 - val_loss: 148.9122\n",
      "Epoch 1769/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 147.5917 - val_loss: 303.0714\n",
      "Epoch 1770/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.8465 - val_loss: 145.5292\n",
      "Epoch 1771/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.0275 - val_loss: 191.8661\n",
      "Epoch 1772/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 154.0435 - val_loss: 159.4048\n",
      "Epoch 1773/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.4357 - val_loss: 140.0620\n",
      "Epoch 1774/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 156.4870 - val_loss: 160.1105\n",
      "Epoch 1775/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 198.9124 - val_loss: 162.5110\n",
      "Epoch 1776/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 153.0066 - val_loss: 152.8248\n",
      "Epoch 1777/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 153.4181 - val_loss: 154.2043\n",
      "Epoch 1778/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.0913 - val_loss: 165.4522\n",
      "Epoch 1779/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.0623 - val_loss: 159.5774\n",
      "Epoch 1780/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.4131 - val_loss: 161.3109\n",
      "Epoch 1781/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 153.8695 - val_loss: 161.4108\n",
      "Epoch 1782/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 161.7543 - val_loss: 183.6527\n",
      "Epoch 1783/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 168.2175 - val_loss: 151.4613\n",
      "Epoch 1784/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 184.8035 - val_loss: 192.7182\n",
      "Epoch 1785/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 150.2866 - val_loss: 154.9590\n",
      "Epoch 1786/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 147.9172 - val_loss: 151.5090\n",
      "Epoch 1787/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 150.7883 - val_loss: 178.5645\n",
      "Epoch 1788/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.3768 - val_loss: 174.4686\n",
      "Epoch 1789/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.2118 - val_loss: 162.6027\n",
      "Epoch 1790/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.2609 - val_loss: 167.0508\n",
      "Epoch 1791/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 145.1834 - val_loss: 139.8183\n",
      "Epoch 1792/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.8524 - val_loss: 147.0187\n",
      "Epoch 1793/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.2790 - val_loss: 164.6213\n",
      "Epoch 1794/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 151.7359 - val_loss: 182.3419\n",
      "Epoch 1795/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 150.2555 - val_loss: 152.0382\n",
      "Epoch 1796/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 194.4252 - val_loss: 145.4257\n",
      "Epoch 1797/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.0397 - val_loss: 152.6076\n",
      "Epoch 1798/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 145.7045 - val_loss: 140.9541\n",
      "Epoch 1799/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 161.6023 - val_loss: 174.0234\n",
      "Epoch 1800/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 153.0463 - val_loss: 179.0891\n",
      "Epoch 1801/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 156.8638 - val_loss: 142.6555\n",
      "Epoch 1802/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 147.5334 - val_loss: 183.1173\n",
      "Epoch 1803/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 149.6479 - val_loss: 222.9828\n",
      "Epoch 1804/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 178.6336 - val_loss: 144.8912\n",
      "Epoch 1805/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 188.2765 - val_loss: 154.2004\n",
      "Epoch 1806/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 164.8098 - val_loss: 142.5314\n",
      "Epoch 1807/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 213.8612 - val_loss: 229.4477\n",
      "Epoch 1808/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 170.2697 - val_loss: 141.4988\n",
      "Epoch 1809/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 148.7475 - val_loss: 146.5760\n",
      "Epoch 1810/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.7700 - val_loss: 143.4895\n",
      "Epoch 1811/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 146.2243 - val_loss: 142.7309\n",
      "Epoch 1812/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 152.7499 - val_loss: 162.9315\n",
      "Epoch 1813/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 150.6791 - val_loss: 141.9964\n",
      "Epoch 1814/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 142.6779 - val_loss: 190.7342\n",
      "Epoch 1815/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 57us/step - loss: 150.1058 - val_loss: 152.2027\n",
      "Epoch 1816/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.5056 - val_loss: 166.1533\n",
      "Epoch 1817/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 141.1729 - val_loss: 171.4026\n",
      "Epoch 1818/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 146.1570 - val_loss: 141.1254\n",
      "Epoch 1819/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 142.5172 - val_loss: 177.4827\n",
      "Epoch 1820/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 157.8061 - val_loss: 334.5280\n",
      "Epoch 1821/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.0100 - val_loss: 203.6769\n",
      "Epoch 1822/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 158.2024 - val_loss: 146.6092\n",
      "Epoch 1823/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.5209 - val_loss: 244.1769\n",
      "Epoch 1824/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 197.2889 - val_loss: 307.9981\n",
      "Epoch 1825/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 172.6884 - val_loss: 141.6233\n",
      "Epoch 1826/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 143.4774 - val_loss: 160.4946\n",
      "Epoch 1827/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.9311 - val_loss: 164.3168\n",
      "Epoch 1828/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 174.2984 - val_loss: 198.5847\n",
      "Epoch 1829/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 149.8726 - val_loss: 149.1431\n",
      "Epoch 1830/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.3765 - val_loss: 165.6910\n",
      "Epoch 1831/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.7601 - val_loss: 143.4835\n",
      "Epoch 1832/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.9152 - val_loss: 150.2381\n",
      "Epoch 1833/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 151.7745 - val_loss: 190.3531\n",
      "Epoch 1834/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.9172 - val_loss: 148.2141\n",
      "Epoch 1835/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.7073 - val_loss: 148.8064\n",
      "Epoch 1836/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.1274 - val_loss: 170.1470\n",
      "Epoch 1837/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 143.1474 - val_loss: 140.7246\n",
      "Epoch 1838/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.4496 - val_loss: 161.1833\n",
      "Epoch 1839/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.6845 - val_loss: 156.7929\n",
      "Epoch 1840/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 178.3665 - val_loss: 141.9162\n",
      "Epoch 1841/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 137.4594 - val_loss: 160.8019\n",
      "Epoch 1842/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 158.4751 - val_loss: 159.1218\n",
      "Epoch 1843/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.8031 - val_loss: 138.8639\n",
      "Epoch 1844/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 157.4035 - val_loss: 150.9092\n",
      "Epoch 1845/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 163.0979 - val_loss: 170.1066\n",
      "Epoch 1846/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 155.3709 - val_loss: 160.4993\n",
      "Epoch 1847/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 189.8397 - val_loss: 143.8450\n",
      "Epoch 1848/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 153.0350 - val_loss: 165.7003\n",
      "Epoch 1849/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 177.7110 - val_loss: 174.4943\n",
      "Epoch 1850/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.9951 - val_loss: 152.4576\n",
      "Epoch 1851/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 147.6759 - val_loss: 160.1935\n",
      "Epoch 1852/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.3139 - val_loss: 150.5768\n",
      "Epoch 1853/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.9236 - val_loss: 137.7511\n",
      "Epoch 1854/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.5071 - val_loss: 157.6787\n",
      "Epoch 1855/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 157.0636 - val_loss: 257.4014\n",
      "Epoch 1856/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.1483 - val_loss: 148.6225\n",
      "Epoch 1857/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.5973 - val_loss: 143.7911\n",
      "Epoch 1858/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 158.1309 - val_loss: 164.0053\n",
      "Epoch 1859/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 162.9728 - val_loss: 192.8825\n",
      "Epoch 1860/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 164.8364 - val_loss: 143.1539\n",
      "Epoch 1861/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 148.7347 - val_loss: 139.2212\n",
      "Epoch 1862/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 170.6936 - val_loss: 142.1795\n",
      "Epoch 1863/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.2228 - val_loss: 145.6590\n",
      "Epoch 1864/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 132.1945 - val_loss: 160.5797\n",
      "Epoch 1865/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.2487 - val_loss: 156.0075\n",
      "Epoch 1866/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.7531 - val_loss: 255.8861\n",
      "Epoch 1867/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.6827 - val_loss: 154.5046\n",
      "Epoch 1868/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 142.2075 - val_loss: 149.9605\n",
      "Epoch 1869/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 145.8534 - val_loss: 153.0408\n",
      "Epoch 1870/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 164.3853 - val_loss: 138.2700\n",
      "Epoch 1871/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 144.9843 - val_loss: 160.9859\n",
      "Epoch 1872/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 151.9698 - val_loss: 141.1302\n",
      "Epoch 1873/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.4888 - val_loss: 141.4482\n",
      "Epoch 1874/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 157.2527 - val_loss: 172.0141\n",
      "Epoch 1875/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.2248 - val_loss: 214.3086\n",
      "Epoch 1876/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.9898 - val_loss: 140.6648\n",
      "Epoch 1877/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 152.2009 - val_loss: 253.6257\n",
      "Epoch 1878/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 141.0314 - val_loss: 139.0479\n",
      "Epoch 1879/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.6432 - val_loss: 152.1270\n",
      "Epoch 1880/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 159.0138 - val_loss: 145.0950\n",
      "Epoch 1881/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 173.4911 - val_loss: 146.4824\n",
      "Epoch 1882/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 154.4631 - val_loss: 143.2901\n",
      "Epoch 1883/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 167.7684 - val_loss: 137.8320\n",
      "Epoch 1884/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.6373 - val_loss: 137.7632\n",
      "Epoch 1885/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 160.4820 - val_loss: 175.2897\n",
      "Epoch 1886/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 165.1012 - val_loss: 140.6387\n",
      "Epoch 1887/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 63us/step - loss: 177.9978 - val_loss: 142.3582\n",
      "Epoch 1888/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 148.0329 - val_loss: 138.9292\n",
      "Epoch 1889/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 142.5305 - val_loss: 141.2545\n",
      "Epoch 1890/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 210.0569 - val_loss: 191.4306\n",
      "Epoch 1891/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 198.1544 - val_loss: 142.6139\n",
      "Epoch 1892/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 147.3107 - val_loss: 146.7484\n",
      "Epoch 1893/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.7410 - val_loss: 143.7472\n",
      "Epoch 1894/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 158.5278 - val_loss: 198.2418\n",
      "Epoch 1895/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.9705 - val_loss: 183.0701\n",
      "Epoch 1896/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 131.6021 - val_loss: 142.1343\n",
      "Epoch 1897/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 150.5929 - val_loss: 143.0776\n",
      "Epoch 1898/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 172.9375 - val_loss: 176.0201\n",
      "Epoch 1899/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 166.8037 - val_loss: 139.4110\n",
      "Epoch 1900/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.1979 - val_loss: 172.1672\n",
      "Epoch 1901/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 153.3939 - val_loss: 170.5171\n",
      "Epoch 1902/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 139.7373 - val_loss: 156.8803\n",
      "Epoch 1903/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.7500 - val_loss: 144.7022\n",
      "Epoch 1904/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.7964 - val_loss: 144.4798\n",
      "Epoch 1905/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.2836 - val_loss: 153.4847\n",
      "Epoch 1906/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 173.0933 - val_loss: 143.3178\n",
      "Epoch 1907/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 145.4218 - val_loss: 152.2089\n",
      "Epoch 1908/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.0584 - val_loss: 178.7099\n",
      "Epoch 1909/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.1480 - val_loss: 149.6367\n",
      "Epoch 1910/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.0929 - val_loss: 147.0573\n",
      "Epoch 1911/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.2376 - val_loss: 173.7580\n",
      "Epoch 1912/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.8117 - val_loss: 155.8987\n",
      "Epoch 1913/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 156.0379 - val_loss: 147.6277\n",
      "Epoch 1914/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 145.9335 - val_loss: 136.8990\n",
      "Epoch 1915/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 166.0463 - val_loss: 156.7951\n",
      "Epoch 1916/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 142.9228 - val_loss: 158.9536\n",
      "Epoch 1917/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 142.6577 - val_loss: 146.6701\n",
      "Epoch 1918/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.2346 - val_loss: 150.6855\n",
      "Epoch 1919/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 154.7077 - val_loss: 140.9089\n",
      "Epoch 1920/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 148.4417 - val_loss: 137.8130\n",
      "Epoch 1921/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 154.4819 - val_loss: 159.3215\n",
      "Epoch 1922/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 138.4398 - val_loss: 149.5037\n",
      "Epoch 1923/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.3809 - val_loss: 166.2508\n",
      "Epoch 1924/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.3486 - val_loss: 174.9524\n",
      "Epoch 1925/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 200.1891 - val_loss: 153.0902\n",
      "Epoch 1926/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 149.9790 - val_loss: 156.2893\n",
      "Epoch 1927/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 136.9902 - val_loss: 177.0129\n",
      "Epoch 1928/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 150.2521 - val_loss: 178.9974\n",
      "Epoch 1929/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.0562 - val_loss: 139.0897\n",
      "Epoch 1930/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 142.5980 - val_loss: 206.4197\n",
      "Epoch 1931/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 155.9113 - val_loss: 148.4362\n",
      "Epoch 1932/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 147.5017 - val_loss: 163.7278\n",
      "Epoch 1933/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 138.4086 - val_loss: 146.5909\n",
      "Epoch 1934/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 166.2675 - val_loss: 147.5989\n",
      "Epoch 1935/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 148.9045 - val_loss: 167.5091\n",
      "Epoch 1936/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.2392 - val_loss: 144.7649\n",
      "Epoch 1937/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 148.6785 - val_loss: 152.8830\n",
      "Epoch 1938/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 145.6850 - val_loss: 191.1365\n",
      "Epoch 1939/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.1811 - val_loss: 139.4935\n",
      "Epoch 1940/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 147.1735 - val_loss: 147.3678\n",
      "Epoch 1941/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.6237 - val_loss: 146.7935\n",
      "Epoch 1942/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 131.3922 - val_loss: 141.7038\n",
      "Epoch 1943/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 176.7194 - val_loss: 154.2859\n",
      "Epoch 1944/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 149.7269 - val_loss: 148.5170\n",
      "Epoch 1945/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 135.4884 - val_loss: 146.5482\n",
      "Epoch 1946/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 143.1059 - val_loss: 152.7953\n",
      "Epoch 1947/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 189.2875 - val_loss: 207.8689\n",
      "Epoch 1948/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.9841 - val_loss: 153.7539\n",
      "Epoch 1949/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 150.1919 - val_loss: 146.3624\n",
      "Epoch 1950/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.1563 - val_loss: 138.7957\n",
      "Epoch 1951/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.7120 - val_loss: 139.4092\n",
      "Epoch 1952/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.6617 - val_loss: 160.4878\n",
      "Epoch 1953/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 142.8810 - val_loss: 155.3538\n",
      "Epoch 1954/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 145.2177 - val_loss: 145.1041\n",
      "Epoch 1955/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.7343 - val_loss: 204.8211\n",
      "Epoch 1956/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 179.1214 - val_loss: 301.1505\n",
      "Epoch 1957/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.6366 - val_loss: 182.0031\n",
      "Epoch 1958/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 146.3449 - val_loss: 170.4865\n",
      "Epoch 1959/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 156.3637 - val_loss: 137.1913\n",
      "Epoch 1960/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 149.7345 - val_loss: 139.7950\n",
      "Epoch 1961/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 150.1768 - val_loss: 144.7446\n",
      "Epoch 1962/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 146.5773 - val_loss: 167.2205\n",
      "Epoch 1963/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.9216 - val_loss: 138.8490\n",
      "Epoch 1964/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 165.1584 - val_loss: 150.2029\n",
      "Epoch 1965/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.7933 - val_loss: 148.4787\n",
      "Epoch 1966/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.6534 - val_loss: 140.2019\n",
      "Epoch 1967/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.1535 - val_loss: 138.5044\n",
      "Epoch 1968/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 191.5870 - val_loss: 163.6604\n",
      "Epoch 1969/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.4104 - val_loss: 155.8253\n",
      "Epoch 1970/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.5279 - val_loss: 150.6542\n",
      "Epoch 1971/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.8023 - val_loss: 173.2128\n",
      "Epoch 1972/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.1213 - val_loss: 141.2196\n",
      "Epoch 1973/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.1122 - val_loss: 177.6189\n",
      "Epoch 1974/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 165.3762 - val_loss: 144.3855\n",
      "Epoch 1975/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 155.4331 - val_loss: 138.1987\n",
      "Epoch 1976/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 181.6733 - val_loss: 163.7109\n",
      "Epoch 1977/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 156.6014 - val_loss: 141.1926\n",
      "Epoch 1978/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.4447 - val_loss: 145.0473\n",
      "Epoch 1979/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.2038 - val_loss: 214.4257\n",
      "Epoch 1980/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 143.0394 - val_loss: 152.7914\n",
      "Epoch 1981/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.9550 - val_loss: 153.2009\n",
      "Epoch 1982/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 176.0179 - val_loss: 325.4854\n",
      "Epoch 1983/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 152.4134 - val_loss: 159.5131\n",
      "Epoch 1984/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.2767 - val_loss: 157.7067\n",
      "Epoch 1985/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.4332 - val_loss: 139.7363\n",
      "Epoch 1986/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.2229 - val_loss: 137.7753\n",
      "Epoch 1987/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 170.6078 - val_loss: 161.5825\n",
      "Epoch 1988/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.8593 - val_loss: 136.5200\n",
      "Epoch 1989/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 154.2808 - val_loss: 146.7334\n",
      "Epoch 1990/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.2340 - val_loss: 216.9435\n",
      "Epoch 1991/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 145.3981 - val_loss: 143.8580\n",
      "Epoch 1992/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 139.7755 - val_loss: 164.9298\n",
      "Epoch 1993/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.1614 - val_loss: 136.5124\n",
      "Epoch 1994/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 147.4773 - val_loss: 176.7620\n",
      "Epoch 1995/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.2744 - val_loss: 162.7773\n",
      "Epoch 1996/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 152.7071 - val_loss: 143.9316\n",
      "Epoch 1997/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.2187 - val_loss: 141.8620\n",
      "Epoch 1998/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 158.7707 - val_loss: 158.1812\n",
      "Epoch 1999/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.4053 - val_loss: 176.7840\n",
      "Epoch 2000/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 167.3691 - val_loss: 318.3331\n",
      "Epoch 2001/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 164.9196 - val_loss: 138.9218\n",
      "Epoch 2002/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.9147 - val_loss: 149.3341\n",
      "Epoch 2003/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.1907 - val_loss: 174.1696\n",
      "Epoch 2004/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 168.8166 - val_loss: 170.5357\n",
      "Epoch 2005/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 149.7507 - val_loss: 138.5936\n",
      "Epoch 2006/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.3328 - val_loss: 141.6790\n",
      "Epoch 2007/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.8617 - val_loss: 271.7290\n",
      "Epoch 2008/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 160.3919 - val_loss: 191.7576\n",
      "Epoch 2009/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 138.1752 - val_loss: 148.0966\n",
      "Epoch 2010/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.5083 - val_loss: 169.7080\n",
      "Epoch 2011/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 146.3728 - val_loss: 183.3167\n",
      "Epoch 2012/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 140.2134 - val_loss: 140.4276\n",
      "Epoch 2013/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.5231 - val_loss: 189.5626\n",
      "Epoch 2014/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 159.3835 - val_loss: 140.2193\n",
      "Epoch 2015/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.6198 - val_loss: 150.2019\n",
      "Epoch 2016/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.0023 - val_loss: 142.6809\n",
      "Epoch 2017/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 152.7985 - val_loss: 188.6412\n",
      "Epoch 2018/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 272.4362 - val_loss: 147.3896\n",
      "Epoch 2019/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 153.6743 - val_loss: 141.6963\n",
      "Epoch 2020/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.5320 - val_loss: 214.0084\n",
      "Epoch 2021/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.1096 - val_loss: 167.0833\n",
      "Epoch 2022/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.6298 - val_loss: 142.7256\n",
      "Epoch 2023/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.5137 - val_loss: 150.2363\n",
      "Epoch 2024/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.1993 - val_loss: 243.5960\n",
      "Epoch 2025/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.1311 - val_loss: 164.6695\n",
      "Epoch 2026/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.8681 - val_loss: 150.0423\n",
      "Epoch 2027/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 183.9903 - val_loss: 176.6378\n",
      "Epoch 2028/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.3169 - val_loss: 142.8501\n",
      "Epoch 2029/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.6293 - val_loss: 165.1035\n",
      "Epoch 2030/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.3634 - val_loss: 154.0753\n",
      "Epoch 2031/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 139.4837 - val_loss: 163.9465\n",
      "Epoch 2032/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.7135 - val_loss: 147.4397\n",
      "Epoch 2033/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 144.8135 - val_loss: 157.9009\n",
      "Epoch 2034/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.4087 - val_loss: 147.7930\n",
      "Epoch 2035/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.1684 - val_loss: 149.9524\n",
      "Epoch 2036/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 144.9542 - val_loss: 147.8186\n",
      "Epoch 2037/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 144.8922 - val_loss: 153.2363\n",
      "Epoch 2038/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 152.6222 - val_loss: 150.1953\n",
      "Epoch 2039/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.1674 - val_loss: 163.3483\n",
      "Epoch 2040/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 135.9768 - val_loss: 177.5600\n",
      "Epoch 2041/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 160.2010 - val_loss: 148.9992\n",
      "Epoch 2042/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 141.9828 - val_loss: 149.6791\n",
      "Epoch 2043/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.1117 - val_loss: 158.8436\n",
      "Epoch 2044/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.0338 - val_loss: 145.0243\n",
      "Epoch 2045/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 148.8736 - val_loss: 153.0764\n",
      "Epoch 2046/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 146.3623 - val_loss: 138.7188\n",
      "Epoch 2047/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.9897 - val_loss: 156.0324\n",
      "Epoch 2048/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 169.6778 - val_loss: 167.7092\n",
      "Epoch 2049/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.7661 - val_loss: 164.0992\n",
      "Epoch 2050/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.0368 - val_loss: 271.7258\n",
      "Epoch 2051/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.0921 - val_loss: 178.0270\n",
      "Epoch 2052/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.4857 - val_loss: 196.6510\n",
      "Epoch 2053/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 148.5272 - val_loss: 146.8336\n",
      "Epoch 2054/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 147.4989 - val_loss: 241.2044\n",
      "Epoch 2055/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 150.8908 - val_loss: 162.2672\n",
      "Epoch 2056/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 147.2885 - val_loss: 137.5836\n",
      "Epoch 2057/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 133.4892 - val_loss: 147.4419\n",
      "Epoch 2058/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 190.9268 - val_loss: 185.7447\n",
      "Epoch 2059/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.7083 - val_loss: 143.8061\n",
      "Epoch 2060/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.3846 - val_loss: 174.3175\n",
      "Epoch 2061/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 175.1485 - val_loss: 139.8310\n",
      "Epoch 2062/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 145.9460 - val_loss: 159.6724\n",
      "Epoch 2063/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 156.4298 - val_loss: 152.9845\n",
      "Epoch 2064/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.9029 - val_loss: 147.8149\n",
      "Epoch 2065/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.5989 - val_loss: 165.8311\n",
      "Epoch 2066/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.0616 - val_loss: 169.3021\n",
      "Epoch 2067/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 136.1851 - val_loss: 141.9736\n",
      "Epoch 2068/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.7996 - val_loss: 144.4014\n",
      "Epoch 2069/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.4594 - val_loss: 139.4075\n",
      "Epoch 2070/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.1040 - val_loss: 147.3311\n",
      "Epoch 2071/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 151.1817 - val_loss: 148.8063\n",
      "Epoch 2072/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 160.4980 - val_loss: 147.6989\n",
      "Epoch 2073/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 164.6954 - val_loss: 143.6546\n",
      "Epoch 2074/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 143.1607 - val_loss: 177.2381\n",
      "Epoch 2075/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.1669 - val_loss: 150.0092\n",
      "Epoch 2076/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 160.4009 - val_loss: 141.3197\n",
      "Epoch 2077/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 148.0718 - val_loss: 138.4631\n",
      "Epoch 2078/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 140.9267 - val_loss: 299.9323\n",
      "Epoch 2079/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 153.2613 - val_loss: 171.1804\n",
      "Epoch 2080/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.9508 - val_loss: 153.1062\n",
      "Epoch 2081/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.5873 - val_loss: 196.0408\n",
      "Epoch 2082/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 144.7625 - val_loss: 141.0746\n",
      "Epoch 2083/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 142.7435 - val_loss: 145.8899\n",
      "Epoch 2084/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 147.3138 - val_loss: 143.6702\n",
      "Epoch 2085/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 144.4379 - val_loss: 140.9683\n",
      "Epoch 2086/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.7323 - val_loss: 397.5190\n",
      "Epoch 2087/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.8305 - val_loss: 159.4435\n",
      "Epoch 2088/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.8974 - val_loss: 148.3267\n",
      "Epoch 2089/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 155.3589 - val_loss: 140.1450\n",
      "Epoch 2090/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 133.6712 - val_loss: 238.7246\n",
      "Epoch 2091/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.4625 - val_loss: 193.5684\n",
      "Epoch 2092/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 167.7337 - val_loss: 203.8448\n",
      "Epoch 2093/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 150.4451 - val_loss: 169.6138\n",
      "Epoch 2094/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 183.2578 - val_loss: 178.1784\n",
      "Epoch 2095/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.1336 - val_loss: 163.8695\n",
      "Epoch 2096/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 132.4909 - val_loss: 138.2628\n",
      "Epoch 2097/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.9659 - val_loss: 141.5129\n",
      "Epoch 2098/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 177.0044 - val_loss: 159.2284\n",
      "Epoch 2099/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 145.3670 - val_loss: 169.2249\n",
      "Epoch 2100/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 165.2034 - val_loss: 152.5046\n",
      "Epoch 2101/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 143.6334 - val_loss: 145.3761\n",
      "Epoch 2102/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 143.3482 - val_loss: 141.9316\n",
      "Epoch 2103/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 56us/step - loss: 142.8122 - val_loss: 161.6891\n",
      "Epoch 2104/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.2075 - val_loss: 196.5655\n",
      "Epoch 2105/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.7905 - val_loss: 138.1778\n",
      "Epoch 2106/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.9592 - val_loss: 150.7548\n",
      "Epoch 2107/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.3205 - val_loss: 145.1370\n",
      "Epoch 2108/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.4834 - val_loss: 145.2511\n",
      "Epoch 2109/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 139.6519 - val_loss: 137.2127\n",
      "Epoch 2110/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.4555 - val_loss: 157.6194\n",
      "Epoch 2111/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.5985 - val_loss: 169.9379\n",
      "Epoch 2112/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 136.1626 - val_loss: 170.6011\n",
      "Epoch 2113/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 147.4399 - val_loss: 141.6100\n",
      "Epoch 2114/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 152.8393 - val_loss: 156.9471\n",
      "Epoch 2115/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 142.6470 - val_loss: 192.8739\n",
      "Epoch 2116/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.1757 - val_loss: 145.3867\n",
      "Epoch 2117/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.1930 - val_loss: 146.9214\n",
      "Epoch 2118/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 150.3952 - val_loss: 144.2237\n",
      "Epoch 2119/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 139.0224 - val_loss: 141.7446\n",
      "Epoch 2120/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 150.9821 - val_loss: 166.9001\n",
      "Epoch 2121/10000\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 147.1743 - val_loss: 170.9242\n",
      "Epoch 2122/10000\n",
      "8000/8000 [==============================] - 1s 143us/step - loss: 163.6801 - val_loss: 168.6278\n",
      "Epoch 2123/10000\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 153.1007 - val_loss: 142.9538\n",
      "Epoch 2124/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 231.9629 - val_loss: 139.9113\n",
      "Epoch 2125/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.8748 - val_loss: 137.9388\n",
      "Epoch 2126/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 140.0050 - val_loss: 160.1158\n",
      "Epoch 2127/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 141.7145 - val_loss: 158.9975\n",
      "Epoch 2128/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.3332 - val_loss: 143.5112\n",
      "Epoch 2129/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.7238 - val_loss: 140.3503\n",
      "Epoch 2130/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 144.8598 - val_loss: 140.4505\n",
      "Epoch 2131/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 155.3471 - val_loss: 148.3943\n",
      "Epoch 2132/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 146.1080 - val_loss: 168.2158\n",
      "Epoch 2133/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 135.1034 - val_loss: 146.8797\n",
      "Epoch 2134/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 168.2458 - val_loss: 150.5059\n",
      "Epoch 2135/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 168.8239 - val_loss: 146.4910\n",
      "Epoch 2136/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.7511 - val_loss: 144.5221\n",
      "Epoch 2137/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.4602 - val_loss: 208.8740\n",
      "Epoch 2138/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 134.8103 - val_loss: 149.7646\n",
      "Epoch 2139/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 136.9166 - val_loss: 183.9626\n",
      "Epoch 2140/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 196.5123 - val_loss: 199.6587\n",
      "Epoch 2141/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 167.7964 - val_loss: 144.6694\n",
      "Epoch 2142/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 152.9590 - val_loss: 140.4396\n",
      "Epoch 2143/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 136.9037 - val_loss: 160.0799\n",
      "Epoch 2144/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.9991 - val_loss: 151.7004\n",
      "Epoch 2145/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.1121 - val_loss: 180.3042\n",
      "Epoch 2146/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 183.4010 - val_loss: 139.7603\n",
      "Epoch 2147/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 171.7549 - val_loss: 205.7636\n",
      "Epoch 2148/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 154.5985 - val_loss: 220.3007\n",
      "Epoch 2149/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 172.8128 - val_loss: 150.5438\n",
      "Epoch 2150/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 132.0896 - val_loss: 152.1535\n",
      "Epoch 2151/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 151.6151 - val_loss: 144.4616\n",
      "Epoch 2152/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 142.2814 - val_loss: 205.8539\n",
      "Epoch 2153/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 158.9596 - val_loss: 153.7189\n",
      "Epoch 2154/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.7637 - val_loss: 153.9960\n",
      "Epoch 2155/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.4086 - val_loss: 234.1353\n",
      "Epoch 2156/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.6070 - val_loss: 153.1770\n",
      "Epoch 2157/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.9811 - val_loss: 149.8943\n",
      "Epoch 2158/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.5459 - val_loss: 144.5079\n",
      "Epoch 2159/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 169.7975 - val_loss: 192.0163\n",
      "Epoch 2160/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.7200 - val_loss: 146.6605\n",
      "Epoch 2161/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.0643 - val_loss: 192.0722\n",
      "Epoch 2162/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.1889 - val_loss: 256.2112\n",
      "Epoch 2163/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.0836 - val_loss: 142.8988\n",
      "Epoch 2164/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 173.5908 - val_loss: 144.3014\n",
      "Epoch 2165/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.1967 - val_loss: 158.7075\n",
      "Epoch 2166/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.1023 - val_loss: 146.9656\n",
      "Epoch 2167/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 172.6402 - val_loss: 153.8250\n",
      "Epoch 2168/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 147.0227 - val_loss: 269.3699\n",
      "Epoch 2169/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.1739 - val_loss: 145.8010\n",
      "Epoch 2170/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.0617 - val_loss: 157.8368\n",
      "Epoch 2171/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.9427 - val_loss: 156.4356\n",
      "Epoch 2172/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 155.2465 - val_loss: 166.3438\n",
      "Epoch 2173/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 175.7686 - val_loss: 190.9811\n",
      "Epoch 2174/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.7670 - val_loss: 138.9533\n",
      "Epoch 2175/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.8561 - val_loss: 277.8541\n",
      "Epoch 2176/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 139.365 - 0s 51us/step - loss: 138.2862 - val_loss: 220.1842\n",
      "Epoch 2177/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.1518 - val_loss: 150.5504\n",
      "Epoch 2178/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.3037 - val_loss: 138.4852\n",
      "Epoch 2179/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 175.9280 - val_loss: 156.3944\n",
      "Epoch 2180/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 175.2446 - val_loss: 139.9039\n",
      "Epoch 2181/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.3150 - val_loss: 158.7077\n",
      "Epoch 2182/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.5781 - val_loss: 173.0835\n",
      "Epoch 2183/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.0301 - val_loss: 211.9606\n",
      "Epoch 2184/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.4425 - val_loss: 141.1221\n",
      "Epoch 2185/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.7021 - val_loss: 161.8755\n",
      "Epoch 2186/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.3513 - val_loss: 141.6620\n",
      "Epoch 2187/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 140.519 - 0s 50us/step - loss: 141.6448 - val_loss: 145.6341\n",
      "Epoch 2188/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 140.7629 - val_loss: 140.5960\n",
      "Epoch 2189/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 166.6113 - val_loss: 177.1316\n",
      "Epoch 2190/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 152.3000 - val_loss: 157.0873\n",
      "Epoch 2191/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.5361 - val_loss: 141.4458\n",
      "Epoch 2192/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.0249 - val_loss: 140.3623\n",
      "Epoch 2193/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.2535 - val_loss: 154.1154\n",
      "Epoch 2194/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 152.7155 - val_loss: 300.6632\n",
      "Epoch 2195/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.4192 - val_loss: 269.4208\n",
      "Epoch 2196/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.1945 - val_loss: 156.6361\n",
      "Epoch 2197/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.9738 - val_loss: 271.0832\n",
      "Epoch 2198/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.9526 - val_loss: 143.5828\n",
      "Epoch 2199/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 173.3829 - val_loss: 138.7951\n",
      "Epoch 2200/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.8258 - val_loss: 139.0166\n",
      "Epoch 2201/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 156.8093 - val_loss: 162.3855\n",
      "Epoch 2202/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.1953 - val_loss: 269.6023\n",
      "Epoch 2203/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 159.6406 - val_loss: 166.6803\n",
      "Epoch 2204/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.5223 - val_loss: 155.2275\n",
      "Epoch 2205/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 148.9086 - val_loss: 144.0307\n",
      "Epoch 2206/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 139.0163 - val_loss: 179.0142\n",
      "Epoch 2207/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 169.9488 - val_loss: 160.1286\n",
      "Epoch 2208/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.4469 - val_loss: 147.2371\n",
      "Epoch 2209/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 136.4821 - val_loss: 201.6162\n",
      "Epoch 2210/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 138.2673 - val_loss: 137.1522\n",
      "Epoch 2211/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.5104 - val_loss: 157.4087\n",
      "Epoch 2212/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.4177 - val_loss: 155.9723\n",
      "Epoch 2213/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.1594 - val_loss: 148.3076\n",
      "Epoch 2214/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.7933 - val_loss: 137.9790\n",
      "Epoch 2215/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.5374 - val_loss: 158.1410\n",
      "Epoch 2216/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.8295 - val_loss: 138.6128\n",
      "Epoch 2217/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.8503 - val_loss: 140.2643\n",
      "Epoch 2218/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.6143 - val_loss: 151.6583\n",
      "Epoch 2219/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 168.4021 - val_loss: 243.2935\n",
      "Epoch 2220/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.8502 - val_loss: 139.3879\n",
      "Epoch 2221/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 150.5233 - val_loss: 145.6165\n",
      "Epoch 2222/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.1605 - val_loss: 161.9069\n",
      "Epoch 2223/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.1912 - val_loss: 165.1340\n",
      "Epoch 2224/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.1871 - val_loss: 176.4436\n",
      "Epoch 2225/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.6168 - val_loss: 155.9234\n",
      "Epoch 2226/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.1037 - val_loss: 184.0236\n",
      "Epoch 2227/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.3528 - val_loss: 146.6009\n",
      "Epoch 2228/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.7936 - val_loss: 144.2600\n",
      "Epoch 2229/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 147.9561 - val_loss: 144.0255\n",
      "Epoch 2230/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.7639 - val_loss: 210.9933\n",
      "Epoch 2231/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 147.3321 - val_loss: 158.6276\n",
      "Epoch 2232/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.2211 - val_loss: 161.6745\n",
      "Epoch 2233/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 160.3252 - val_loss: 142.1391\n",
      "Epoch 2234/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.8759 - val_loss: 149.1433\n",
      "Epoch 2235/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 163.7337 - val_loss: 141.3331\n",
      "Epoch 2236/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.1515 - val_loss: 228.2289\n",
      "Epoch 2237/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 159.4862 - val_loss: 150.8563\n",
      "Epoch 2238/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.2850 - val_loss: 138.2813\n",
      "Epoch 2239/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.7587 - val_loss: 166.3821\n",
      "Epoch 2240/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 185.0605 - val_loss: 207.7927\n",
      "Epoch 2241/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 161.1912 - val_loss: 135.9030\n",
      "Epoch 2242/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.9720 - val_loss: 146.5424\n",
      "Epoch 2243/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 149.6341 - val_loss: 156.3527\n",
      "Epoch 2244/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.0939 - val_loss: 146.9293\n",
      "Epoch 2245/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.4267 - val_loss: 181.1345\n",
      "Epoch 2246/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 233.8548 - val_loss: 159.7317\n",
      "Epoch 2247/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 176.0352 - val_loss: 144.4766\n",
      "Epoch 2248/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 152.2261 - val_loss: 138.9196\n",
      "Epoch 2249/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.1102 - val_loss: 148.9961\n",
      "Epoch 2250/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.9867 - val_loss: 177.0402\n",
      "Epoch 2251/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.6631 - val_loss: 140.4806\n",
      "Epoch 2252/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.0659 - val_loss: 162.1022\n",
      "Epoch 2253/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 138.0341 - val_loss: 141.0045\n",
      "Epoch 2254/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.6590 - val_loss: 139.7863\n",
      "Epoch 2255/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.6439 - val_loss: 249.7637\n",
      "Epoch 2256/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 141.5039 - val_loss: 178.9739\n",
      "Epoch 2257/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 257.4060 - val_loss: 141.8784\n",
      "Epoch 2258/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.4722 - val_loss: 150.6453\n",
      "Epoch 2259/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 210.8131 - val_loss: 161.6720\n",
      "Epoch 2260/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 139.7286 - val_loss: 144.5122\n",
      "Epoch 2261/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 136.4186 - val_loss: 143.6179\n",
      "Epoch 2262/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.5288 - val_loss: 170.9004\n",
      "Epoch 2263/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 134.4801 - val_loss: 140.7885\n",
      "Epoch 2264/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 159.9837 - val_loss: 184.8853\n",
      "Epoch 2265/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 140.4415 - val_loss: 135.1914\n",
      "Epoch 2266/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 147.4715 - val_loss: 144.3063\n",
      "Epoch 2267/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 157.1442 - val_loss: 148.8928\n",
      "Epoch 2268/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.4053 - val_loss: 155.6792\n",
      "Epoch 2269/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.4355 - val_loss: 140.3927\n",
      "Epoch 2270/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.4149 - val_loss: 153.0165\n",
      "Epoch 2271/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.8241 - val_loss: 143.3276\n",
      "Epoch 2272/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.6101 - val_loss: 140.3251\n",
      "Epoch 2273/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.9174 - val_loss: 142.8652\n",
      "Epoch 2274/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 162.6293 - val_loss: 227.4415\n",
      "Epoch 2275/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 196.6058 - val_loss: 140.4001\n",
      "Epoch 2276/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.8215 - val_loss: 175.0141\n",
      "Epoch 2277/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 135.8113 - val_loss: 200.5955\n",
      "Epoch 2278/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 144.5257 - val_loss: 153.9418\n",
      "Epoch 2279/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.7684 - val_loss: 140.6700\n",
      "Epoch 2280/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.1490 - val_loss: 139.6559\n",
      "Epoch 2281/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 139.5002 - val_loss: 162.1266\n",
      "Epoch 2282/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 148.5151 - val_loss: 139.9073\n",
      "Epoch 2283/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 143.9541 - val_loss: 158.6345\n",
      "Epoch 2284/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 138.7540 - val_loss: 243.1304\n",
      "Epoch 2285/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.8309 - val_loss: 141.9524\n",
      "Epoch 2286/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 146.6836 - val_loss: 155.7081\n",
      "Epoch 2287/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 131.0506 - val_loss: 166.7630\n",
      "Epoch 2288/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 140.4621 - val_loss: 140.0306\n",
      "Epoch 2289/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 141.3356 - val_loss: 137.8418\n",
      "Epoch 2290/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.1563 - val_loss: 135.8982\n",
      "Epoch 2291/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.2539 - val_loss: 142.2438\n",
      "Epoch 2292/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.2830 - val_loss: 217.1082\n",
      "Epoch 2293/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.7119 - val_loss: 168.9764\n",
      "Epoch 2294/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.8145 - val_loss: 199.5983\n",
      "Epoch 2295/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 149.4458 - val_loss: 145.4076\n",
      "Epoch 2296/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 148.5450 - val_loss: 201.9015\n",
      "Epoch 2297/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 141.5097 - val_loss: 142.5017\n",
      "Epoch 2298/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 167.0323 - val_loss: 166.9806\n",
      "Epoch 2299/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.7692 - val_loss: 145.4927\n",
      "Epoch 2300/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 136.1933 - val_loss: 143.0869\n",
      "Epoch 2301/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 154.866 - 0s 57us/step - loss: 154.4415 - val_loss: 152.6881\n",
      "Epoch 2302/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 171.7236 - val_loss: 156.8325\n",
      "Epoch 2303/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.5368 - val_loss: 165.1321\n",
      "Epoch 2304/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.9530 - val_loss: 139.0934\n",
      "Epoch 2305/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 145.1128 - val_loss: 138.3325\n",
      "Epoch 2306/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.6197 - val_loss: 140.5472\n",
      "Epoch 2307/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.7914 - val_loss: 147.7684\n",
      "Epoch 2308/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 146.5393 - val_loss: 146.9778\n",
      "Epoch 2309/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.4421 - val_loss: 154.1575\n",
      "Epoch 2310/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.0780 - val_loss: 139.4194\n",
      "Epoch 2311/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 138.7907 - val_loss: 141.1275\n",
      "Epoch 2312/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 139.4363 - val_loss: 160.2219\n",
      "Epoch 2313/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.2375 - val_loss: 138.6185\n",
      "Epoch 2314/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 151.8244 - val_loss: 139.1959\n",
      "Epoch 2315/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 162.7434 - val_loss: 143.7915\n",
      "Epoch 2316/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.1752 - val_loss: 152.8037\n",
      "Epoch 2317/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 144.8727 - val_loss: 152.8425\n",
      "Epoch 2318/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.5090 - val_loss: 158.3283\n",
      "Epoch 2319/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 57us/step - loss: 148.5740 - val_loss: 173.8940\n",
      "Epoch 2320/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 161.1123 - val_loss: 149.0342\n",
      "Epoch 2321/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 151.4087 - val_loss: 147.0845\n",
      "Epoch 2322/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 138.7088 - val_loss: 166.6071\n",
      "Epoch 2323/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.6544 - val_loss: 189.0351\n",
      "Epoch 2324/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 141.8111 - val_loss: 202.3797\n",
      "Epoch 2325/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 144.9980 - val_loss: 149.9964\n",
      "Epoch 2326/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 155.0782 - val_loss: 169.9508\n",
      "Epoch 2327/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 132.2773 - val_loss: 144.6472\n",
      "Epoch 2328/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 136.8813 - val_loss: 177.7123\n",
      "Epoch 2329/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 139.8848 - val_loss: 141.7953\n",
      "Epoch 2330/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 150.2031 - val_loss: 155.3834\n",
      "Epoch 2331/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 140.2190 - val_loss: 177.1919\n",
      "Epoch 2332/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 140.2532 - val_loss: 168.6732\n",
      "Epoch 2333/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 158.8407 - val_loss: 144.5180\n",
      "Epoch 2334/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 144.221 - 0s 57us/step - loss: 144.5004 - val_loss: 160.0600\n",
      "Epoch 2335/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 144.9125 - val_loss: 154.0699\n",
      "Epoch 2336/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.7860 - val_loss: 192.2036\n",
      "Epoch 2337/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.3308 - val_loss: 171.2356\n",
      "Epoch 2338/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 131.9237 - val_loss: 146.7867\n",
      "Epoch 2339/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 147.8016 - val_loss: 160.3974\n",
      "Epoch 2340/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.8455 - val_loss: 150.5377\n",
      "Epoch 2341/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 140.2904 - val_loss: 148.6462\n",
      "Epoch 2342/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 146.0931 - val_loss: 179.9316\n",
      "Epoch 2343/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 139.6635 - val_loss: 154.1469\n",
      "Epoch 2344/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 162.8862 - val_loss: 208.5247\n",
      "Epoch 2345/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 204.6251 - val_loss: 177.3325\n",
      "Epoch 2346/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.7273 - val_loss: 143.0038\n",
      "Epoch 2347/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 159.5226 - val_loss: 141.5883\n",
      "Epoch 2348/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 136.4317 - val_loss: 140.3641\n",
      "Epoch 2349/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 137.3228 - val_loss: 203.7818\n",
      "Epoch 2350/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 162.9223 - val_loss: 144.0974\n",
      "Epoch 2351/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 147.1034 - val_loss: 142.3535\n",
      "Epoch 2352/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.7474 - val_loss: 160.5372\n",
      "Epoch 2353/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 146.6171 - val_loss: 179.6992\n",
      "Epoch 2354/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.5505 - val_loss: 179.0653\n",
      "Epoch 2355/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.9796 - val_loss: 162.6174\n",
      "Epoch 2356/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.3457 - val_loss: 145.9883\n",
      "Epoch 2357/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 165.8828 - val_loss: 184.5921\n",
      "Epoch 2358/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.9804 - val_loss: 176.0496\n",
      "Epoch 2359/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 141.7595 - val_loss: 156.0365\n",
      "Epoch 2360/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.1005 - val_loss: 222.5929\n",
      "Epoch 2361/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 140.8540 - val_loss: 144.4108\n",
      "Epoch 2362/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 133.7202 - val_loss: 154.6709\n",
      "Epoch 2363/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 142.8228 - val_loss: 153.5325\n",
      "Epoch 2364/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 153.4916 - val_loss: 143.2692\n",
      "Epoch 2365/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.7737 - val_loss: 150.4790\n",
      "Epoch 2366/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 133.4556 - val_loss: 135.9938\n",
      "Epoch 2367/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 145.7926 - val_loss: 150.0938\n",
      "Epoch 2368/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 144.8104 - val_loss: 203.0403\n",
      "Epoch 2369/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.4504 - val_loss: 138.0591\n",
      "Epoch 2370/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 133.0324 - val_loss: 175.4189\n",
      "Epoch 2371/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.1416 - val_loss: 187.5445\n",
      "Epoch 2372/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 163.7152 - val_loss: 155.1618\n",
      "Epoch 2373/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 135.4392 - val_loss: 141.8262\n",
      "Epoch 2374/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.1715 - val_loss: 141.8676\n",
      "Epoch 2375/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.7798 - val_loss: 174.5212\n",
      "Epoch 2376/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 141.3108 - val_loss: 137.0494\n",
      "Epoch 2377/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 144.6200 - val_loss: 193.6220\n",
      "Epoch 2378/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 147.0793 - val_loss: 141.2790\n",
      "Epoch 2379/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 136.7486 - val_loss: 152.9115\n",
      "Epoch 2380/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 139.1168 - val_loss: 142.9268\n",
      "Epoch 2381/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.9767 - val_loss: 138.9962\n",
      "Epoch 2382/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 149.8416 - val_loss: 150.5878\n",
      "Epoch 2383/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 141.5084 - val_loss: 204.9927\n",
      "Epoch 2384/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 142.0062 - val_loss: 240.6187\n",
      "Epoch 2385/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 178.9186 - val_loss: 250.9244\n",
      "Epoch 2386/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 164.0961 - val_loss: 166.2524\n",
      "Epoch 2387/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.5347 - val_loss: 144.7863\n",
      "Epoch 2388/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.1587 - val_loss: 171.5142\n",
      "Epoch 2389/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.7875 - val_loss: 150.7572\n",
      "Epoch 2390/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.9929 - val_loss: 187.9065\n",
      "Epoch 2391/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 219.3400 - val_loss: 158.7074\n",
      "Epoch 2392/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 144.2130 - val_loss: 140.9948\n",
      "Epoch 2393/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 146.2408 - val_loss: 146.4499\n",
      "Epoch 2394/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.1818 - val_loss: 184.3001\n",
      "Epoch 2395/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 141.6691 - val_loss: 175.3658\n",
      "Epoch 2396/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 137.2810 - val_loss: 153.3818\n",
      "Epoch 2397/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.0355 - val_loss: 155.2891\n",
      "Epoch 2398/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 141.0750 - val_loss: 181.5397\n",
      "Epoch 2399/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 145.2402 - val_loss: 170.1993\n",
      "Epoch 2400/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 177.6859 - val_loss: 157.1352\n",
      "Epoch 2401/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 138.7926 - val_loss: 152.7190\n",
      "Epoch 2402/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 144.4843 - val_loss: 151.2799\n",
      "Epoch 2403/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 149.9464 - val_loss: 150.3915\n",
      "Epoch 2404/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 140.3664 - val_loss: 211.7541\n",
      "Epoch 2405/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 143.9857 - val_loss: 147.0019\n",
      "Epoch 2406/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.7819 - val_loss: 143.1269\n",
      "Epoch 2407/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 135.6000 - val_loss: 147.1900\n",
      "Epoch 2408/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 154.6745 - val_loss: 150.6550\n",
      "Epoch 2409/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 143.1599 - val_loss: 171.6252\n",
      "Epoch 2410/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 151.3493 - val_loss: 173.8672\n",
      "Epoch 2411/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.7112 - val_loss: 153.5264\n",
      "Epoch 2412/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 142.0208 - val_loss: 159.4084\n",
      "Epoch 2413/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 148.3644 - val_loss: 138.2557\n",
      "Epoch 2414/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 139.2852 - val_loss: 137.1965\n",
      "Epoch 2415/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.2874 - val_loss: 141.6582\n",
      "Epoch 2416/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.0652 - val_loss: 266.2967\n",
      "Epoch 2417/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.9656 - val_loss: 158.9055\n",
      "Epoch 2418/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 146.9565 - val_loss: 162.7209\n",
      "Epoch 2419/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 159.5415 - val_loss: 183.5973\n",
      "Epoch 2420/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.6500 - val_loss: 153.9167\n",
      "Epoch 2421/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 139.8907 - val_loss: 167.8567\n",
      "Epoch 2422/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 136.1925 - val_loss: 157.8881\n",
      "Epoch 2423/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 163.0519 - val_loss: 181.6067\n",
      "Epoch 2424/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 152.4426 - val_loss: 150.1748\n",
      "Epoch 2425/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 130.1725 - val_loss: 148.2641\n",
      "Epoch 2426/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 131.4697 - val_loss: 146.1854\n",
      "Epoch 2427/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 138.5577 - val_loss: 154.5925\n",
      "Epoch 2428/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 134.2920 - val_loss: 151.8649\n",
      "Epoch 2429/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 141.3556 - val_loss: 271.8841\n",
      "Epoch 2430/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 151.9346 - val_loss: 148.1434\n",
      "Epoch 2431/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 151.2370 - val_loss: 154.9442\n",
      "Epoch 2432/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 134.9052 - val_loss: 148.5082\n",
      "Epoch 2433/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 148.9096 - val_loss: 199.8821\n",
      "Epoch 2434/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.8617 - val_loss: 198.0359\n",
      "Epoch 2435/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 132.7602 - val_loss: 172.2905\n",
      "Epoch 2436/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 139.7395 - val_loss: 143.9701\n",
      "Epoch 2437/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 166.2831 - val_loss: 138.1568\n",
      "Epoch 2438/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 176.6314 - val_loss: 161.9068\n",
      "Epoch 2439/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.9124 - val_loss: 142.6478\n",
      "Epoch 2440/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.4079 - val_loss: 144.4122\n",
      "Epoch 2441/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.7321 - val_loss: 153.2680\n",
      "Epoch 2442/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.7288 - val_loss: 144.0242\n",
      "Epoch 2443/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 153.0182 - val_loss: 165.6555\n",
      "Epoch 2444/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.4199 - val_loss: 138.4279\n",
      "Epoch 2445/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 145.3452 - val_loss: 142.7130\n",
      "Epoch 2446/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.3866 - val_loss: 141.2514\n",
      "Epoch 2447/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.6966 - val_loss: 135.0559\n",
      "Epoch 2448/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.4435 - val_loss: 138.7538\n",
      "Epoch 2449/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.0354 - val_loss: 185.0238\n",
      "Epoch 2450/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 166.3329 - val_loss: 169.0118\n",
      "Epoch 2451/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 134.9176 - val_loss: 139.8043\n",
      "Epoch 2452/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.1995 - val_loss: 144.2295\n",
      "Epoch 2453/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 148.3230 - val_loss: 180.2517\n",
      "Epoch 2454/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 143.6955 - val_loss: 138.2709\n",
      "Epoch 2455/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 151.3409 - val_loss: 143.9223\n",
      "Epoch 2456/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.0995 - val_loss: 214.7371\n",
      "Epoch 2457/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 129.0099 - val_loss: 138.2758\n",
      "Epoch 2458/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 137.5461 - val_loss: 144.5285\n",
      "Epoch 2459/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 131.7306 - val_loss: 145.2694\n",
      "Epoch 2460/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.4790 - val_loss: 145.0907\n",
      "Epoch 2461/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.6293 - val_loss: 148.4417\n",
      "Epoch 2462/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 148.2409 - val_loss: 138.7757\n",
      "Epoch 2463/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 152.1637 - val_loss: 192.5969\n",
      "Epoch 2464/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.3798 - val_loss: 151.3931\n",
      "Epoch 2465/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 142.5708 - val_loss: 154.2004\n",
      "Epoch 2466/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 225.8688 - val_loss: 154.4323\n",
      "Epoch 2467/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 162.1932 - val_loss: 160.5838\n",
      "Epoch 2468/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 139.0833 - val_loss: 145.1555\n",
      "Epoch 2469/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 144.4126 - val_loss: 148.3447\n",
      "Epoch 2470/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 139.0837 - val_loss: 140.9970\n",
      "Epoch 2471/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 136.0065 - val_loss: 161.5370\n",
      "Epoch 2472/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 151.5446 - val_loss: 156.4371\n",
      "Epoch 2473/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 146.1134 - val_loss: 149.5090\n",
      "Epoch 2474/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 148.8456 - val_loss: 145.0739\n",
      "Epoch 2475/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 140.4430 - val_loss: 154.5159\n",
      "Epoch 2476/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.2388 - val_loss: 163.3685\n",
      "Epoch 2477/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.7530 - val_loss: 147.6028\n",
      "Epoch 2478/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.0335 - val_loss: 141.3171\n",
      "Epoch 2479/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 133.5380 - val_loss: 147.3687\n",
      "Epoch 2480/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.3073 - val_loss: 148.1067\n",
      "Epoch 2481/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 160.0286 - val_loss: 150.7725\n",
      "Epoch 2482/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 166.5317 - val_loss: 144.1515\n",
      "Epoch 2483/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.0230 - val_loss: 140.2398\n",
      "Epoch 2484/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 136.5505 - val_loss: 175.7751\n",
      "Epoch 2485/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.3954 - val_loss: 143.0986\n",
      "Epoch 2486/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.6213 - val_loss: 136.8935\n",
      "Epoch 2487/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 140.4917 - val_loss: 201.3857\n",
      "Epoch 2488/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 159.4620 - val_loss: 178.4775\n",
      "Epoch 2489/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 143.7625 - val_loss: 151.5231\n",
      "Epoch 2490/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.1154 - val_loss: 331.6832\n",
      "Epoch 2491/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.0564 - val_loss: 161.1924\n",
      "Epoch 2492/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.7771 - val_loss: 141.2111\n",
      "Epoch 2493/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 146.8353 - val_loss: 138.9813\n",
      "Epoch 2494/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.0166 - val_loss: 254.5229\n",
      "Epoch 2495/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 167.1194 - val_loss: 148.3973\n",
      "Epoch 2496/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.6202 - val_loss: 151.9809\n",
      "Epoch 2497/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 153.9751 - val_loss: 173.0439\n",
      "Epoch 2498/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 142.5050 - val_loss: 156.6807\n",
      "Epoch 2499/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 138.9526 - val_loss: 146.4350\n",
      "Epoch 2500/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.6742 - val_loss: 177.2094\n",
      "Epoch 2501/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 146.5669 - val_loss: 145.9234\n",
      "Epoch 2502/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.0852 - val_loss: 154.4846\n",
      "Epoch 2503/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 160.8631 - val_loss: 395.1845\n",
      "Epoch 2504/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 165.2277 - val_loss: 152.1389\n",
      "Epoch 2505/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.5100 - val_loss: 154.1867\n",
      "Epoch 2506/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.1054 - val_loss: 200.8130\n",
      "Epoch 2507/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.6297 - val_loss: 165.6426\n",
      "Epoch 2508/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.7352 - val_loss: 140.9042\n",
      "Epoch 2509/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.4851 - val_loss: 159.6520\n",
      "Epoch 2510/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 143.6517 - val_loss: 144.8370\n",
      "Epoch 2511/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 149.5803 - val_loss: 155.6575\n",
      "Epoch 2512/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 148.8721 - val_loss: 161.2615\n",
      "Epoch 2513/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 142.6171 - val_loss: 153.7229\n",
      "Epoch 2514/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 140.7532 - val_loss: 168.5914\n",
      "Epoch 2515/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.7930 - val_loss: 147.2674\n",
      "Epoch 2516/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 147.4363 - val_loss: 163.4928\n",
      "Epoch 2517/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 143.7698 - val_loss: 142.3580\n",
      "Epoch 2518/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 142.6987 - val_loss: 139.3431\n",
      "Epoch 2519/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 142.2182 - val_loss: 187.9740\n",
      "Epoch 2520/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.1436 - val_loss: 154.4264\n",
      "Epoch 2521/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 164.4474 - val_loss: 138.9759\n",
      "Epoch 2522/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 132.1706 - val_loss: 182.3806\n",
      "Epoch 2523/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.5668 - val_loss: 271.4591\n",
      "Epoch 2524/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.1569 - val_loss: 173.9074\n",
      "Epoch 2525/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 125.3934 - val_loss: 133.7339\n",
      "Epoch 2526/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 153.3337 - val_loss: 143.8857\n",
      "Epoch 2527/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 160.9082 - val_loss: 144.3574\n",
      "Epoch 2528/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.9272 - val_loss: 144.0876\n",
      "Epoch 2529/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 136.1912 - val_loss: 143.7672\n",
      "Epoch 2530/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.5589 - val_loss: 152.5057\n",
      "Epoch 2531/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 153.1816 - val_loss: 138.7642\n",
      "Epoch 2532/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.3016 - val_loss: 147.1882\n",
      "Epoch 2533/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 136.1434 - val_loss: 194.0663\n",
      "Epoch 2534/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.7863 - val_loss: 166.9141\n",
      "Epoch 2535/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.0672 - val_loss: 139.2747\n",
      "Epoch 2536/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 147.9663 - val_loss: 150.0680\n",
      "Epoch 2537/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.1719 - val_loss: 165.9465\n",
      "Epoch 2538/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 157.0973 - val_loss: 153.0048\n",
      "Epoch 2539/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.0860 - val_loss: 235.5257\n",
      "Epoch 2540/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 148.8887 - val_loss: 140.7747\n",
      "Epoch 2541/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.1854 - val_loss: 137.6141\n",
      "Epoch 2542/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 139.0689 - val_loss: 181.8482\n",
      "Epoch 2543/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 139.5206 - val_loss: 137.5897\n",
      "Epoch 2544/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 148.3240 - val_loss: 183.1014\n",
      "Epoch 2545/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 188.9824 - val_loss: 159.1318\n",
      "Epoch 2546/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 168.3936 - val_loss: 186.3454\n",
      "Epoch 2547/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 152.2738 - val_loss: 178.3240\n",
      "Epoch 2548/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 132.8403 - val_loss: 148.9690\n",
      "Epoch 2549/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 130.2944 - val_loss: 136.6778\n",
      "Epoch 2550/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 135.8193 - val_loss: 159.5334\n",
      "Epoch 2551/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 163.5409 - val_loss: 144.2851\n",
      "Epoch 2552/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.5407 - val_loss: 144.9951\n",
      "Epoch 2553/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.1297 - val_loss: 140.5834\n",
      "Epoch 2554/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.7942 - val_loss: 181.0230\n",
      "Epoch 2555/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.2739 - val_loss: 149.2402\n",
      "Epoch 2556/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 141.8940 - val_loss: 140.8967\n",
      "Epoch 2557/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 128.3603 - val_loss: 147.0754\n",
      "Epoch 2558/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 144.7354 - val_loss: 141.6599\n",
      "Epoch 2559/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 143.5661 - val_loss: 155.3542\n",
      "Epoch 2560/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 144.4342 - val_loss: 151.9071\n",
      "Epoch 2561/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 130.0008 - val_loss: 135.2246\n",
      "Epoch 2562/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 148.2843 - val_loss: 162.9243\n",
      "Epoch 2563/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.0372 - val_loss: 219.0535\n",
      "Epoch 2564/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 142.3096 - val_loss: 153.3200\n",
      "Epoch 2565/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 150.3659 - val_loss: 327.1554\n",
      "Epoch 2566/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.5582 - val_loss: 148.4016\n",
      "Epoch 2567/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 129.3675 - val_loss: 161.0014\n",
      "Epoch 2568/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 188.3507 - val_loss: 193.8520\n",
      "Epoch 2569/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.6625 - val_loss: 249.7484\n",
      "Epoch 2570/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.7037 - val_loss: 189.3385\n",
      "Epoch 2571/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.6552 - val_loss: 142.1696\n",
      "Epoch 2572/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.4927 - val_loss: 137.7445\n",
      "Epoch 2573/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.3202 - val_loss: 141.6973\n",
      "Epoch 2574/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.8580 - val_loss: 141.0093\n",
      "Epoch 2575/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.0809 - val_loss: 151.9493\n",
      "Epoch 2576/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 161.4749 - val_loss: 144.0542\n",
      "Epoch 2577/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 141.4061 - val_loss: 179.4690\n",
      "Epoch 2578/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 135.5038 - val_loss: 205.5426\n",
      "Epoch 2579/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.4082 - val_loss: 138.3378\n",
      "Epoch 2580/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 135.1302 - val_loss: 149.8306\n",
      "Epoch 2581/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.9116 - val_loss: 149.3383\n",
      "Epoch 2582/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.2725 - val_loss: 219.4589\n",
      "Epoch 2583/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 139.0968 - val_loss: 189.9299\n",
      "Epoch 2584/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.8168 - val_loss: 175.9410\n",
      "Epoch 2585/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 183.1541 - val_loss: 162.2734\n",
      "Epoch 2586/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 178.5419 - val_loss: 168.9005\n",
      "Epoch 2587/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.8307 - val_loss: 188.6953\n",
      "Epoch 2588/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 165.3180 - val_loss: 146.5610\n",
      "Epoch 2589/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 156.2645 - val_loss: 147.5361\n",
      "Epoch 2590/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 143.0522 - val_loss: 144.3142\n",
      "Epoch 2591/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 138.1766 - val_loss: 144.5012\n",
      "Epoch 2592/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 215.4930 - val_loss: 136.6136\n",
      "Epoch 2593/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 139.2125 - val_loss: 141.1919\n",
      "Epoch 2594/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 174.2911 - val_loss: 139.8187\n",
      "Epoch 2595/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.5583 - val_loss: 145.4105\n",
      "Epoch 2596/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 146.6347 - val_loss: 146.7846\n",
      "Epoch 2597/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 142.3390 - val_loss: 143.1031\n",
      "Epoch 2598/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 191.0891 - val_loss: 140.2557\n",
      "Epoch 2599/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 152.7926 - val_loss: 155.9737\n",
      "Epoch 2600/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.5736 - val_loss: 148.7985\n",
      "Epoch 2601/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 187.6775 - val_loss: 181.1888\n",
      "Epoch 2602/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 151.8245 - val_loss: 155.8548\n",
      "Epoch 2603/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.3422 - val_loss: 146.9475\n",
      "Epoch 2604/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.0452 - val_loss: 175.1585\n",
      "Epoch 2605/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.4446 - val_loss: 144.7516\n",
      "Epoch 2606/10000\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 146.0374 - val_loss: 168.1575\n",
      "Epoch 2607/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 96us/step - loss: 138.9441 - val_loss: 141.4850\n",
      "Epoch 2608/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.6890 - val_loss: 145.6444\n",
      "Epoch 2609/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.8432 - val_loss: 146.0835\n",
      "Epoch 2610/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 176.2056 - val_loss: 157.2765\n",
      "Epoch 2611/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.6383 - val_loss: 187.9151\n",
      "Epoch 2612/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.7434 - val_loss: 143.1831\n",
      "Epoch 2613/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.7559 - val_loss: 145.2565\n",
      "Epoch 2614/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.2431 - val_loss: 145.9346\n",
      "Epoch 2615/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 139.7688 - val_loss: 138.1150\n",
      "Epoch 2616/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.0428 - val_loss: 140.4229\n",
      "Epoch 2617/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 193.6512 - val_loss: 152.8558\n",
      "Epoch 2618/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 167.0184 - val_loss: 148.5153\n",
      "Epoch 2619/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 160.8162 - val_loss: 149.5244\n",
      "Epoch 2620/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.1607 - val_loss: 173.3528\n",
      "Epoch 2621/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.4935 - val_loss: 145.4398\n",
      "Epoch 2622/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.4860 - val_loss: 138.9545\n",
      "Epoch 2623/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.8034 - val_loss: 142.6107\n",
      "Epoch 2624/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 165.7887 - val_loss: 148.8578\n",
      "Epoch 2625/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 162.6425 - val_loss: 174.0571\n",
      "Epoch 2626/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.8469 - val_loss: 138.2742\n",
      "Epoch 2627/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.8544 - val_loss: 179.6425\n",
      "Epoch 2628/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.5223 - val_loss: 139.4676\n",
      "Epoch 2629/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.3233 - val_loss: 184.0768\n",
      "Epoch 2630/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.9745 - val_loss: 134.7319\n",
      "Epoch 2631/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.3670 - val_loss: 181.2493\n",
      "Epoch 2632/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.1944 - val_loss: 151.8697\n",
      "Epoch 2633/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 136.2798 - val_loss: 137.7593\n",
      "Epoch 2634/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.3535 - val_loss: 148.7870\n",
      "Epoch 2635/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.4577 - val_loss: 144.9645\n",
      "Epoch 2636/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.1969 - val_loss: 204.8368\n",
      "Epoch 2637/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.4684 - val_loss: 137.2535\n",
      "Epoch 2638/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.7569 - val_loss: 156.7270\n",
      "Epoch 2639/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.8272 - val_loss: 141.2904\n",
      "Epoch 2640/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 132.9991 - val_loss: 141.8339\n",
      "Epoch 2641/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.6160 - val_loss: 139.0315\n",
      "Epoch 2642/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.2219 - val_loss: 155.0074\n",
      "Epoch 2643/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.9077 - val_loss: 137.6112\n",
      "Epoch 2644/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.7521 - val_loss: 138.1289\n",
      "Epoch 2645/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.2024 - val_loss: 171.9314\n",
      "Epoch 2646/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 174.9178 - val_loss: 166.6563\n",
      "Epoch 2647/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 136.6864 - val_loss: 161.5291\n",
      "Epoch 2648/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.1263 - val_loss: 146.2675\n",
      "Epoch 2649/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 136.8873 - val_loss: 141.3801\n",
      "Epoch 2650/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 135.1853 - val_loss: 146.2783\n",
      "Epoch 2651/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 136.1381 - val_loss: 148.0307\n",
      "Epoch 2652/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 140.2262 - val_loss: 213.4228\n",
      "Epoch 2653/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 154.6572 - val_loss: 196.5557\n",
      "Epoch 2654/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 142.2677 - val_loss: 139.5393\n",
      "Epoch 2655/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 141.9226 - val_loss: 139.0385\n",
      "Epoch 2656/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 204.8129 - val_loss: 137.2890\n",
      "Epoch 2657/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.3949 - val_loss: 152.9111\n",
      "Epoch 2658/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 127.2301 - val_loss: 147.8686\n",
      "Epoch 2659/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 136.9701 - val_loss: 155.9984\n",
      "Epoch 2660/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 134.1661 - val_loss: 177.7363\n",
      "Epoch 2661/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 133.4587 - val_loss: 136.5442\n",
      "Epoch 2662/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 165.4490 - val_loss: 145.8587\n",
      "Epoch 2663/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.9861 - val_loss: 143.1223\n",
      "Epoch 2664/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 135.9683 - val_loss: 144.7406\n",
      "Epoch 2665/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 135.486 - 1s 63us/step - loss: 134.5971 - val_loss: 155.3096\n",
      "Epoch 2666/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.1388 - val_loss: 163.3181\n",
      "Epoch 2667/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.0205 - val_loss: 140.0394\n",
      "Epoch 2668/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 135.7417 - val_loss: 157.5315\n",
      "Epoch 2669/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 131.8837 - val_loss: 136.5954\n",
      "Epoch 2670/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 126.3134 - val_loss: 147.5461\n",
      "Epoch 2671/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 148.1047 - val_loss: 139.2609\n",
      "Epoch 2672/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.1906 - val_loss: 156.7503\n",
      "Epoch 2673/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.5489 - val_loss: 160.8972\n",
      "Epoch 2674/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.6110 - val_loss: 162.5402\n",
      "Epoch 2675/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 126.2244 - val_loss: 139.1414\n",
      "Epoch 2676/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 137.0340 - val_loss: 141.3500\n",
      "Epoch 2677/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.8456 - val_loss: 147.1488\n",
      "Epoch 2678/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 141.1530 - val_loss: 148.2456\n",
      "Epoch 2679/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 64us/step - loss: 136.2675 - val_loss: 151.2652\n",
      "Epoch 2680/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.0120 - val_loss: 143.1659\n",
      "Epoch 2681/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 160.3368 - val_loss: 146.1586\n",
      "Epoch 2682/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 137.0400 - val_loss: 159.6326\n",
      "Epoch 2683/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 152.4212 - val_loss: 146.1582\n",
      "Epoch 2684/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 134.0132 - val_loss: 140.2689\n",
      "Epoch 2685/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 135.6033 - val_loss: 145.8522\n",
      "Epoch 2686/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 152.9559 - val_loss: 147.4052\n",
      "Epoch 2687/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 140.3769 - val_loss: 150.5936\n",
      "Epoch 2688/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 142.2584 - val_loss: 243.4380\n",
      "Epoch 2689/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 138.5453 - val_loss: 144.9374\n",
      "Epoch 2690/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 128.3202 - val_loss: 146.9633\n",
      "Epoch 2691/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 128.9170 - val_loss: 136.9181\n",
      "Epoch 2692/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.8378 - val_loss: 143.1586\n",
      "Epoch 2693/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 169.6299 - val_loss: 215.2941\n",
      "Epoch 2694/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 153.2837 - val_loss: 152.8446\n",
      "Epoch 2695/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 143.8582 - val_loss: 147.6066\n",
      "Epoch 2696/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 137.6419 - val_loss: 152.4162\n",
      "Epoch 2697/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.9159 - val_loss: 179.7699- ETA: 0s - loss: 14\n",
      "Epoch 2698/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.5690 - val_loss: 188.0733\n",
      "Epoch 2699/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 133.0147 - val_loss: 148.6465\n",
      "Epoch 2700/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 148.1833 - val_loss: 174.2689\n",
      "Epoch 2701/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 151.0238 - val_loss: 154.3482\n",
      "Epoch 2702/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.6301 - val_loss: 160.2853\n",
      "Epoch 2703/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.4602 - val_loss: 136.2936\n",
      "Epoch 2704/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 129.9658 - val_loss: 144.4626\n",
      "Epoch 2705/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.6442 - val_loss: 151.4972\n",
      "Epoch 2706/10000\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 139.5768 - val_loss: 148.2758\n",
      "Epoch 2707/10000\n",
      "8000/8000 [==============================] - 1s 129us/step - loss: 141.5503 - val_loss: 171.4242\n",
      "Epoch 2708/10000\n",
      "8000/8000 [==============================] - 1s 140us/step - loss: 139.1798 - val_loss: 185.6383\n",
      "Epoch 2709/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 135.3529 - val_loss: 200.8587\n",
      "Epoch 2710/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.3187 - val_loss: 141.5688\n",
      "Epoch 2711/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 146.3951 - val_loss: 142.3640\n",
      "Epoch 2712/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 135.3663 - val_loss: 202.5508\n",
      "Epoch 2713/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 143.6109 - val_loss: 141.8735\n",
      "Epoch 2714/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 142.7560 - val_loss: 136.1297\n",
      "Epoch 2715/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.6299 - val_loss: 144.9933\n",
      "Epoch 2716/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 149.9402 - val_loss: 141.0748\n",
      "Epoch 2717/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.7245 - val_loss: 157.4543\n",
      "Epoch 2718/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.9227 - val_loss: 136.7397\n",
      "Epoch 2719/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.6337 - val_loss: 139.6213\n",
      "Epoch 2720/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.1465 - val_loss: 161.8522\n",
      "Epoch 2721/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 141.1801 - val_loss: 153.0730\n",
      "Epoch 2722/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.3545 - val_loss: 249.7585\n",
      "Epoch 2723/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.6577 - val_loss: 166.3703\n",
      "Epoch 2724/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 156.8826 - val_loss: 191.1796\n",
      "Epoch 2725/10000\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 135.1317 - val_loss: 196.2063\n",
      "Epoch 2726/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.5907 - val_loss: 144.9332\n",
      "Epoch 2727/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 138.8353 - val_loss: 186.4523\n",
      "Epoch 2728/10000\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 136.7777 - val_loss: 148.7221\n",
      "Epoch 2729/10000\n",
      "8000/8000 [==============================] - 1s 152us/step - loss: 143.2695 - val_loss: 145.5184\n",
      "Epoch 2730/10000\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 135.1907 - val_loss: 139.7444\n",
      "Epoch 2731/10000\n",
      "8000/8000 [==============================] - 1s 129us/step - loss: 138.0612 - val_loss: 139.7988\n",
      "Epoch 2732/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 132.4859 - val_loss: 143.9177\n",
      "Epoch 2733/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 128.7466 - val_loss: 139.6616\n",
      "Epoch 2734/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.9228 - val_loss: 137.3655\n",
      "Epoch 2735/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 130.1400 - val_loss: 140.4531\n",
      "Epoch 2736/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 148.6108 - val_loss: 157.8834\n",
      "Epoch 2737/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.9254 - val_loss: 192.0202\n",
      "Epoch 2738/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 139.3492 - val_loss: 153.9720\n",
      "Epoch 2739/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 131.7709 - val_loss: 150.7825\n",
      "Epoch 2740/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.9726 - val_loss: 137.1342\n",
      "Epoch 2741/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 144.3402 - val_loss: 174.7802\n",
      "Epoch 2742/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.8003 - val_loss: 186.6358\n",
      "Epoch 2743/10000\n",
      "8000/8000 [==============================] - 1s 129us/step - loss: 143.3782 - val_loss: 159.8769\n",
      "Epoch 2744/10000\n",
      "8000/8000 [==============================] - 1s 140us/step - loss: 162.1131 - val_loss: 146.8245\n",
      "Epoch 2745/10000\n",
      "8000/8000 [==============================] - 1s 142us/step - loss: 136.8973 - val_loss: 139.9839\n",
      "Epoch 2746/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 140.2977 - val_loss: 169.3823\n",
      "Epoch 2747/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 127.4096 - val_loss: 166.3517\n",
      "Epoch 2748/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.1789 - val_loss: 162.0121\n",
      "Epoch 2749/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 138.3548 - val_loss: 184.9631\n",
      "Epoch 2750/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.6205 - val_loss: 195.0858\n",
      "Epoch 2751/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.5915 - val_loss: 154.1966\n",
      "Epoch 2752/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.4687 - val_loss: 152.4059\n",
      "Epoch 2753/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 179.1336 - val_loss: 153.6754\n",
      "Epoch 2754/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.5029 - val_loss: 182.7395\n",
      "Epoch 2755/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 130.1942 - val_loss: 142.6991\n",
      "Epoch 2756/10000\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 137.3755 - val_loss: 147.1253\n",
      "Epoch 2757/10000\n",
      "8000/8000 [==============================] - 1s 130us/step - loss: 152.2403 - val_loss: 145.2696\n",
      "Epoch 2758/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 128.9385 - val_loss: 141.0408\n",
      "Epoch 2759/10000\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 139.1385 - val_loss: 154.3431\n",
      "Epoch 2760/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 140.6078 - val_loss: 144.8772\n",
      "Epoch 2761/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 143.0648 - val_loss: 142.4271\n",
      "Epoch 2762/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 136.2364 - val_loss: 140.6822\n",
      "Epoch 2763/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 129.7348 - val_loss: 145.5291\n",
      "Epoch 2764/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 132.1357 - val_loss: 142.4930\n",
      "Epoch 2765/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.2614 - val_loss: 157.2849\n",
      "Epoch 2766/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 136.2989 - val_loss: 145.3969\n",
      "Epoch 2767/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 129.6287 - val_loss: 161.9213\n",
      "Epoch 2768/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.5421 - val_loss: 192.4836\n",
      "Epoch 2769/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 172.7920 - val_loss: 232.1313\n",
      "Epoch 2770/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.2085 - val_loss: 170.9212\n",
      "Epoch 2771/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 143.0545 - val_loss: 161.2749\n",
      "Epoch 2772/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 146.8173 - val_loss: 246.9361\n",
      "Epoch 2773/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 157.5809 - val_loss: 140.2958\n",
      "Epoch 2774/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 130.9645 - val_loss: 142.1468\n",
      "Epoch 2775/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 136.5092 - val_loss: 200.2029\n",
      "Epoch 2776/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.9148 - val_loss: 148.2691\n",
      "Epoch 2777/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.4741 - val_loss: 160.4871\n",
      "Epoch 2778/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.7390 - val_loss: 146.0380\n",
      "Epoch 2779/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 143.4439 - val_loss: 144.5538\n",
      "Epoch 2780/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.1764 - val_loss: 138.9246\n",
      "Epoch 2781/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.4203 - val_loss: 148.0057\n",
      "Epoch 2782/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 134.6525 - val_loss: 155.8044\n",
      "Epoch 2783/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 129.8398 - val_loss: 169.5990\n",
      "Epoch 2784/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 134.2885 - val_loss: 142.3424\n",
      "Epoch 2785/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 139.2604 - val_loss: 137.8480\n",
      "Epoch 2786/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 132.2014 - val_loss: 138.6276\n",
      "Epoch 2787/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 126.8627 - val_loss: 141.6151\n",
      "Epoch 2788/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.7577 - val_loss: 138.6970\n",
      "Epoch 2789/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 180.4770 - val_loss: 140.4525\n",
      "Epoch 2790/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 127.6156 - val_loss: 183.3379\n",
      "Epoch 2791/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 137.0964 - val_loss: 154.1948\n",
      "Epoch 2792/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 138.6312 - val_loss: 135.5433\n",
      "Epoch 2793/10000\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 125.9362 - val_loss: 137.4292\n",
      "Epoch 2794/10000\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 149.4611 - val_loss: 157.8958\n",
      "Epoch 2795/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 126.6062 - val_loss: 199.8121\n",
      "Epoch 2796/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 202.6339 - val_loss: 166.6017\n",
      "Epoch 2797/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 228.6860 - val_loss: 153.6240\n",
      "Epoch 2798/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 195.2893 - val_loss: 206.7506\n",
      "Epoch 2799/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 167.0329 - val_loss: 147.5241\n",
      "Epoch 2800/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 132.5343 - val_loss: 143.2102\n",
      "Epoch 2801/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.8075 - val_loss: 172.9557\n",
      "Epoch 2802/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 146.7168 - val_loss: 157.5419\n",
      "Epoch 2803/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.0821 - val_loss: 135.8477\n",
      "Epoch 2804/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.3081 - val_loss: 142.6427\n",
      "Epoch 2805/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.2402 - val_loss: 233.5079\n",
      "Epoch 2806/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 201.6251 - val_loss: 146.0309\n",
      "Epoch 2807/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.6552 - val_loss: 153.0233\n",
      "Epoch 2808/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 129.9587 - val_loss: 151.9202\n",
      "Epoch 2809/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.0868 - val_loss: 153.8395\n",
      "Epoch 2810/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 134.7607 - val_loss: 144.5285\n",
      "Epoch 2811/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 133.1472 - val_loss: 181.5749\n",
      "Epoch 2812/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 129.7804 - val_loss: 140.4612\n",
      "Epoch 2813/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 156.9667 - val_loss: 142.8638\n",
      "Epoch 2814/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.8295 - val_loss: 144.6655\n",
      "Epoch 2815/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.4384 - val_loss: 148.2738\n",
      "Epoch 2816/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.6134 - val_loss: 275.8038\n",
      "Epoch 2817/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 129.7502 - val_loss: 144.9362\n",
      "Epoch 2818/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.9443 - val_loss: 182.8551\n",
      "Epoch 2819/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.2607 - val_loss: 145.7055\n",
      "Epoch 2820/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 136.5926 - val_loss: 153.9694\n",
      "Epoch 2821/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 132.1007 - val_loss: 141.4105\n",
      "Epoch 2822/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 132.3207 - val_loss: 141.7721\n",
      "Epoch 2823/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 55us/step - loss: 133.4677 - val_loss: 146.7769\n",
      "Epoch 2824/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 138.3589 - val_loss: 144.3515\n",
      "Epoch 2825/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 181.6705 - val_loss: 145.0335\n",
      "Epoch 2826/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.0520 - val_loss: 152.9389\n",
      "Epoch 2827/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 135.0020 - val_loss: 133.4857\n",
      "Epoch 2828/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.2140 - val_loss: 140.5522\n",
      "Epoch 2829/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.9982 - val_loss: 168.4529\n",
      "Epoch 2830/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.4278 - val_loss: 233.1808\n",
      "Epoch 2831/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 157.3028 - val_loss: 143.0341\n",
      "Epoch 2832/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.1011 - val_loss: 142.4373\n",
      "Epoch 2833/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 139.7829 - val_loss: 156.6566\n",
      "Epoch 2834/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.5920 - val_loss: 207.0894\n",
      "Epoch 2835/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.3978 - val_loss: 178.9978\n",
      "Epoch 2836/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.4850 - val_loss: 160.4966\n",
      "Epoch 2837/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.6229 - val_loss: 136.4055\n",
      "Epoch 2838/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.1480 - val_loss: 136.0358\n",
      "Epoch 2839/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.0059 - val_loss: 139.6288\n",
      "Epoch 2840/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.3270 - val_loss: 143.4342\n",
      "Epoch 2841/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.4353 - val_loss: 183.8542\n",
      "Epoch 2842/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.2671 - val_loss: 183.6735\n",
      "Epoch 2843/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 143.6274 - val_loss: 138.1339\n",
      "Epoch 2844/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 139.6113 - val_loss: 139.3791\n",
      "Epoch 2845/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.1087 - val_loss: 142.5226\n",
      "Epoch 2846/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.3174 - val_loss: 147.3235\n",
      "Epoch 2847/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 190.7516 - val_loss: 146.7575\n",
      "Epoch 2848/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 138.0570 - val_loss: 153.4462\n",
      "Epoch 2849/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.9745 - val_loss: 138.3257\n",
      "Epoch 2850/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 138.5566 - val_loss: 138.7810\n",
      "Epoch 2851/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.1220 - val_loss: 141.9341\n",
      "Epoch 2852/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 125.5312 - val_loss: 181.3654\n",
      "Epoch 2853/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 138.9311 - val_loss: 194.5688\n",
      "Epoch 2854/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 146.8033 - val_loss: 188.0809\n",
      "Epoch 2855/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.8179 - val_loss: 136.6085\n",
      "Epoch 2856/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.2758 - val_loss: 195.3952\n",
      "Epoch 2857/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 158.6706 - val_loss: 140.4024\n",
      "Epoch 2858/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 142.7877 - val_loss: 148.3244\n",
      "Epoch 2859/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 132.2809 - val_loss: 135.5842\n",
      "Epoch 2860/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 148.9058 - val_loss: 159.8492\n",
      "Epoch 2861/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.9544 - val_loss: 152.9113\n",
      "Epoch 2862/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.3359 - val_loss: 161.4465\n",
      "Epoch 2863/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.3641 - val_loss: 142.6650\n",
      "Epoch 2864/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 133.1446 - val_loss: 168.0187\n",
      "Epoch 2865/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 133.1204 - val_loss: 175.3679\n",
      "Epoch 2866/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 139.5716 - val_loss: 309.3175\n",
      "Epoch 2867/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.6799 - val_loss: 142.5092\n",
      "Epoch 2868/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.8276 - val_loss: 150.0755\n",
      "Epoch 2869/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 130.0770 - val_loss: 134.7706\n",
      "Epoch 2870/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 156.4855 - val_loss: 149.6275\n",
      "Epoch 2871/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 130.1516 - val_loss: 153.3075\n",
      "Epoch 2872/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 147.2479 - val_loss: 168.5798\n",
      "Epoch 2873/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.8703 - val_loss: 135.6144\n",
      "Epoch 2874/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.5637 - val_loss: 165.8725\n",
      "Epoch 2875/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.4351 - val_loss: 138.5938\n",
      "Epoch 2876/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 132.7229 - val_loss: 161.5537\n",
      "Epoch 2877/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 135.8199 - val_loss: 141.1037\n",
      "Epoch 2878/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 142.1122 - val_loss: 159.8328\n",
      "Epoch 2879/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.8245 - val_loss: 180.5817\n",
      "Epoch 2880/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 132.4471 - val_loss: 147.3593\n",
      "Epoch 2881/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.6083 - val_loss: 142.2290\n",
      "Epoch 2882/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 126.6701 - val_loss: 171.0693\n",
      "Epoch 2883/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.0251 - val_loss: 156.9130\n",
      "Epoch 2884/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.8533 - val_loss: 148.7943\n",
      "Epoch 2885/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 148.5553 - val_loss: 151.7911\n",
      "Epoch 2886/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 140.2905 - val_loss: 147.9593\n",
      "Epoch 2887/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 159.2495 - val_loss: 148.5903\n",
      "Epoch 2888/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 153.2648 - val_loss: 217.1327\n",
      "Epoch 2889/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 134.6195 - val_loss: 143.0563\n",
      "Epoch 2890/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 127.8816 - val_loss: 133.8782\n",
      "Epoch 2891/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.5830 - val_loss: 135.2241\n",
      "Epoch 2892/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.9954 - val_loss: 157.3633\n",
      "Epoch 2893/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.0334 - val_loss: 142.7811\n",
      "Epoch 2894/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.3281 - val_loss: 149.8424\n",
      "Epoch 2895/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 79us/step - loss: 150.9311 - val_loss: 168.2037\n",
      "Epoch 2896/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 228.0872 - val_loss: 167.3604\n",
      "Epoch 2897/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 169.4597 - val_loss: 358.9881\n",
      "Epoch 2898/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 154.8628 - val_loss: 144.8333\n",
      "Epoch 2899/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.2710 - val_loss: 142.8907\n",
      "Epoch 2900/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 132.5549 - val_loss: 152.7191\n",
      "Epoch 2901/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 134.7853 - val_loss: 143.5102\n",
      "Epoch 2902/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 134.1164 - val_loss: 164.1459\n",
      "Epoch 2903/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 128.4365 - val_loss: 156.7618\n",
      "Epoch 2904/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 140.9141 - val_loss: 143.4265\n",
      "Epoch 2905/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.3216 - val_loss: 143.0253\n",
      "Epoch 2906/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.0572 - val_loss: 137.0280\n",
      "Epoch 2907/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.1940 - val_loss: 157.0379\n",
      "Epoch 2908/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 138.8438 - val_loss: 144.3395\n",
      "Epoch 2909/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 130.3197 - val_loss: 152.3444\n",
      "Epoch 2910/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.0776 - val_loss: 165.9059\n",
      "Epoch 2911/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.5230 - val_loss: 136.9302\n",
      "Epoch 2912/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.9465 - val_loss: 162.3857\n",
      "Epoch 2913/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 144.6910 - val_loss: 173.3215\n",
      "Epoch 2914/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 152.9321 - val_loss: 143.9226\n",
      "Epoch 2915/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.9955 - val_loss: 137.0345\n",
      "Epoch 2916/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.4191 - val_loss: 161.1987\n",
      "Epoch 2917/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 135.5208 - val_loss: 150.8905\n",
      "Epoch 2918/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 135.3419 - val_loss: 150.4934\n",
      "Epoch 2919/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 155.3341 - val_loss: 174.5328\n",
      "Epoch 2920/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.0893 - val_loss: 138.3563\n",
      "Epoch 2921/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 130.4000 - val_loss: 157.8438\n",
      "Epoch 2922/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.8148 - val_loss: 144.5557\n",
      "Epoch 2923/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 138.2805 - val_loss: 139.1687\n",
      "Epoch 2924/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.3127 - val_loss: 255.9752\n",
      "Epoch 2925/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 138.8561 - val_loss: 226.4960\n",
      "Epoch 2926/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 171.2308 - val_loss: 144.5931\n",
      "Epoch 2927/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.1160 - val_loss: 204.3872\n",
      "Epoch 2928/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.1228 - val_loss: 154.3733\n",
      "Epoch 2929/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.2372 - val_loss: 155.8183\n",
      "Epoch 2930/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.2087 - val_loss: 141.4187\n",
      "Epoch 2931/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.8769 - val_loss: 145.2655\n",
      "Epoch 2932/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.6039 - val_loss: 161.1784\n",
      "Epoch 2933/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 145.6706 - val_loss: 277.0905\n",
      "Epoch 2934/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 149.7976 - val_loss: 135.5867\n",
      "Epoch 2935/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 173.5601 - val_loss: 263.4316\n",
      "Epoch 2936/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 131.4573 - val_loss: 169.4988\n",
      "Epoch 2937/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 129.8621 - val_loss: 138.4940\n",
      "Epoch 2938/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.8717 - val_loss: 140.0516\n",
      "Epoch 2939/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.8377 - val_loss: 175.5187\n",
      "Epoch 2940/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 136.1582 - val_loss: 137.4536\n",
      "Epoch 2941/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 127.3732 - val_loss: 134.2518\n",
      "Epoch 2942/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 129.5483 - val_loss: 137.4969\n",
      "Epoch 2943/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 129.9611 - val_loss: 155.5063\n",
      "Epoch 2944/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.7876 - val_loss: 143.5550\n",
      "Epoch 2945/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.4345 - val_loss: 148.9744\n",
      "Epoch 2946/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.0975 - val_loss: 139.0036\n",
      "Epoch 2947/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.7275 - val_loss: 173.4485\n",
      "Epoch 2948/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.0288 - val_loss: 149.6060\n",
      "Epoch 2949/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.5228 - val_loss: 158.3310\n",
      "Epoch 2950/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 156.0831 - val_loss: 142.1517\n",
      "Epoch 2951/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.3581 - val_loss: 152.4470\n",
      "Epoch 2952/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 149.8449 - val_loss: 192.7028\n",
      "Epoch 2953/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 132.2862 - val_loss: 148.9814\n",
      "Epoch 2954/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.9169 - val_loss: 138.5484\n",
      "Epoch 2955/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.2415 - val_loss: 147.0410\n",
      "Epoch 2956/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 133.4813 - val_loss: 138.2708\n",
      "Epoch 2957/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 147.4769 - val_loss: 210.9755\n",
      "Epoch 2958/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 160.0273 - val_loss: 141.3656\n",
      "Epoch 2959/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 136.1719 - val_loss: 149.9829\n",
      "Epoch 2960/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.6388 - val_loss: 149.7219\n",
      "Epoch 2961/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 127.8096 - val_loss: 145.4646\n",
      "Epoch 2962/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.0339 - val_loss: 168.7563\n",
      "Epoch 2963/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 141.1408 - val_loss: 211.3917\n",
      "Epoch 2964/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.1825 - val_loss: 139.5463\n",
      "Epoch 2965/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 125.7768 - val_loss: 152.6489\n",
      "Epoch 2966/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 174.6475 - val_loss: 137.9005\n",
      "Epoch 2967/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 56us/step - loss: 128.0273 - val_loss: 181.0760\n",
      "Epoch 2968/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.4811 - val_loss: 153.6018\n",
      "Epoch 2969/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.3931 - val_loss: 186.1659\n",
      "Epoch 2970/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.4437 - val_loss: 148.0382\n",
      "Epoch 2971/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.7846 - val_loss: 219.5365\n",
      "Epoch 2972/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 141.8844 - val_loss: 139.5096\n",
      "Epoch 2973/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 130.5485 - val_loss: 137.4495\n",
      "Epoch 2974/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.4966 - val_loss: 137.8204\n",
      "Epoch 2975/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 147.7009 - val_loss: 156.7617\n",
      "Epoch 2976/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.0508 - val_loss: 140.8802\n",
      "Epoch 2977/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.3198 - val_loss: 139.0125\n",
      "Epoch 2978/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 129.1975 - val_loss: 181.6636\n",
      "Epoch 2979/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 137.0946 - val_loss: 177.9281\n",
      "Epoch 2980/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.8800 - val_loss: 143.6737\n",
      "Epoch 2981/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.6570 - val_loss: 137.8756\n",
      "Epoch 2982/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.5917 - val_loss: 156.5725\n",
      "Epoch 2983/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.7283 - val_loss: 150.4405\n",
      "Epoch 2984/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 193.7002 - val_loss: 143.4602\n",
      "Epoch 2985/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 150.9016 - val_loss: 146.0431\n",
      "Epoch 2986/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 152.2358 - val_loss: 138.5346\n",
      "Epoch 2987/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.7255 - val_loss: 165.1187\n",
      "Epoch 2988/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 164.0970 - val_loss: 138.5215\n",
      "Epoch 2989/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.8056 - val_loss: 155.8746\n",
      "Epoch 2990/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 204.7014 - val_loss: 135.0232\n",
      "Epoch 2991/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.6980 - val_loss: 134.4411\n",
      "Epoch 2992/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.0234 - val_loss: 141.1215\n",
      "Epoch 2993/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.2747 - val_loss: 146.5379\n",
      "Epoch 2994/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 133.6019 - val_loss: 157.1949\n",
      "Epoch 2995/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 144.5218 - val_loss: 138.6409\n",
      "Epoch 2996/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.7958 - val_loss: 137.2201\n",
      "Epoch 2997/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.3617 - val_loss: 142.2339\n",
      "Epoch 2998/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 140.6372 - val_loss: 190.4250\n",
      "Epoch 2999/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.0116 - val_loss: 153.3569\n",
      "Epoch 3000/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 123.6096 - val_loss: 140.2792\n",
      "Epoch 3001/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 142.4610 - val_loss: 145.3081\n",
      "Epoch 3002/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 130.5761 - val_loss: 135.5997\n",
      "Epoch 3003/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 135.4734 - val_loss: 145.4017\n",
      "Epoch 3004/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.0863 - val_loss: 138.4225\n",
      "Epoch 3005/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 140.6268 - val_loss: 146.9568\n",
      "Epoch 3006/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 170.8774 - val_loss: 150.6984\n",
      "Epoch 3007/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 145.0340 - val_loss: 286.9697\n",
      "Epoch 3008/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.6690 - val_loss: 147.9378\n",
      "Epoch 3009/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 146.4106 - val_loss: 136.8661\n",
      "Epoch 3010/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 141.9114 - val_loss: 144.2984\n",
      "Epoch 3011/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 141.9519 - val_loss: 149.3751\n",
      "Epoch 3012/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.7882 - val_loss: 153.5963\n",
      "Epoch 3013/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 153.6141 - val_loss: 153.1219\n",
      "Epoch 3014/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.3280 - val_loss: 140.1408\n",
      "Epoch 3015/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 128.5912 - val_loss: 136.9916\n",
      "Epoch 3016/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 137.9762 - val_loss: 134.7319\n",
      "Epoch 3017/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 131.4304 - val_loss: 155.8055\n",
      "Epoch 3018/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.3788 - val_loss: 135.5545\n",
      "Epoch 3019/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.9956 - val_loss: 144.8135\n",
      "Epoch 3020/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 139.9187 - val_loss: 191.8921\n",
      "Epoch 3021/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 138.9435 - val_loss: 201.6482\n",
      "Epoch 3022/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 135.2843 - val_loss: 137.3339\n",
      "Epoch 3023/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 123.5614 - val_loss: 150.4107\n",
      "Epoch 3024/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.6197 - val_loss: 176.3531\n",
      "Epoch 3025/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.0804 - val_loss: 195.8127\n",
      "Epoch 3026/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 136.6344 - val_loss: 212.3121\n",
      "Epoch 3027/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 183.1062 - val_loss: 178.5217\n",
      "Epoch 3028/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.2233 - val_loss: 148.9702\n",
      "Epoch 3029/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.5531 - val_loss: 152.9971\n",
      "Epoch 3030/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 147.5040 - val_loss: 215.2202\n",
      "Epoch 3031/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 142.0949 - val_loss: 154.6737\n",
      "Epoch 3032/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.0916 - val_loss: 145.0091\n",
      "Epoch 3033/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 148.4275 - val_loss: 153.7236\n",
      "Epoch 3034/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.8377 - val_loss: 149.3180\n",
      "Epoch 3035/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 148.0757 - val_loss: 151.4153\n",
      "Epoch 3036/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 157.7857 - val_loss: 175.9362\n",
      "Epoch 3037/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.4944 - val_loss: 142.0777\n",
      "Epoch 3038/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.5603 - val_loss: 145.5684\n",
      "Epoch 3039/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.3265 - val_loss: 137.7912\n",
      "Epoch 3040/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.5515 - val_loss: 146.5756\n",
      "Epoch 3041/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.4687 - val_loss: 150.1427\n",
      "Epoch 3042/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.1452 - val_loss: 177.3909\n",
      "Epoch 3043/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 139.0472 - val_loss: 145.5696\n",
      "Epoch 3044/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.3555 - val_loss: 147.6053\n",
      "Epoch 3045/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 143.7761 - val_loss: 137.1862\n",
      "Epoch 3046/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 140.1120 - val_loss: 147.9881\n",
      "Epoch 3047/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.0853 - val_loss: 140.0511\n",
      "Epoch 3048/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.0138 - val_loss: 168.9179\n",
      "Epoch 3049/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 155.9148 - val_loss: 143.7525\n",
      "Epoch 3050/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 128.9196 - val_loss: 140.0845\n",
      "Epoch 3051/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 129.1602 - val_loss: 134.8047\n",
      "Epoch 3052/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 157.9584 - val_loss: 176.7996\n",
      "Epoch 3053/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.6682 - val_loss: 157.5380\n",
      "Epoch 3054/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.2953 - val_loss: 168.9508\n",
      "Epoch 3055/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.2296 - val_loss: 167.6606\n",
      "Epoch 3056/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.6860 - val_loss: 142.9132\n",
      "Epoch 3057/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.6272 - val_loss: 138.5420\n",
      "Epoch 3058/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 131.2457 - val_loss: 138.7022\n",
      "Epoch 3059/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.8515 - val_loss: 173.0931\n",
      "Epoch 3060/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 139.5462 - val_loss: 156.5364\n",
      "Epoch 3061/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 131.3440 - val_loss: 185.5109\n",
      "Epoch 3062/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 127.4473 - val_loss: 136.9599\n",
      "Epoch 3063/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 134.5228 - val_loss: 136.0649\n",
      "Epoch 3064/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 130.2731 - val_loss: 139.4692\n",
      "Epoch 3065/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.1399 - val_loss: 146.4059\n",
      "Epoch 3066/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.7605 - val_loss: 168.7236\n",
      "Epoch 3067/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 128.9262 - val_loss: 147.2795\n",
      "Epoch 3068/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 132.8896 - val_loss: 151.3921\n",
      "Epoch 3069/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.0856 - val_loss: 149.4416\n",
      "Epoch 3070/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.5137 - val_loss: 166.7929\n",
      "Epoch 3071/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.6199 - val_loss: 137.0690\n",
      "Epoch 3072/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.1163 - val_loss: 135.2471\n",
      "Epoch 3073/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 127.8087 - val_loss: 138.5104\n",
      "Epoch 3074/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 129.1659 - val_loss: 145.6891\n",
      "Epoch 3075/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.6351 - val_loss: 140.4537\n",
      "Epoch 3076/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.2691 - val_loss: 149.4366\n",
      "Epoch 3077/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.4646 - val_loss: 138.8442\n",
      "Epoch 3078/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.1476 - val_loss: 137.8610\n",
      "Epoch 3079/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.6394 - val_loss: 137.4575\n",
      "Epoch 3080/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 135.0554 - val_loss: 165.4568\n",
      "Epoch 3081/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.0540 - val_loss: 143.1863\n",
      "Epoch 3082/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 132.1749 - val_loss: 139.7724\n",
      "Epoch 3083/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 143.2767 - val_loss: 138.3209\n",
      "Epoch 3084/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 148.2336 - val_loss: 157.9622\n",
      "Epoch 3085/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 137.7577 - val_loss: 163.5159\n",
      "Epoch 3086/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.5308 - val_loss: 137.1095\n",
      "Epoch 3087/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.1547 - val_loss: 163.9438\n",
      "Epoch 3088/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.8698 - val_loss: 191.2357\n",
      "Epoch 3089/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.1643 - val_loss: 135.6537\n",
      "Epoch 3090/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.0333 - val_loss: 141.4968\n",
      "Epoch 3091/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.3858 - val_loss: 136.6582\n",
      "Epoch 3092/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 138.3939 - val_loss: 166.8165\n",
      "Epoch 3093/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.1909 - val_loss: 139.6447\n",
      "Epoch 3094/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 125.1044 - val_loss: 136.5563\n",
      "Epoch 3095/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.4222 - val_loss: 214.1288\n",
      "Epoch 3096/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.4991 - val_loss: 143.8180\n",
      "Epoch 3097/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.1366 - val_loss: 155.2062\n",
      "Epoch 3098/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.4154 - val_loss: 142.8264\n",
      "Epoch 3099/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.5206 - val_loss: 139.9264\n",
      "Epoch 3100/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.7060 - val_loss: 157.6957\n",
      "Epoch 3101/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.7464 - val_loss: 144.0063\n",
      "Epoch 3102/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.4371 - val_loss: 164.0442\n",
      "Epoch 3103/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 127.3715 - val_loss: 141.0244\n",
      "Epoch 3104/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 133.4234 - val_loss: 155.6255\n",
      "Epoch 3105/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 129.7206 - val_loss: 178.4973\n",
      "Epoch 3106/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.8658 - val_loss: 151.9546\n",
      "Epoch 3107/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 131.0651 - val_loss: 139.4776\n",
      "Epoch 3108/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.4901 - val_loss: 136.7082\n",
      "Epoch 3109/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 174.2162 - val_loss: 143.6764\n",
      "Epoch 3110/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.6939 - val_loss: 142.7924\n",
      "Epoch 3111/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 131.0220 - val_loss: 168.6682\n",
      "Epoch 3112/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.7826 - val_loss: 142.5524\n",
      "Epoch 3113/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 133.6426 - val_loss: 147.2781\n",
      "Epoch 3114/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 195.9504 - val_loss: 442.8878\n",
      "Epoch 3115/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 156.5698 - val_loss: 143.7481\n",
      "Epoch 3116/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 139.4841 - val_loss: 193.6201\n",
      "Epoch 3117/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.8382 - val_loss: 144.0964\n",
      "Epoch 3118/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 141.0961 - val_loss: 142.7054\n",
      "Epoch 3119/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 173.5522 - val_loss: 239.4024\n",
      "Epoch 3120/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 142.3594 - val_loss: 140.9060\n",
      "Epoch 3121/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 132.4712 - val_loss: 212.4624\n",
      "Epoch 3122/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 131.4288 - val_loss: 144.2748\n",
      "Epoch 3123/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 132.9572 - val_loss: 156.5261\n",
      "Epoch 3124/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.0719 - val_loss: 139.5996\n",
      "Epoch 3125/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.5771 - val_loss: 141.5568\n",
      "Epoch 3126/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 132.7809 - val_loss: 142.3162\n",
      "Epoch 3127/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.7278 - val_loss: 137.0907\n",
      "Epoch 3128/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 160.3284 - val_loss: 206.2752\n",
      "Epoch 3129/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.6496 - val_loss: 143.0827\n",
      "Epoch 3130/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 150.1890 - val_loss: 183.2461\n",
      "Epoch 3131/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.7327 - val_loss: 226.5089\n",
      "Epoch 3132/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.1022 - val_loss: 143.6788\n",
      "Epoch 3133/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.1655 - val_loss: 135.7712\n",
      "Epoch 3134/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.5533 - val_loss: 148.0336\n",
      "Epoch 3135/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.8889 - val_loss: 136.6691\n",
      "Epoch 3136/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.3416 - val_loss: 156.3542\n",
      "Epoch 3137/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.3450 - val_loss: 134.3335\n",
      "Epoch 3138/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.6765 - val_loss: 144.9854\n",
      "Epoch 3139/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.2736 - val_loss: 141.6774\n",
      "Epoch 3140/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.2405 - val_loss: 146.2500\n",
      "Epoch 3141/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.5516 - val_loss: 135.0759\n",
      "Epoch 3142/10000\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 147.9287 - val_loss: 188.6885\n",
      "Epoch 3143/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 132.7909 - val_loss: 156.7564\n",
      "Epoch 3144/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 129.3798 - val_loss: 155.4687\n",
      "Epoch 3145/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.6584 - val_loss: 182.7177\n",
      "Epoch 3146/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.5921 - val_loss: 155.6779\n",
      "Epoch 3147/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 133.7198 - val_loss: 140.7758\n",
      "Epoch 3148/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 132.1530 - val_loss: 143.8430\n",
      "Epoch 3149/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 135.2578 - val_loss: 151.3431\n",
      "Epoch 3150/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.9695 - val_loss: 134.7711\n",
      "Epoch 3151/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 126.3260 - val_loss: 142.9900\n",
      "Epoch 3152/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 155.3281 - val_loss: 138.5588\n",
      "Epoch 3153/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.8974 - val_loss: 165.6747\n",
      "Epoch 3154/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.4900 - val_loss: 179.5864\n",
      "Epoch 3155/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.1503 - val_loss: 149.7835\n",
      "Epoch 3156/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 144.0322 - val_loss: 152.3963\n",
      "Epoch 3157/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 150.0828 - val_loss: 173.1521\n",
      "Epoch 3158/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 137.1945 - val_loss: 183.8403\n",
      "Epoch 3159/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 131.6850 - val_loss: 138.1392\n",
      "Epoch 3160/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 136.3442 - val_loss: 137.5810\n",
      "Epoch 3161/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 127.0849 - val_loss: 136.0258\n",
      "Epoch 3162/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 125.5261 - val_loss: 149.8364\n",
      "Epoch 3163/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.3188 - val_loss: 207.3202\n",
      "Epoch 3164/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.9208 - val_loss: 178.2711\n",
      "Epoch 3165/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.5902 - val_loss: 141.1902\n",
      "Epoch 3166/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.8268 - val_loss: 151.8719\n",
      "Epoch 3167/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 160.4460 - val_loss: 152.3958\n",
      "Epoch 3168/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 128.8969 - val_loss: 139.2266\n",
      "Epoch 3169/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.1278 - val_loss: 153.3925\n",
      "Epoch 3170/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 126.2919 - val_loss: 151.7806\n",
      "Epoch 3171/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 131.5590 - val_loss: 249.0871\n",
      "Epoch 3172/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.4265 - val_loss: 151.7566\n",
      "Epoch 3173/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 142.5015 - val_loss: 136.8134\n",
      "Epoch 3174/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.3098 - val_loss: 159.8532\n",
      "Epoch 3175/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 144.3961 - val_loss: 154.0550\n",
      "Epoch 3176/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.2490 - val_loss: 161.5396\n",
      "Epoch 3177/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 134.1781 - val_loss: 138.0817\n",
      "Epoch 3178/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.9779 - val_loss: 136.5206\n",
      "Epoch 3179/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 133.8369 - val_loss: 145.5945\n",
      "Epoch 3180/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.1582 - val_loss: 144.6785\n",
      "Epoch 3181/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 140.4130 - val_loss: 145.4159\n",
      "Epoch 3182/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.5068 - val_loss: 183.0672\n",
      "Epoch 3183/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 57us/step - loss: 133.4107 - val_loss: 152.4952\n",
      "Epoch 3184/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 139.8939 - val_loss: 137.6175\n",
      "Epoch 3185/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 131.726 - 0s 57us/step - loss: 132.8457 - val_loss: 143.8374\n",
      "Epoch 3186/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.8311 - val_loss: 168.9177\n",
      "Epoch 3187/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 149.2704 - val_loss: 153.1593\n",
      "Epoch 3188/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 126.7561 - val_loss: 147.9592\n",
      "Epoch 3189/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 160.6911 - val_loss: 176.8665\n",
      "Epoch 3190/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.8819 - val_loss: 139.9884\n",
      "Epoch 3191/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.5778 - val_loss: 181.2661\n",
      "Epoch 3192/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 131.3464 - val_loss: 178.7329\n",
      "Epoch 3193/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 147.0983 - val_loss: 232.0153\n",
      "Epoch 3194/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 182.5662 - val_loss: 146.8356\n",
      "Epoch 3195/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.7777 - val_loss: 140.1789\n",
      "Epoch 3196/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.2600 - val_loss: 135.6178\n",
      "Epoch 3197/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.2499 - val_loss: 147.4094\n",
      "Epoch 3198/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 127.9197 - val_loss: 143.7615\n",
      "Epoch 3199/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.0074 - val_loss: 135.4826\n",
      "Epoch 3200/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.0425 - val_loss: 136.6877\n",
      "Epoch 3201/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.7928 - val_loss: 156.0911\n",
      "Epoch 3202/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 181.1610 - val_loss: 208.7272\n",
      "Epoch 3203/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.0077 - val_loss: 144.9866\n",
      "Epoch 3204/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.5724 - val_loss: 138.8595\n",
      "Epoch 3205/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 130.8722 - val_loss: 142.2000\n",
      "Epoch 3206/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 126.2892 - val_loss: 143.1777\n",
      "Epoch 3207/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.7952 - val_loss: 140.3779\n",
      "Epoch 3208/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.8801 - val_loss: 160.0988\n",
      "Epoch 3209/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 133.8353 - val_loss: 249.8285\n",
      "Epoch 3210/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.6418 - val_loss: 189.1580\n",
      "Epoch 3211/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.1007 - val_loss: 199.9388\n",
      "Epoch 3212/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.7272 - val_loss: 147.4733\n",
      "Epoch 3213/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.3668 - val_loss: 143.7956\n",
      "Epoch 3214/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 127.1836 - val_loss: 143.5868\n",
      "Epoch 3215/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 135.6069 - val_loss: 204.7211\n",
      "Epoch 3216/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 133.2069 - val_loss: 152.0820\n",
      "Epoch 3217/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 136.7411 - val_loss: 145.8111\n",
      "Epoch 3218/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.3894 - val_loss: 136.4333\n",
      "Epoch 3219/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.4266 - val_loss: 147.2990\n",
      "Epoch 3220/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 147.2267 - val_loss: 140.5346\n",
      "Epoch 3221/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.9968 - val_loss: 140.2174\n",
      "Epoch 3222/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 145.8859 - val_loss: 156.9149\n",
      "Epoch 3223/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 145.8630 - val_loss: 145.1135\n",
      "Epoch 3224/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.2849 - val_loss: 158.3885\n",
      "Epoch 3225/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 131.7070 - val_loss: 158.5225\n",
      "Epoch 3226/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.5826 - val_loss: 146.3958\n",
      "Epoch 3227/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.0515 - val_loss: 153.5712\n",
      "Epoch 3228/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.2581 - val_loss: 163.0971\n",
      "Epoch 3229/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.4920 - val_loss: 144.1806\n",
      "Epoch 3230/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.2480 - val_loss: 148.5936\n",
      "Epoch 3231/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.3963 - val_loss: 144.3562\n",
      "Epoch 3232/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 128.1894 - val_loss: 138.9964\n",
      "Epoch 3233/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 164.0201 - val_loss: 199.4788\n",
      "Epoch 3234/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 179.3400 - val_loss: 158.1923\n",
      "Epoch 3235/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 143.0328 - val_loss: 154.6942\n",
      "Epoch 3236/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 140.7415 - val_loss: 141.8310\n",
      "Epoch 3237/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.3344 - val_loss: 150.7866\n",
      "Epoch 3238/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.8201 - val_loss: 150.3664\n",
      "Epoch 3239/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 136.6164 - val_loss: 193.5778\n",
      "Epoch 3240/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 145.5961 - val_loss: 167.4732\n",
      "Epoch 3241/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.3404 - val_loss: 147.0985\n",
      "Epoch 3242/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.0185 - val_loss: 137.6373\n",
      "Epoch 3243/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 143.3837 - val_loss: 223.4391\n",
      "Epoch 3244/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 144.1900 - val_loss: 137.5805\n",
      "Epoch 3245/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.6853 - val_loss: 145.5284\n",
      "Epoch 3246/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 126.2975 - val_loss: 150.6053\n",
      "Epoch 3247/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.5388 - val_loss: 152.0982\n",
      "Epoch 3248/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.8585 - val_loss: 183.0007\n",
      "Epoch 3249/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 167.0858 - val_loss: 159.5793\n",
      "Epoch 3250/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 138.6118 - val_loss: 238.2503\n",
      "Epoch 3251/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 132.5297 - val_loss: 153.7736\n",
      "Epoch 3252/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 126.4176 - val_loss: 142.9919\n",
      "Epoch 3253/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 224.4345 - val_loss: 145.1531\n",
      "Epoch 3254/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.9949 - val_loss: 158.4438\n",
      "Epoch 3255/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.2131 - val_loss: 171.4620\n",
      "Epoch 3256/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.6665 - val_loss: 157.5325\n",
      "Epoch 3257/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.8161 - val_loss: 138.0098\n",
      "Epoch 3258/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.8543 - val_loss: 161.4425\n",
      "Epoch 3259/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.9083 - val_loss: 137.0573\n",
      "Epoch 3260/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.5490 - val_loss: 139.4693\n",
      "Epoch 3261/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.4261 - val_loss: 152.5877\n",
      "Epoch 3262/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 125.9946 - val_loss: 152.4211\n",
      "Epoch 3263/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.1257 - val_loss: 143.2040\n",
      "Epoch 3264/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.9724 - val_loss: 142.5709\n",
      "Epoch 3265/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.7260 - val_loss: 176.6644\n",
      "Epoch 3266/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 136.7349 - val_loss: 140.4105\n",
      "Epoch 3267/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 126.8277 - val_loss: 147.7733\n",
      "Epoch 3268/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 137.2147 - val_loss: 150.0817\n",
      "Epoch 3269/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 144.1966 - val_loss: 140.8023\n",
      "Epoch 3270/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 133.2281 - val_loss: 154.3085\n",
      "Epoch 3271/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 128.420 - 0s 51us/step - loss: 128.2146 - val_loss: 157.3988\n",
      "Epoch 3272/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.8345 - val_loss: 158.8623\n",
      "Epoch 3273/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.1491 - val_loss: 159.8962\n",
      "Epoch 3274/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.3685 - val_loss: 157.0172\n",
      "Epoch 3275/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.9609 - val_loss: 142.8102\n",
      "Epoch 3276/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.5756 - val_loss: 135.4094\n",
      "Epoch 3277/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.5489 - val_loss: 145.3513\n",
      "Epoch 3278/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 131.1470 - val_loss: 140.9945\n",
      "Epoch 3279/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.8276 - val_loss: 171.5220\n",
      "Epoch 3280/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 138.7758 - val_loss: 143.5509\n",
      "Epoch 3281/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 169.0408 - val_loss: 135.3384\n",
      "Epoch 3282/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 131.3501 - val_loss: 151.8699\n",
      "Epoch 3283/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 131.7656 - val_loss: 141.4214\n",
      "Epoch 3284/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.4115 - val_loss: 159.7155\n",
      "Epoch 3285/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 128.0327 - val_loss: 140.3812\n",
      "Epoch 3286/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.7288 - val_loss: 169.6540\n",
      "Epoch 3287/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 133.2932 - val_loss: 192.6443\n",
      "Epoch 3288/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.7269 - val_loss: 138.7737\n",
      "Epoch 3289/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 136.2573 - val_loss: 138.8799\n",
      "Epoch 3290/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 160.6107 - val_loss: 151.3951\n",
      "Epoch 3291/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.8209 - val_loss: 217.8149\n",
      "Epoch 3292/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 131.6185 - val_loss: 147.2034\n",
      "Epoch 3293/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 141.1563 - val_loss: 153.0658\n",
      "Epoch 3294/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.7820 - val_loss: 145.1942\n",
      "Epoch 3295/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 129.6546 - val_loss: 149.7303\n",
      "Epoch 3296/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.1481 - val_loss: 144.8571\n",
      "Epoch 3297/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.4627 - val_loss: 135.3042\n",
      "Epoch 3298/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 131.4014 - val_loss: 176.2663\n",
      "Epoch 3299/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 135.4927 - val_loss: 147.3574\n",
      "Epoch 3300/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 155.8300 - val_loss: 146.7661\n",
      "Epoch 3301/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 138.0813 - val_loss: 161.5969\n",
      "Epoch 3302/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 138.3209 - val_loss: 155.1677\n",
      "Epoch 3303/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.9043 - val_loss: 136.1699\n",
      "Epoch 3304/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.7462 - val_loss: 154.7331\n",
      "Epoch 3305/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.7129 - val_loss: 254.9192\n",
      "Epoch 3306/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.8513 - val_loss: 230.8292\n",
      "Epoch 3307/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 127.6304 - val_loss: 137.3875\n",
      "Epoch 3308/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 159.3088 - val_loss: 146.5108\n",
      "Epoch 3309/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.4981 - val_loss: 169.6174\n",
      "Epoch 3310/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.5937 - val_loss: 160.5461\n",
      "Epoch 3311/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.2351 - val_loss: 186.2960\n",
      "Epoch 3312/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 155.0076 - val_loss: 151.4001\n",
      "Epoch 3313/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 131.3192 - val_loss: 136.0590\n",
      "Epoch 3314/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 126.4701 - val_loss: 141.9917\n",
      "Epoch 3315/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 125.8586 - val_loss: 152.8831\n",
      "Epoch 3316/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.8698 - val_loss: 141.9370\n",
      "Epoch 3317/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 132.4541 - val_loss: 136.1285\n",
      "Epoch 3318/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.7924 - val_loss: 156.3507\n",
      "Epoch 3319/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 133.0020 - val_loss: 136.1276\n",
      "Epoch 3320/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 140.4134 - val_loss: 152.8079\n",
      "Epoch 3321/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 128.5291 - val_loss: 163.7470\n",
      "Epoch 3322/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.6733 - val_loss: 175.6829\n",
      "Epoch 3323/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 137.8521 - val_loss: 162.3959\n",
      "Epoch 3324/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 130.9951 - val_loss: 160.4098\n",
      "Epoch 3325/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 138.6485 - val_loss: 148.6670\n",
      "Epoch 3326/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.6281 - val_loss: 181.0145\n",
      "Epoch 3327/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.9798 - val_loss: 137.5729\n",
      "Epoch 3328/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.4740 - val_loss: 167.6066\n",
      "Epoch 3329/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.1431 - val_loss: 138.3671\n",
      "Epoch 3330/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 127.2340 - val_loss: 185.0135\n",
      "Epoch 3331/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.3074 - val_loss: 135.1601\n",
      "Epoch 3332/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.3981 - val_loss: 205.4171\n",
      "Epoch 3333/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.1201 - val_loss: 162.5714\n",
      "Epoch 3334/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.7486 - val_loss: 232.2636\n",
      "Epoch 3335/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 126.5100 - val_loss: 139.9641\n",
      "Epoch 3336/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 131.0727 - val_loss: 173.3648\n",
      "Epoch 3337/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 140.7350 - val_loss: 141.9384\n",
      "Epoch 3338/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.5431 - val_loss: 138.5382\n",
      "Epoch 3339/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 188.0029 - val_loss: 181.7752\n",
      "Epoch 3340/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.4523 - val_loss: 148.5873\n",
      "Epoch 3341/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.1685 - val_loss: 159.5308\n",
      "Epoch 3342/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.8319 - val_loss: 144.5244\n",
      "Epoch 3343/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.5935 - val_loss: 138.2212\n",
      "Epoch 3344/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 145.1861 - val_loss: 142.7573\n",
      "Epoch 3345/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.1817 - val_loss: 163.3715\n",
      "Epoch 3346/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.6466 - val_loss: 135.7748\n",
      "Epoch 3347/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.8645 - val_loss: 136.4316\n",
      "Epoch 3348/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 189.0033 - val_loss: 147.3902\n",
      "Epoch 3349/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.8154 - val_loss: 150.6419\n",
      "Epoch 3350/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 126.6310 - val_loss: 149.0656\n",
      "Epoch 3351/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 127.3139 - val_loss: 180.1056\n",
      "Epoch 3352/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 166.4521 - val_loss: 184.5027\n",
      "Epoch 3353/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 164.5225 - val_loss: 163.6933\n",
      "Epoch 3354/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 148.2022 - val_loss: 167.4089\n",
      "Epoch 3355/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 125.5986 - val_loss: 137.7709\n",
      "Epoch 3356/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.7772 - val_loss: 136.4914\n",
      "Epoch 3357/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 123.6999 - val_loss: 140.0598\n",
      "Epoch 3358/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.6018 - val_loss: 143.8308\n",
      "Epoch 3359/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.5848 - val_loss: 135.4652\n",
      "Epoch 3360/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.3258 - val_loss: 143.2999\n",
      "Epoch 3361/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.4412 - val_loss: 143.3414\n",
      "Epoch 3362/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.7051 - val_loss: 164.2842\n",
      "Epoch 3363/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 140.8765 - val_loss: 138.1394\n",
      "Epoch 3364/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 141.0756 - val_loss: 145.6353\n",
      "Epoch 3365/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.9512 - val_loss: 160.6275\n",
      "Epoch 3366/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 138.2832 - val_loss: 159.1093\n",
      "Epoch 3367/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.6640 - val_loss: 146.3751\n",
      "Epoch 3368/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 127.9463 - val_loss: 142.6334\n",
      "Epoch 3369/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 133.2752 - val_loss: 143.8030\n",
      "Epoch 3370/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 160.3087 - val_loss: 148.7893\n",
      "Epoch 3371/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.4045 - val_loss: 155.2923\n",
      "Epoch 3372/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.9892 - val_loss: 172.8077\n",
      "Epoch 3373/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 129.0330 - val_loss: 169.4148\n",
      "Epoch 3374/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.9140 - val_loss: 162.7985\n",
      "Epoch 3375/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 124.1379 - val_loss: 135.5591\n",
      "Epoch 3376/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 182.1572 - val_loss: 156.5896\n",
      "Epoch 3377/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 148.3338 - val_loss: 153.0240\n",
      "Epoch 3378/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.2526 - val_loss: 141.4032\n",
      "Epoch 3379/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 195.9851 - val_loss: 148.3821\n",
      "Epoch 3380/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 158.5853 - val_loss: 197.4032\n",
      "Epoch 3381/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.4038 - val_loss: 150.3747\n",
      "Epoch 3382/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 128.7618 - val_loss: 136.2152\n",
      "Epoch 3383/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 130.6836 - val_loss: 156.0637\n",
      "Epoch 3384/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.4208 - val_loss: 169.7228\n",
      "Epoch 3385/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 128.2153 - val_loss: 154.0192\n",
      "Epoch 3386/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 124.8021 - val_loss: 136.5008\n",
      "Epoch 3387/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 142.0742 - val_loss: 166.9950\n",
      "Epoch 3388/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.1197 - val_loss: 154.5535\n",
      "Epoch 3389/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 133.1475 - val_loss: 162.1476\n",
      "Epoch 3390/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.9692 - val_loss: 143.6780\n",
      "Epoch 3391/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 131.6897 - val_loss: 141.8617\n",
      "Epoch 3392/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 137.9798 - val_loss: 164.8005\n",
      "Epoch 3393/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.7660 - val_loss: 139.9448\n",
      "Epoch 3394/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 129.1925 - val_loss: 145.1078\n",
      "Epoch 3395/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 144.7604 - val_loss: 134.8392\n",
      "Epoch 3396/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.7454 - val_loss: 139.9991\n",
      "Epoch 3397/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.9869 - val_loss: 144.1050\n",
      "Epoch 3398/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.3322 - val_loss: 176.9779\n",
      "Epoch 3399/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.9517 - val_loss: 156.3970\n",
      "Epoch 3400/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.7638 - val_loss: 144.2314\n",
      "Epoch 3401/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.8288 - val_loss: 141.5799\n",
      "Epoch 3402/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 145.0553 - val_loss: 153.9099\n",
      "Epoch 3403/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 148.0690 - val_loss: 154.5623\n",
      "Epoch 3404/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.0794 - val_loss: 141.0218\n",
      "Epoch 3405/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 124.2792 - val_loss: 144.3862\n",
      "Epoch 3406/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.3871 - val_loss: 176.0150\n",
      "Epoch 3407/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.2675 - val_loss: 153.7656\n",
      "Epoch 3408/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.0353 - val_loss: 182.1699\n",
      "Epoch 3409/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.2140 - val_loss: 155.9865\n",
      "Epoch 3410/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.8152 - val_loss: 158.8789\n",
      "Epoch 3411/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.5846 - val_loss: 146.0249\n",
      "Epoch 3412/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.2067 - val_loss: 150.2702\n",
      "Epoch 3413/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.8939 - val_loss: 143.5065\n",
      "Epoch 3414/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 173.8466 - val_loss: 145.5515\n",
      "Epoch 3415/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 144.8187 - val_loss: 152.3333\n",
      "Epoch 3416/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 172.1905 - val_loss: 165.8693\n",
      "Epoch 3417/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.0814 - val_loss: 139.9758\n",
      "Epoch 3418/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.5430 - val_loss: 145.3212\n",
      "Epoch 3419/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 137.6990 - val_loss: 154.0033\n",
      "Epoch 3420/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 160.6044 - val_loss: 281.5366\n",
      "Epoch 3421/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 184.2516 - val_loss: 142.6167\n",
      "Epoch 3422/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 132.4251 - val_loss: 142.6085\n",
      "Epoch 3423/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.8095 - val_loss: 149.1645\n",
      "Epoch 3424/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.8188 - val_loss: 142.8175\n",
      "Epoch 3425/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 140.0667 - val_loss: 146.7471\n",
      "Epoch 3426/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.7776 - val_loss: 145.9550\n",
      "Epoch 3427/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 133.4203 - val_loss: 171.0803\n",
      "Epoch 3428/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.6269 - val_loss: 145.3214\n",
      "Epoch 3429/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.4993 - val_loss: 146.8762\n",
      "Epoch 3430/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 137.0391 - val_loss: 136.8070\n",
      "Epoch 3431/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.5571 - val_loss: 166.9241\n",
      "Epoch 3432/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 144.9505 - val_loss: 153.9593\n",
      "Epoch 3433/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 138.6923 - val_loss: 183.0892\n",
      "Epoch 3434/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 133.3302 - val_loss: 190.5669\n",
      "Epoch 3435/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.7931 - val_loss: 201.2262\n",
      "Epoch 3436/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.1167 - val_loss: 207.7252\n",
      "Epoch 3437/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.7051 - val_loss: 151.0708\n",
      "Epoch 3438/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.1570 - val_loss: 162.8841\n",
      "Epoch 3439/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.1533 - val_loss: 188.2617\n",
      "Epoch 3440/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.0818 - val_loss: 168.3392\n",
      "Epoch 3441/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.9694 - val_loss: 141.6995\n",
      "Epoch 3442/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 130.6443 - val_loss: 143.9974\n",
      "Epoch 3443/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.4654 - val_loss: 146.6818\n",
      "Epoch 3444/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.3669 - val_loss: 151.2958\n",
      "Epoch 3445/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.0444 - val_loss: 135.8002\n",
      "Epoch 3446/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.5671 - val_loss: 144.2595\n",
      "Epoch 3447/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 134.8378 - val_loss: 146.9652\n",
      "Epoch 3448/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.5050 - val_loss: 138.7035\n",
      "Epoch 3449/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 123.3950 - val_loss: 157.9402\n",
      "Epoch 3450/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.6326 - val_loss: 142.4476\n",
      "Epoch 3451/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.3596 - val_loss: 147.4483\n",
      "Epoch 3452/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 307.3917 - val_loss: 203.0021\n",
      "Epoch 3453/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.3320 - val_loss: 154.5220\n",
      "Epoch 3454/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 153.6033 - val_loss: 151.9769\n",
      "Epoch 3455/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 153.6368 - val_loss: 156.6515\n",
      "Epoch 3456/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.4326 - val_loss: 143.2694\n",
      "Epoch 3457/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 181.2990 - val_loss: 166.3123\n",
      "Epoch 3458/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.3487 - val_loss: 144.4389\n",
      "Epoch 3459/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.1854 - val_loss: 170.9295\n",
      "Epoch 3460/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 157.8499 - val_loss: 177.2919\n",
      "Epoch 3461/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 139.7267 - val_loss: 138.1692\n",
      "Epoch 3462/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 149.0355 - val_loss: 142.7726\n",
      "Epoch 3463/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.4802 - val_loss: 148.8065\n",
      "Epoch 3464/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.9607 - val_loss: 246.8055\n",
      "Epoch 3465/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.8145 - val_loss: 178.3423\n",
      "Epoch 3466/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 231.4471 - val_loss: 176.9356\n",
      "Epoch 3467/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 163.1088 - val_loss: 145.1022\n",
      "Epoch 3468/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.2224 - val_loss: 206.2242\n",
      "Epoch 3469/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 139.0005 - val_loss: 153.6189\n",
      "Epoch 3470/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.1537 - val_loss: 137.1204\n",
      "Epoch 3471/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 140.4327 - val_loss: 248.9497\n",
      "Epoch 3472/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.2510 - val_loss: 141.6234\n",
      "Epoch 3473/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.3792 - val_loss: 143.0283\n",
      "Epoch 3474/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.7354 - val_loss: 146.8083\n",
      "Epoch 3475/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 128.0344 - val_loss: 143.2132\n",
      "Epoch 3476/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.3197 - val_loss: 141.3627\n",
      "Epoch 3477/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.9558 - val_loss: 182.7015\n",
      "Epoch 3478/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.7929 - val_loss: 144.2878\n",
      "Epoch 3479/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.4433 - val_loss: 144.8544\n",
      "Epoch 3480/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.0826 - val_loss: 193.3486\n",
      "Epoch 3481/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.0791 - val_loss: 159.1102\n",
      "Epoch 3482/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.1464 - val_loss: 138.4649\n",
      "Epoch 3483/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.1791 - val_loss: 182.1021\n",
      "Epoch 3484/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 133.4252 - val_loss: 149.7799\n",
      "Epoch 3485/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.9992 - val_loss: 144.3399\n",
      "Epoch 3486/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 164.4329 - val_loss: 184.0420\n",
      "Epoch 3487/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 131.4195 - val_loss: 158.2840\n",
      "Epoch 3488/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 145.4792 - val_loss: 136.5153\n",
      "Epoch 3489/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 140.4419 - val_loss: 143.5179\n",
      "Epoch 3490/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.6237 - val_loss: 143.1845\n",
      "Epoch 3491/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.5774 - val_loss: 134.9243\n",
      "Epoch 3492/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.7425 - val_loss: 171.0280\n",
      "Epoch 3493/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.6505 - val_loss: 140.9038\n",
      "Epoch 3494/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.5783 - val_loss: 174.9690\n",
      "Epoch 3495/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.0481 - val_loss: 151.4477\n",
      "Epoch 3496/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.5633 - val_loss: 148.5097\n",
      "Epoch 3497/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.9023 - val_loss: 144.4662\n",
      "Epoch 3498/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.8450 - val_loss: 164.9397\n",
      "Epoch 3499/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 140.1361 - val_loss: 139.7190\n",
      "Epoch 3500/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.9533 - val_loss: 191.4672\n",
      "Epoch 3501/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.1829 - val_loss: 156.9788\n",
      "Epoch 3502/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 147.0134 - val_loss: 213.2596\n",
      "Epoch 3503/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 157.6714 - val_loss: 146.2890\n",
      "Epoch 3504/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 194.7141 - val_loss: 167.1512\n",
      "Epoch 3505/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 158.6671 - val_loss: 153.7081\n",
      "Epoch 3506/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.5638 - val_loss: 147.3855\n",
      "Epoch 3507/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.0376 - val_loss: 134.1072\n",
      "Epoch 3508/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 148.3391 - val_loss: 215.4955\n",
      "Epoch 3509/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.2065 - val_loss: 138.3707\n",
      "Epoch 3510/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.6499 - val_loss: 144.0900\n",
      "Epoch 3511/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.8113 - val_loss: 137.0011\n",
      "Epoch 3512/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.2667 - val_loss: 139.5032\n",
      "Epoch 3513/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 126.3248 - val_loss: 144.2589\n",
      "Epoch 3514/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.9137 - val_loss: 177.5733\n",
      "Epoch 3515/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.0308 - val_loss: 135.7441\n",
      "Epoch 3516/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.0191 - val_loss: 198.0046\n",
      "Epoch 3517/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 144.4407 - val_loss: 150.6768\n",
      "Epoch 3518/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.0517 - val_loss: 157.7347\n",
      "Epoch 3519/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.2604 - val_loss: 186.8392\n",
      "Epoch 3520/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.8922 - val_loss: 136.1915\n",
      "Epoch 3521/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.2875 - val_loss: 146.2501\n",
      "Epoch 3522/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 136.6111 - val_loss: 175.0248\n",
      "Epoch 3523/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.8229 - val_loss: 156.2417\n",
      "Epoch 3524/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.0877 - val_loss: 138.1519\n",
      "Epoch 3525/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 128.6056 - val_loss: 149.0492\n",
      "Epoch 3526/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.4785 - val_loss: 146.6040\n",
      "Epoch 3527/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 130.9116 - val_loss: 154.3131\n",
      "Epoch 3528/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.3572 - val_loss: 137.2015\n",
      "Epoch 3529/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.5529 - val_loss: 135.7678\n",
      "Epoch 3530/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 139.3918 - val_loss: 143.2339\n",
      "Epoch 3531/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.6328 - val_loss: 153.2676\n",
      "Epoch 3532/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.2773 - val_loss: 185.9981\n",
      "Epoch 3533/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 136.6843 - val_loss: 157.5998\n",
      "Epoch 3534/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.8619 - val_loss: 142.0145\n",
      "Epoch 3535/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.4613 - val_loss: 160.3478\n",
      "Epoch 3536/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.3486 - val_loss: 147.9659\n",
      "Epoch 3537/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.8172 - val_loss: 263.7494\n",
      "Epoch 3538/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.1587 - val_loss: 143.8666\n",
      "Epoch 3539/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.4880 - val_loss: 141.5205\n",
      "Epoch 3540/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.4254 - val_loss: 159.5081\n",
      "Epoch 3541/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.6677 - val_loss: 139.6919\n",
      "Epoch 3542/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.1562 - val_loss: 182.4008\n",
      "Epoch 3543/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.9409 - val_loss: 185.1410\n",
      "Epoch 3544/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.0297 - val_loss: 140.8377\n",
      "Epoch 3545/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.3407 - val_loss: 141.2304\n",
      "Epoch 3546/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.2426 - val_loss: 140.3804\n",
      "Epoch 3547/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.3405 - val_loss: 142.4582\n",
      "Epoch 3548/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.7288 - val_loss: 186.3173\n",
      "Epoch 3549/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.6323 - val_loss: 140.1197\n",
      "Epoch 3550/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 132.2733 - val_loss: 183.8694\n",
      "Epoch 3551/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 128.6196 - val_loss: 140.8705\n",
      "Epoch 3552/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.7406 - val_loss: 166.5215\n",
      "Epoch 3553/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.3851 - val_loss: 145.6220\n",
      "Epoch 3554/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 128.3513 - val_loss: 146.1413\n",
      "Epoch 3555/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 140.7540 - val_loss: 137.3555\n",
      "Epoch 3556/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 144.1884 - val_loss: 157.6156\n",
      "Epoch 3557/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 131.3952 - val_loss: 144.3183\n",
      "Epoch 3558/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.2152 - val_loss: 137.2684\n",
      "Epoch 3559/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.1377 - val_loss: 168.3569\n",
      "Epoch 3560/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.6959 - val_loss: 153.9298\n",
      "Epoch 3561/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.5593 - val_loss: 144.2368\n",
      "Epoch 3562/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.3874 - val_loss: 141.6487\n",
      "Epoch 3563/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 150.5698 - val_loss: 148.2291\n",
      "Epoch 3564/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 167.2967 - val_loss: 172.4955\n",
      "Epoch 3565/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 153.1366 - val_loss: 165.7262\n",
      "Epoch 3566/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 147.7990 - val_loss: 151.8948\n",
      "Epoch 3567/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 142.3706 - val_loss: 150.3771\n",
      "Epoch 3568/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 143.6018 - val_loss: 145.1300\n",
      "Epoch 3569/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.6701 - val_loss: 149.7202\n",
      "Epoch 3570/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 139.2248 - val_loss: 140.1949\n",
      "Epoch 3571/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.3423 - val_loss: 149.4492\n",
      "Epoch 3572/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.3259 - val_loss: 144.1118\n",
      "Epoch 3573/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 152.6154 - val_loss: 141.9788\n",
      "Epoch 3574/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 157.9763 - val_loss: 140.8389\n",
      "Epoch 3575/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.4025 - val_loss: 150.1162\n",
      "Epoch 3576/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 136.7729 - val_loss: 137.3899\n",
      "Epoch 3577/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.4835 - val_loss: 142.6315\n",
      "Epoch 3578/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 165.7347 - val_loss: 158.6407\n",
      "Epoch 3579/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 145.2964 - val_loss: 137.7483\n",
      "Epoch 3580/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 124.8593 - val_loss: 163.7677\n",
      "Epoch 3581/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 133.0226 - val_loss: 142.9059\n",
      "Epoch 3582/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 138.6074 - val_loss: 135.9481\n",
      "Epoch 3583/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 131.8156 - val_loss: 142.8079\n",
      "Epoch 3584/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 132.8512 - val_loss: 139.3390\n",
      "Epoch 3585/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.1378 - val_loss: 137.1462\n",
      "Epoch 3586/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.7472 - val_loss: 137.4779\n",
      "Epoch 3587/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.0326 - val_loss: 217.0051\n",
      "Epoch 3588/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.9038 - val_loss: 148.3496\n",
      "Epoch 3589/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 157.1921 - val_loss: 220.6749\n",
      "Epoch 3590/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 157.2885 - val_loss: 148.9781\n",
      "Epoch 3591/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.1958 - val_loss: 141.8868\n",
      "Epoch 3592/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.7257 - val_loss: 161.0208\n",
      "Epoch 3593/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.1090 - val_loss: 149.8300\n",
      "Epoch 3594/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 205.5431 - val_loss: 152.2471\n",
      "Epoch 3595/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.8176 - val_loss: 233.6504\n",
      "Epoch 3596/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.7196 - val_loss: 188.1559\n",
      "Epoch 3597/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.2355 - val_loss: 152.8203\n",
      "Epoch 3598/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.9179 - val_loss: 143.1247\n",
      "Epoch 3599/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.1653 - val_loss: 145.0254\n",
      "Epoch 3600/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.1034 - val_loss: 141.8932\n",
      "Epoch 3601/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 139.7619 - val_loss: 190.9199\n",
      "Epoch 3602/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.0211 - val_loss: 143.0876\n",
      "Epoch 3603/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.5112 - val_loss: 155.9325\n",
      "Epoch 3604/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.4028 - val_loss: 145.8653\n",
      "Epoch 3605/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.7100 - val_loss: 156.4888\n",
      "Epoch 3606/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 123.8162 - val_loss: 143.9115\n",
      "Epoch 3607/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 151.3366 - val_loss: 148.9053\n",
      "Epoch 3608/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 149.9462 - val_loss: 148.9322\n",
      "Epoch 3609/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.4149 - val_loss: 139.5888\n",
      "Epoch 3610/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.9301 - val_loss: 146.3232\n",
      "Epoch 3611/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 143.1137 - val_loss: 161.5620\n",
      "Epoch 3612/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 142.3936 - val_loss: 155.6810\n",
      "Epoch 3613/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.6893 - val_loss: 136.3298\n",
      "Epoch 3614/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 128.0110 - val_loss: 156.2809\n",
      "Epoch 3615/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 128.8924 - val_loss: 177.7030\n",
      "Epoch 3616/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 153.0074 - val_loss: 155.2717\n",
      "Epoch 3617/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.8228 - val_loss: 135.7978\n",
      "Epoch 3618/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.5115 - val_loss: 137.1697\n",
      "Epoch 3619/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.2426 - val_loss: 149.5941\n",
      "Epoch 3620/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.6634 - val_loss: 178.3954\n",
      "Epoch 3621/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.1495 - val_loss: 145.1725\n",
      "Epoch 3622/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 134.3391 - val_loss: 134.8795\n",
      "Epoch 3623/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.1752 - val_loss: 144.2004\n",
      "Epoch 3624/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 129.2609 - val_loss: 146.9316\n",
      "Epoch 3625/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 132.6337 - val_loss: 138.4420\n",
      "Epoch 3626/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 129.8493 - val_loss: 159.8203\n",
      "Epoch 3627/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 147.4206 - val_loss: 156.0842\n",
      "Epoch 3628/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.0312 - val_loss: 228.4871\n",
      "Epoch 3629/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 170.5395 - val_loss: 150.1319\n",
      "Epoch 3630/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.0900 - val_loss: 139.6326\n",
      "Epoch 3631/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.6025 - val_loss: 139.5375\n",
      "Epoch 3632/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.7259 - val_loss: 140.6820\n",
      "Epoch 3633/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.4212 - val_loss: 169.0081\n",
      "Epoch 3634/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 167.2155 - val_loss: 287.3566\n",
      "Epoch 3635/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.5004 - val_loss: 164.8294\n",
      "Epoch 3636/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.1109 - val_loss: 137.3763\n",
      "Epoch 3637/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.5779 - val_loss: 144.1051\n",
      "Epoch 3638/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.9251 - val_loss: 179.1400\n",
      "Epoch 3639/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 437.3133 - val_loss: 158.6768\n",
      "Epoch 3640/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 181.5118 - val_loss: 182.5022\n",
      "Epoch 3641/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.3139 - val_loss: 229.4527\n",
      "Epoch 3642/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.8655 - val_loss: 145.9288\n",
      "Epoch 3643/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.6836 - val_loss: 148.1860\n",
      "Epoch 3644/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.7732 - val_loss: 141.0395\n",
      "Epoch 3645/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.1387 - val_loss: 139.8869\n",
      "Epoch 3646/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.4664 - val_loss: 142.9632\n",
      "Epoch 3647/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.1467 - val_loss: 141.9739\n",
      "Epoch 3648/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 143.8186 - val_loss: 147.5524\n",
      "Epoch 3649/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.7629 - val_loss: 138.9393\n",
      "Epoch 3650/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 127.8393 - val_loss: 148.9300\n",
      "Epoch 3651/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.4587 - val_loss: 140.5816\n",
      "Epoch 3652/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.3294 - val_loss: 139.8085\n",
      "Epoch 3653/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.0982 - val_loss: 146.6054\n",
      "Epoch 3654/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.6605 - val_loss: 145.7791\n",
      "Epoch 3655/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.2016 - val_loss: 140.1919\n",
      "Epoch 3656/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.0923 - val_loss: 144.3750\n",
      "Epoch 3657/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.2580 - val_loss: 141.5487\n",
      "Epoch 3658/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 142.5252 - val_loss: 139.5662\n",
      "Epoch 3659/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 131.7682 - val_loss: 146.9644\n",
      "Epoch 3660/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.6918 - val_loss: 136.9131\n",
      "Epoch 3661/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.2484 - val_loss: 146.9551\n",
      "Epoch 3662/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.1761 - val_loss: 248.0664\n",
      "Epoch 3663/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 144.7097 - val_loss: 162.8871\n",
      "Epoch 3664/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.2457 - val_loss: 166.7790\n",
      "Epoch 3665/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.1853 - val_loss: 138.4050\n",
      "Epoch 3666/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.9365 - val_loss: 139.0746\n",
      "Epoch 3667/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.3251 - val_loss: 141.4415\n",
      "Epoch 3668/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.0811 - val_loss: 142.9223\n",
      "Epoch 3669/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.8669 - val_loss: 134.9506\n",
      "Epoch 3670/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.8862 - val_loss: 141.5982\n",
      "Epoch 3671/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.5762 - val_loss: 188.3421\n",
      "Epoch 3672/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.6759 - val_loss: 140.1398\n",
      "Epoch 3673/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.4987 - val_loss: 189.0317\n",
      "Epoch 3674/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.8462 - val_loss: 238.7324\n",
      "Epoch 3675/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.9811 - val_loss: 135.3493\n",
      "Epoch 3676/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 145.9783 - val_loss: 212.6746\n",
      "Epoch 3677/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.2037 - val_loss: 150.7581\n",
      "Epoch 3678/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.6442 - val_loss: 138.6179\n",
      "Epoch 3679/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 133.0126 - val_loss: 146.0876\n",
      "Epoch 3680/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.4147 - val_loss: 156.0231\n",
      "Epoch 3681/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 129.1205 - val_loss: 144.7942\n",
      "Epoch 3682/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.1181 - val_loss: 135.9824\n",
      "Epoch 3683/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 143.7741 - val_loss: 165.2084\n",
      "Epoch 3684/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 136.8126 - val_loss: 210.2668\n",
      "Epoch 3685/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 144.7352 - val_loss: 446.4057\n",
      "Epoch 3686/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.8804 - val_loss: 138.4772\n",
      "Epoch 3687/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.1436 - val_loss: 161.7373\n",
      "Epoch 3688/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 126.1961 - val_loss: 170.3240\n",
      "Epoch 3689/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.1122 - val_loss: 196.1625\n",
      "Epoch 3690/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.5775 - val_loss: 156.3972\n",
      "Epoch 3691/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.9985 - val_loss: 138.9874\n",
      "Epoch 3692/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.1622 - val_loss: 135.1353\n",
      "Epoch 3693/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.3442 - val_loss: 144.3081\n",
      "Epoch 3694/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.4902 - val_loss: 167.1142\n",
      "Epoch 3695/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 133.8113 - val_loss: 149.1672\n",
      "Epoch 3696/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 130.7918 - val_loss: 133.8302\n",
      "Epoch 3697/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 133.6223 - val_loss: 171.3531\n",
      "Epoch 3698/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 147.0016 - val_loss: 191.5982\n",
      "Epoch 3699/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.2195 - val_loss: 165.8784\n",
      "Epoch 3700/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.6691 - val_loss: 150.9897\n",
      "Epoch 3701/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.0194 - val_loss: 163.8470\n",
      "Epoch 3702/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.8153 - val_loss: 164.4629\n",
      "Epoch 3703/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.2964 - val_loss: 139.9022\n",
      "Epoch 3704/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.9315 - val_loss: 141.1853\n",
      "Epoch 3705/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.5465 - val_loss: 147.2858\n",
      "Epoch 3706/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.8609 - val_loss: 152.2312\n",
      "Epoch 3707/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.6308 - val_loss: 163.2806\n",
      "Epoch 3708/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 166.4378 - val_loss: 159.7651\n",
      "Epoch 3709/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.4790 - val_loss: 145.7374\n",
      "Epoch 3710/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.1702 - val_loss: 148.6247\n",
      "Epoch 3711/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 132.1470 - val_loss: 146.8516\n",
      "Epoch 3712/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.0732 - val_loss: 185.2919\n",
      "Epoch 3713/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 131.5871 - val_loss: 135.3895\n",
      "Epoch 3714/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 150.4412 - val_loss: 136.0478\n",
      "Epoch 3715/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.1458 - val_loss: 142.6617\n",
      "Epoch 3716/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.2623 - val_loss: 147.3802\n",
      "Epoch 3717/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.0530 - val_loss: 392.8442\n",
      "Epoch 3718/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 172.5161 - val_loss: 245.1865\n",
      "Epoch 3719/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 151.4533 - val_loss: 141.0784\n",
      "Epoch 3720/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.1182 - val_loss: 157.0421\n",
      "Epoch 3721/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 126.6108 - val_loss: 140.4421\n",
      "Epoch 3722/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.2142 - val_loss: 166.9851\n",
      "Epoch 3723/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 133.0332 - val_loss: 139.2644\n",
      "Epoch 3724/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 136.3642 - val_loss: 143.0581\n",
      "Epoch 3725/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.1277 - val_loss: 143.1434\n",
      "Epoch 3726/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 154.8494 - val_loss: 206.4697\n",
      "Epoch 3727/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 179.8437 - val_loss: 144.7447\n",
      "Epoch 3728/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.5123 - val_loss: 142.3416\n",
      "Epoch 3729/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 137.7346 - val_loss: 139.1470\n",
      "Epoch 3730/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 147.2677 - val_loss: 164.0622\n",
      "Epoch 3731/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 133.9879 - val_loss: 152.9538\n",
      "Epoch 3732/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 131.7884 - val_loss: 134.3144\n",
      "Epoch 3733/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.1575 - val_loss: 145.4279\n",
      "Epoch 3734/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.6067 - val_loss: 135.7556\n",
      "Epoch 3735/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.7668 - val_loss: 188.3092\n",
      "Epoch 3736/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.2693 - val_loss: 232.2850\n",
      "Epoch 3737/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.7276 - val_loss: 139.8586\n",
      "Epoch 3738/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 127.2826 - val_loss: 252.3268\n",
      "Epoch 3739/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.3404 - val_loss: 163.8875\n",
      "Epoch 3740/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 128.2139 - val_loss: 139.1891\n",
      "Epoch 3741/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.9035 - val_loss: 136.6525\n",
      "Epoch 3742/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 153.9072 - val_loss: 150.2963\n",
      "Epoch 3743/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.1507 - val_loss: 176.0482\n",
      "Epoch 3744/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.5340 - val_loss: 183.9947\n",
      "Epoch 3745/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 175.7846 - val_loss: 377.0905\n",
      "Epoch 3746/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 159.0934 - val_loss: 142.1881\n",
      "Epoch 3747/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 191.3152 - val_loss: 158.7314\n",
      "Epoch 3748/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 162.9521 - val_loss: 177.8204\n",
      "Epoch 3749/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.5213 - val_loss: 143.2543\n",
      "Epoch 3750/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.7118 - val_loss: 237.0882\n",
      "Epoch 3751/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 148.4170 - val_loss: 145.7995\n",
      "Epoch 3752/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.7190 - val_loss: 141.2947\n",
      "Epoch 3753/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 131.1883 - val_loss: 144.2053\n",
      "Epoch 3754/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.0667 - val_loss: 150.5981\n",
      "Epoch 3755/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.1709 - val_loss: 142.2615\n",
      "Epoch 3756/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 144.4567 - val_loss: 142.1426\n",
      "Epoch 3757/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 134.016 - 0s 57us/step - loss: 133.3519 - val_loss: 140.5164\n",
      "Epoch 3758/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.8762 - val_loss: 145.8018\n",
      "Epoch 3759/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.1263 - val_loss: 173.2894\n",
      "Epoch 3760/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 143.5143 - val_loss: 142.4960\n",
      "Epoch 3761/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.9537 - val_loss: 137.8663\n",
      "Epoch 3762/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.4615 - val_loss: 148.2624\n",
      "Epoch 3763/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 140.3568 - val_loss: 143.0458\n",
      "Epoch 3764/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.2029 - val_loss: 139.3141\n",
      "Epoch 3765/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 136.1086 - val_loss: 150.7089\n",
      "Epoch 3766/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 163.9457 - val_loss: 209.6688\n",
      "Epoch 3767/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.0849 - val_loss: 144.2464\n",
      "Epoch 3768/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 134.3206 - val_loss: 180.2362\n",
      "Epoch 3769/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 161.7529 - val_loss: 146.6111\n",
      "Epoch 3770/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 136.2264 - val_loss: 143.3639\n",
      "Epoch 3771/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.9374 - val_loss: 147.3735\n",
      "Epoch 3772/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 140.4564 - val_loss: 146.1219\n",
      "Epoch 3773/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 143.9338 - val_loss: 143.5577\n",
      "Epoch 3774/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 125.7187 - val_loss: 151.8811\n",
      "Epoch 3775/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 132.9605 - val_loss: 136.5489\n",
      "Epoch 3776/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 125.5924 - val_loss: 158.1860\n",
      "Epoch 3777/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.9265 - val_loss: 138.8078\n",
      "Epoch 3778/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.3502 - val_loss: 148.0726\n",
      "Epoch 3779/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.1554 - val_loss: 148.6852\n",
      "Epoch 3780/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.3179 - val_loss: 180.8440\n",
      "Epoch 3781/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.6199 - val_loss: 142.1428\n",
      "Epoch 3782/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 131.4441 - val_loss: 138.9885\n",
      "Epoch 3783/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.5593 - val_loss: 146.8185\n",
      "Epoch 3784/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.2091 - val_loss: 140.9116\n",
      "Epoch 3785/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.7115 - val_loss: 137.6344\n",
      "Epoch 3786/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.6342 - val_loss: 151.1272\n",
      "Epoch 3787/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 129.6748 - val_loss: 219.6554\n",
      "Epoch 3788/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.6726 - val_loss: 138.2401\n",
      "Epoch 3789/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.8682 - val_loss: 158.3237\n",
      "Epoch 3790/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.0949 - val_loss: 142.1763\n",
      "Epoch 3791/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.8586 - val_loss: 161.6969\n",
      "Epoch 3792/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.0757 - val_loss: 137.4034\n",
      "Epoch 3793/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.6539 - val_loss: 137.3997\n",
      "Epoch 3794/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.5908 - val_loss: 139.5041\n",
      "Epoch 3795/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 126.1859 - val_loss: 146.8119\n",
      "Epoch 3796/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.3599 - val_loss: 136.2960\n",
      "Epoch 3797/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.9419 - val_loss: 148.7003\n",
      "Epoch 3798/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.7047 - val_loss: 190.2585\n",
      "Epoch 3799/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.0076 - val_loss: 138.5447\n",
      "Epoch 3800/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.9292 - val_loss: 138.4830\n",
      "Epoch 3801/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 133.2503 - val_loss: 160.3176\n",
      "Epoch 3802/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.0572 - val_loss: 140.7209\n",
      "Epoch 3803/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.7262 - val_loss: 172.0010\n",
      "Epoch 3804/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 163.3159 - val_loss: 146.6747\n",
      "Epoch 3805/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 145.8455 - val_loss: 155.1323\n",
      "Epoch 3806/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.2821 - val_loss: 136.9098\n",
      "Epoch 3807/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 127.8480 - val_loss: 150.0226\n",
      "Epoch 3808/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 167.6461 - val_loss: 165.7464\n",
      "Epoch 3809/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 132.5447 - val_loss: 142.2967\n",
      "Epoch 3810/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 124.2986 - val_loss: 156.9791\n",
      "Epoch 3811/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 141.7393 - val_loss: 145.0163\n",
      "Epoch 3812/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.3382 - val_loss: 139.0064\n",
      "Epoch 3813/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.4137 - val_loss: 148.0203\n",
      "Epoch 3814/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.1257 - val_loss: 165.3571\n",
      "Epoch 3815/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.9463 - val_loss: 138.9245\n",
      "Epoch 3816/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 174.8920 - val_loss: 178.4576\n",
      "Epoch 3817/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 168.5346 - val_loss: 213.1626\n",
      "Epoch 3818/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 151.9933 - val_loss: 151.2045\n",
      "Epoch 3819/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 131.7773 - val_loss: 155.6941\n",
      "Epoch 3820/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 129.3214 - val_loss: 138.5831\n",
      "Epoch 3821/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.3233 - val_loss: 143.5625\n",
      "Epoch 3822/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 161.0566 - val_loss: 234.4114\n",
      "Epoch 3823/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 191.0477 - val_loss: 140.6818\n",
      "Epoch 3824/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 159.0658 - val_loss: 152.8961\n",
      "Epoch 3825/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 151.2823 - val_loss: 137.7037\n",
      "Epoch 3826/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.6929 - val_loss: 142.7933\n",
      "Epoch 3827/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.9092 - val_loss: 176.0646\n",
      "Epoch 03827: early stopping\n",
      "Fold score (RMSE): 13.085084915161133\n",
      "Fold #2\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 21574.0787 - val_loss: 5410.5429\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 4685.5675 - val_loss: 5402.3391\n",
      "Epoch 3/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 4568.6206 - val_loss: 4809.2409\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 4321.9971 - val_loss: 4594.1867\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 4184.0522 - val_loss: 4601.6693\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 4109.7419 - val_loss: 4438.4839\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 4006.9898 - val_loss: 4221.3467\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 3944.2935 - val_loss: 4255.9494\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 3844.0652 - val_loss: 4159.2088\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 3785.3024 - val_loss: 4106.5037\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 3816.2628 - val_loss: 3922.7523\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 3643.6179 - val_loss: 3902.9309\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 3525.2751 - val_loss: 3951.7769\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 3318.6764 - val_loss: 3272.5270\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 3148.1278 - val_loss: 3769.6132\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 3060.9050 - val_loss: 2855.2998\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 2788.0735 - val_loss: 2713.4015\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 2463.6342 - val_loss: 2398.2260\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 2379.7439 - val_loss: 1918.0881\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 1871.8432 - val_loss: 1502.7924\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 1932.1597 - val_loss: 2676.4524\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 2027.0171 - val_loss: 1575.0180\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 1413.6285 - val_loss: 941.3682\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 1164.0424 - val_loss: 1075.7351\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 1127.6085 - val_loss: 747.4293\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 1030.3371 - val_loss: 674.8623\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 938.1852 - val_loss: 649.7184\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 992.2761 - val_loss: 626.8887\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 800.7012 - val_loss: 554.9773\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 639.7330 - val_loss: 614.9026\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 807.6855 - val_loss: 728.9595\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 809.1815 - val_loss: 554.8590\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 633.9568 - val_loss: 1035.5669\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 683.5009 - val_loss: 607.3983\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 610.9081 - val_loss: 447.0748\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 646.3253 - val_loss: 598.4296\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 548.3650 - val_loss: 671.8595\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 723.7896 - val_loss: 626.6847\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 598.4343 - val_loss: 454.7792\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 624.8653 - val_loss: 990.9924\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 469.1934 - val_loss: 473.2408\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 594.2551 - val_loss: 563.2506\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 574.3908 - val_loss: 396.7715\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 474.7953 - val_loss: 496.9315\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 512.5043 - val_loss: 337.3757\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 505.6297 - val_loss: 926.8786\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 392.7705 - val_loss: 680.3863\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 433.7188 - val_loss: 316.3411\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 646.875 - 0s 51us/step - loss: 642.4881 - val_loss: 387.6791\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 485.2546 - val_loss: 325.3419\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 558.2296 - val_loss: 577.5781\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 389.2624 - val_loss: 414.4758\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 451.5691 - val_loss: 361.1351\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 446.9723 - val_loss: 708.0406\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 439.3215 - val_loss: 451.1972\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 384.7411 - val_loss: 406.4917\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 368.9592 - val_loss: 372.2104\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 441.0886 - val_loss: 416.6386\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 461.8203 - val_loss: 405.0928\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 403.1439 - val_loss: 641.3152\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 463.3779 - val_loss: 875.1626\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 375.5613 - val_loss: 889.5973\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 379.9586 - val_loss: 391.9993\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 341.8962 - val_loss: 317.0864\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 492.1129 - val_loss: 320.2342\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 393.7511 - val_loss: 315.5400\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 408.6635 - val_loss: 306.7323\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 444.8123 - val_loss: 532.0290\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 324.1632 - val_loss: 268.7525\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 374.0215 - val_loss: 289.8904\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 336.2402 - val_loss: 487.1349\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 370.5692 - val_loss: 466.2191\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 347.2569 - val_loss: 286.1718\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 444.0159 - val_loss: 292.6345\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 347.9355 - val_loss: 289.0616\n",
      "Epoch 76/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 332.1233 - val_loss: 311.7369\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 319.5656 - val_loss: 315.2072\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 362.2752 - val_loss: 604.8705\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 377.6541 - val_loss: 353.4505\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 322.2804 - val_loss: 314.8541\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 366.5429 - val_loss: 245.6032\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 302.8778 - val_loss: 353.1274\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 347.3738 - val_loss: 718.4597\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 419.4839 - val_loss: 281.6792\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 355.1403 - val_loss: 883.7319\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 334.6202 - val_loss: 271.6824\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 359.7865 - val_loss: 278.5689\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 299.8347 - val_loss: 471.9016\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 307.9408 - val_loss: 271.2937\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 315.5122 - val_loss: 258.3270\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 320.7539 - val_loss: 228.6342\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 379.3957 - val_loss: 261.3520\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 298.7818 - val_loss: 279.3767\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 311.1890 - val_loss: 510.5698\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 332.5418 - val_loss: 342.5778\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 296.2639 - val_loss: 264.6836\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 298.5591 - val_loss: 615.7875\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 379.1226 - val_loss: 241.7681\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 383.1488 - val_loss: 667.4580\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 386.9932 - val_loss: 568.1498\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 312.0281 - val_loss: 323.8046\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 285.8545 - val_loss: 232.9839\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 299.2615 - val_loss: 311.4652\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 287.8256 - val_loss: 554.8589\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 443.5622 - val_loss: 1040.5178\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 334.3424 - val_loss: 283.4606\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 289.2008 - val_loss: 303.4236\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 245.5030 - val_loss: 312.7091\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 278.6511 - val_loss: 257.1639\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 269.6217 - val_loss: 248.1851\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 312.1856 - val_loss: 229.9740\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 300.5518 - val_loss: 246.1758\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 299.1944 - val_loss: 297.9717\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 279.6940 - val_loss: 247.4761\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 269.7162 - val_loss: 214.8420\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 282.9208 - val_loss: 337.9557\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 311.5918 - val_loss: 213.3341\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 367.7469 - val_loss: 750.2321\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 278.0432 - val_loss: 237.3562\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 324.0561 - val_loss: 381.6612\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 449.0164 - val_loss: 280.4454\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 266.6389 - val_loss: 215.3466\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 246.7931 - val_loss: 234.2142\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 236.3935 - val_loss: 219.4312\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 240.0919 - val_loss: 280.4219\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 319.2470 - val_loss: 236.7672\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 234.6876 - val_loss: 287.3000\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 419.0092 - val_loss: 220.6297\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 293.5648 - val_loss: 252.7291\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 317.9488 - val_loss: 281.1099\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 232.7519 - val_loss: 203.3804\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 260.7848 - val_loss: 677.0574\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 315.6505 - val_loss: 248.2474\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 266.9423 - val_loss: 702.6694\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 307.2381 - val_loss: 211.4570\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 264.2180 - val_loss: 436.8985\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 259.8895 - val_loss: 245.9445\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 275.1838 - val_loss: 228.5461\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 269.6006 - val_loss: 241.5022\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 267.3621 - val_loss: 260.1871\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 230.3279 - val_loss: 258.4199\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 260.7485 - val_loss: 372.4470\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 282.4007 - val_loss: 225.2130\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 252.0792 - val_loss: 209.8777\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 216.4905 - val_loss: 193.7940\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 335.8117 - val_loss: 285.2439\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 219.0719 - val_loss: 202.6394\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 236.4905 - val_loss: 220.7927\n",
      "Epoch 149/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 256.7978 - val_loss: 244.9328\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 318.3164 - val_loss: 402.4203\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 228.8346 - val_loss: 190.9039\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 261.4820 - val_loss: 458.6930\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 237.9180 - val_loss: 342.5496\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 249.9675 - val_loss: 196.5960\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 220.5055 - val_loss: 201.1351\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 262.7870 - val_loss: 200.9126\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 374.6074 - val_loss: 252.8041\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 255.4405 - val_loss: 256.4969\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 319.3118 - val_loss: 246.6999\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 205.8933 - val_loss: 197.2949\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 245.6321 - val_loss: 205.4938\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 250.3998 - val_loss: 231.1197\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 224.5078 - val_loss: 178.6278\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 197.4251 - val_loss: 293.9397\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 235.2509 - val_loss: 215.2905\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 234.6937 - val_loss: 227.4964\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 224.7131 - val_loss: 259.4507\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 251.5956 - val_loss: 209.7962\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 233.5831 - val_loss: 287.3231\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 251.4383 - val_loss: 198.9339\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 236.5322 - val_loss: 199.3774\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 243.6269 - val_loss: 411.4257\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 232.2882 - val_loss: 187.9454\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 192.3101 - val_loss: 230.2319\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 258.7079 - val_loss: 451.3894\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 292.4142 - val_loss: 423.9919\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 225.2469 - val_loss: 206.3789\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 227.0529 - val_loss: 388.1899\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 250.9366 - val_loss: 191.9228\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 257.2776 - val_loss: 220.2456\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 214.3366 - val_loss: 207.2242\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 365.3802 - val_loss: 215.5777\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 223.4456 - val_loss: 198.3721\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 233.4696 - val_loss: 226.5545\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 194.2826 - val_loss: 189.3706\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 290.6485 - val_loss: 345.2021\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 206.6629 - val_loss: 192.3144\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 198.4338 - val_loss: 223.8833\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 228.7623 - val_loss: 503.5106\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 231.9241 - val_loss: 179.1054\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 242.0640 - val_loss: 199.5159\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 207.8701 - val_loss: 180.3370\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 291.8864 - val_loss: 214.9502\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 257.2103 - val_loss: 217.5536\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 277.6961 - val_loss: 204.2255\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 193.7305 - val_loss: 176.3562\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 233.0307 - val_loss: 256.1349\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 280.1161 - val_loss: 411.9463\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 239.0875 - val_loss: 258.4294\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 203.7641 - val_loss: 229.6002\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 209.8856 - val_loss: 186.4459\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 190.6263 - val_loss: 387.0338\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 204.7939 - val_loss: 332.9746\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 334.9779 - val_loss: 337.4287\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 217.3378 - val_loss: 177.7154\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 194.4542 - val_loss: 180.1364\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 190.7534 - val_loss: 206.2241\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 302.5134 - val_loss: 206.5661\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 234.8243 - val_loss: 182.1484\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 204.7372 - val_loss: 181.2819\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 210.5225 - val_loss: 315.6303\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 200.1754 - val_loss: 217.4096\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 212.0370 - val_loss: 168.2938\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 215.0168 - val_loss: 202.8021\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 309.0984 - val_loss: 196.1092\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 206.3080 - val_loss: 203.4980\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 199.4011 - val_loss: 171.8177\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 221.0112 - val_loss: 227.0610\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 187.4368 - val_loss: 287.0236\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 261.0846 - val_loss: 334.9562\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 247.2836 - val_loss: 187.4175\n",
      "Epoch 222/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 56us/step - loss: 251.9263 - val_loss: 219.5512\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 226.7454 - val_loss: 236.2588\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 279.0620 - val_loss: 230.1978\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 222.9250 - val_loss: 226.3886\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 219.5370 - val_loss: 258.4969\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 189.2258 - val_loss: 326.9544\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 207.9598 - val_loss: 174.4390\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 219.2408 - val_loss: 222.9658\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 265.5096 - val_loss: 209.5323\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 203.4251 - val_loss: 174.6066\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 185.7257 - val_loss: 222.0939\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 226.9952 - val_loss: 174.4869\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 187.3950 - val_loss: 171.5675\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 279.8125 - val_loss: 356.2387\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 224.367 - 0s 51us/step - loss: 226.9718 - val_loss: 384.6364\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 246.7415 - val_loss: 191.5436\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 180.7505 - val_loss: 200.0248\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 183.3844 - val_loss: 166.8554\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 189.8803 - val_loss: 331.1633\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 199.2928 - val_loss: 214.1603\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 300.7090 - val_loss: 186.1497\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 187.7394 - val_loss: 198.0766\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 180.1866 - val_loss: 219.4555\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 192.3839 - val_loss: 197.2684\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 210.7389 - val_loss: 198.6178\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 208.6292 - val_loss: 175.7533\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 198.8862 - val_loss: 183.0481\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 224.7560 - val_loss: 161.5489\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 204.2653 - val_loss: 479.3372\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 215.2945 - val_loss: 223.9683\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 185.3692 - val_loss: 188.1477\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 213.8917 - val_loss: 293.2206\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 198.3855 - val_loss: 173.6796\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 178.629 - 0s 57us/step - loss: 179.2992 - val_loss: 182.1908\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 207.2636 - val_loss: 285.9911\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 269.1727 - val_loss: 181.9291\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 176.0500 - val_loss: 160.8747\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 256.2301 - val_loss: 175.4403\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 195.5959 - val_loss: 196.5144\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 224.7698 - val_loss: 249.1186\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 171.4179 - val_loss: 165.5414\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 185.6427 - val_loss: 164.3999\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 173.4522 - val_loss: 183.0637\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 197.8150 - val_loss: 168.1268\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 194.2668 - val_loss: 452.5999\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 558.0755 - val_loss: 292.5649\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 285.5110 - val_loss: 263.3532\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 278.7756 - val_loss: 310.7349\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 277.9074 - val_loss: 302.1561\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 207.7767 - val_loss: 330.0418\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 269.2575 - val_loss: 188.9490\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 260.2969 - val_loss: 185.8863\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 200.8118 - val_loss: 259.1397\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 224.7565 - val_loss: 177.9390\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 260.6842 - val_loss: 195.0698\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 190.9458 - val_loss: 169.5134\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 212.7592 - val_loss: 175.7619\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 171.0449 - val_loss: 175.5157\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 184.8926 - val_loss: 169.1281\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 213.0815 - val_loss: 185.0459\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 335.8445 - val_loss: 187.0968\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 179.7626 - val_loss: 199.3770\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 189.2655 - val_loss: 166.5574\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 181.3798 - val_loss: 230.6906\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.1891 - val_loss: 180.8940\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 175.4712 - val_loss: 179.1687\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 214.2231 - val_loss: 176.4048\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 324.7206 - val_loss: 195.1628\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 201.8480 - val_loss: 184.8221\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 180.2999 - val_loss: 171.6386\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 190.1694 - val_loss: 191.3998\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 192.0416 - val_loss: 179.5146\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 188.2247 - val_loss: 244.8090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 215.9611 - val_loss: 176.1446\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 176.4300 - val_loss: 175.0679\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 237.7524 - val_loss: 173.6762\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 190.0028 - val_loss: 191.9383\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 175.2026 - val_loss: 155.6360\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.6445 - val_loss: 165.4456\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 187.7124 - val_loss: 182.8705\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 169.0405 - val_loss: 180.5604\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 247.1466 - val_loss: 283.8037\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 245.1611 - val_loss: 174.5951\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 164.1414 - val_loss: 181.2317\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 177.9807 - val_loss: 185.2629\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 180.0063 - val_loss: 172.8929\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 195.6639 - val_loss: 352.7129\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 186.2337 - val_loss: 210.0688\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 175.5575 - val_loss: 395.5386\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 188.6496 - val_loss: 285.5629\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 208.9308 - val_loss: 242.3004\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 183.5398 - val_loss: 162.8003\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 168.9843 - val_loss: 183.7202\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.2946 - val_loss: 162.2475\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 215.2436 - val_loss: 167.4323\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 189.3098 - val_loss: 212.9497\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 285.6282 - val_loss: 339.5928\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 174.9316 - val_loss: 204.4135\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 251.3224 - val_loss: 4323.6643\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 532.7377 - val_loss: 232.1709\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 236.7184 - val_loss: 224.0532\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 212.3293 - val_loss: 319.6861\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 228.3509 - val_loss: 226.0162\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 201.4054 - val_loss: 209.6591\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 197.0082 - val_loss: 275.7218\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 238.8801 - val_loss: 201.9834\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 242.9901 - val_loss: 262.7271\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 190.4794 - val_loss: 300.8572\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 218.9181 - val_loss: 221.6251\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 227.0588 - val_loss: 503.6142\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 263.6063 - val_loss: 207.7232\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 188.3767 - val_loss: 192.6720\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 228.9237 - val_loss: 173.2608\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 230.6235 - val_loss: 450.6862\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 316.1914 - val_loss: 195.8281\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 190.2430 - val_loss: 208.0455\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 174.9911 - val_loss: 170.6987\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 188.8895 - val_loss: 291.4689\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 192.8462 - val_loss: 212.5766\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 212.8342 - val_loss: 175.9633\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 193.8202 - val_loss: 203.3478\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 237.1902 - val_loss: 168.1542\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 187.9646 - val_loss: 286.4734\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 278.2646 - val_loss: 167.1689\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 181.6529 - val_loss: 249.5254\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 181.3905 - val_loss: 258.1713\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 199.2445 - val_loss: 186.4935\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 387.3367 - val_loss: 206.5471\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 194.6575 - val_loss: 177.0546\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 217.2774 - val_loss: 184.0118\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 195.8074 - val_loss: 163.7679\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 204.6434 - val_loss: 195.2891\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 178.3458 - val_loss: 248.5503\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 184.4848 - val_loss: 196.4754\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 225.7050 - val_loss: 176.8343\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 191.8067 - val_loss: 165.7231\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 227.3931 - val_loss: 675.4554\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 475.5978 - val_loss: 257.8713\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 194.0229 - val_loss: 175.7092\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 199.9633 - val_loss: 258.0861\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 216.5694 - val_loss: 346.9793\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 183.3530 - val_loss: 192.4384\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 186.0141 - val_loss: 206.7463\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 169.5280 - val_loss: 163.5933\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 184.0515 - val_loss: 170.2198\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 348.1942 - val_loss: 195.3584\n",
      "Epoch 368/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step - loss: 250.6428 - val_loss: 592.7802\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 222.1208 - val_loss: 170.6985\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 192.1041 - val_loss: 182.4842\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 195.1771 - val_loss: 211.9267\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 233.3195 - val_loss: 179.6825\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 194.7611 - val_loss: 229.2948\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 206.0321 - val_loss: 171.2146\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 195.3071 - val_loss: 207.8681\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 205.0159 - val_loss: 197.4389\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 192.9946 - val_loss: 214.7778\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 210.7762 - val_loss: 213.4990\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 170.6375 - val_loss: 165.7862\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 229.9699 - val_loss: 398.7453\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 243.7825 - val_loss: 205.9125\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 189.4092 - val_loss: 209.6421\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 208.0400 - val_loss: 160.7037\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 179.5670 - val_loss: 162.9635\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 215.2710 - val_loss: 410.6424\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 204.9729 - val_loss: 200.7030\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 200.6162 - val_loss: 181.0411\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 186.3965 - val_loss: 267.5243\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 184.8498 - val_loss: 185.3584\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 421.0382 - val_loss: 332.3604\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 284.8289 - val_loss: 275.9948\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 200.7939 - val_loss: 196.0533\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 193.0777 - val_loss: 234.5404\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 180.9487 - val_loss: 172.6253\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 187.8535 - val_loss: 169.4616\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 173.6200 - val_loss: 187.8391\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 172.7323 - val_loss: 177.8189\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 180.7076 - val_loss: 159.1966\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 177.6408 - val_loss: 224.9078\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 183.8424 - val_loss: 192.0499\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 180.3810 - val_loss: 207.5523\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 288.1933 - val_loss: 255.7962\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 185.9499 - val_loss: 226.8965\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.4431 - val_loss: 219.9144\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 166.9240 - val_loss: 181.2817\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.8190 - val_loss: 174.1324\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.4026 - val_loss: 173.7614\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 190.8083 - val_loss: 159.1303\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 176.8842 - val_loss: 205.5658\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 175.9120 - val_loss: 185.0803\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.3350 - val_loss: 170.1102\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 174.3276 - val_loss: 261.4910\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 206.3064 - val_loss: 184.8686\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 161.9051 - val_loss: 195.1562\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 225.6238 - val_loss: 170.0685\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 226.9332 - val_loss: 173.4825\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 173.3933 - val_loss: 291.8396\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 226.2475 - val_loss: 158.6386\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 182.4948 - val_loss: 210.8629\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.1903 - val_loss: 160.0682\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 251.2668 - val_loss: 213.8501\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 160.3410 - val_loss: 157.9110\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.5505 - val_loss: 357.0331\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 233.3175 - val_loss: 193.8335\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 250.5461 - val_loss: 179.4796\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 170.6717 - val_loss: 199.4693\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 191.5119 - val_loss: 425.0115\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 191.1384 - val_loss: 182.3925\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 173.8352 - val_loss: 166.4189\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 156.0643 - val_loss: 173.3983\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 219.7968 - val_loss: 179.6512\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 181.3784 - val_loss: 379.6015\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 205.0540 - val_loss: 175.3665\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 184.0268 - val_loss: 190.1197\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 203.5053 - val_loss: 757.6985\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 228.4959 - val_loss: 189.3912\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 161.7684 - val_loss: 175.1925\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 155.4462 - val_loss: 159.2122\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 184.7526 - val_loss: 162.4661\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 184.9333 - val_loss: 183.1220\n",
      "Epoch 441/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 206.0868 - val_loss: 184.2678\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 203.5225 - val_loss: 206.7519\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 159.8472 - val_loss: 244.8097\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 192.7864 - val_loss: 176.7647\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 174.4301 - val_loss: 166.9456\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 189.0952 - val_loss: 190.4824\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 158.0481 - val_loss: 167.3145\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 212.9689 - val_loss: 222.2538\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 166.4907 - val_loss: 177.7157\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 180.4909 - val_loss: 166.1863\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 181.2268 - val_loss: 226.0704\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 163.1669 - val_loss: 163.0456\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 232.4117 - val_loss: 152.8307\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 156.3107 - val_loss: 238.3052\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 180.2556 - val_loss: 165.9990\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 162.5474 - val_loss: 156.8634\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 192.1055 - val_loss: 457.4069\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 179.0801 - val_loss: 154.4979\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 163.1985 - val_loss: 171.0276\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 169.9560 - val_loss: 187.8039\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 394.4003 - val_loss: 162.4549\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.7559 - val_loss: 151.2634\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 155.1799 - val_loss: 202.6478\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 188.2368 - val_loss: 176.7270\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 158.7131 - val_loss: 165.7555\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 154.0403 - val_loss: 225.1960\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 165.8746 - val_loss: 164.6965\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 187.2500 - val_loss: 176.0690\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 156.8866 - val_loss: 187.1001\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.0804 - val_loss: 164.9642\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 163.0454 - val_loss: 216.8387\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 175.6344 - val_loss: 218.0338\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 240.0150 - val_loss: 159.6318\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 207.6882 - val_loss: 946.8750\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 266.7343 - val_loss: 406.3733\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 166.8184 - val_loss: 217.0001\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.5711 - val_loss: 223.4901\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 199.2339 - val_loss: 272.2744\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 174.0435 - val_loss: 200.0068\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 178.7889 - val_loss: 630.9014\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 207.7296 - val_loss: 158.9214\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.7674 - val_loss: 275.1823\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.3734 - val_loss: 173.4151\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.3672 - val_loss: 176.6123\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 178.6038 - val_loss: 149.8200\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 234.5625 - val_loss: 176.4816\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.6888 - val_loss: 154.6658\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 159.0072 - val_loss: 214.5935\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 160.9970 - val_loss: 177.6865\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 151.8639 - val_loss: 170.8698\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.9055 - val_loss: 168.4777\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 174.4170 - val_loss: 347.4709\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 162.1656 - val_loss: 175.4147\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 167.4364 - val_loss: 153.6826\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 163.1813 - val_loss: 181.3458\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 160.2805 - val_loss: 157.5279\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 177.1890 - val_loss: 160.1739\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 188.0040 - val_loss: 152.2025\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 179.3533 - val_loss: 170.3784\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 174.8464 - val_loss: 549.0921\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 209.6294 - val_loss: 166.5415\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 196.6531 - val_loss: 173.5430\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 156.7342 - val_loss: 209.6436\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 212.0921 - val_loss: 352.5488\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 149.9743 - val_loss: 192.7914\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 156.3540 - val_loss: 161.6597\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 175.1974 - val_loss: 194.8008\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 363.5335 - val_loss: 224.4040\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 164.0069 - val_loss: 164.3488\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.4865 - val_loss: 149.2200\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 155.4240 - val_loss: 156.2531\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.8880 - val_loss: 161.7112\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 165.7138 - val_loss: 220.5323\n",
      "Epoch 514/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 57us/step - loss: 155.9700 - val_loss: 168.1864\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 409.2029 - val_loss: 605.9484\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 304.8059 - val_loss: 195.7116\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 223.6206 - val_loss: 177.1117\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 184.8764 - val_loss: 172.3955\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 188.2777 - val_loss: 195.3035\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 189.9077 - val_loss: 200.6791\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 218.8880 - val_loss: 255.5413\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 183.4841 - val_loss: 183.0547\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 212.9389 - val_loss: 216.4505\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 181.8868 - val_loss: 191.0906\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 203.6524 - val_loss: 298.5294\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 421.7942 - val_loss: 202.5121\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 175.4353 - val_loss: 169.9111\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 204.6981 - val_loss: 159.7897\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 164.7762 - val_loss: 261.3064\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 166.8767 - val_loss: 200.4122\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.0210 - val_loss: 186.6974\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 201.2493 - val_loss: 305.7940\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 200.2530 - val_loss: 181.6382\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 171.3143 - val_loss: 171.0543\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 179.3910 - val_loss: 174.1435\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 354.9452 - val_loss: 201.8532\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 169.6588 - val_loss: 218.4497\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 189.0674 - val_loss: 268.6542\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 169.4408 - val_loss: 192.2716\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.8883 - val_loss: 180.5041\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.8095 - val_loss: 154.4370\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 212.4175 - val_loss: 176.6091\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 179.1570 - val_loss: 160.2005\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 182.3129 - val_loss: 220.6985\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 262.4978 - val_loss: 223.0268\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 162.8395 - val_loss: 184.7380\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 166.6520 - val_loss: 169.3771\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 183.0523 - val_loss: 175.7374\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 172.0153 - val_loss: 153.5262\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 161.6094 - val_loss: 227.2752\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 200.4559 - val_loss: 219.1024\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 158.6888 - val_loss: 152.9575\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 160.2407 - val_loss: 162.8889\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 169.9743 - val_loss: 539.5577\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 169.1302 - val_loss: 164.4624\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 255.7662 - val_loss: 199.4699\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 158.9094 - val_loss: 171.6226\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 173.5605 - val_loss: 169.0245\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 212.6719 - val_loss: 165.8389\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 411.4532 - val_loss: 358.9575\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 344.4490 - val_loss: 476.6496\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 257.9999 - val_loss: 303.3040\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 191.6408 - val_loss: 158.5530\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 170.4699 - val_loss: 187.2782\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 173.2473 - val_loss: 207.7417\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 182.7159 - val_loss: 185.8167\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 159.8921 - val_loss: 248.6179\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 197.3511 - val_loss: 177.2757\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 204.7607 - val_loss: 159.3982\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.5679 - val_loss: 187.9857\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 177.4357 - val_loss: 228.2468\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 176.8121 - val_loss: 181.7691\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 159.9000 - val_loss: 180.2109\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 175.9658 - val_loss: 182.0881\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 174.1836 - val_loss: 166.6799\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 259.4078 - val_loss: 166.9023\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 164.8247 - val_loss: 178.1711\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.0981 - val_loss: 188.3432\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 171.1006 - val_loss: 167.2221\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 169.4894 - val_loss: 155.9570\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 168.4619 - val_loss: 164.1947\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 212.4987 - val_loss: 193.4401\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 203.5782 - val_loss: 166.1394\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 160.2262 - val_loss: 173.1495\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 187.4507 - val_loss: 770.8306\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 215.8425 - val_loss: 199.8853\n",
      "Epoch 587/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 170.4306 - val_loss: 165.2791\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 185.5749 - val_loss: 189.2803\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 185.2152 - val_loss: 186.2970\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 276.9158 - val_loss: 166.3780\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 155.8591 - val_loss: 160.5817\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.0582 - val_loss: 220.9360\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 158.3019 - val_loss: 172.2782\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 198.8361 - val_loss: 290.5913\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 609.9902 - val_loss: 269.9361\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 222.4510 - val_loss: 176.6661\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 164.1428 - val_loss: 154.1631\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 152.6575 - val_loss: 154.9305\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.2531 - val_loss: 175.6718\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 164.4048 - val_loss: 193.4146\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.3241 - val_loss: 190.0611\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 171.4087 - val_loss: 167.9959\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 161.3517 - val_loss: 155.2339\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 160.5611 - val_loss: 174.8473\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 206.5050 - val_loss: 192.9835\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 174.6195 - val_loss: 177.4187\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.3744 - val_loss: 154.3090\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 174.9033 - val_loss: 157.1997\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 153.7955 - val_loss: 180.1728\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 200.9287 - val_loss: 220.8968\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 185.1940 - val_loss: 180.9841\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.8038 - val_loss: 160.4151\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 186.9515 - val_loss: 176.7705\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 197.8329 - val_loss: 245.4345\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 175.5402 - val_loss: 198.9441\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 174.6080 - val_loss: 160.9736\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 176.1853 - val_loss: 182.9165\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 166.4478 - val_loss: 164.8236\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 179.5558 - val_loss: 156.6945\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 163.5851 - val_loss: 201.3628\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 168.0189 - val_loss: 209.6669\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 178.5487 - val_loss: 150.9281\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 234.3822 - val_loss: 206.2531\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 155.3245 - val_loss: 150.3713\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.6677 - val_loss: 168.8098\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 447.3179 - val_loss: 249.0118\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 212.5050 - val_loss: 156.7663\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 201.5321 - val_loss: 195.8349\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 176.3239 - val_loss: 172.5799\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 179.9827 - val_loss: 177.8989\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.0414 - val_loss: 155.1527\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.7731 - val_loss: 157.1277\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 153.4147 - val_loss: 176.9899\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.1850 - val_loss: 166.6968\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 155.4612 - val_loss: 166.5440\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 158.5454 - val_loss: 188.0923\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.4574 - val_loss: 161.3838\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.4474 - val_loss: 172.7160\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 383.4096 - val_loss: 168.8588\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 170.2491 - val_loss: 157.6453\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 158.1990 - val_loss: 173.4477\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 146.9620 - val_loss: 161.6286\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 162.6845 - val_loss: 213.9616\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.0584 - val_loss: 158.4333\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 177.8601 - val_loss: 449.8286\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 159.8648 - val_loss: 161.3640\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 160.6330 - val_loss: 160.5135\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 196.4743 - val_loss: 300.6221\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 264.9395 - val_loss: 176.2907\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 244.9745 - val_loss: 199.8329\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 155.2640 - val_loss: 223.0490\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 162.2854 - val_loss: 188.7973\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.3391 - val_loss: 157.1696\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.1070 - val_loss: 155.7850\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 150.9870 - val_loss: 216.9805\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.9222 - val_loss: 152.0545\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 183.1641 - val_loss: 298.7888\n",
      "Epoch 658/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 224.1620 - val_loss: 151.1386\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.0155 - val_loss: 160.9778\n",
      "Epoch 660/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 76us/step - loss: 160.4618 - val_loss: 232.2643\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 150.5459 - val_loss: 150.5190\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 151.3601 - val_loss: 166.0807\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.9779 - val_loss: 155.3169\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 151.7009 - val_loss: 183.0835\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 418.3819 - val_loss: 1786.7679\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 228.5798 - val_loss: 193.4775\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 148.4685 - val_loss: 157.9264\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.9134 - val_loss: 192.7555\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.0251 - val_loss: 157.7707\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.8492 - val_loss: 183.8164\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.4670 - val_loss: 174.2471\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.5435 - val_loss: 166.2621\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 148.2065 - val_loss: 188.3113\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 183.9275 - val_loss: 369.2101\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 281.7568 - val_loss: 187.9004\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 167.1724 - val_loss: 149.1532\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.4352 - val_loss: 265.9047\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.0404 - val_loss: 157.8680\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 299.7307 - val_loss: 234.6184\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 163.0420 - val_loss: 159.8096\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 280.4542 - val_loss: 300.1145\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 188.1253 - val_loss: 157.5620\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.6621 - val_loss: 157.0048\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 155.0329 - val_loss: 223.2312\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 149.3866 - val_loss: 161.0954\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.2340 - val_loss: 183.0230\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 179.3796 - val_loss: 150.5236\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 147.6714 - val_loss: 163.8680\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.7901 - val_loss: 153.1999\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.9070 - val_loss: 211.6139\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 163.0757 - val_loss: 154.1287\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 294.5949 - val_loss: 155.1545\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 151.5163 - val_loss: 212.4191\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 146.1822 - val_loss: 217.7911\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.3428 - val_loss: 205.8692\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 149.3652 - val_loss: 159.5458\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.7982 - val_loss: 150.1969\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 149.2581 - val_loss: 197.1179\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 196.9323 - val_loss: 175.9666\n",
      "Epoch 700/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.9884 - val_loss: 193.4443\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 162.3680 - val_loss: 211.6784\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 164.8293 - val_loss: 152.8648\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 154.1590 - val_loss: 220.5486\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 171.1257 - val_loss: 188.7498\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 162.3571 - val_loss: 160.9412\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 180.4728 - val_loss: 257.8486\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 216.4866 - val_loss: 210.7331\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 599.1823 - val_loss: 158.2151\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 160.5632 - val_loss: 179.5811\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 177.8934 - val_loss: 188.4523\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 149.6910 - val_loss: 163.3313\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.7730 - val_loss: 167.5519\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.9788 - val_loss: 161.7233\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.0013 - val_loss: 196.7400\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 237.2412 - val_loss: 227.1335\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.0000 - val_loss: 255.4939\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 170.1329 - val_loss: 154.1705\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 153.6367 - val_loss: 184.2158\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 151.9598 - val_loss: 173.9496\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 151.1559 - val_loss: 151.7276\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 211.3710 - val_loss: 176.5277\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 158.3243 - val_loss: 149.3842\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.3634 - val_loss: 161.4767\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 142.3801 - val_loss: 152.7219\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 153.3773 - val_loss: 150.3100\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 153.5827 - val_loss: 161.3898\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.5663 - val_loss: 147.9362\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 150.7820 - val_loss: 171.3121\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 463.1844 - val_loss: 427.2110\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 400.0945 - val_loss: 267.7494\n",
      "Epoch 731/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 289.9501 - val_loss: 239.0873\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 274.3062 - val_loss: 223.1580\n",
      "Epoch 733/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 56us/step - loss: 262.6072 - val_loss: 236.6958\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 234.9859 - val_loss: 271.8187\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 251.5665 - val_loss: 241.8980\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 206.1015 - val_loss: 211.7483\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 213.1994 - val_loss: 199.6234\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 253.9927 - val_loss: 230.8461\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 209.9235 - val_loss: 213.6822\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 199.7918 - val_loss: 323.4239\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 210.6630 - val_loss: 228.9378\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 240.9103 - val_loss: 176.3222\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 188.6044 - val_loss: 170.3087\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.9099 - val_loss: 171.2380\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 191.1690 - val_loss: 178.9530\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 197.6964 - val_loss: 189.9803\n",
      "Epoch 747/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 182.7288 - val_loss: 179.4822\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 226.4820 - val_loss: 177.7061\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 227.2683 - val_loss: 218.4687\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 188.4172 - val_loss: 166.8529\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 184.1373 - val_loss: 177.6895\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 184.0520 - val_loss: 197.5668\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 185.6029 - val_loss: 212.3164\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 190.8635 - val_loss: 197.7573\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 182.6723 - val_loss: 224.5581\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 176.2021 - val_loss: 184.9100\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.2974 - val_loss: 201.6472\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 181.0514 - val_loss: 169.9076\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 175.3956 - val_loss: 168.5308\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 188.4157 - val_loss: 193.7511\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.5329 - val_loss: 226.0229\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 175.6866 - val_loss: 171.1808\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.1218 - val_loss: 205.6299\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 178.8186 - val_loss: 163.6450\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 213.3349 - val_loss: 164.2545\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 169.2683 - val_loss: 290.3820\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 187.7234 - val_loss: 212.5005\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.9725 - val_loss: 245.5233\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.3046 - val_loss: 171.7759\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 172.6754 - val_loss: 201.2226\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.2262 - val_loss: 209.7725\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 178.8570 - val_loss: 177.1708\n",
      "Epoch 773/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 165.4342 - val_loss: 172.1635\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 156.8237 - val_loss: 200.5689\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 208.3114 - val_loss: 156.9397\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.0565 - val_loss: 150.4553\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 164.0616 - val_loss: 229.8033\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 164.6848 - val_loss: 257.4574\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 169.7975 - val_loss: 154.0103\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.5113 - val_loss: 222.1347\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 204.7505 - val_loss: 187.7772\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 168.9666 - val_loss: 167.1335\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 179.4958 - val_loss: 168.9589\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 200.9661 - val_loss: 242.6319\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 165.9472 - val_loss: 179.2392\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 155.1630 - val_loss: 174.8874\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 171.8297 - val_loss: 168.4358\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 162.7040 - val_loss: 164.3437\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 197.2755 - val_loss: 171.1938\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.3337 - val_loss: 164.2440\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.9322 - val_loss: 166.4293\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 168.5609 - val_loss: 165.1164\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.4532 - val_loss: 173.6108\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 167.4355 - val_loss: 294.5286\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 162.2235 - val_loss: 156.8605\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.0114 - val_loss: 231.5943\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 173.7936 - val_loss: 187.0977\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 180.7984 - val_loss: 173.5769\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 178.6581 - val_loss: 150.4432\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 163.2869 - val_loss: 164.8155\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.4806 - val_loss: 172.1286\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 187.9037 - val_loss: 170.1353\n",
      "Epoch 803/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 173.8508 - val_loss: 194.2036\n",
      "Epoch 804/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.6012 - val_loss: 202.2116\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 184.6737 - val_loss: 161.1225\n",
      "Epoch 806/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.7220 - val_loss: 190.4476\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 205.0248 - val_loss: 235.2623\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 179.4091 - val_loss: 257.5826\n",
      "Epoch 809/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.5667 - val_loss: 168.6606\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.6375 - val_loss: 275.4556\n",
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.8931 - val_loss: 149.9709\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.4726 - val_loss: 148.8267\n",
      "Epoch 813/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 151.3814 - val_loss: 157.0226\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.2188 - val_loss: 182.6520\n",
      "Epoch 815/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 177.7649 - val_loss: 149.3792\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 151.0878 - val_loss: 162.6005\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.0126 - val_loss: 186.0796\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.7960 - val_loss: 149.8457\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 217.1879 - val_loss: 181.4619\n",
      "Epoch 820/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 187.6886 - val_loss: 183.3150\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 172.3262 - val_loss: 162.3194\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.5100 - val_loss: 155.3345\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.8723 - val_loss: 161.2866\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 167.4741 - val_loss: 155.9473\n",
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 195.5500 - val_loss: 225.4125\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 170.7419 - val_loss: 198.4222\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 190.7076 - val_loss: 236.1868\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.2756 - val_loss: 168.4296\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 164.2300 - val_loss: 200.1108\n",
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 173.3807 - val_loss: 160.9725\n",
      "Epoch 831/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 198.3293 - val_loss: 164.8861\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 173.6402 - val_loss: 261.0001\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 154.0444 - val_loss: 152.8526\n",
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.7223 - val_loss: 154.0369\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 158.8547 - val_loss: 151.6635\n",
      "Epoch 836/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 175.2922 - val_loss: 195.3771\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.3185 - val_loss: 156.0266\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 174.6309 - val_loss: 150.3041\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 156.1881 - val_loss: 168.2753\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 158.4966 - val_loss: 149.4054\n",
      "Epoch 841/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 148.3862 - val_loss: 189.6145\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.9470 - val_loss: 176.0602\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 147.3138 - val_loss: 218.6924\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 165.3678 - val_loss: 268.6502\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 154.7752 - val_loss: 160.2376\n",
      "Epoch 846/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 162.2560 - val_loss: 175.4240\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 164.4809 - val_loss: 151.3381\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.3992 - val_loss: 149.4396\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.5798 - val_loss: 168.2718\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.8022 - val_loss: 153.1581\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 165.0765 - val_loss: 164.8765\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 155.1625 - val_loss: 159.8245\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 148.1300 - val_loss: 147.9829\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 206.4252 - val_loss: 261.8202\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 174.9959 - val_loss: 169.8068\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 157.4846 - val_loss: 165.2285\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 147.5128 - val_loss: 209.3009\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 164.4731 - val_loss: 151.4354\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.9791 - val_loss: 149.4440\n",
      "Epoch 860/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 155.0306 - val_loss: 143.8822\n",
      "Epoch 861/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 160.5940 - val_loss: 275.3621\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 180.7427 - val_loss: 262.7742\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 163.0483 - val_loss: 159.5561\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 179.2042 - val_loss: 159.8099\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.9565 - val_loss: 196.1757\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 161.6550 - val_loss: 223.7373\n",
      "Epoch 867/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 158.2454 - val_loss: 171.5324\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 155.5759 - val_loss: 157.0619\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 163.9604 - val_loss: 174.5751\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 190.3701 - val_loss: 164.7391\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 171.6869 - val_loss: 235.9763\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 216.3801 - val_loss: 201.4772\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 170.6142 - val_loss: 200.6797\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.0991 - val_loss: 182.7052\n",
      "Epoch 875/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 164.8431 - val_loss: 173.7777\n",
      "Epoch 876/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 144.9469 - val_loss: 160.7117\n",
      "Epoch 877/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 203.4439 - val_loss: 177.7751\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.3558 - val_loss: 215.7793\n",
      "Epoch 879/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.5597 - val_loss: 148.7801\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.4174 - val_loss: 187.7326\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 163.9403 - val_loss: 156.2312\n",
      "Epoch 882/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.6619 - val_loss: 149.7599\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 158.8302 - val_loss: 165.0275\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 161.2819 - val_loss: 181.9731\n",
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 158.8055 - val_loss: 143.9213\n",
      "Epoch 886/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.6281 - val_loss: 168.9312\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 158.2683 - val_loss: 151.6402\n",
      "Epoch 888/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 151.7560 - val_loss: 673.9852\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 175.0407 - val_loss: 184.1026\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 156.2416 - val_loss: 352.8762\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.2231 - val_loss: 280.5178\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 170.8156 - val_loss: 147.9792\n",
      "Epoch 893/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.2892 - val_loss: 143.9002\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 154.0190 - val_loss: 182.1251\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 254.3471 - val_loss: 149.9797\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.0708 - val_loss: 196.7437\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 155.4638 - val_loss: 146.6135\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.4516 - val_loss: 213.6596\n",
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.2671 - val_loss: 188.9429\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 151.0291 - val_loss: 193.0489\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.1468 - val_loss: 149.9992ETA: 0s - loss: 15\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 189.8269 - val_loss: 181.5004\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 187.9928 - val_loss: 182.2078\n",
      "Epoch 904/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 150.967 - 0s 50us/step - loss: 150.4478 - val_loss: 148.2421\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.4663 - val_loss: 186.1959\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.9182 - val_loss: 151.1067\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 166.0211 - val_loss: 190.7071\n",
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.3612 - val_loss: 152.2523\n",
      "Epoch 909/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.8865 - val_loss: 158.3456\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.0072 - val_loss: 153.2848\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.2503 - val_loss: 155.9621\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 155.7547 - val_loss: 169.4803\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 165.6919 - val_loss: 168.3000\n",
      "Epoch 914/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 144.4007 - val_loss: 171.5851\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 163.3850 - val_loss: 162.4344\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 173.7411 - val_loss: 210.5704\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 144.8166 - val_loss: 147.0353\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 171.2246 - val_loss: 156.1765\n",
      "Epoch 919/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.3857 - val_loss: 168.2058\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.9418 - val_loss: 169.9957\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.5000 - val_loss: 189.8681\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 151.6015 - val_loss: 236.4399\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 158.5470 - val_loss: 185.6411\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 148.1588 - val_loss: 146.0476\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 182.9031 - val_loss: 158.9735\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 155.3696 - val_loss: 173.2381\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 205.1886 - val_loss: 268.5974\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 175.0071 - val_loss: 263.5693\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 204.5971 - val_loss: 165.4414\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 146.7519 - val_loss: 145.5292\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 186.9379 - val_loss: 206.7173\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.4851 - val_loss: 161.9029\n",
      "Epoch 933/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 152.3649 - val_loss: 164.7908\n",
      "Epoch 934/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.8105 - val_loss: 165.7484\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.2296 - val_loss: 206.1707\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.4537 - val_loss: 238.8311\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.8728 - val_loss: 191.7439\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.7438 - val_loss: 243.8534\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.8339 - val_loss: 173.3880\n",
      "Epoch 940/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.6554 - val_loss: 207.4760\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 147.6058 - val_loss: 174.6727\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 164.9781 - val_loss: 197.0126\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.6847 - val_loss: 184.6042\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.4120 - val_loss: 153.7292\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.4430 - val_loss: 243.2211\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 178.9499 - val_loss: 513.3817\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 224.8351 - val_loss: 167.9427\n",
      "Epoch 948/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 147.6491 - val_loss: 157.7605\n",
      "Epoch 949/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 185.0312 - val_loss: 149.5025\n",
      "Epoch 950/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.8737 - val_loss: 163.4387\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 143.3452 - val_loss: 157.6205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.8706 - val_loss: 146.7811\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.1841 - val_loss: 188.6495\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.5189 - val_loss: 151.2179\n",
      "Epoch 955/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.0902 - val_loss: 254.5829\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.6904 - val_loss: 202.8118\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.6603 - val_loss: 144.0612\n",
      "Epoch 958/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 170.5809 - val_loss: 249.3601\n",
      "Epoch 959/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.3063 - val_loss: 158.0089\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 153.2394 - val_loss: 160.5498\n",
      "Epoch 961/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 147.7030 - val_loss: 163.2266\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.7972 - val_loss: 247.0766\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.1223 - val_loss: 169.8809\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.7174 - val_loss: 155.7911\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.9872 - val_loss: 264.7059\n",
      "Epoch 966/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 211.4554 - val_loss: 160.1818\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.1678 - val_loss: 196.8165\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 243.1743 - val_loss: 161.9627\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 146.4518 - val_loss: 145.1874\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 145.8834 - val_loss: 164.9540\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.5072 - val_loss: 155.0276\n",
      "Epoch 972/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 165.8463 - val_loss: 291.2240\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 166.2578 - val_loss: 146.1035\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.8022 - val_loss: 161.1145\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.4772 - val_loss: 210.6997\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 218.7464 - val_loss: 160.9891\n",
      "Epoch 977/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 170.5229 - val_loss: 153.6764\n",
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.3025 - val_loss: 228.7923\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 144.0457 - val_loss: 143.5004\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.9602 - val_loss: 182.2465\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.8472 - val_loss: 157.4884\n",
      "Epoch 982/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 167.2108 - val_loss: 171.3729\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 187.6528 - val_loss: 149.3489\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.9316 - val_loss: 155.1300\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.0246 - val_loss: 231.0086\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 158.9108 - val_loss: 154.7883\n",
      "Epoch 987/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 140.6360 - val_loss: 162.7767\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 162.3187 - val_loss: 148.3425\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 240.7850 - val_loss: 218.8037\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 162.9098 - val_loss: 172.1820\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.5650 - val_loss: 143.8654\n",
      "Epoch 992/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.5075 - val_loss: 159.4223\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 162.8853 - val_loss: 162.9292\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 171.8825 - val_loss: 156.2768\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 149.2852 - val_loss: 202.8763\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 244.4832 - val_loss: 192.4831\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.3496 - val_loss: 195.9298\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 156.8800 - val_loss: 158.7828\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 149.5193 - val_loss: 194.5998\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.5273 - val_loss: 169.8320\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 162.4182 - val_loss: 162.5473\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.9023 - val_loss: 151.2636\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 141.7045 - val_loss: 277.9373\n",
      "Epoch 1004/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 161.8918 - val_loss: 183.5783\n",
      "Epoch 1005/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.2034 - val_loss: 175.0246\n",
      "Epoch 1006/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 179.3577 - val_loss: 169.4545\n",
      "Epoch 1007/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.3378 - val_loss: 164.7829\n",
      "Epoch 1008/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 198.1280 - val_loss: 215.2438\n",
      "Epoch 1009/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 179.7696 - val_loss: 151.3427\n",
      "Epoch 1010/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 157.4263 - val_loss: 144.9217\n",
      "Epoch 1011/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.6295 - val_loss: 189.4806\n",
      "Epoch 1012/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 161.2474 - val_loss: 145.3246\n",
      "Epoch 1013/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 159.132 - 0s 51us/step - loss: 158.0764 - val_loss: 157.3704\n",
      "Epoch 1014/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.2829 - val_loss: 172.6308\n",
      "Epoch 1015/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.8282 - val_loss: 192.1777\n",
      "Epoch 1016/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 166.3559 - val_loss: 168.5738\n",
      "Epoch 1017/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.1559 - val_loss: 158.6003\n",
      "Epoch 1018/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.1922 - val_loss: 148.3868\n",
      "Epoch 1019/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 165.2392 - val_loss: 142.9083\n",
      "Epoch 1020/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 146.4286 - val_loss: 183.3835\n",
      "Epoch 1021/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 178.0445 - val_loss: 510.3348\n",
      "Epoch 1022/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 387.4890 - val_loss: 211.4764\n",
      "Epoch 1023/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 291.3607 - val_loss: 275.8510\n",
      "Epoch 1024/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 256.1713 - val_loss: 204.0594\n",
      "Epoch 1025/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 166.6073 - val_loss: 184.3968\n",
      "Epoch 1026/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 166.3514 - val_loss: 168.9771\n",
      "Epoch 1027/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 166.8199 - val_loss: 156.1416\n",
      "Epoch 1028/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 155.1527 - val_loss: 189.7810\n",
      "Epoch 1029/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 164.3567 - val_loss: 182.6663\n",
      "Epoch 1030/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 191.5386 - val_loss: 158.4793\n",
      "Epoch 1031/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 146.3783 - val_loss: 175.5358\n",
      "Epoch 1032/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 159.8252 - val_loss: 167.5700\n",
      "Epoch 1033/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.4683 - val_loss: 151.2838\n",
      "Epoch 1034/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 146.2367 - val_loss: 164.7988\n",
      "Epoch 1035/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.2940 - val_loss: 155.0834\n",
      "Epoch 1036/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 153.0307 - val_loss: 153.4738\n",
      "Epoch 1037/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.9917 - val_loss: 149.3346\n",
      "Epoch 1038/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.2781 - val_loss: 161.0346\n",
      "Epoch 1039/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 147.0133 - val_loss: 176.2412\n",
      "Epoch 1040/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.4260 - val_loss: 149.7800\n",
      "Epoch 1041/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 162.7100 - val_loss: 153.3675\n",
      "Epoch 1042/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 155.2250 - val_loss: 153.9156\n",
      "Epoch 1043/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.1486 - val_loss: 148.0694\n",
      "Epoch 1044/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 155.6440 - val_loss: 179.1801\n",
      "Epoch 1045/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.9515 - val_loss: 191.1490\n",
      "Epoch 1046/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.5332 - val_loss: 153.8038\n",
      "Epoch 1047/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 190.7301 - val_loss: 148.6831\n",
      "Epoch 1048/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 147.1080 - val_loss: 154.5797\n",
      "Epoch 1049/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.9607 - val_loss: 202.8202\n",
      "Epoch 1050/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.5782 - val_loss: 163.2581\n",
      "Epoch 1051/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 160.3479 - val_loss: 149.2941\n",
      "Epoch 1052/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 181.6298 - val_loss: 186.7943\n",
      "Epoch 1053/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.3262 - val_loss: 149.3925\n",
      "Epoch 1054/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.9406 - val_loss: 369.1258\n",
      "Epoch 1055/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 149.9667 - val_loss: 172.6327\n",
      "Epoch 1056/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 151.7726 - val_loss: 157.0490\n",
      "Epoch 1057/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.1590 - val_loss: 250.0592\n",
      "Epoch 1058/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 147.0836 - val_loss: 180.0660\n",
      "Epoch 1059/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 138.7015 - val_loss: 160.6291\n",
      "Epoch 1060/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 148.9700 - val_loss: 188.2660\n",
      "Epoch 1061/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 216.0131 - val_loss: 159.8737\n",
      "Epoch 1062/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 156.3743 - val_loss: 191.2834\n",
      "Epoch 1063/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 203.0087 - val_loss: 188.9082\n",
      "Epoch 1064/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.7904 - val_loss: 150.1347\n",
      "Epoch 1065/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.0823 - val_loss: 155.6309\n",
      "Epoch 1066/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.5368 - val_loss: 177.4936\n",
      "Epoch 1067/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 149.9280 - val_loss: 164.7891\n",
      "Epoch 1068/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.1718 - val_loss: 154.9307\n",
      "Epoch 1069/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 190.1786 - val_loss: 170.9662\n",
      "Epoch 1070/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 165.0296 - val_loss: 180.3419\n",
      "Epoch 1071/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 162.4355 - val_loss: 162.0122\n",
      "Epoch 1072/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 172.5961 - val_loss: 154.3098\n",
      "Epoch 1073/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 154.0626 - val_loss: 271.3864\n",
      "Epoch 1074/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.8390 - val_loss: 146.9727\n",
      "Epoch 1075/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.9528 - val_loss: 204.5812\n",
      "Epoch 1076/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.1130 - val_loss: 194.3948\n",
      "Epoch 1077/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 238.8196 - val_loss: 165.7977\n",
      "Epoch 1078/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 165.6800 - val_loss: 159.0680\n",
      "Epoch 1079/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 190.6446 - val_loss: 168.8977\n",
      "Epoch 1080/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 141.9271 - val_loss: 144.3267\n",
      "Epoch 1081/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 155.0345 - val_loss: 148.0291\n",
      "Epoch 1082/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.4370 - val_loss: 148.0238\n",
      "Epoch 1083/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.4971 - val_loss: 170.5350\n",
      "Epoch 1084/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 157.5710 - val_loss: 306.4571\n",
      "Epoch 1085/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 183.2173 - val_loss: 147.9908\n",
      "Epoch 1086/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.0597 - val_loss: 164.7532\n",
      "Epoch 1087/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 177.8278 - val_loss: 198.8565\n",
      "Epoch 1088/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 158.0502 - val_loss: 147.0858\n",
      "Epoch 1089/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 163.5698 - val_loss: 150.3543\n",
      "Epoch 1090/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.7368 - val_loss: 158.4880\n",
      "Epoch 1091/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.4624 - val_loss: 164.5480\n",
      "Epoch 1092/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 186.8167 - val_loss: 151.4663\n",
      "Epoch 1093/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.9649 - val_loss: 143.9573\n",
      "Epoch 1094/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 165.7406 - val_loss: 167.3747\n",
      "Epoch 1095/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 174.9038 - val_loss: 161.1227\n",
      "Epoch 1096/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.0597 - val_loss: 153.5158\n",
      "Epoch 1097/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 157.5958 - val_loss: 154.9939\n",
      "Epoch 1098/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.0759 - val_loss: 231.6266\n",
      "Epoch 1099/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 193.5683 - val_loss: 148.9103\n",
      "Epoch 1100/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.8672 - val_loss: 144.8422\n",
      "Epoch 1101/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 146.8406 - val_loss: 171.0149\n",
      "Epoch 1102/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 213.8146 - val_loss: 178.6451\n",
      "Epoch 1103/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 154.9341 - val_loss: 198.9859\n",
      "Epoch 1104/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.4616 - val_loss: 158.1327\n",
      "Epoch 1105/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 158.0799 - val_loss: 153.9587\n",
      "Epoch 1106/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.7206 - val_loss: 151.1442\n",
      "Epoch 1107/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.5988 - val_loss: 173.3894\n",
      "Epoch 1108/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.8241 - val_loss: 147.8490\n",
      "Epoch 1109/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 153.8067 - val_loss: 159.6544\n",
      "Epoch 1110/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.4732 - val_loss: 150.3950\n",
      "Epoch 1111/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.6187 - val_loss: 147.7485\n",
      "Epoch 1112/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 171.2496 - val_loss: 144.9624\n",
      "Epoch 1113/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 158.9340 - val_loss: 142.1468\n",
      "Epoch 1114/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 162.2252 - val_loss: 144.1013\n",
      "Epoch 1115/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.1806 - val_loss: 147.0230\n",
      "Epoch 1116/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.8642 - val_loss: 179.8865\n",
      "Epoch 1117/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 202.4128 - val_loss: 174.7911\n",
      "Epoch 1118/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.3998 - val_loss: 151.5807\n",
      "Epoch 1119/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.7944 - val_loss: 155.6577\n",
      "Epoch 1120/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 142.1869 - val_loss: 155.1066\n",
      "Epoch 1121/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.8204 - val_loss: 199.4474\n",
      "Epoch 1122/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.7291 - val_loss: 181.5930\n",
      "Epoch 1123/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 160.1901 - val_loss: 151.9242\n",
      "Epoch 1124/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 177.3528 - val_loss: 286.8160\n",
      "Epoch 1125/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 180.5134 - val_loss: 157.7283\n",
      "Epoch 1126/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.5320 - val_loss: 159.0878\n",
      "Epoch 1127/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.1174 - val_loss: 177.6744\n",
      "Epoch 1128/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 205.9758 - val_loss: 152.8731\n",
      "Epoch 1129/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.9379 - val_loss: 146.0214\n",
      "Epoch 1130/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 155.3208 - val_loss: 163.9518\n",
      "Epoch 1131/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 157.5962 - val_loss: 178.3727\n",
      "Epoch 1132/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.8193 - val_loss: 159.2744\n",
      "Epoch 1133/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.7598 - val_loss: 244.1447\n",
      "Epoch 1134/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.8940 - val_loss: 148.5861\n",
      "Epoch 1135/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.1494 - val_loss: 155.5827\n",
      "Epoch 1136/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 162.4026 - val_loss: 148.6113\n",
      "Epoch 1137/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.9114 - val_loss: 200.4084\n",
      "Epoch 1138/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 175.8549 - val_loss: 158.9343\n",
      "Epoch 1139/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 142.0542 - val_loss: 177.7890\n",
      "Epoch 1140/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 197.9730 - val_loss: 162.7844\n",
      "Epoch 1141/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 155.7313 - val_loss: 142.5301\n",
      "Epoch 1142/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.6710 - val_loss: 146.5499\n",
      "Epoch 1143/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.3224 - val_loss: 146.5560\n",
      "Epoch 1144/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.1353 - val_loss: 154.8881\n",
      "Epoch 1145/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.3942 - val_loss: 237.9148\n",
      "Epoch 1146/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 240.5620 - val_loss: 216.4048\n",
      "Epoch 1147/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.5643 - val_loss: 157.1280\n",
      "Epoch 1148/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 168.7598 - val_loss: 194.6526\n",
      "Epoch 1149/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.8420 - val_loss: 194.5063\n",
      "Epoch 1150/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 200.8021 - val_loss: 247.8486\n",
      "Epoch 1151/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 154.0062 - val_loss: 159.1508\n",
      "Epoch 1152/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.2678 - val_loss: 161.2970\n",
      "Epoch 1153/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.0781 - val_loss: 224.0256\n",
      "Epoch 1154/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 160.2432 - val_loss: 170.8595\n",
      "Epoch 1155/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 186.2732 - val_loss: 152.2564\n",
      "Epoch 1156/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 159.4419 - val_loss: 143.1134\n",
      "Epoch 1157/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.5254 - val_loss: 145.5956\n",
      "Epoch 1158/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.4101 - val_loss: 174.9038\n",
      "Epoch 1159/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 190.6114 - val_loss: 151.3047\n",
      "Epoch 1160/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.1172 - val_loss: 157.6337\n",
      "Epoch 1161/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 157.1174 - val_loss: 173.2663\n",
      "Epoch 1162/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.3716 - val_loss: 166.4729\n",
      "Epoch 1163/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 158.0922 - val_loss: 143.7979\n",
      "Epoch 1164/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 142.1122 - val_loss: 141.4423\n",
      "Epoch 1165/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 155.0874 - val_loss: 152.9239\n",
      "Epoch 1166/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 217.2726 - val_loss: 232.2181\n",
      "Epoch 1167/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 258.6749 - val_loss: 200.8939\n",
      "Epoch 1168/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 65us/step - loss: 171.1108 - val_loss: 225.0515\n",
      "Epoch 1169/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 180.9463 - val_loss: 190.1527\n",
      "Epoch 1170/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 150.7708 - val_loss: 177.0103\n",
      "Epoch 1171/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.0748 - val_loss: 158.8103\n",
      "Epoch 1172/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 164.5419 - val_loss: 166.5749\n",
      "Epoch 1173/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.5994 - val_loss: 281.4532\n",
      "Epoch 1174/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 163.0591 - val_loss: 160.5306\n",
      "Epoch 1175/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 168.8362 - val_loss: 182.2938\n",
      "Epoch 1176/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 152.5261 - val_loss: 181.2206\n",
      "Epoch 1177/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 159.8472 - val_loss: 169.5530\n",
      "Epoch 1178/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 172.8416 - val_loss: 199.7721\n",
      "Epoch 1179/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.1388 - val_loss: 161.1199\n",
      "Epoch 1180/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 148.7885 - val_loss: 146.4406\n",
      "Epoch 1181/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 139.7624 - val_loss: 188.1865\n",
      "Epoch 1182/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 159.0929 - val_loss: 152.2276\n",
      "Epoch 1183/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.9616 - val_loss: 186.1220\n",
      "Epoch 1184/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 189.1312 - val_loss: 170.3878\n",
      "Epoch 1185/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 167.3317 - val_loss: 165.9106\n",
      "Epoch 1186/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 148.1128 - val_loss: 266.7254\n",
      "Epoch 1187/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 172.5641 - val_loss: 217.0835\n",
      "Epoch 1188/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 190.7963 - val_loss: 159.2637\n",
      "Epoch 1189/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 161.5435 - val_loss: 159.7359\n",
      "Epoch 1190/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.5845 - val_loss: 169.1779\n",
      "Epoch 1191/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.6404 - val_loss: 251.7293\n",
      "Epoch 1192/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.6906 - val_loss: 155.7940\n",
      "Epoch 1193/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 200.6544 - val_loss: 145.6665\n",
      "Epoch 1194/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 140.2446 - val_loss: 233.3566\n",
      "Epoch 1195/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 203.0447 - val_loss: 147.5628\n",
      "Epoch 1196/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.4494 - val_loss: 156.3937\n",
      "Epoch 1197/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.0555 - val_loss: 166.8132\n",
      "Epoch 1198/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 166.9658 - val_loss: 170.0970\n",
      "Epoch 1199/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.4357 - val_loss: 210.2624\n",
      "Epoch 1200/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 148.6592 - val_loss: 236.9477\n",
      "Epoch 1201/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 161.092 - 0s 50us/step - loss: 161.3216 - val_loss: 165.3207\n",
      "Epoch 1202/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 186.9284 - val_loss: 182.7545\n",
      "Epoch 1203/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 141.5005 - val_loss: 142.7659\n",
      "Epoch 1204/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.6802 - val_loss: 155.4288\n",
      "Epoch 1205/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 173.1151 - val_loss: 177.6729\n",
      "Epoch 1206/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 184.9071 - val_loss: 148.9961\n",
      "Epoch 1207/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.6738 - val_loss: 146.2333\n",
      "Epoch 1208/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.4180 - val_loss: 269.3153\n",
      "Epoch 1209/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 163.7344 - val_loss: 175.3706\n",
      "Epoch 1210/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 147.5213 - val_loss: 164.4357\n",
      "Epoch 1211/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.3423 - val_loss: 154.0313\n",
      "Epoch 1212/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 169.3533 - val_loss: 174.5859\n",
      "Epoch 1213/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 163.2628 - val_loss: 163.9735\n",
      "Epoch 1214/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.0630 - val_loss: 158.7533\n",
      "Epoch 1215/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 161.3880 - val_loss: 156.8133\n",
      "Epoch 1216/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.8601 - val_loss: 173.7957\n",
      "Epoch 1217/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.1410 - val_loss: 146.7647\n",
      "Epoch 1218/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 149.6831 - val_loss: 199.5150\n",
      "Epoch 1219/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 162.5570 - val_loss: 162.7332\n",
      "Epoch 1220/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 155.4145 - val_loss: 141.9131\n",
      "Epoch 1221/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.3998 - val_loss: 203.3503\n",
      "Epoch 1222/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.2838 - val_loss: 155.3819\n",
      "Epoch 1223/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 159.7499 - val_loss: 162.1101\n",
      "Epoch 1224/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 182.2207 - val_loss: 153.1470\n",
      "Epoch 1225/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 151.9741 - val_loss: 153.2544\n",
      "Epoch 1226/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 150.2222 - val_loss: 148.1251\n",
      "Epoch 1227/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 194.3725 - val_loss: 157.5179\n",
      "Epoch 1228/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 146.3025 - val_loss: 158.4389\n",
      "Epoch 1229/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.4720 - val_loss: 153.4104\n",
      "Epoch 1230/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.3871 - val_loss: 274.4691\n",
      "Epoch 1231/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 199.1128 - val_loss: 196.4422\n",
      "Epoch 1232/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 166.5421 - val_loss: 221.5435\n",
      "Epoch 1233/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.6897 - val_loss: 171.3086\n",
      "Epoch 1234/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 162.4547 - val_loss: 160.9116\n",
      "Epoch 1235/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.8069 - val_loss: 148.4717\n",
      "Epoch 1236/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 149.8160 - val_loss: 157.4287\n",
      "Epoch 1237/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.6058 - val_loss: 151.3791\n",
      "Epoch 1238/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.5968 - val_loss: 151.4959\n",
      "Epoch 1239/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.4538 - val_loss: 142.4966\n",
      "Epoch 1240/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.2117 - val_loss: 165.0758\n",
      "Epoch 1241/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 233.4793 - val_loss: 149.4795\n",
      "Epoch 1242/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.8051 - val_loss: 181.0858\n",
      "Epoch 1243/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.1741 - val_loss: 152.8017\n",
      "Epoch 1244/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.5796 - val_loss: 153.1448\n",
      "Epoch 1245/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 147.4478 - val_loss: 163.4736\n",
      "Epoch 1246/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.8855 - val_loss: 184.9972\n",
      "Epoch 1247/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 224.1660 - val_loss: 173.2666\n",
      "Epoch 1248/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 218.8221 - val_loss: 292.0488\n",
      "Epoch 1249/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 222.8408 - val_loss: 247.0371\n",
      "Epoch 1250/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 177.6769 - val_loss: 172.1710\n",
      "Epoch 1251/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 146.3465 - val_loss: 265.7437\n",
      "Epoch 1252/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 141.7683 - val_loss: 189.1944\n",
      "Epoch 1253/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.1984 - val_loss: 203.5973\n",
      "Epoch 1254/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 154.8063 - val_loss: 144.9397\n",
      "Epoch 1255/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 344.4515 - val_loss: 163.6076\n",
      "Epoch 1256/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 179.9484 - val_loss: 150.5673\n",
      "Epoch 1257/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 157.2750 - val_loss: 161.5300\n",
      "Epoch 1258/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 162.9360 - val_loss: 143.4860\n",
      "Epoch 1259/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.4299 - val_loss: 148.9211\n",
      "Epoch 1260/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 170.1921 - val_loss: 162.3471\n",
      "Epoch 1261/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 165.3619 - val_loss: 308.3323\n",
      "Epoch 1262/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.9749 - val_loss: 150.8825\n",
      "Epoch 1263/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 155.6441 - val_loss: 153.7107\n",
      "Epoch 1264/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 143.7184 - val_loss: 152.9654\n",
      "Epoch 1265/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 142.7082 - val_loss: 171.2187\n",
      "Epoch 1266/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 174.8640 - val_loss: 140.7262\n",
      "Epoch 1267/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 207.7201 - val_loss: 164.8151\n",
      "Epoch 1268/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 143.9496 - val_loss: 179.4018\n",
      "Epoch 1269/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 148.5993 - val_loss: 151.0652\n",
      "Epoch 1270/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 141.9864 - val_loss: 165.1741\n",
      "Epoch 1271/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.0958 - val_loss: 150.6821\n",
      "Epoch 1272/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 234.3652 - val_loss: 184.5432\n",
      "Epoch 1273/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 150.9308 - val_loss: 191.1421\n",
      "Epoch 1274/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.7614 - val_loss: 275.8887\n",
      "Epoch 1275/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 180.3569 - val_loss: 162.8680\n",
      "Epoch 1276/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 182.8691 - val_loss: 143.6164\n",
      "Epoch 1277/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.8395 - val_loss: 242.7287\n",
      "Epoch 1278/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.3345 - val_loss: 179.1415\n",
      "Epoch 1279/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.0093 - val_loss: 162.8334\n",
      "Epoch 1280/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.5045 - val_loss: 145.6865\n",
      "Epoch 1281/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 158.4880 - val_loss: 165.9890\n",
      "Epoch 1282/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.6353 - val_loss: 147.5262\n",
      "Epoch 1283/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.2478 - val_loss: 164.4945\n",
      "Epoch 1284/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.2995 - val_loss: 186.3323\n",
      "Epoch 1285/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 282.9168 - val_loss: 419.6764\n",
      "Epoch 1286/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.4924 - val_loss: 168.5063\n",
      "Epoch 1287/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.3257 - val_loss: 178.5231\n",
      "Epoch 1288/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 144.1260 - val_loss: 150.4008\n",
      "Epoch 1289/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.2009 - val_loss: 159.0032\n",
      "Epoch 1290/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.7169 - val_loss: 170.7526\n",
      "Epoch 1291/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 145.0383 - val_loss: 171.9770\n",
      "Epoch 1292/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 146.6934 - val_loss: 146.0851\n",
      "Epoch 1293/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 138.4373 - val_loss: 154.3317\n",
      "Epoch 1294/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 174.6318 - val_loss: 172.8718\n",
      "Epoch 1295/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.1941 - val_loss: 298.8254\n",
      "Epoch 1296/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 191.9743 - val_loss: 157.2361\n",
      "Epoch 1297/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 172.9537 - val_loss: 208.8356\n",
      "Epoch 1298/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 153.4216 - val_loss: 161.1532\n",
      "Epoch 1299/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 167.5480 - val_loss: 268.9335\n",
      "Epoch 1300/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 155.4190 - val_loss: 148.7990\n",
      "Epoch 1301/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.1264 - val_loss: 151.7120\n",
      "Epoch 1302/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.0656 - val_loss: 169.2676\n",
      "Epoch 1303/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 150.3356 - val_loss: 198.0031\n",
      "Epoch 1304/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 159.7376 - val_loss: 144.2513\n",
      "Epoch 1305/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.5098 - val_loss: 190.3761\n",
      "Epoch 1306/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.9277 - val_loss: 230.0386\n",
      "Epoch 1307/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.3047 - val_loss: 186.9715\n",
      "Epoch 1308/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 179.0137 - val_loss: 177.2475\n",
      "Epoch 1309/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 148.9048 - val_loss: 190.2202\n",
      "Epoch 1310/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 155.1081 - val_loss: 179.9618\n",
      "Epoch 1311/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 147.5664 - val_loss: 154.6049\n",
      "Epoch 1312/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.6052 - val_loss: 146.3637\n",
      "Epoch 1313/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 145.0643 - val_loss: 157.9017\n",
      "Epoch 1314/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 184.4385 - val_loss: 223.5980\n",
      "Epoch 1315/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 163.4277 - val_loss: 152.1066\n",
      "Epoch 1316/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.2428 - val_loss: 340.7264\n",
      "Epoch 1317/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 173.8421 - val_loss: 242.6586\n",
      "Epoch 1318/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 152.5061 - val_loss: 172.8818\n",
      "Epoch 1319/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 138.1380 - val_loss: 225.8283\n",
      "Epoch 1320/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 162.9366 - val_loss: 186.7973\n",
      "Epoch 1321/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.9176 - val_loss: 201.6499\n",
      "Epoch 1322/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.9715 - val_loss: 147.4288\n",
      "Epoch 1323/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.6458 - val_loss: 223.7376\n",
      "Epoch 1324/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.0780 - val_loss: 156.4832\n",
      "Epoch 1325/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 143.2086 - val_loss: 145.7201\n",
      "Epoch 1326/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 334.4284 - val_loss: 158.4217\n",
      "Epoch 1327/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 202.2738 - val_loss: 156.4385\n",
      "Epoch 1328/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 174.6914 - val_loss: 266.6702\n",
      "Epoch 1329/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.5590 - val_loss: 151.6711\n",
      "Epoch 1330/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 250.5503 - val_loss: 149.7087\n",
      "Epoch 1331/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.0432 - val_loss: 146.0214\n",
      "Epoch 1332/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 168.7533 - val_loss: 159.1768\n",
      "Epoch 1333/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 189.6411 - val_loss: 165.8045\n",
      "Epoch 1334/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.3900 - val_loss: 164.2512\n",
      "Epoch 1335/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 147.4807 - val_loss: 204.8404\n",
      "Epoch 1336/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.4976 - val_loss: 145.6099\n",
      "Epoch 1337/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.9065 - val_loss: 154.2909\n",
      "Epoch 1338/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.5627 - val_loss: 153.4016\n",
      "Epoch 1339/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.6634 - val_loss: 175.0470\n",
      "Epoch 1340/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.7283 - val_loss: 163.1723\n",
      "Epoch 1341/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 171.5342 - val_loss: 248.8854\n",
      "Epoch 1342/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 165.2842 - val_loss: 175.6715\n",
      "Epoch 1343/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 148.4878 - val_loss: 156.4605\n",
      "Epoch 1344/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 151.2237 - val_loss: 142.6333\n",
      "Epoch 1345/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 158.5414 - val_loss: 246.5023\n",
      "Epoch 1346/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 211.7923 - val_loss: 143.3696\n",
      "Epoch 1347/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.7877 - val_loss: 150.0364\n",
      "Epoch 1348/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.1433 - val_loss: 170.9572\n",
      "Epoch 1349/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.5864 - val_loss: 149.2826\n",
      "Epoch 1350/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.4342 - val_loss: 180.4463\n",
      "Epoch 1351/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.1410 - val_loss: 152.1715\n",
      "Epoch 1352/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 250.6187 - val_loss: 301.9006\n",
      "Epoch 1353/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 302.0919 - val_loss: 164.8780\n",
      "Epoch 1354/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 184.5050 - val_loss: 184.4442\n",
      "Epoch 1355/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 165.9718 - val_loss: 147.5522\n",
      "Epoch 1356/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 190.6053 - val_loss: 166.0688\n",
      "Epoch 1357/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 185.9250 - val_loss: 160.4158\n",
      "Epoch 1358/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 190.2085 - val_loss: 180.7522\n",
      "Epoch 1359/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.8275 - val_loss: 148.2541\n",
      "Epoch 1360/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 184.6733 - val_loss: 143.6903\n",
      "Epoch 1361/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.0987 - val_loss: 151.7658\n",
      "Epoch 1362/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 161.1594 - val_loss: 152.2485\n",
      "Epoch 1363/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 194.7222 - val_loss: 153.4734\n",
      "Epoch 1364/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 204.5092 - val_loss: 158.2080\n",
      "Epoch 1365/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 183.8570 - val_loss: 231.8150\n",
      "Epoch 1366/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 184.4070 - val_loss: 252.8496\n",
      "Epoch 1367/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 194.2637 - val_loss: 194.7192\n",
      "Epoch 1368/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.2639 - val_loss: 161.1984\n",
      "Epoch 1369/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 202.0220 - val_loss: 215.9929\n",
      "Epoch 1370/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.1389 - val_loss: 605.9295\n",
      "Epoch 1371/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 174.0380 - val_loss: 155.9789\n",
      "Epoch 1372/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 193.0803 - val_loss: 142.5427\n",
      "Epoch 1373/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.6096 - val_loss: 183.0782\n",
      "Epoch 1374/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 151.9147 - val_loss: 162.5418\n",
      "Epoch 1375/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 148.1534 - val_loss: 161.6485\n",
      "Epoch 1376/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.8379 - val_loss: 159.5661\n",
      "Epoch 1377/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 153.0143 - val_loss: 161.3110\n",
      "Epoch 1378/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 147.9629 - val_loss: 161.7070\n",
      "Epoch 1379/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 145.4563 - val_loss: 157.0955\n",
      "Epoch 1380/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 176.8267 - val_loss: 196.5196\n",
      "Epoch 1381/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 144.2570 - val_loss: 146.8818\n",
      "Epoch 1382/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 149.1063 - val_loss: 141.6640\n",
      "Epoch 1383/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 144.4562 - val_loss: 271.8150\n",
      "Epoch 1384/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 73us/step - loss: 155.2572 - val_loss: 169.3541\n",
      "Epoch 1385/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 137.8152 - val_loss: 161.5966\n",
      "Epoch 1386/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 152.2703 - val_loss: 141.4556\n",
      "Epoch 1387/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 225.0302 - val_loss: 549.0184\n",
      "Epoch 1388/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 244.4652 - val_loss: 149.4010\n",
      "Epoch 1389/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 215.5083 - val_loss: 190.9370\n",
      "Epoch 1390/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 156.9569 - val_loss: 297.4935\n",
      "Epoch 1391/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 170.8942 - val_loss: 144.1785\n",
      "Epoch 1392/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 147.5241 - val_loss: 146.8011\n",
      "Epoch 1393/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 172.0528 - val_loss: 228.2088\n",
      "Epoch 1394/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 172.0642 - val_loss: 144.9371\n",
      "Epoch 1395/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 154.2149 - val_loss: 191.2362\n",
      "Epoch 1396/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.7932 - val_loss: 212.7157\n",
      "Epoch 1397/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 251.8833 - val_loss: 198.5139\n",
      "Epoch 1398/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 157.5682 - val_loss: 206.8558\n",
      "Epoch 1399/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.5698 - val_loss: 222.5329\n",
      "Epoch 1400/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 157.0096 - val_loss: 170.2852\n",
      "Epoch 1401/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 187.3267 - val_loss: 206.8326\n",
      "Epoch 1402/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 196.4945 - val_loss: 166.8261\n",
      "Epoch 1403/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 162.0298 - val_loss: 175.8673\n",
      "Epoch 1404/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 208.7524 - val_loss: 264.9761\n",
      "Epoch 1405/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 158.7602 - val_loss: 159.2859\n",
      "Epoch 1406/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.3366 - val_loss: 179.9051\n",
      "Epoch 1407/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 153.0010 - val_loss: 153.2657\n",
      "Epoch 1408/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 164.9413 - val_loss: 170.5449\n",
      "Epoch 1409/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.6650 - val_loss: 152.4422\n",
      "Epoch 1410/10000\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 260.8586 - val_loss: 175.4121\n",
      "Epoch 1411/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 181.3122 - val_loss: 169.3076\n",
      "Epoch 1412/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 137.9737 - val_loss: 156.8356\n",
      "Epoch 1413/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 144.3463 - val_loss: 164.7030\n",
      "Epoch 1414/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.6502 - val_loss: 147.6579\n",
      "Epoch 1415/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.9063 - val_loss: 171.8712\n",
      "Epoch 1416/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 139.0133 - val_loss: 155.7360\n",
      "Epoch 1417/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.9700 - val_loss: 182.9304\n",
      "Epoch 1418/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 151.6991 - val_loss: 317.6224\n",
      "Epoch 1419/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 198.0532 - val_loss: 809.0271\n",
      "Epoch 1420/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 226.8846 - val_loss: 275.8086\n",
      "Epoch 1421/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 242.7112 - val_loss: 179.0388\n",
      "Epoch 1422/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.1292 - val_loss: 155.8916\n",
      "Epoch 1423/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 160.8950 - val_loss: 157.1455\n",
      "Epoch 1424/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.0527 - val_loss: 153.7921\n",
      "Epoch 1425/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.0323 - val_loss: 150.3749\n",
      "Epoch 1426/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.0505 - val_loss: 231.2926\n",
      "Epoch 1427/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.9028 - val_loss: 172.6481\n",
      "Epoch 1428/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.8442 - val_loss: 193.7837\n",
      "Epoch 1429/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 158.1791 - val_loss: 174.1017\n",
      "Epoch 1430/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 153.2485 - val_loss: 155.2044\n",
      "Epoch 1431/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.1082 - val_loss: 149.7214\n",
      "Epoch 1432/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 157.5307 - val_loss: 142.3584\n",
      "Epoch 1433/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 155.0516 - val_loss: 156.4599\n",
      "Epoch 1434/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 220.8750 - val_loss: 177.7988\n",
      "Epoch 1435/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 140.1070 - val_loss: 176.4742\n",
      "Epoch 1436/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.5200 - val_loss: 147.7890\n",
      "Epoch 1437/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 142.0632 - val_loss: 167.4856\n",
      "Epoch 1438/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.7132 - val_loss: 156.0294\n",
      "Epoch 1439/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.3320 - val_loss: 179.7392\n",
      "Epoch 1440/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.5497 - val_loss: 161.7074\n",
      "Epoch 1441/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 144.5607 - val_loss: 167.5629\n",
      "Epoch 1442/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 186.8247 - val_loss: 158.3516\n",
      "Epoch 1443/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.0435 - val_loss: 155.2347\n",
      "Epoch 1444/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.5046 - val_loss: 164.2652\n",
      "Epoch 1445/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.7337 - val_loss: 147.7154\n",
      "Epoch 1446/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 150.6271 - val_loss: 244.8048\n",
      "Epoch 1447/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.9380 - val_loss: 183.3293\n",
      "Epoch 1448/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 154.4775 - val_loss: 149.1996\n",
      "Epoch 1449/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.5568 - val_loss: 196.7280\n",
      "Epoch 1450/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 198.9817 - val_loss: 167.5488\n",
      "Epoch 1451/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.6106 - val_loss: 146.4706\n",
      "Epoch 1452/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 184.4842 - val_loss: 381.0642\n",
      "Epoch 1453/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 228.8446 - val_loss: 147.0742\n",
      "Epoch 1454/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 149.7742 - val_loss: 165.6467\n",
      "Epoch 1455/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 142.3606 - val_loss: 151.7011\n",
      "Epoch 1456/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 55us/step - loss: 178.7114 - val_loss: 192.9741\n",
      "Epoch 1457/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 158.1088 - val_loss: 153.6712\n",
      "Epoch 1458/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.3856 - val_loss: 143.9768\n",
      "Epoch 1459/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.3458 - val_loss: 200.5864\n",
      "Epoch 1460/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.1373 - val_loss: 165.1448\n",
      "Epoch 1461/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.4669 - val_loss: 151.5673\n",
      "Epoch 1462/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.3575 - val_loss: 165.8871\n",
      "Epoch 1463/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.0544 - val_loss: 148.7503\n",
      "Epoch 1464/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 147.4271 - val_loss: 157.3392\n",
      "Epoch 1465/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 142.2296 - val_loss: 170.0776\n",
      "Epoch 1466/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.4016 - val_loss: 174.2068\n",
      "Epoch 1467/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 155.1139 - val_loss: 165.8806\n",
      "Epoch 1468/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.5544 - val_loss: 148.4540\n",
      "Epoch 1469/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 208.1569 - val_loss: 157.5734\n",
      "Epoch 1470/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 285.2722 - val_loss: 290.1412\n",
      "Epoch 1471/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 189.1589 - val_loss: 174.8622\n",
      "Epoch 1472/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 157.0215 - val_loss: 165.6295\n",
      "Epoch 1473/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.5239 - val_loss: 149.5879\n",
      "Epoch 1474/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.1085 - val_loss: 158.7795\n",
      "Epoch 1475/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.6349 - val_loss: 144.6308\n",
      "Epoch 1476/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 161.1770 - val_loss: 148.0167\n",
      "Epoch 1477/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.9414 - val_loss: 148.1469\n",
      "Epoch 1478/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.6570 - val_loss: 149.6031\n",
      "Epoch 1479/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.0325 - val_loss: 280.6343\n",
      "Epoch 1480/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 201.4461 - val_loss: 160.4873\n",
      "Epoch 1481/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.4782 - val_loss: 142.2460\n",
      "Epoch 1482/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 232.2611 - val_loss: 146.4453\n",
      "Epoch 1483/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 158.9761 - val_loss: 149.4208\n",
      "Epoch 1484/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.6820 - val_loss: 142.2258\n",
      "Epoch 1485/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.7025 - val_loss: 145.1286\n",
      "Epoch 1486/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.4991 - val_loss: 167.9463\n",
      "Epoch 1487/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 261.7767 - val_loss: 169.4171\n",
      "Epoch 1488/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 157.9637 - val_loss: 161.4596\n",
      "Epoch 1489/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.9717 - val_loss: 151.3378\n",
      "Epoch 1490/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 167.8246 - val_loss: 156.5256\n",
      "Epoch 1491/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.3490 - val_loss: 301.6638\n",
      "Epoch 1492/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 187.5568 - val_loss: 193.9056\n",
      "Epoch 1493/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.2185 - val_loss: 151.0362\n",
      "Epoch 1494/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 170.4456 - val_loss: 166.8223\n",
      "Epoch 1495/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 156.8516 - val_loss: 161.5299\n",
      "Epoch 1496/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 154.7177 - val_loss: 243.1033\n",
      "Epoch 1497/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.0733 - val_loss: 144.5605\n",
      "Epoch 1498/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.0015 - val_loss: 161.1317\n",
      "Epoch 1499/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.0022 - val_loss: 146.1790\n",
      "Epoch 1500/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 341.6719 - val_loss: 173.4678\n",
      "Epoch 1501/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 157.1707 - val_loss: 159.4950\n",
      "Epoch 1502/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.8401 - val_loss: 161.8009\n",
      "Epoch 1503/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 148.3288 - val_loss: 144.6308\n",
      "Epoch 1504/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.8605 - val_loss: 222.8425\n",
      "Epoch 1505/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.9878 - val_loss: 170.9370\n",
      "Epoch 1506/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 255.2400 - val_loss: 228.0044\n",
      "Epoch 1507/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 184.5482 - val_loss: 162.2892\n",
      "Epoch 1508/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 158.1867 - val_loss: 190.8786\n",
      "Epoch 1509/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 168.3014 - val_loss: 145.3905\n",
      "Epoch 1510/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.2888 - val_loss: 143.8144\n",
      "Epoch 1511/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.0797 - val_loss: 165.1806\n",
      "Epoch 1512/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 177.7291 - val_loss: 173.5519\n",
      "Epoch 1513/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.0016 - val_loss: 176.4262\n",
      "Epoch 1514/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 168.0350 - val_loss: 161.9411\n",
      "Epoch 1515/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 171.3924 - val_loss: 163.8185\n",
      "Epoch 1516/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.7658 - val_loss: 168.2719\n",
      "Epoch 1517/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 153.1612 - val_loss: 152.1429\n",
      "Epoch 1518/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.7363 - val_loss: 221.6153\n",
      "Epoch 1519/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 239.5240 - val_loss: 168.1998\n",
      "Epoch 1520/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 185.4677 - val_loss: 151.7382\n",
      "Epoch 1521/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 165.0861 - val_loss: 147.4630\n",
      "Epoch 1522/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.2018 - val_loss: 205.1529\n",
      "Epoch 1523/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 162.9095 - val_loss: 193.6820\n",
      "Epoch 1524/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 148.3476 - val_loss: 147.9304\n",
      "Epoch 1525/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 158.4653 - val_loss: 142.3962\n",
      "Epoch 1526/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.4921 - val_loss: 152.5402\n",
      "Epoch 1527/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 166.2579 - val_loss: 156.5167\n",
      "Epoch 1528/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 441.4892 - val_loss: 179.8366\n",
      "Epoch 1529/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 158.9241 - val_loss: 152.5089\n",
      "Epoch 1530/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.1823 - val_loss: 180.9280\n",
      "Epoch 1531/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 168.9205 - val_loss: 156.0390\n",
      "Epoch 1532/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 230.6013 - val_loss: 165.9940\n",
      "Epoch 1533/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 162.2227 - val_loss: 193.7653\n",
      "Epoch 1534/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.0191 - val_loss: 182.5446\n",
      "Epoch 1535/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 150.9576 - val_loss: 274.4633\n",
      "Epoch 1536/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 149.2184 - val_loss: 172.7050\n",
      "Epoch 1537/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 141.3417 - val_loss: 170.9025\n",
      "Epoch 1538/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 157.3432 - val_loss: 169.7377\n",
      "Epoch 1539/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 150.9867 - val_loss: 149.4560\n",
      "Epoch 1540/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 139.1243 - val_loss: 182.3180\n",
      "Epoch 1541/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.4988 - val_loss: 172.8635\n",
      "Epoch 1542/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.3090 - val_loss: 286.4992\n",
      "Epoch 1543/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 183.8128 - val_loss: 145.6309\n",
      "Epoch 1544/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 174.5891 - val_loss: 184.7274\n",
      "Epoch 1545/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 155.8713 - val_loss: 154.2728\n",
      "Epoch 1546/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 146.5978 - val_loss: 141.9488\n",
      "Epoch 1547/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 148.3649 - val_loss: 146.1737\n",
      "Epoch 1548/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.1253 - val_loss: 185.8240\n",
      "Epoch 1549/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.8614 - val_loss: 153.2314\n",
      "Epoch 1550/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 179.6131 - val_loss: 219.1950\n",
      "Epoch 1551/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 182.9685 - val_loss: 147.2378\n",
      "Epoch 1552/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.2474 - val_loss: 150.2943\n",
      "Epoch 1553/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.7532 - val_loss: 141.7594\n",
      "Epoch 1554/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 153.8153 - val_loss: 148.1546\n",
      "Epoch 1555/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 236.9277 - val_loss: 226.0378\n",
      "Epoch 1556/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 176.0123 - val_loss: 195.4507\n",
      "Epoch 1557/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.8442 - val_loss: 161.7811\n",
      "Epoch 1558/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 173.7040 - val_loss: 143.6785\n",
      "Epoch 1559/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 154.4432 - val_loss: 164.2260\n",
      "Epoch 1560/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.2085 - val_loss: 153.1342\n",
      "Epoch 1561/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.5215 - val_loss: 184.1969\n",
      "Epoch 1562/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.3698 - val_loss: 145.8737\n",
      "Epoch 1563/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 141.9636 - val_loss: 147.2503\n",
      "Epoch 1564/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.2943 - val_loss: 153.3019\n",
      "Epoch 1565/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.0356 - val_loss: 160.1411\n",
      "Epoch 1566/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 156.6837 - val_loss: 159.8858\n",
      "Epoch 1567/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 147.4696 - val_loss: 151.4469\n",
      "Epoch 1568/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 146.3034 - val_loss: 157.8526\n",
      "Epoch 1569/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 222.3864 - val_loss: 167.8119\n",
      "Epoch 1570/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.2711 - val_loss: 156.3608\n",
      "Epoch 1571/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.1409 - val_loss: 167.5837\n",
      "Epoch 1572/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.4913 - val_loss: 178.9811\n",
      "Epoch 1573/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.4680 - val_loss: 185.6061\n",
      "Epoch 1574/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 412.5116 - val_loss: 216.6790\n",
      "Epoch 1575/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 247.5705 - val_loss: 178.4712\n",
      "Epoch 1576/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 194.0801 - val_loss: 172.6335\n",
      "Epoch 1577/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 187.9475 - val_loss: 185.2317\n",
      "Epoch 1578/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 168.8174 - val_loss: 178.9043\n",
      "Epoch 1579/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 194.9263 - val_loss: 166.1697\n",
      "Epoch 1580/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 163.8258 - val_loss: 157.2068\n",
      "Epoch 1581/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 151.6790 - val_loss: 186.0017\n",
      "Epoch 1582/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 201.5160 - val_loss: 171.4547\n",
      "Epoch 1583/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 165.9851 - val_loss: 151.0364\n",
      "Epoch 1584/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 156.5971 - val_loss: 148.9956\n",
      "Epoch 1585/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 174.1279 - val_loss: 300.9927\n",
      "Epoch 1586/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 205.1466 - val_loss: 153.4845\n",
      "Epoch 1587/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 154.2469 - val_loss: 153.6491\n",
      "Epoch 1588/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 179.0344 - val_loss: 158.4287\n",
      "Epoch 1589/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.3924 - val_loss: 179.2233\n",
      "Epoch 1590/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 158.7283 - val_loss: 174.1512\n",
      "Epoch 1591/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 174.9336 - val_loss: 169.1363\n",
      "Epoch 1592/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 191.2361 - val_loss: 203.2053\n",
      "Epoch 1593/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.5665 - val_loss: 158.6206\n",
      "Epoch 1594/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.8984 - val_loss: 151.0064\n",
      "Epoch 1595/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.5981 - val_loss: 246.2283\n",
      "Epoch 1596/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 168.6482 - val_loss: 198.4503\n",
      "Epoch 1597/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 194.4770 - val_loss: 291.1852\n",
      "Epoch 1598/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 200.7399 - val_loss: 269.6860\n",
      "Epoch 1599/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 168.5336 - val_loss: 194.5312\n",
      "Epoch 1600/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 186.9902 - val_loss: 174.8253\n",
      "Epoch 1601/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 146.0760 - val_loss: 161.1747\n",
      "Epoch 1602/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 241.7843 - val_loss: 218.8807\n",
      "Epoch 1603/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 155.1234 - val_loss: 150.9913\n",
      "Epoch 1604/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 163.4089 - val_loss: 290.3206\n",
      "Epoch 1605/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 159.0888 - val_loss: 168.3719\n",
      "Epoch 1606/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 150.5056 - val_loss: 156.8376\n",
      "Epoch 1607/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 192.4648 - val_loss: 300.1763\n",
      "Epoch 1608/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 216.6168 - val_loss: 159.1432\n",
      "Epoch 1609/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.7982 - val_loss: 210.1823\n",
      "Epoch 1610/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 175.7196 - val_loss: 158.4764\n",
      "Epoch 1611/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.0377 - val_loss: 152.0131\n",
      "Epoch 1612/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 144.3557 - val_loss: 156.7312\n",
      "Epoch 1613/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 155.0144 - val_loss: 151.2178\n",
      "Epoch 1614/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 155.4028 - val_loss: 146.8661\n",
      "Epoch 1615/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 158.8524 - val_loss: 291.0287\n",
      "Epoch 1616/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.5737 - val_loss: 148.4569\n",
      "Epoch 1617/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.2701 - val_loss: 162.5437\n",
      "Epoch 1618/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 143.1425 - val_loss: 146.9851\n",
      "Epoch 1619/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 148.7062 - val_loss: 204.9460\n",
      "Epoch 1620/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 149.0969 - val_loss: 171.0906\n",
      "Epoch 1621/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 158.2947 - val_loss: 212.0655\n",
      "Epoch 1622/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.2843 - val_loss: 181.7312\n",
      "Epoch 1623/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 173.8333 - val_loss: 170.5325\n",
      "Epoch 1624/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 273.5533 - val_loss: 167.7460\n",
      "Epoch 1625/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 168.0310 - val_loss: 153.3564\n",
      "Epoch 1626/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 149.3730 - val_loss: 170.7844\n",
      "Epoch 1627/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.3848 - val_loss: 184.7552\n",
      "Epoch 1628/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 195.3743 - val_loss: 157.4802\n",
      "Epoch 1629/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 163.0422 - val_loss: 173.8579\n",
      "Epoch 1630/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 189.0376 - val_loss: 343.0769\n",
      "Epoch 1631/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 166.6238 - val_loss: 249.5588\n",
      "Epoch 1632/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 148.0134 - val_loss: 157.0846\n",
      "Epoch 1633/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.0866 - val_loss: 181.7841\n",
      "Epoch 1634/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.5458 - val_loss: 220.6449\n",
      "Epoch 1635/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 244.3347 - val_loss: 176.5868\n",
      "Epoch 1636/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 152.6097 - val_loss: 148.5055\n",
      "Epoch 1637/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 155.9655 - val_loss: 164.7664\n",
      "Epoch 1638/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 150.9854 - val_loss: 155.6213\n",
      "Epoch 1639/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.9408 - val_loss: 148.5328\n",
      "Epoch 1640/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 144.3388 - val_loss: 161.2857\n",
      "Epoch 1641/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 147.1989 - val_loss: 153.5117\n",
      "Epoch 1642/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 184.6561 - val_loss: 262.9985\n",
      "Epoch 1643/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 147.0394 - val_loss: 440.8974\n",
      "Epoch 1644/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 152.9911 - val_loss: 166.4489\n",
      "Epoch 1645/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 157.4329 - val_loss: 147.1309\n",
      "Epoch 1646/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 142.0669 - val_loss: 148.5843\n",
      "Epoch 1647/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.7715 - val_loss: 153.6833\n",
      "Epoch 1648/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.6051 - val_loss: 174.0071\n",
      "Epoch 1649/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.2320 - val_loss: 146.6337\n",
      "Epoch 1650/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.7593 - val_loss: 144.3091\n",
      "Epoch 1651/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 151.9439 - val_loss: 177.3149\n",
      "Epoch 1652/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 211.8385 - val_loss: 161.6036\n",
      "Epoch 1653/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 147.6512 - val_loss: 177.4020\n",
      "Epoch 1654/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 144.8965 - val_loss: 185.5310\n",
      "Epoch 1655/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 156.5006 - val_loss: 253.1733\n",
      "Epoch 1656/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 163.9446 - val_loss: 174.9484\n",
      "Epoch 1657/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 172.5532 - val_loss: 159.0754\n",
      "Epoch 1658/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 165.8906 - val_loss: 240.4295\n",
      "Epoch 1659/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 155.6351 - val_loss: 192.7402\n",
      "Epoch 1660/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 185.9671 - val_loss: 165.7568\n",
      "Epoch 1661/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 156.7862 - val_loss: 152.2604\n",
      "Epoch 1662/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 162.3992 - val_loss: 157.8549\n",
      "Epoch 1663/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 144.7398 - val_loss: 171.4854\n",
      "Epoch 1664/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 165.6023 - val_loss: 165.4852\n",
      "Epoch 1665/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.9593 - val_loss: 175.9553\n",
      "Epoch 1666/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.4455 - val_loss: 159.8700\n",
      "Epoch 1667/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 151.9469 - val_loss: 152.0186\n",
      "Epoch 1668/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 141.4921 - val_loss: 162.1747\n",
      "Epoch 1669/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.7843 - val_loss: 210.5135\n",
      "Epoch 1670/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 141.554 - 0s 51us/step - loss: 143.0426 - val_loss: 183.9894\n",
      "Epoch 1671/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 145.6174 - val_loss: 154.0117\n",
      "Epoch 1672/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step - loss: 140.6575 - val_loss: 158.5395\n",
      "Epoch 1673/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 155.7243 - val_loss: 164.4103\n",
      "Epoch 1674/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 161.4313 - val_loss: 163.6331\n",
      "Epoch 1675/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.6391 - val_loss: 159.2283\n",
      "Epoch 1676/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 147.5027 - val_loss: 162.1035\n",
      "Epoch 1677/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.7713 - val_loss: 156.3836\n",
      "Epoch 1678/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.9894 - val_loss: 166.2634\n",
      "Epoch 1679/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 146.3648 - val_loss: 143.9599\n",
      "Epoch 1680/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 154.5688 - val_loss: 229.3567\n",
      "Epoch 1681/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 143.6257 - val_loss: 151.1336\n",
      "Epoch 1682/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.7991 - val_loss: 147.2258\n",
      "Epoch 1683/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 155.1701 - val_loss: 149.7482\n",
      "Epoch 1684/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 152.1654 - val_loss: 181.1052\n",
      "Epoch 1685/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.9546 - val_loss: 209.9759\n",
      "Epoch 1686/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 142.2445 - val_loss: 166.7406\n",
      "Epoch 1687/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.1220 - val_loss: 147.4753\n",
      "Epoch 1688/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.6688 - val_loss: 171.0451\n",
      "Epoch 1689/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.8692 - val_loss: 148.4193\n",
      "Epoch 1690/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.1966 - val_loss: 240.4622\n",
      "Epoch 1691/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.2565 - val_loss: 161.6310\n",
      "Epoch 1692/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 173.8141 - val_loss: 327.8487\n",
      "Epoch 1693/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 428.7455 - val_loss: 239.5731\n",
      "Epoch 1694/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 233.0587 - val_loss: 258.7106\n",
      "Epoch 1695/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 214.8485 - val_loss: 192.4129\n",
      "Epoch 1696/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 180.5537 - val_loss: 206.2833\n",
      "Epoch 1697/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 198.0996 - val_loss: 184.3115\n",
      "Epoch 1698/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 182.3078 - val_loss: 318.3628\n",
      "Epoch 1699/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 172.8926 - val_loss: 170.2634\n",
      "Epoch 1700/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 164.1795 - val_loss: 167.9348\n",
      "Epoch 1701/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 162.1747 - val_loss: 174.7643\n",
      "Epoch 1702/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 173.6388 - val_loss: 171.6850\n",
      "Epoch 1703/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 154.9185 - val_loss: 152.3296\n",
      "Epoch 1704/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 190.8795 - val_loss: 154.3181\n",
      "Epoch 1705/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.4477 - val_loss: 150.9832\n",
      "Epoch 1706/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 169.5049 - val_loss: 155.5760\n",
      "Epoch 1707/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 142.108 - 0s 50us/step - loss: 141.4735 - val_loss: 146.5909\n",
      "Epoch 1708/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.3901 - val_loss: 156.3876\n",
      "Epoch 1709/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 167.0511 - val_loss: 159.3797\n",
      "Epoch 1710/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 149.5430 - val_loss: 182.2483\n",
      "Epoch 1711/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.5181 - val_loss: 154.5869\n",
      "Epoch 1712/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 150.2076 - val_loss: 151.9729\n",
      "Epoch 1713/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.9311 - val_loss: 248.0580\n",
      "Epoch 1714/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 170.0219 - val_loss: 142.3457\n",
      "Epoch 1715/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.0926 - val_loss: 195.2529\n",
      "Epoch 1716/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.8641 - val_loss: 153.9709\n",
      "Epoch 1717/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.1725 - val_loss: 148.0018\n",
      "Epoch 1718/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.1172 - val_loss: 181.0106\n",
      "Epoch 1719/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 183.6151 - val_loss: 157.7535\n",
      "Epoch 1720/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.0710 - val_loss: 235.0484\n",
      "Epoch 1721/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.3583 - val_loss: 141.5203\n",
      "Epoch 1722/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.0632 - val_loss: 168.9483\n",
      "Epoch 1723/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.4727 - val_loss: 157.7841\n",
      "Epoch 1724/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 249.7881 - val_loss: 158.3933\n",
      "Epoch 1725/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 162.1087 - val_loss: 220.5803\n",
      "Epoch 1726/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.3457 - val_loss: 161.0343\n",
      "Epoch 1727/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 158.1274 - val_loss: 158.9307\n",
      "Epoch 1728/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 174.0114 - val_loss: 176.2703\n",
      "Epoch 1729/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 136.5615 - val_loss: 201.2646\n",
      "Epoch 1730/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 162.4676 - val_loss: 164.3895\n",
      "Epoch 1731/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 153.6104 - val_loss: 152.8277\n",
      "Epoch 1732/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 185.3925 - val_loss: 161.5550\n",
      "Epoch 1733/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.4793 - val_loss: 244.4937\n",
      "Epoch 1734/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.3345 - val_loss: 141.5052\n",
      "Epoch 1735/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.2763 - val_loss: 172.6871\n",
      "Epoch 1736/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 207.8074 - val_loss: 169.1364\n",
      "Epoch 1737/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.3745 - val_loss: 144.5000\n",
      "Epoch 1738/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.4411 - val_loss: 153.8106\n",
      "Epoch 1739/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 140.0199 - val_loss: 149.5674\n",
      "Epoch 1740/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.9052 - val_loss: 145.0622\n",
      "Epoch 1741/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 146.2134 - val_loss: 154.5527\n",
      "Epoch 1742/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.6032 - val_loss: 148.1341\n",
      "Epoch 1743/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.1522 - val_loss: 376.1842\n",
      "Epoch 1744/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step - loss: 148.6503 - val_loss: 148.6055\n",
      "Epoch 1745/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.9113 - val_loss: 205.3034\n",
      "Epoch 1746/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 179.7963 - val_loss: 171.7410\n",
      "Epoch 1747/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.9228 - val_loss: 156.8641\n",
      "Epoch 1748/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 166.5660 - val_loss: 156.6845\n",
      "Epoch 1749/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 200.7063 - val_loss: 153.4616\n",
      "Epoch 1750/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.4216 - val_loss: 183.4072\n",
      "Epoch 1751/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.7528 - val_loss: 152.7854\n",
      "Epoch 1752/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 135.3927 - val_loss: 144.0957\n",
      "Epoch 1753/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 155.3981 - val_loss: 158.0814\n",
      "Epoch 1754/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 147.7476 - val_loss: 153.4818\n",
      "Epoch 1755/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 131.0364 - val_loss: 160.6299\n",
      "Epoch 1756/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 170.2938 - val_loss: 170.2754\n",
      "Epoch 1757/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 145.3107 - val_loss: 150.8861\n",
      "Epoch 1758/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 150.7466 - val_loss: 385.4899\n",
      "Epoch 1759/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.2566 - val_loss: 165.3943\n",
      "Epoch 1760/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.4642 - val_loss: 529.1935\n",
      "Epoch 1761/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 235.2668 - val_loss: 149.4965\n",
      "Epoch 1762/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 143.9189 - val_loss: 158.3898\n",
      "Epoch 1763/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.1187 - val_loss: 147.7776\n",
      "Epoch 1764/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 131.6825 - val_loss: 143.3223\n",
      "Epoch 1765/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.8633 - val_loss: 147.2571\n",
      "Epoch 1766/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 184.0902 - val_loss: 510.2339\n",
      "Epoch 1767/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 201.6525 - val_loss: 148.1548\n",
      "Epoch 1768/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.7340 - val_loss: 147.8325\n",
      "Epoch 1769/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.4411 - val_loss: 144.1066\n",
      "Epoch 1770/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 137.1490 - val_loss: 151.0692\n",
      "Epoch 1771/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 141.0240 - val_loss: 162.4180\n",
      "Epoch 1772/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 135.7971 - val_loss: 152.4287\n",
      "Epoch 1773/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 163.9163 - val_loss: 143.2663\n",
      "Epoch 1774/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 186.9936 - val_loss: 149.8015\n",
      "Epoch 1775/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 147.3208 - val_loss: 163.2070\n",
      "Epoch 1776/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 160.7404 - val_loss: 149.6232\n",
      "Epoch 1777/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 178.0494 - val_loss: 218.3425\n",
      "Epoch 1778/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 145.8273 - val_loss: 159.1931\n",
      "Epoch 1779/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 141.9468 - val_loss: 246.9256\n",
      "Epoch 1780/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 135.1329 - val_loss: 171.3912\n",
      "Epoch 1781/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 161.8181 - val_loss: 185.2551\n",
      "Epoch 1782/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 144.2149 - val_loss: 152.3088\n",
      "Epoch 1783/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.1824 - val_loss: 271.4312\n",
      "Epoch 1784/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.1369 - val_loss: 168.4211\n",
      "Epoch 1785/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 135.2049 - val_loss: 167.6962\n",
      "Epoch 1786/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 157.5214 - val_loss: 156.0973\n",
      "Epoch 1787/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 143.7417 - val_loss: 160.7493\n",
      "Epoch 1788/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.5803 - val_loss: 143.6473\n",
      "Epoch 1789/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 139.7631 - val_loss: 158.0865\n",
      "Epoch 1790/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 141.0683 - val_loss: 156.2168\n",
      "Epoch 1791/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 186.9608 - val_loss: 143.9222\n",
      "Epoch 1792/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 212.2631 - val_loss: 208.8307\n",
      "Epoch 1793/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 167.3416 - val_loss: 162.1881\n",
      "Epoch 1794/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 129.9026 - val_loss: 149.5643\n",
      "Epoch 1795/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 164.4476 - val_loss: 158.1467\n",
      "Epoch 1796/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 153.2857 - val_loss: 152.2880\n",
      "Epoch 1797/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 146.6564 - val_loss: 167.5000\n",
      "Epoch 1798/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.0847 - val_loss: 156.3991\n",
      "Epoch 1799/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 146.3019 - val_loss: 179.0798\n",
      "Epoch 1800/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.4501 - val_loss: 157.1597\n",
      "Epoch 1801/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 141.6939 - val_loss: 153.4602\n",
      "Epoch 1802/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 143.6526 - val_loss: 160.5387\n",
      "Epoch 1803/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.4453 - val_loss: 157.1758\n",
      "Epoch 1804/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 172.1396 - val_loss: 179.9187\n",
      "Epoch 1805/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 155.3340 - val_loss: 146.0424\n",
      "Epoch 1806/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 138.3786 - val_loss: 178.2250\n",
      "Epoch 1807/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 140.4440 - val_loss: 188.9106\n",
      "Epoch 1808/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 206.9233 - val_loss: 164.6718\n",
      "Epoch 1809/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 164.0341 - val_loss: 144.9590\n",
      "Epoch 1810/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.0314 - val_loss: 149.2884\n",
      "Epoch 1811/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 147.1394 - val_loss: 160.5631\n",
      "Epoch 1812/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 135.6346 - val_loss: 162.9985\n",
      "Epoch 1813/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.3273 - val_loss: 161.5274\n",
      "Epoch 1814/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.4713 - val_loss: 143.5238\n",
      "Epoch 1815/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 137.5307 - val_loss: 155.7257\n",
      "Epoch 1816/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step - loss: 140.3734 - val_loss: 172.1463\n",
      "Epoch 1817/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 259.0636 - val_loss: 200.6865\n",
      "Epoch 1818/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 157.2389 - val_loss: 156.6065\n",
      "Epoch 1819/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 144.9146 - val_loss: 182.8146\n",
      "Epoch 1820/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.4804 - val_loss: 152.7565\n",
      "Epoch 1821/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.4935 - val_loss: 144.2044\n",
      "Epoch 1822/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.5276 - val_loss: 155.1063\n",
      "Epoch 1823/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.4795 - val_loss: 142.9740\n",
      "Epoch 1824/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.7410 - val_loss: 154.5463\n",
      "Epoch 1825/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 194.6763 - val_loss: 166.4541\n",
      "Epoch 1826/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.6630 - val_loss: 145.5453\n",
      "Epoch 1827/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.3002 - val_loss: 161.8528\n",
      "Epoch 1828/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.5432 - val_loss: 182.9675\n",
      "Epoch 1829/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.4196 - val_loss: 163.4662\n",
      "Epoch 1830/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.0555 - val_loss: 219.6018\n",
      "Epoch 1831/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 140.0432 - val_loss: 169.8431\n",
      "Epoch 1832/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.6534 - val_loss: 159.1028\n",
      "Epoch 1833/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.5984 - val_loss: 149.1546\n",
      "Epoch 1834/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 260.6912 - val_loss: 312.5126\n",
      "Epoch 1835/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 170.3777 - val_loss: 150.0268\n",
      "Epoch 1836/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.4032 - val_loss: 148.6631\n",
      "Epoch 1837/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.0340 - val_loss: 162.5683\n",
      "Epoch 1838/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.9393 - val_loss: 155.9758\n",
      "Epoch 1839/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 146.668 - 0s 51us/step - loss: 146.6436 - val_loss: 172.7070\n",
      "Epoch 1840/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.4175 - val_loss: 158.7630\n",
      "Epoch 1841/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.1918 - val_loss: 173.1923\n",
      "Epoch 1842/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.9478 - val_loss: 147.4937\n",
      "Epoch 1843/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.6195 - val_loss: 160.3890\n",
      "Epoch 1844/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.4972 - val_loss: 153.9477\n",
      "Epoch 1845/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.4314 - val_loss: 260.1166\n",
      "Epoch 1846/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.3946 - val_loss: 146.2550\n",
      "Epoch 1847/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 209.7849 - val_loss: 145.7475\n",
      "Epoch 1848/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.9399 - val_loss: 141.7168\n",
      "Epoch 1849/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.4372 - val_loss: 148.1349\n",
      "Epoch 1850/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.0855 - val_loss: 180.3534\n",
      "Epoch 1851/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.7067 - val_loss: 140.7216\n",
      "Epoch 1852/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 292.2802 - val_loss: 146.4428\n",
      "Epoch 1853/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.7350 - val_loss: 160.7329\n",
      "Epoch 1854/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 130.7363 - val_loss: 156.7258\n",
      "Epoch 1855/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 128.4852 - val_loss: 172.8063\n",
      "Epoch 1856/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.9922 - val_loss: 195.5706\n",
      "Epoch 1857/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 164.7776 - val_loss: 143.3723\n",
      "Epoch 1858/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.2054 - val_loss: 280.8924\n",
      "Epoch 1859/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.6114 - val_loss: 159.5917\n",
      "Epoch 1860/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.5844 - val_loss: 166.0396\n",
      "Epoch 1861/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.6568 - val_loss: 198.2068\n",
      "Epoch 1862/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 226.0913 - val_loss: 173.2836\n",
      "Epoch 1863/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.3525 - val_loss: 172.0884\n",
      "Epoch 1864/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 150.0458 - val_loss: 295.4006\n",
      "Epoch 1865/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 170.0569 - val_loss: 161.5428\n",
      "Epoch 1866/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.5319 - val_loss: 149.1093\n",
      "Epoch 1867/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 216.8521 - val_loss: 165.5984\n",
      "Epoch 1868/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.5523 - val_loss: 165.9871\n",
      "Epoch 1869/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.2964 - val_loss: 219.8400\n",
      "Epoch 1870/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.0294 - val_loss: 162.2982\n",
      "Epoch 1871/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.7362 - val_loss: 154.1545\n",
      "Epoch 1872/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.5346 - val_loss: 151.0682\n",
      "Epoch 1873/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.7734 - val_loss: 144.1927\n",
      "Epoch 1874/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 168.9065 - val_loss: 161.3153\n",
      "Epoch 1875/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 171.5757 - val_loss: 178.1193\n",
      "Epoch 1876/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 146.9085 - val_loss: 142.4602\n",
      "Epoch 1877/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 151.6286 - val_loss: 158.5775\n",
      "Epoch 1878/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.7348 - val_loss: 165.8411\n",
      "Epoch 1879/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.4104 - val_loss: 174.0142\n",
      "Epoch 1880/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.4260 - val_loss: 179.5513\n",
      "Epoch 1881/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 141.5045 - val_loss: 386.6025\n",
      "Epoch 1882/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 230.5333 - val_loss: 160.4817\n",
      "Epoch 1883/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.8728 - val_loss: 147.4478\n",
      "Epoch 1884/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 137.4098 - val_loss: 188.6010\n",
      "Epoch 1885/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.5205 - val_loss: 147.2969\n",
      "Epoch 1886/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 147.2061 - val_loss: 144.6226\n",
      "Epoch 1887/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 148.0750 - val_loss: 237.3159\n",
      "Epoch 1888/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.6033 - val_loss: 148.7036\n",
      "Epoch 1889/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.3472 - val_loss: 147.9294\n",
      "Epoch 1890/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.7730 - val_loss: 159.8739\n",
      "Epoch 1891/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.0696 - val_loss: 147.0745\n",
      "Epoch 1892/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 143.7811 - val_loss: 153.8979\n",
      "Epoch 1893/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.3325 - val_loss: 156.1927\n",
      "Epoch 1894/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.7209 - val_loss: 150.6463\n",
      "Epoch 1895/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.4263 - val_loss: 186.3948\n",
      "Epoch 1896/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.3642 - val_loss: 160.1046\n",
      "Epoch 1897/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 154.0753 - val_loss: 198.3203\n",
      "Epoch 1898/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 277.3059 - val_loss: 143.3464\n",
      "Epoch 1899/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.1611 - val_loss: 145.0790\n",
      "Epoch 1900/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.9386 - val_loss: 152.0676\n",
      "Epoch 1901/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.3024 - val_loss: 140.4459\n",
      "Epoch 1902/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 130.0661 - val_loss: 143.5723\n",
      "Epoch 1903/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.0832 - val_loss: 174.5267\n",
      "Epoch 1904/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.9468 - val_loss: 182.4618\n",
      "Epoch 1905/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.0263 - val_loss: 184.6033\n",
      "Epoch 1906/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 147.0766 - val_loss: 144.0113\n",
      "Epoch 1907/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.3120 - val_loss: 201.6719\n",
      "Epoch 1908/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 239.9853 - val_loss: 897.9391\n",
      "Epoch 1909/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 216.3447 - val_loss: 145.8230\n",
      "Epoch 1910/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 140.3784 - val_loss: 199.5235\n",
      "Epoch 1911/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.4867 - val_loss: 175.2581\n",
      "Epoch 1912/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 137.8125 - val_loss: 144.9606\n",
      "Epoch 1913/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 139.4542 - val_loss: 146.4939\n",
      "Epoch 1914/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 133.9623 - val_loss: 149.1052\n",
      "Epoch 1915/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 140.1646 - val_loss: 194.1991\n",
      "Epoch 1916/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 138.8362 - val_loss: 143.3014\n",
      "Epoch 1917/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.8415 - val_loss: 196.4573\n",
      "Epoch 1918/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.5194 - val_loss: 145.2547\n",
      "Epoch 1919/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 174.6974 - val_loss: 167.0326\n",
      "Epoch 1920/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.4092 - val_loss: 151.1786\n",
      "Epoch 1921/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 159.2699 - val_loss: 267.3471\n",
      "Epoch 1922/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 178.7805 - val_loss: 370.3561\n",
      "Epoch 1923/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.8275 - val_loss: 207.7932\n",
      "Epoch 1924/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.8743 - val_loss: 143.2757\n",
      "Epoch 1925/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.7892 - val_loss: 157.3285\n",
      "Epoch 1926/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.6777 - val_loss: 140.3686\n",
      "Epoch 1927/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 144.0728 - val_loss: 148.0201\n",
      "Epoch 1928/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.7041 - val_loss: 153.3184\n",
      "Epoch 1929/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.0357 - val_loss: 167.7325\n",
      "Epoch 1930/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.3280 - val_loss: 279.0896\n",
      "Epoch 1931/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.1646 - val_loss: 164.1996\n",
      "Epoch 1932/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.2037 - val_loss: 165.2851\n",
      "Epoch 1933/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.4727 - val_loss: 140.5239\n",
      "Epoch 1934/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 185.2932 - val_loss: 167.9033\n",
      "Epoch 1935/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.0606 - val_loss: 148.3035\n",
      "Epoch 1936/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.1607 - val_loss: 183.0265\n",
      "Epoch 1937/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.7273 - val_loss: 217.0940\n",
      "Epoch 1938/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.3929 - val_loss: 145.4216\n",
      "Epoch 1939/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.6238 - val_loss: 155.9784\n",
      "Epoch 1940/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.9337 - val_loss: 145.6161\n",
      "Epoch 1941/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.1383 - val_loss: 149.7429\n",
      "Epoch 1942/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 140.8888 - val_loss: 152.9304\n",
      "Epoch 1943/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 157.9985 - val_loss: 176.2197\n",
      "Epoch 1944/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.2312 - val_loss: 157.5596\n",
      "Epoch 1945/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.2424 - val_loss: 143.1409\n",
      "Epoch 1946/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.4362 - val_loss: 145.0827\n",
      "Epoch 1947/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 130.2077 - val_loss: 190.4977\n",
      "Epoch 1948/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.5195 - val_loss: 152.8403\n",
      "Epoch 1949/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.3298 - val_loss: 178.0479\n",
      "Epoch 1950/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.9612 - val_loss: 174.6200\n",
      "Epoch 1951/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.8049 - val_loss: 140.7413\n",
      "Epoch 1952/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 336.2597 - val_loss: 361.0303\n",
      "Epoch 1953/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 248.5619 - val_loss: 398.5620\n",
      "Epoch 1954/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 198.8646 - val_loss: 187.6853\n",
      "Epoch 1955/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 178.4967 - val_loss: 198.3302\n",
      "Epoch 1956/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 165.1243 - val_loss: 282.2678\n",
      "Epoch 1957/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 157.5695 - val_loss: 175.7013\n",
      "Epoch 1958/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 146.3368 - val_loss: 157.5378\n",
      "Epoch 1959/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 157.5699 - val_loss: 151.8842\n",
      "Epoch 1960/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.9376 - val_loss: 238.9861\n",
      "Epoch 1961/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 161.8687 - val_loss: 169.1612\n",
      "Epoch 1962/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.8567 - val_loss: 186.3458\n",
      "Epoch 1963/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 145.5121 - val_loss: 178.9818\n",
      "Epoch 1964/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.4886 - val_loss: 149.4164\n",
      "Epoch 1965/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.7786 - val_loss: 147.7131\n",
      "Epoch 1966/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.0114 - val_loss: 182.9141\n",
      "Epoch 1967/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.4331 - val_loss: 174.5798\n",
      "Epoch 1968/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.5617 - val_loss: 146.2163\n",
      "Epoch 1969/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 154.5101 - val_loss: 168.6626\n",
      "Epoch 1970/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 165.7581 - val_loss: 155.1582\n",
      "Epoch 1971/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.7174 - val_loss: 176.5510\n",
      "Epoch 1972/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.2599 - val_loss: 150.5813\n",
      "Epoch 1973/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.5504 - val_loss: 170.4771\n",
      "Epoch 1974/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 140.5955 - val_loss: 159.3029\n",
      "Epoch 1975/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.2021 - val_loss: 146.0641\n",
      "Epoch 1976/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.5156 - val_loss: 163.7523\n",
      "Epoch 1977/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.1955 - val_loss: 149.0372\n",
      "Epoch 1978/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.5191 - val_loss: 147.4273\n",
      "Epoch 1979/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 163.1182 - val_loss: 246.1714\n",
      "Epoch 1980/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 165.5630 - val_loss: 212.7696\n",
      "Epoch 1981/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.7225 - val_loss: 152.3509\n",
      "Epoch 1982/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.1601 - val_loss: 150.8071\n",
      "Epoch 1983/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.9747 - val_loss: 168.2125\n",
      "Epoch 1984/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.2372 - val_loss: 171.1685\n",
      "Epoch 1985/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.7418 - val_loss: 226.5954\n",
      "Epoch 1986/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 239.6552 - val_loss: 175.9916\n",
      "Epoch 1987/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.0224 - val_loss: 144.3564\n",
      "Epoch 1988/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 194.0957 - val_loss: 173.3941\n",
      "Epoch 1989/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.8096 - val_loss: 142.8322\n",
      "Epoch 1990/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.6471 - val_loss: 144.1253\n",
      "Epoch 1991/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.7591 - val_loss: 168.1474\n",
      "Epoch 1992/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.8520 - val_loss: 185.1579\n",
      "Epoch 1993/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.8386 - val_loss: 143.8872\n",
      "Epoch 1994/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.7643 - val_loss: 162.4829\n",
      "Epoch 1995/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.4569 - val_loss: 143.1511\n",
      "Epoch 1996/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 148.4244 - val_loss: 162.4379\n",
      "Epoch 1997/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 178.9096 - val_loss: 183.6219\n",
      "Epoch 1998/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 149.7052 - val_loss: 171.2488\n",
      "Epoch 1999/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.3717 - val_loss: 161.2760\n",
      "Epoch 2000/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.4992 - val_loss: 146.7768\n",
      "Epoch 2001/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.8324 - val_loss: 164.2374\n",
      "Epoch 2002/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.0716 - val_loss: 150.1617\n",
      "Epoch 2003/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.2915 - val_loss: 141.6559\n",
      "Epoch 2004/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 139.771 - 0s 51us/step - loss: 139.2023 - val_loss: 142.1783\n",
      "Epoch 2005/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.0238 - val_loss: 162.7822\n",
      "Epoch 2006/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.9751 - val_loss: 213.5497\n",
      "Epoch 2007/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.2300 - val_loss: 160.4405\n",
      "Epoch 2008/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 144.9872 - val_loss: 157.1687\n",
      "Epoch 2009/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.6415 - val_loss: 153.3852\n",
      "Epoch 2010/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.3120 - val_loss: 145.9731\n",
      "Epoch 2011/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.3313 - val_loss: 141.9938\n",
      "Epoch 2012/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.0031 - val_loss: 169.9567\n",
      "Epoch 2013/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.4220 - val_loss: 147.0015\n",
      "Epoch 2014/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 306.1173 - val_loss: 233.8738\n",
      "Epoch 2015/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 239.4789 - val_loss: 349.9552\n",
      "Epoch 2016/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 188.5227 - val_loss: 192.4627\n",
      "Epoch 2017/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.3279 - val_loss: 192.2984\n",
      "Epoch 2018/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 154.6220 - val_loss: 250.4027\n",
      "Epoch 2019/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 175.8827 - val_loss: 325.6601\n",
      "Epoch 2020/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 165.2171 - val_loss: 157.3300\n",
      "Epoch 2021/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 158.6018 - val_loss: 155.5036\n",
      "Epoch 2022/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 212.0488 - val_loss: 181.4180\n",
      "Epoch 2023/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 163.0213 - val_loss: 187.8128\n",
      "Epoch 2024/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 158.5450 - val_loss: 182.2271\n",
      "Epoch 2025/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 179.5893 - val_loss: 184.3857\n",
      "Epoch 2026/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 160.5040 - val_loss: 251.8881\n",
      "Epoch 2027/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 163.0700 - val_loss: 214.8591\n",
      "Epoch 2028/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 180.4377 - val_loss: 286.8638\n",
      "Epoch 2029/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 165.2250 - val_loss: 188.5440\n",
      "Epoch 2030/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.3796 - val_loss: 176.0289\n",
      "Epoch 2031/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 159.4306 - val_loss: 156.5922\n",
      "Epoch 2032/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.7635 - val_loss: 157.6374\n",
      "Epoch 2033/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 148.5227 - val_loss: 161.9825\n",
      "Epoch 2034/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 158.6539 - val_loss: 174.0997\n",
      "Epoch 2035/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.3409 - val_loss: 153.0432\n",
      "Epoch 2036/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.3507 - val_loss: 166.3862\n",
      "Epoch 2037/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 161.8755 - val_loss: 180.9079\n",
      "Epoch 2038/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.6339 - val_loss: 158.9454\n",
      "Epoch 2039/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.0494 - val_loss: 182.4567\n",
      "Epoch 2040/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.3147 - val_loss: 144.8259\n",
      "Epoch 2041/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.4154 - val_loss: 171.0900\n",
      "Epoch 2042/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 151.6972 - val_loss: 183.3224\n",
      "Epoch 2043/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 149.4750 - val_loss: 156.0947\n",
      "Epoch 2044/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.4796 - val_loss: 150.0524\n",
      "Epoch 2045/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.9459 - val_loss: 175.6284\n",
      "Epoch 2046/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.4945 - val_loss: 182.7725\n",
      "Epoch 2047/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.3084 - val_loss: 158.7744\n",
      "Epoch 2048/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.6637 - val_loss: 149.4073\n",
      "Epoch 2049/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 147.3562 - val_loss: 149.2266\n",
      "Epoch 2050/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.5310 - val_loss: 154.0790\n",
      "Epoch 2051/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.6290 - val_loss: 156.7895\n",
      "Epoch 2052/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 165.8473 - val_loss: 148.5383\n",
      "Epoch 2053/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 143.360 - 0s 51us/step - loss: 143.8944 - val_loss: 180.7539\n",
      "Epoch 2054/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 151.9895 - val_loss: 166.0110\n",
      "Epoch 2055/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.0686 - val_loss: 159.6948\n",
      "Epoch 2056/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.5045 - val_loss: 162.3024\n",
      "Epoch 2057/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 131.5170 - val_loss: 146.4247\n",
      "Epoch 2058/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 171.9846 - val_loss: 186.2061\n",
      "Epoch 2059/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.4600 - val_loss: 154.3533\n",
      "Epoch 2060/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.1800 - val_loss: 195.3785\n",
      "Epoch 2061/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.9924 - val_loss: 168.6359\n",
      "Epoch 2062/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.3067 - val_loss: 140.5222\n",
      "Epoch 2063/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.5683 - val_loss: 202.8998\n",
      "Epoch 2064/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 146.5822 - val_loss: 213.0910\n",
      "Epoch 2065/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.5412 - val_loss: 147.3199\n",
      "Epoch 2066/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.9323 - val_loss: 198.6481\n",
      "Epoch 2067/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 158.7278 - val_loss: 202.8542\n",
      "Epoch 2068/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.1152 - val_loss: 167.2298\n",
      "Epoch 2069/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.3273 - val_loss: 159.6735\n",
      "Epoch 2070/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.6427 - val_loss: 145.7671\n",
      "Epoch 2071/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 142.4607 - val_loss: 148.6527\n",
      "Epoch 2072/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.4419 - val_loss: 161.0049\n",
      "Epoch 2073/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.1567 - val_loss: 148.1362\n",
      "Epoch 2074/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 143.5583 - val_loss: 142.5685\n",
      "Epoch 2075/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 166.1291 - val_loss: 158.8007\n",
      "Epoch 2076/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 158.1414 - val_loss: 160.4114\n",
      "Epoch 2077/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 151.8849 - val_loss: 148.7924\n",
      "Epoch 2078/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.7797 - val_loss: 152.9376\n",
      "Epoch 2079/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 172.5424 - val_loss: 212.2344\n",
      "Epoch 2080/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.6317 - val_loss: 154.0188\n",
      "Epoch 2081/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 140.2321 - val_loss: 168.7400\n",
      "Epoch 2082/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.8859 - val_loss: 169.6321\n",
      "Epoch 2083/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.6339 - val_loss: 188.1287\n",
      "Epoch 2084/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 140.9858 - val_loss: 150.7303\n",
      "Epoch 2085/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 159.6527 - val_loss: 275.1595\n",
      "Epoch 2086/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.9191 - val_loss: 189.2057\n",
      "Epoch 2087/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 142.1794 - val_loss: 149.8909\n",
      "Epoch 2088/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 149.5504 - val_loss: 190.3915\n",
      "Epoch 2089/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.1707 - val_loss: 146.4579\n",
      "Epoch 2090/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.2925 - val_loss: 151.2323\n",
      "Epoch 2091/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 144.3241 - val_loss: 159.6414\n",
      "Epoch 2092/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.6729 - val_loss: 195.9972\n",
      "Epoch 2093/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.2157 - val_loss: 142.9403\n",
      "Epoch 2094/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.9277 - val_loss: 283.3901\n",
      "Epoch 2095/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.3869 - val_loss: 251.9045\n",
      "Epoch 2096/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 142.2844 - val_loss: 140.6495\n",
      "Epoch 2097/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 194.5044 - val_loss: 186.5709\n",
      "Epoch 2098/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 147.0703 - val_loss: 146.1042\n",
      "Epoch 2099/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.6201 - val_loss: 190.2375\n",
      "Epoch 2100/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.6176 - val_loss: 166.5386\n",
      "Epoch 2101/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 148.8744 - val_loss: 178.2020\n",
      "Epoch 2102/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.8610 - val_loss: 178.8454\n",
      "Epoch 2103/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 148.0998 - val_loss: 166.3262\n",
      "Epoch 2104/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.5498 - val_loss: 149.4381\n",
      "Epoch 2105/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.8864 - val_loss: 148.6842\n",
      "Epoch 2106/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.5855 - val_loss: 159.2334\n",
      "Epoch 2107/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 149.7668 - val_loss: 152.6561\n",
      "Epoch 2108/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.4609 - val_loss: 154.4483\n",
      "Epoch 2109/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.6078 - val_loss: 169.6205\n",
      "Epoch 2110/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 161.4016 - val_loss: 189.0076\n",
      "Epoch 2111/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.1359 - val_loss: 158.0821\n",
      "Epoch 2112/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.9146 - val_loss: 152.7744\n",
      "Epoch 2113/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 151.9314 - val_loss: 152.6913\n",
      "Epoch 2114/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.4665 - val_loss: 167.9872\n",
      "Epoch 2115/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.3565 - val_loss: 156.2899\n",
      "Epoch 2116/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 140.0159 - val_loss: 142.8474\n",
      "Epoch 2117/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.4684 - val_loss: 159.6582\n",
      "Epoch 2118/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.6144 - val_loss: 143.7263\n",
      "Epoch 2119/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 134.7924 - val_loss: 152.2546\n",
      "Epoch 2120/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.0301 - val_loss: 177.2573\n",
      "Epoch 2121/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.7993 - val_loss: 152.4961\n",
      "Epoch 2122/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.9581 - val_loss: 191.4772\n",
      "Epoch 2123/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.6938 - val_loss: 161.2499\n",
      "Epoch 2124/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 172.4469 - val_loss: 194.7271\n",
      "Epoch 2125/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 163.6328 - val_loss: 144.5360\n",
      "Epoch 2126/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.8743 - val_loss: 186.4285\n",
      "Epoch 2127/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.6988 - val_loss: 148.5180\n",
      "Epoch 2128/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.5724 - val_loss: 155.2697\n",
      "Epoch 2129/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.3723 - val_loss: 148.4085\n",
      "Epoch 2130/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.7034 - val_loss: 160.0952\n",
      "Epoch 2131/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.2939 - val_loss: 188.1171\n",
      "Epoch 2132/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.2457 - val_loss: 154.8422\n",
      "Epoch 2133/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.3080 - val_loss: 144.1181\n",
      "Epoch 2134/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.2703 - val_loss: 146.8554\n",
      "Epoch 2135/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.3848 - val_loss: 138.9248\n",
      "Epoch 2136/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 134.919 - 0s 51us/step - loss: 134.5774 - val_loss: 147.4020\n",
      "Epoch 2137/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.2097 - val_loss: 179.7164\n",
      "Epoch 2138/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.0614 - val_loss: 138.6605\n",
      "Epoch 2139/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.0574 - val_loss: 255.6263\n",
      "Epoch 2140/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.4743 - val_loss: 178.3947\n",
      "Epoch 2141/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 173.5773 - val_loss: 167.1295\n",
      "Epoch 2142/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.5971 - val_loss: 150.5208\n",
      "Epoch 2143/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.1572 - val_loss: 147.4673\n",
      "Epoch 2144/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.1008 - val_loss: 220.4703\n",
      "Epoch 2145/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.8934 - val_loss: 157.0299\n",
      "Epoch 2146/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.5822 - val_loss: 169.0303\n",
      "Epoch 2147/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.0684 - val_loss: 142.5924\n",
      "Epoch 2148/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.9879 - val_loss: 144.7229\n",
      "Epoch 2149/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.4980 - val_loss: 146.3129\n",
      "Epoch 2150/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.0088 - val_loss: 151.2974\n",
      "Epoch 2151/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.6278 - val_loss: 145.2176\n",
      "Epoch 2152/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.5358 - val_loss: 152.2543\n",
      "Epoch 2153/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 142.962 - 0s 50us/step - loss: 143.7261 - val_loss: 154.7022\n",
      "Epoch 2154/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 147.5434 - val_loss: 146.0193\n",
      "Epoch 2155/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 156.4722 - val_loss: 139.3261\n",
      "Epoch 2156/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 133.7226 - val_loss: 155.0357\n",
      "Epoch 2157/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 136.4210 - val_loss: 160.2011\n",
      "Epoch 2158/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.0700 - val_loss: 171.0491\n",
      "Epoch 2159/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 145.5916 - val_loss: 145.8842\n",
      "Epoch 2160/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 142.2817 - val_loss: 153.6613\n",
      "Epoch 2161/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 161.4818 - val_loss: 151.2706\n",
      "Epoch 2162/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 154.3135 - val_loss: 233.2385\n",
      "Epoch 2163/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 146.4532 - val_loss: 159.6260\n",
      "Epoch 2164/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.6223 - val_loss: 173.8529\n",
      "Epoch 2165/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.5406 - val_loss: 166.3675\n",
      "Epoch 2166/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.5590 - val_loss: 154.9926\n",
      "Epoch 2167/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 139.2625 - val_loss: 158.1379\n",
      "Epoch 2168/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.3659 - val_loss: 144.6932\n",
      "Epoch 2169/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.0844 - val_loss: 165.6874\n",
      "Epoch 2170/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 176.2630 - val_loss: 162.1258\n",
      "Epoch 2171/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.3320 - val_loss: 171.6668\n",
      "Epoch 2172/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 162.7528 - val_loss: 158.9599\n",
      "Epoch 2173/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.7942 - val_loss: 203.6485\n",
      "Epoch 2174/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.5717 - val_loss: 143.5155\n",
      "Epoch 2175/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.1888 - val_loss: 147.3861\n",
      "Epoch 2176/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.2064 - val_loss: 150.8723\n",
      "Epoch 2177/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.6283 - val_loss: 156.3356\n",
      "Epoch 2178/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.7407 - val_loss: 146.3843\n",
      "Epoch 2179/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.5824 - val_loss: 144.1405\n",
      "Epoch 2180/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.2447 - val_loss: 140.6687\n",
      "Epoch 2181/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.9194 - val_loss: 157.6418\n",
      "Epoch 2182/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 141.3827 - val_loss: 156.6957\n",
      "Epoch 2183/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 153.5658 - val_loss: 188.6689\n",
      "Epoch 2184/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.3972 - val_loss: 145.6593\n",
      "Epoch 2185/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.0338 - val_loss: 153.6520\n",
      "Epoch 2186/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.9671 - val_loss: 192.5372\n",
      "Epoch 2187/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.0072 - val_loss: 172.6244\n",
      "Epoch 2188/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.0938 - val_loss: 150.9239\n",
      "Epoch 2189/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.2533 - val_loss: 161.6170\n",
      "Epoch 2190/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.0136 - val_loss: 143.4403\n",
      "Epoch 2191/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 133.6281 - val_loss: 166.5242\n",
      "Epoch 2192/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.0534 - val_loss: 153.9724\n",
      "Epoch 2193/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.4865 - val_loss: 176.3830\n",
      "Epoch 2194/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.0777 - val_loss: 141.2726\n",
      "Epoch 2195/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.6400 - val_loss: 151.2836\n",
      "Epoch 2196/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.2022 - val_loss: 147.4939\n",
      "Epoch 2197/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.8537 - val_loss: 284.5639\n",
      "Epoch 2198/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 154.0202 - val_loss: 150.2957\n",
      "Epoch 2199/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 172.9278 - val_loss: 155.7535\n",
      "Epoch 2200/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 155.9051 - val_loss: 213.0483\n",
      "Epoch 2201/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.7564 - val_loss: 146.4651\n",
      "Epoch 2202/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 141.0984 - val_loss: 144.6364\n",
      "Epoch 2203/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.8799 - val_loss: 172.9773\n",
      "Epoch 2204/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 173.9562 - val_loss: 164.9483\n",
      "Epoch 2205/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.0336 - val_loss: 172.0204\n",
      "Epoch 2206/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.0869 - val_loss: 151.2944\n",
      "Epoch 2207/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.6934 - val_loss: 160.8579\n",
      "Epoch 2208/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 156.6115 - val_loss: 154.7076\n",
      "Epoch 2209/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.1938 - val_loss: 138.5629\n",
      "Epoch 2210/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.8182 - val_loss: 195.2746\n",
      "Epoch 2211/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.8446 - val_loss: 236.2983\n",
      "Epoch 2212/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.0858 - val_loss: 143.4526\n",
      "Epoch 2213/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 131.7713 - val_loss: 144.1623\n",
      "Epoch 2214/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 137.0692 - val_loss: 144.9785\n",
      "Epoch 2215/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 147.6902 - val_loss: 173.4421\n",
      "Epoch 2216/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 158.3534 - val_loss: 257.4743\n",
      "Epoch 2217/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 264.5236 - val_loss: 161.0781\n",
      "Epoch 2218/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.2447 - val_loss: 142.2365\n",
      "Epoch 2219/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.8776 - val_loss: 157.2949\n",
      "Epoch 2220/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.7272 - val_loss: 141.8114\n",
      "Epoch 2221/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.6759 - val_loss: 142.8427\n",
      "Epoch 2222/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.7823 - val_loss: 174.3737\n",
      "Epoch 2223/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 135.0225 - val_loss: 167.2994\n",
      "Epoch 2224/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 137.1783 - val_loss: 141.3052\n",
      "Epoch 2225/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.4620 - val_loss: 230.4812\n",
      "Epoch 2226/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 156.7830 - val_loss: 153.9620\n",
      "Epoch 2227/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.2823 - val_loss: 142.5615\n",
      "Epoch 2228/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 142.2264 - val_loss: 221.9023\n",
      "Epoch 2229/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.1022 - val_loss: 140.0887\n",
      "Epoch 2230/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 143.8717 - val_loss: 154.6371\n",
      "Epoch 2231/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 166.6581 - val_loss: 237.3694\n",
      "Epoch 2232/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 192.0365 - val_loss: 207.8816\n",
      "Epoch 2233/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 173.3527 - val_loss: 165.7331\n",
      "Epoch 2234/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.8005 - val_loss: 141.4180\n",
      "Epoch 2235/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 134.7338 - val_loss: 214.3070\n",
      "Epoch 2236/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.5511 - val_loss: 160.5023\n",
      "Epoch 2237/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.0934 - val_loss: 160.1103\n",
      "Epoch 2238/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 129.3754 - val_loss: 141.2673\n",
      "Epoch 2239/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.7735 - val_loss: 146.1862\n",
      "Epoch 2240/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 171.2290 - val_loss: 152.9465\n",
      "Epoch 2241/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 132.0551 - val_loss: 145.6343\n",
      "Epoch 2242/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 142.1827 - val_loss: 163.2017\n",
      "Epoch 2243/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.0274 - val_loss: 155.1112\n",
      "Epoch 2244/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.7359 - val_loss: 153.2072\n",
      "Epoch 2245/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.2467 - val_loss: 147.0693\n",
      "Epoch 2246/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.3546 - val_loss: 152.2997\n",
      "Epoch 2247/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.5629 - val_loss: 162.1360\n",
      "Epoch 2248/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 190.9998 - val_loss: 173.0783\n",
      "Epoch 2249/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.7203 - val_loss: 153.1986\n",
      "Epoch 2250/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.1926 - val_loss: 141.9384\n",
      "Epoch 2251/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.2997 - val_loss: 161.3472\n",
      "Epoch 2252/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.8631 - val_loss: 145.7159\n",
      "Epoch 2253/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 186.2948 - val_loss: 152.9358\n",
      "Epoch 2254/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.4324 - val_loss: 186.2336\n",
      "Epoch 2255/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.9089 - val_loss: 236.7953\n",
      "Epoch 2256/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.9389 - val_loss: 153.3384\n",
      "Epoch 2257/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 127.3375 - val_loss: 152.9862\n",
      "Epoch 2258/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 151.4301 - val_loss: 891.8626\n",
      "Epoch 2259/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 171.9650 - val_loss: 146.4975\n",
      "Epoch 2260/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.7259 - val_loss: 147.7537\n",
      "Epoch 2261/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.7194 - val_loss: 166.9258\n",
      "Epoch 2262/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.3842 - val_loss: 169.1533\n",
      "Epoch 2263/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.4037 - val_loss: 153.1963\n",
      "Epoch 2264/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.3859 - val_loss: 145.8654\n",
      "Epoch 2265/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.4214 - val_loss: 145.5707\n",
      "Epoch 2266/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.2516 - val_loss: 159.2860\n",
      "Epoch 2267/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.8059 - val_loss: 143.6307\n",
      "Epoch 2268/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.6287 - val_loss: 148.9752\n",
      "Epoch 2269/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.9380 - val_loss: 248.6592\n",
      "Epoch 2270/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.0060 - val_loss: 159.0423\n",
      "Epoch 2271/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.6895 - val_loss: 155.7905\n",
      "Epoch 2272/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 125.9171 - val_loss: 151.6857\n",
      "Epoch 2273/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.1039 - val_loss: 163.1445\n",
      "Epoch 2274/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 136.0387 - val_loss: 179.8391\n",
      "Epoch 2275/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.9111 - val_loss: 176.4534\n",
      "Epoch 2276/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.4976 - val_loss: 151.3420\n",
      "Epoch 2277/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.2950 - val_loss: 173.8350\n",
      "Epoch 2278/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 133.4293 - val_loss: 159.1540\n",
      "Epoch 2279/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.2140 - val_loss: 144.1844\n",
      "Epoch 2280/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 144.1108 - val_loss: 156.0321\n",
      "Epoch 2281/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.7499 - val_loss: 198.6912\n",
      "Epoch 2282/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.8156 - val_loss: 150.0595\n",
      "Epoch 2283/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 149.2283 - val_loss: 161.6007\n",
      "Epoch 2284/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 142.8566 - val_loss: 162.1950\n",
      "Epoch 2285/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 135.5320 - val_loss: 163.8265\n",
      "Epoch 2286/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.2737 - val_loss: 152.2403\n",
      "Epoch 2287/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.8931 - val_loss: 151.2165\n",
      "Epoch 2288/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.7673 - val_loss: 181.3780\n",
      "Epoch 2289/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 150.3016 - val_loss: 152.5545\n",
      "Epoch 2290/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.7826 - val_loss: 319.2143\n",
      "Epoch 2291/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.6448 - val_loss: 148.3361\n",
      "Epoch 2292/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 151.3311 - val_loss: 198.7334\n",
      "Epoch 2293/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.9579 - val_loss: 143.1454\n",
      "Epoch 2294/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.6448 - val_loss: 205.2779\n",
      "Epoch 2295/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.7995 - val_loss: 194.0913\n",
      "Epoch 2296/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.4531 - val_loss: 157.9183\n",
      "Epoch 2297/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.2014 - val_loss: 149.6101\n",
      "Epoch 2298/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.8139 - val_loss: 150.7492\n",
      "Epoch 2299/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.1309 - val_loss: 156.9622\n",
      "Epoch 2300/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.7852 - val_loss: 139.7257\n",
      "Epoch 2301/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 131.5782 - val_loss: 164.7258\n",
      "Epoch 2302/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 136.3310 - val_loss: 172.5203\n",
      "Epoch 2303/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.3303 - val_loss: 164.0011\n",
      "Epoch 2304/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.6335 - val_loss: 214.0500\n",
      "Epoch 2305/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 145.9593 - val_loss: 144.9135\n",
      "Epoch 2306/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 140.6822 - val_loss: 154.6359\n",
      "Epoch 2307/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.2697 - val_loss: 150.3449\n",
      "Epoch 2308/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.8689 - val_loss: 151.8711\n",
      "Epoch 2309/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 127.1836 - val_loss: 179.6620\n",
      "Epoch 2310/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.4540 - val_loss: 148.3339\n",
      "Epoch 2311/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.6148 - val_loss: 176.2075\n",
      "Epoch 2312/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.6036 - val_loss: 139.8956\n",
      "Epoch 2313/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.8545 - val_loss: 142.1457\n",
      "Epoch 2314/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.2052 - val_loss: 148.1072\n",
      "Epoch 2315/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.3279 - val_loss: 188.6573\n",
      "Epoch 2316/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.7315 - val_loss: 211.5227\n",
      "Epoch 2317/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.9676 - val_loss: 145.1403\n",
      "Epoch 2318/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.3441 - val_loss: 144.1747\n",
      "Epoch 2319/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.6699 - val_loss: 146.8288\n",
      "Epoch 2320/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.8411 - val_loss: 190.5336\n",
      "Epoch 2321/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.8350 - val_loss: 149.0586\n",
      "Epoch 2322/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.5170 - val_loss: 149.6801\n",
      "Epoch 2323/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 146.9535 - val_loss: 146.6189\n",
      "Epoch 2324/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.1682 - val_loss: 157.7000\n",
      "Epoch 2325/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.5929 - val_loss: 150.2402\n",
      "Epoch 2326/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.7122 - val_loss: 139.2485\n",
      "Epoch 2327/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.4493 - val_loss: 178.6956\n",
      "Epoch 2328/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.6962 - val_loss: 194.7439\n",
      "Epoch 2329/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 160.6999 - val_loss: 206.8116\n",
      "Epoch 2330/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 142.8458 - val_loss: 155.7665\n",
      "Epoch 2331/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.2398 - val_loss: 163.6574\n",
      "Epoch 2332/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.2893 - val_loss: 153.5630\n",
      "Epoch 2333/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.5556 - val_loss: 150.9410\n",
      "Epoch 2334/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.4026 - val_loss: 185.8375\n",
      "Epoch 2335/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.7929 - val_loss: 164.0957\n",
      "Epoch 2336/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.8695 - val_loss: 146.8338\n",
      "Epoch 2337/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.1693 - val_loss: 166.0024\n",
      "Epoch 2338/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.4952 - val_loss: 143.0588\n",
      "Epoch 2339/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.7232 - val_loss: 198.4543\n",
      "Epoch 2340/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.2701 - val_loss: 155.1203\n",
      "Epoch 2341/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.7900 - val_loss: 153.1920\n",
      "Epoch 2342/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 143.3479 - val_loss: 148.9802\n",
      "Epoch 2343/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.5254 - val_loss: 163.8378\n",
      "Epoch 2344/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 141.1294 - val_loss: 150.6742\n",
      "Epoch 2345/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.0257 - val_loss: 148.3433\n",
      "Epoch 2346/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.0853 - val_loss: 149.8033\n",
      "Epoch 2347/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.4740 - val_loss: 170.1006\n",
      "Epoch 2348/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 152.6176 - val_loss: 148.2456\n",
      "Epoch 2349/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.1596 - val_loss: 142.7343\n",
      "Epoch 2350/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.3724 - val_loss: 159.5838\n",
      "Epoch 2351/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 178.1042 - val_loss: 177.0422\n",
      "Epoch 2352/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.4927 - val_loss: 152.2679\n",
      "Epoch 2353/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.2864 - val_loss: 143.0755\n",
      "Epoch 2354/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.6970 - val_loss: 155.8923\n",
      "Epoch 2355/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.4336 - val_loss: 145.4289\n",
      "Epoch 2356/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.4075 - val_loss: 149.2594\n",
      "Epoch 2357/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.0664 - val_loss: 164.9989\n",
      "Epoch 2358/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.9011 - val_loss: 184.4097\n",
      "Epoch 2359/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.9123 - val_loss: 146.0715\n",
      "Epoch 2360/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.2587 - val_loss: 163.3169\n",
      "Epoch 2361/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 161.4395 - val_loss: 185.3859\n",
      "Epoch 2362/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 252.0318 - val_loss: 161.6265\n",
      "Epoch 2363/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.8541 - val_loss: 164.5059\n",
      "Epoch 2364/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.1615 - val_loss: 195.5166\n",
      "Epoch 2365/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.8214 - val_loss: 146.4943\n",
      "Epoch 2366/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 131.7158 - val_loss: 139.4873\n",
      "Epoch 2367/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.8908 - val_loss: 140.2676\n",
      "Epoch 2368/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.7507 - val_loss: 139.9628\n",
      "Epoch 2369/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.8721 - val_loss: 158.1473\n",
      "Epoch 2370/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.3188 - val_loss: 145.0894\n",
      "Epoch 2371/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.5208 - val_loss: 163.5237\n",
      "Epoch 2372/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.3693 - val_loss: 143.7174\n",
      "Epoch 2373/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.1372 - val_loss: 150.1170\n",
      "Epoch 2374/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.4774 - val_loss: 154.9335\n",
      "Epoch 2375/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.3770 - val_loss: 184.3795\n",
      "Epoch 2376/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.6170 - val_loss: 200.8058\n",
      "Epoch 2377/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.1922 - val_loss: 163.3764\n",
      "Epoch 2378/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.8803 - val_loss: 161.6432\n",
      "Epoch 2379/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.9190 - val_loss: 160.0330\n",
      "Epoch 2380/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.2332 - val_loss: 173.4373\n",
      "Epoch 2381/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 175.6786 - val_loss: 171.4842\n",
      "Epoch 2382/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 131.8939 - val_loss: 142.5849\n",
      "Epoch 2383/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 126.2013 - val_loss: 153.7762\n",
      "Epoch 2384/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 136.5202 - val_loss: 155.2254\n",
      "Epoch 2385/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 140.4067 - val_loss: 215.3629\n",
      "Epoch 2386/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 167.1229 - val_loss: 198.0165\n",
      "Epoch 2387/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 178.2199 - val_loss: 152.7800\n",
      "Epoch 2388/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.4107 - val_loss: 174.0575\n",
      "Epoch 2389/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.2205 - val_loss: 159.7182\n",
      "Epoch 2390/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 137.8054 - val_loss: 145.5966\n",
      "Epoch 2391/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 133.0873 - val_loss: 146.9773\n",
      "Epoch 2392/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.4792 - val_loss: 162.7717\n",
      "Epoch 2393/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.5434 - val_loss: 171.7750\n",
      "Epoch 2394/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.1248 - val_loss: 168.6039\n",
      "Epoch 2395/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.5572 - val_loss: 145.4871\n",
      "Epoch 2396/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.9468 - val_loss: 236.3976\n",
      "Epoch 2397/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 143.4534 - val_loss: 138.3940\n",
      "Epoch 2398/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.9415 - val_loss: 181.0762\n",
      "Epoch 2399/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.7429 - val_loss: 158.9093\n",
      "Epoch 2400/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.3047 - val_loss: 142.2519\n",
      "Epoch 2401/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 137.7652 - val_loss: 146.5228\n",
      "Epoch 2402/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.2140 - val_loss: 180.2078\n",
      "Epoch 2403/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.9248 - val_loss: 148.8017\n",
      "Epoch 2404/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.6712 - val_loss: 201.8249\n",
      "Epoch 2405/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.9888 - val_loss: 160.2933\n",
      "Epoch 2406/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.3857 - val_loss: 154.6296\n",
      "Epoch 2407/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 168.8336 - val_loss: 275.5511\n",
      "Epoch 2408/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 173.0592 - val_loss: 152.9554\n",
      "Epoch 2409/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.7552 - val_loss: 151.8320\n",
      "Epoch 2410/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.2689 - val_loss: 251.0038\n",
      "Epoch 2411/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.8351 - val_loss: 146.2096\n",
      "Epoch 2412/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 127.9249 - val_loss: 144.4255\n",
      "Epoch 2413/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.6538 - val_loss: 156.6978\n",
      "Epoch 2414/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.9477 - val_loss: 158.3433\n",
      "Epoch 2415/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.4161 - val_loss: 154.5012\n",
      "Epoch 2416/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.5530 - val_loss: 151.2842\n",
      "Epoch 2417/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.0143 - val_loss: 156.9828\n",
      "Epoch 2418/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.4751 - val_loss: 145.7722\n",
      "Epoch 2419/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.0551 - val_loss: 149.9807\n",
      "Epoch 2420/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.5828 - val_loss: 155.3399\n",
      "Epoch 2421/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.3210 - val_loss: 143.3558\n",
      "Epoch 2422/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.8436 - val_loss: 189.4865\n",
      "Epoch 2423/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.7812 - val_loss: 162.5777\n",
      "Epoch 2424/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.6119 - val_loss: 146.0191\n",
      "Epoch 2425/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.0023 - val_loss: 218.3069\n",
      "Epoch 2426/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.9153 - val_loss: 209.1735\n",
      "Epoch 2427/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 126.8148 - val_loss: 195.4402\n",
      "Epoch 2428/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 159.7356 - val_loss: 241.0318\n",
      "Epoch 2429/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 159.6900 - val_loss: 175.7471\n",
      "Epoch 2430/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.6125 - val_loss: 201.1912\n",
      "Epoch 2431/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 147.7605 - val_loss: 139.2882\n",
      "Epoch 2432/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.6218 - val_loss: 162.7592\n",
      "Epoch 2433/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.4204 - val_loss: 155.6628\n",
      "Epoch 2434/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.4612 - val_loss: 149.9954\n",
      "Epoch 2435/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 131.0236 - val_loss: 152.2888\n",
      "Epoch 2436/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.4472 - val_loss: 158.5711\n",
      "Epoch 2437/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 143.0604 - val_loss: 150.1905\n",
      "Epoch 2438/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.9327 - val_loss: 145.6701\n",
      "Epoch 2439/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 177.0873 - val_loss: 168.4963\n",
      "Epoch 2440/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.7480 - val_loss: 164.4939\n",
      "Epoch 2441/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.2562 - val_loss: 154.2401\n",
      "Epoch 2442/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 187.6111 - val_loss: 164.2317\n",
      "Epoch 2443/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.0310 - val_loss: 191.1526\n",
      "Epoch 2444/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.5458 - val_loss: 189.7818\n",
      "Epoch 2445/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.3575 - val_loss: 176.9621\n",
      "Epoch 2446/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 130.5198 - val_loss: 142.9072\n",
      "Epoch 2447/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.1114 - val_loss: 154.7145\n",
      "Epoch 2448/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 138.1820 - val_loss: 244.4144\n",
      "Epoch 2449/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 133.1301 - val_loss: 166.0948\n",
      "Epoch 2450/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 132.0785 - val_loss: 154.2309\n",
      "Epoch 2451/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.7577 - val_loss: 227.8568\n",
      "Epoch 2452/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 146.6950 - val_loss: 157.7662\n",
      "Epoch 2453/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 133.8219 - val_loss: 168.9742\n",
      "Epoch 2454/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.4944 - val_loss: 140.8207\n",
      "Epoch 2455/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.7249 - val_loss: 147.9416\n",
      "Epoch 2456/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 167.1023 - val_loss: 199.0639\n",
      "Epoch 2457/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.4790 - val_loss: 208.4597\n",
      "Epoch 2458/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.4351 - val_loss: 243.6127\n",
      "Epoch 2459/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 187.7912 - val_loss: 185.8863\n",
      "Epoch 2460/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 210.1369 - val_loss: 153.6266\n",
      "Epoch 2461/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.4549 - val_loss: 152.5990\n",
      "Epoch 2462/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.2691 - val_loss: 180.4252\n",
      "Epoch 2463/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.2625 - val_loss: 148.1170\n",
      "Epoch 2464/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.4945 - val_loss: 161.6883\n",
      "Epoch 2465/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 177.8515 - val_loss: 174.8776\n",
      "Epoch 2466/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.0061 - val_loss: 155.7966\n",
      "Epoch 2467/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.3468 - val_loss: 140.6090\n",
      "Epoch 2468/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.2306 - val_loss: 148.5240\n",
      "Epoch 2469/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.1962 - val_loss: 144.6565\n",
      "Epoch 2470/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.9291 - val_loss: 142.1902\n",
      "Epoch 2471/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.3839 - val_loss: 139.4695\n",
      "Epoch 2472/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.0043 - val_loss: 171.2325\n",
      "Epoch 2473/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.2596 - val_loss: 158.1652\n",
      "Epoch 2474/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.3380 - val_loss: 141.8574\n",
      "Epoch 2475/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.5642 - val_loss: 160.0496\n",
      "Epoch 2476/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.6590 - val_loss: 152.5815\n",
      "Epoch 2477/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.8040 - val_loss: 149.7926\n",
      "Epoch 2478/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.2156 - val_loss: 161.4765\n",
      "Epoch 2479/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 171.2521 - val_loss: 139.2213\n",
      "Epoch 2480/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.8148 - val_loss: 143.4969\n",
      "Epoch 2481/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.1677 - val_loss: 161.6243\n",
      "Epoch 2482/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.5256 - val_loss: 160.1054\n",
      "Epoch 2483/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.2672 - val_loss: 138.9690\n",
      "Epoch 2484/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.9728 - val_loss: 185.5261\n",
      "Epoch 2485/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.5134 - val_loss: 170.3015\n",
      "Epoch 2486/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.6922 - val_loss: 179.7287\n",
      "Epoch 2487/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.7047 - val_loss: 150.6993\n",
      "Epoch 2488/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.3865 - val_loss: 179.4872\n",
      "Epoch 2489/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 206.9900 - val_loss: 142.0038\n",
      "Epoch 2490/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.8326 - val_loss: 150.0883\n",
      "Epoch 2491/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 137.837 - 0s 51us/step - loss: 138.2323 - val_loss: 158.3998\n",
      "Epoch 2492/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 156.1256 - val_loss: 151.1887\n",
      "Epoch 2493/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.3970 - val_loss: 157.3998\n",
      "Epoch 2494/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.8723 - val_loss: 243.4580\n",
      "Epoch 2495/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.4156 - val_loss: 155.1256\n",
      "Epoch 2496/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 132.7519 - val_loss: 153.3822\n",
      "Epoch 2497/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.3516 - val_loss: 208.3580\n",
      "Epoch 2498/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.8400 - val_loss: 171.7306\n",
      "Epoch 2499/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 129.3403 - val_loss: 147.5174\n",
      "Epoch 2500/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 221.8056 - val_loss: 156.1303\n",
      "Epoch 2501/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 147.7473 - val_loss: 201.6522\n",
      "Epoch 2502/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.3197 - val_loss: 212.4447\n",
      "Epoch 2503/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 177.5212 - val_loss: 153.4986\n",
      "Epoch 2504/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.1631 - val_loss: 142.9012\n",
      "Epoch 2505/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.8629 - val_loss: 157.4762\n",
      "Epoch 2506/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 152.8798 - val_loss: 213.1586\n",
      "Epoch 2507/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 165.0267 - val_loss: 163.0825\n",
      "Epoch 2508/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.7376 - val_loss: 150.2677\n",
      "Epoch 2509/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 142.2133 - val_loss: 202.2971\n",
      "Epoch 2510/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.3628 - val_loss: 207.5397\n",
      "Epoch 2511/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.0439 - val_loss: 143.3160\n",
      "Epoch 2512/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.6524 - val_loss: 166.7358\n",
      "Epoch 2513/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 180.1348 - val_loss: 159.8894\n",
      "Epoch 2514/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 157.3662 - val_loss: 171.9446\n",
      "Epoch 2515/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.1641 - val_loss: 215.0450\n",
      "Epoch 2516/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.3935 - val_loss: 179.0869\n",
      "Epoch 2517/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.3098 - val_loss: 175.9981\n",
      "Epoch 2518/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.4750 - val_loss: 236.9884\n",
      "Epoch 2519/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 149.4094 - val_loss: 212.4113\n",
      "Epoch 2520/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.9872 - val_loss: 193.5638\n",
      "Epoch 2521/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.5310 - val_loss: 144.9735\n",
      "Epoch 2522/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.0090 - val_loss: 139.0436\n",
      "Epoch 2523/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.1834 - val_loss: 157.0153\n",
      "Epoch 2524/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.0755 - val_loss: 165.9205\n",
      "Epoch 2525/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 142.3734 - val_loss: 152.6373\n",
      "Epoch 2526/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.8128 - val_loss: 152.6934\n",
      "Epoch 2527/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 135.226 - 0s 51us/step - loss: 136.1349 - val_loss: 144.6120\n",
      "Epoch 2528/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.5259 - val_loss: 153.0650\n",
      "Epoch 2529/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.7467 - val_loss: 139.1756\n",
      "Epoch 2530/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.6494 - val_loss: 148.7527\n",
      "Epoch 2531/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.9693 - val_loss: 145.1061\n",
      "Epoch 2532/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.1412 - val_loss: 148.3675\n",
      "Epoch 2533/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.0429 - val_loss: 156.4073\n",
      "Epoch 2534/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.3018 - val_loss: 142.9651\n",
      "Epoch 2535/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.6354 - val_loss: 142.7124\n",
      "Epoch 2536/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.4089 - val_loss: 135.8036\n",
      "Epoch 2537/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.1221 - val_loss: 149.7395\n",
      "Epoch 2538/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 184.9720 - val_loss: 166.8091\n",
      "Epoch 2539/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.7342 - val_loss: 150.5217\n",
      "Epoch 2540/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.2347 - val_loss: 147.5985\n",
      "Epoch 2541/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.1458 - val_loss: 142.4041\n",
      "Epoch 2542/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.6921 - val_loss: 136.0531\n",
      "Epoch 2543/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 182.5511 - val_loss: 176.6388\n",
      "Epoch 2544/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.9094 - val_loss: 145.3188\n",
      "Epoch 2545/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 218.6887 - val_loss: 149.3778\n",
      "Epoch 2546/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.1838 - val_loss: 164.5936\n",
      "Epoch 2547/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.9161 - val_loss: 149.3453\n",
      "Epoch 2548/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.9765 - val_loss: 149.8463\n",
      "Epoch 2549/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 153.643 - 0s 50us/step - loss: 156.0477 - val_loss: 163.4800\n",
      "Epoch 2550/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 158.2220 - val_loss: 169.4433\n",
      "Epoch 2551/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 143.4068 - val_loss: 155.2368\n",
      "Epoch 2552/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.6943 - val_loss: 190.2772\n",
      "Epoch 2553/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.3054 - val_loss: 148.3829\n",
      "Epoch 2554/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 155.3913 - val_loss: 138.9424\n",
      "Epoch 2555/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.0943 - val_loss: 141.2133\n",
      "Epoch 2556/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.4003 - val_loss: 144.0727\n",
      "Epoch 2557/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 142.5345 - val_loss: 138.8529\n",
      "Epoch 2558/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.5448 - val_loss: 226.5782\n",
      "Epoch 2559/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.5556 - val_loss: 152.6357\n",
      "Epoch 2560/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.3418 - val_loss: 168.6676\n",
      "Epoch 2561/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 163.1244 - val_loss: 144.8426\n",
      "Epoch 2562/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.1170 - val_loss: 152.5717\n",
      "Epoch 2563/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 143.9205 - val_loss: 167.3273\n",
      "Epoch 2564/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 128.0212 - val_loss: 139.7391\n",
      "Epoch 2565/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.0297 - val_loss: 155.8542\n",
      "Epoch 2566/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.9325 - val_loss: 148.1775\n",
      "Epoch 2567/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.3249 - val_loss: 153.2461\n",
      "Epoch 2568/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 135.2594 - val_loss: 162.3474\n",
      "Epoch 2569/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 130.9017 - val_loss: 155.2643\n",
      "Epoch 2570/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.3179 - val_loss: 141.7272\n",
      "Epoch 2571/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.5883 - val_loss: 141.8848\n",
      "Epoch 2572/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.0108 - val_loss: 144.5384\n",
      "Epoch 2573/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 151.9995 - val_loss: 150.5441\n",
      "Epoch 2574/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.0445 - val_loss: 145.7710\n",
      "Epoch 2575/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.1335 - val_loss: 146.9317\n",
      "Epoch 2576/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 160.9459 - val_loss: 150.7569\n",
      "Epoch 2577/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 146.1026 - val_loss: 197.2133\n",
      "Epoch 2578/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.7396 - val_loss: 138.3658\n",
      "Epoch 2579/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 159.2085 - val_loss: 180.0417\n",
      "Epoch 2580/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.4382 - val_loss: 174.4426\n",
      "Epoch 2581/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.8324 - val_loss: 144.6886\n",
      "Epoch 2582/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.9169 - val_loss: 153.7557\n",
      "Epoch 2583/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.0188 - val_loss: 141.8005\n",
      "Epoch 2584/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.1854 - val_loss: 146.5389\n",
      "Epoch 2585/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 128.0795 - val_loss: 160.8231\n",
      "Epoch 2586/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.9271 - val_loss: 163.5404\n",
      "Epoch 2587/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.4712 - val_loss: 150.6372\n",
      "Epoch 2588/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.8036 - val_loss: 156.0842\n",
      "Epoch 2589/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.7580 - val_loss: 139.9287\n",
      "Epoch 2590/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.7977 - val_loss: 181.0106\n",
      "Epoch 2591/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.9417 - val_loss: 170.3761\n",
      "Epoch 2592/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.7306 - val_loss: 171.0392\n",
      "Epoch 2593/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.2746 - val_loss: 160.3770\n",
      "Epoch 2594/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 176.6119 - val_loss: 414.7967\n",
      "Epoch 2595/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.0716 - val_loss: 139.0570\n",
      "Epoch 2596/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.2063 - val_loss: 137.9092\n",
      "Epoch 2597/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.9706 - val_loss: 141.1479\n",
      "Epoch 2598/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 139.8255 - val_loss: 138.6918\n",
      "Epoch 2599/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 195.0105 - val_loss: 157.6034\n",
      "Epoch 2600/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.5445 - val_loss: 157.6376\n",
      "Epoch 2601/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.2183 - val_loss: 162.9684\n",
      "Epoch 2602/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.8000 - val_loss: 135.7074\n",
      "Epoch 2603/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.3453 - val_loss: 161.9615\n",
      "Epoch 2604/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 225.6840 - val_loss: 155.2632\n",
      "Epoch 2605/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 148.6428 - val_loss: 165.9187\n",
      "Epoch 2606/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.5192 - val_loss: 139.4756\n",
      "Epoch 2607/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.8347 - val_loss: 152.1599\n",
      "Epoch 2608/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.2341 - val_loss: 154.5239\n",
      "Epoch 2609/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.8861 - val_loss: 138.9152\n",
      "Epoch 2610/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.1611 - val_loss: 146.7548\n",
      "Epoch 2611/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.3809 - val_loss: 215.5779\n",
      "Epoch 2612/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.1789 - val_loss: 168.9086\n",
      "Epoch 2613/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.8210 - val_loss: 138.8976\n",
      "Epoch 2614/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.1322 - val_loss: 141.8251\n",
      "Epoch 2615/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.3239 - val_loss: 147.6706\n",
      "Epoch 2616/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.0010 - val_loss: 157.9712\n",
      "Epoch 2617/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.7267 - val_loss: 198.1447\n",
      "Epoch 2618/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 213.0458 - val_loss: 241.2521\n",
      "Epoch 2619/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 180.2992 - val_loss: 142.9980\n",
      "Epoch 2620/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.8314 - val_loss: 176.5466\n",
      "Epoch 2621/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.5551 - val_loss: 162.3130\n",
      "Epoch 2622/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.4285 - val_loss: 153.4865\n",
      "Epoch 2623/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 190.2197 - val_loss: 141.9889\n",
      "Epoch 2624/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 151.6543 - val_loss: 160.2883\n",
      "Epoch 2625/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.6913 - val_loss: 181.7493\n",
      "Epoch 2626/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.7585 - val_loss: 145.1053\n",
      "Epoch 2627/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.0281 - val_loss: 149.9376\n",
      "Epoch 2628/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.4142 - val_loss: 149.9928\n",
      "Epoch 2629/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.7129 - val_loss: 158.8662\n",
      "Epoch 2630/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 149.5134 - val_loss: 170.5853\n",
      "Epoch 2631/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.2666 - val_loss: 158.3295\n",
      "Epoch 2632/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.8076 - val_loss: 147.9069\n",
      "Epoch 2633/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 159.1510 - val_loss: 144.9235\n",
      "Epoch 2634/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.2796 - val_loss: 152.8444\n",
      "Epoch 2635/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.1201 - val_loss: 333.2607\n",
      "Epoch 2636/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.5452 - val_loss: 140.8165\n",
      "Epoch 2637/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.6732 - val_loss: 147.1783\n",
      "Epoch 2638/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.2181 - val_loss: 167.7546\n",
      "Epoch 2639/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.2124 - val_loss: 161.5938\n",
      "Epoch 2640/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.7922 - val_loss: 169.9601\n",
      "Epoch 2641/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.1032 - val_loss: 143.4509\n",
      "Epoch 2642/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.3588 - val_loss: 149.1421\n",
      "Epoch 2643/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 162.1416 - val_loss: 207.1502\n",
      "Epoch 2644/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 150.2511 - val_loss: 157.8932\n",
      "Epoch 2645/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.0564 - val_loss: 156.3012\n",
      "Epoch 2646/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 140.2979 - val_loss: 139.1450\n",
      "Epoch 2647/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 160.9095 - val_loss: 159.7398\n",
      "Epoch 2648/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.0428 - val_loss: 156.4543\n",
      "Epoch 2649/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.0400 - val_loss: 183.7827\n",
      "Epoch 2650/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.7849 - val_loss: 166.6657\n",
      "Epoch 2651/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 127.564 - 0s 51us/step - loss: 127.8707 - val_loss: 141.2952\n",
      "Epoch 2652/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 192.9385 - val_loss: 159.5328\n",
      "Epoch 2653/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 154.3348 - val_loss: 141.0271\n",
      "Epoch 2654/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.1694 - val_loss: 144.5281\n",
      "Epoch 2655/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 140.1481 - val_loss: 147.6877\n",
      "Epoch 2656/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.4374 - val_loss: 146.0317\n",
      "Epoch 2657/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 135.158 - 0s 51us/step - loss: 134.6037 - val_loss: 182.0656\n",
      "Epoch 2658/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.6084 - val_loss: 156.3390\n",
      "Epoch 2659/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.5814 - val_loss: 152.4008\n",
      "Epoch 2660/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.4326 - val_loss: 156.7758\n",
      "Epoch 2661/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 188.0823 - val_loss: 161.4541\n",
      "Epoch 2662/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.6463 - val_loss: 178.1418\n",
      "Epoch 2663/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.7291 - val_loss: 151.5491\n",
      "Epoch 2664/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.3700 - val_loss: 140.7318\n",
      "Epoch 2665/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.0753 - val_loss: 210.7643\n",
      "Epoch 2666/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.1888 - val_loss: 143.1227\n",
      "Epoch 2667/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.3226 - val_loss: 143.8016\n",
      "Epoch 2668/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 147.7376 - val_loss: 171.4221\n",
      "Epoch 2669/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.3611 - val_loss: 141.3049\n",
      "Epoch 2670/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 133.7311 - val_loss: 183.6032\n",
      "Epoch 2671/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.1319 - val_loss: 148.3181\n",
      "Epoch 2672/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.1845 - val_loss: 479.0888\n",
      "Epoch 2673/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 169.0458 - val_loss: 144.6024\n",
      "Epoch 2674/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.3612 - val_loss: 161.4583\n",
      "Epoch 2675/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.0847 - val_loss: 138.5361\n",
      "Epoch 2676/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.2456 - val_loss: 186.3443\n",
      "Epoch 2677/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 155.3872 - val_loss: 159.9506\n",
      "Epoch 2678/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 136.0502 - val_loss: 146.4238\n",
      "Epoch 2679/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.0590 - val_loss: 143.6222\n",
      "Epoch 2680/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.6679 - val_loss: 154.4035\n",
      "Epoch 2681/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.4368 - val_loss: 184.5709\n",
      "Epoch 2682/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 178.6175 - val_loss: 433.8630\n",
      "Epoch 2683/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 165.8508 - val_loss: 137.6397\n",
      "Epoch 2684/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.2052 - val_loss: 139.9615\n",
      "Epoch 2685/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.7220 - val_loss: 182.0481\n",
      "Epoch 2686/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.9065 - val_loss: 148.8335\n",
      "Epoch 2687/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 206.2899 - val_loss: 151.1083\n",
      "Epoch 2688/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.0541 - val_loss: 168.1532\n",
      "Epoch 2689/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.7122 - val_loss: 141.9982\n",
      "Epoch 2690/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.9995 - val_loss: 142.6623\n",
      "Epoch 2691/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.1412 - val_loss: 151.2114\n",
      "Epoch 2692/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.9050 - val_loss: 314.7696\n",
      "Epoch 2693/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 156.2369 - val_loss: 160.3740\n",
      "Epoch 2694/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.8145 - val_loss: 145.5025\n",
      "Epoch 2695/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.9151 - val_loss: 139.1279\n",
      "Epoch 2696/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.7071 - val_loss: 149.1430\n",
      "Epoch 2697/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.5919 - val_loss: 153.9012\n",
      "Epoch 2698/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.1685 - val_loss: 144.7435\n",
      "Epoch 2699/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.4210 - val_loss: 152.6104\n",
      "Epoch 2700/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.3864 - val_loss: 248.9676\n",
      "Epoch 2701/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.3885 - val_loss: 148.4710\n",
      "Epoch 2702/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.4828 - val_loss: 242.5134\n",
      "Epoch 2703/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.4716 - val_loss: 178.5365\n",
      "Epoch 2704/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.8570 - val_loss: 211.3931\n",
      "Epoch 2705/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.2437 - val_loss: 145.3957\n",
      "Epoch 2706/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.2009 - val_loss: 227.0229\n",
      "Epoch 2707/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.2712 - val_loss: 199.6941\n",
      "Epoch 2708/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.8276 - val_loss: 219.8999\n",
      "Epoch 2709/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 132.5873 - val_loss: 153.2034\n",
      "Epoch 2710/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.4515 - val_loss: 157.4712\n",
      "Epoch 2711/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 149.9822 - val_loss: 149.0468\n",
      "Epoch 2712/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 209.0409 - val_loss: 141.5015\n",
      "Epoch 2713/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 133.4611 - val_loss: 146.3124\n",
      "Epoch 2714/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 130.8646 - val_loss: 146.2662\n",
      "Epoch 2715/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 142.5494 - val_loss: 215.9396\n",
      "Epoch 2716/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 183.3878 - val_loss: 454.9007\n",
      "Epoch 2717/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 208.6995 - val_loss: 161.1023\n",
      "Epoch 2718/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.6747 - val_loss: 144.7854\n",
      "Epoch 2719/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.0555 - val_loss: 167.9892\n",
      "Epoch 2720/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.4359 - val_loss: 141.0527\n",
      "Epoch 2721/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.3575 - val_loss: 142.7364\n",
      "Epoch 2722/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.3035 - val_loss: 151.5231\n",
      "Epoch 2723/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.9069 - val_loss: 146.3349\n",
      "Epoch 2724/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.3662 - val_loss: 154.5160\n",
      "Epoch 2725/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.3333 - val_loss: 142.6890\n",
      "Epoch 2726/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.7201 - val_loss: 148.0419\n",
      "Epoch 2727/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.4586 - val_loss: 191.5614\n",
      "Epoch 2728/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.9363 - val_loss: 146.7629\n",
      "Epoch 2729/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.7099 - val_loss: 181.6766\n",
      "Epoch 2730/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.0775 - val_loss: 141.9759\n",
      "Epoch 2731/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.1080 - val_loss: 144.4009\n",
      "Epoch 2732/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.4741 - val_loss: 159.6423\n",
      "Epoch 2733/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.3271 - val_loss: 219.6776\n",
      "Epoch 2734/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.8511 - val_loss: 169.8380\n",
      "Epoch 2735/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.2024 - val_loss: 151.2760\n",
      "Epoch 2736/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.2578 - val_loss: 160.2387\n",
      "Epoch 2737/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 165.6831 - val_loss: 167.9797\n",
      "Epoch 2738/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.4663 - val_loss: 163.1517\n",
      "Epoch 2739/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.9347 - val_loss: 142.1013\n",
      "Epoch 2740/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.0716 - val_loss: 148.6492\n",
      "Epoch 2741/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.9626 - val_loss: 142.6185\n",
      "Epoch 2742/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.0322 - val_loss: 151.7830\n",
      "Epoch 2743/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.7873 - val_loss: 145.7816\n",
      "Epoch 2744/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.7582 - val_loss: 166.0787\n",
      "Epoch 2745/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.1333 - val_loss: 142.6050\n",
      "Epoch 2746/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.2113 - val_loss: 177.4131\n",
      "Epoch 2747/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.7794 - val_loss: 140.0281\n",
      "Epoch 2748/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.3490 - val_loss: 150.0584\n",
      "Epoch 2749/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.0659 - val_loss: 152.7901\n",
      "Epoch 2750/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.7439 - val_loss: 166.8163\n",
      "Epoch 2751/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.5643 - val_loss: 148.2560\n",
      "Epoch 2752/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.4613 - val_loss: 157.9252\n",
      "Epoch 2753/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.3656 - val_loss: 162.1277\n",
      "Epoch 2754/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.6097 - val_loss: 165.4270\n",
      "Epoch 2755/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.1677 - val_loss: 140.9922\n",
      "Epoch 2756/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.9521 - val_loss: 143.2070\n",
      "Epoch 2757/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.2871 - val_loss: 136.9199\n",
      "Epoch 2758/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 159.2841 - val_loss: 154.5165\n",
      "Epoch 2759/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.7932 - val_loss: 147.0816\n",
      "Epoch 2760/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.7820 - val_loss: 140.4946\n",
      "Epoch 2761/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.3495 - val_loss: 156.2168\n",
      "Epoch 2762/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.3680 - val_loss: 139.8814\n",
      "Epoch 2763/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.2722 - val_loss: 138.2716\n",
      "Epoch 2764/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.1797 - val_loss: 151.3092\n",
      "Epoch 2765/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.6517 - val_loss: 148.6236\n",
      "Epoch 2766/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.1537 - val_loss: 141.8969\n",
      "Epoch 2767/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.3193 - val_loss: 145.9147\n",
      "Epoch 2768/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.2423 - val_loss: 150.4028\n",
      "Epoch 2769/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.2390 - val_loss: 164.3250\n",
      "Epoch 2770/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.1940 - val_loss: 156.5135\n",
      "Epoch 2771/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.2787 - val_loss: 169.0910\n",
      "Epoch 2772/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 170.0078 - val_loss: 143.4507\n",
      "Epoch 2773/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.0148 - val_loss: 155.6446\n",
      "Epoch 2774/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.2943 - val_loss: 153.3090\n",
      "Epoch 2775/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.2352 - val_loss: 274.5293\n",
      "Epoch 2776/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.1448 - val_loss: 140.2189\n",
      "Epoch 2777/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.8080 - val_loss: 152.2671\n",
      "Epoch 2778/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 161.8307 - val_loss: 145.7933\n",
      "Epoch 2779/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 166.8929 - val_loss: 139.3152\n",
      "Epoch 2780/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.3080 - val_loss: 144.0958\n",
      "Epoch 2781/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.1892 - val_loss: 184.5699\n",
      "Epoch 2782/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.3203 - val_loss: 174.7251\n",
      "Epoch 2783/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.6937 - val_loss: 139.0196\n",
      "Epoch 2784/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.6981 - val_loss: 150.7580\n",
      "Epoch 2785/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.8657 - val_loss: 151.6477\n",
      "Epoch 2786/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.2636 - val_loss: 141.6013\n",
      "Epoch 2787/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.6064 - val_loss: 164.5444\n",
      "Epoch 2788/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.3550 - val_loss: 172.8395\n",
      "Epoch 2789/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.0182 - val_loss: 146.9655\n",
      "Epoch 2790/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 244.0078 - val_loss: 201.3975\n",
      "Epoch 2791/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 179.2389 - val_loss: 172.3271\n",
      "Epoch 2792/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.0415 - val_loss: 188.1264\n",
      "Epoch 2793/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.2512 - val_loss: 148.9351\n",
      "Epoch 2794/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.9839 - val_loss: 144.5908\n",
      "Epoch 2795/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.3570 - val_loss: 146.1004\n",
      "Epoch 2796/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.7824 - val_loss: 194.5380\n",
      "Epoch 2797/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 130.3592 - val_loss: 154.6729\n",
      "Epoch 2798/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 130.6445 - val_loss: 151.7160\n",
      "Epoch 2799/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.0120 - val_loss: 138.4664\n",
      "Epoch 2800/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.9310 - val_loss: 151.4188\n",
      "Epoch 2801/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.6914 - val_loss: 144.2782\n",
      "Epoch 2802/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.0972 - val_loss: 195.3196\n",
      "Epoch 2803/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.5389 - val_loss: 141.9192\n",
      "Epoch 2804/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.9892 - val_loss: 146.9040\n",
      "Epoch 2805/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.9101 - val_loss: 150.5621\n",
      "Epoch 2806/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.4350 - val_loss: 156.6910\n",
      "Epoch 2807/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.6085 - val_loss: 167.8818\n",
      "Epoch 2808/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.4017 - val_loss: 184.7834\n",
      "Epoch 2809/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.4394 - val_loss: 148.4291\n",
      "Epoch 2810/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.1491 - val_loss: 159.4949\n",
      "Epoch 2811/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.5862 - val_loss: 136.6242\n",
      "Epoch 2812/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.1093 - val_loss: 149.5411\n",
      "Epoch 2813/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.0032 - val_loss: 155.3799\n",
      "Epoch 2814/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.7709 - val_loss: 151.5427\n",
      "Epoch 2815/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.3288 - val_loss: 166.8902\n",
      "Epoch 2816/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 164.5598 - val_loss: 154.4105\n",
      "Epoch 2817/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.0092 - val_loss: 144.1203\n",
      "Epoch 2818/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.1483 - val_loss: 157.4176\n",
      "Epoch 2819/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.0048 - val_loss: 259.7501\n",
      "Epoch 2820/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.8294 - val_loss: 151.8626\n",
      "Epoch 2821/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.6014 - val_loss: 139.7885\n",
      "Epoch 2822/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.1708 - val_loss: 186.0754\n",
      "Epoch 2823/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.0498 - val_loss: 158.9899\n",
      "Epoch 2824/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.3475 - val_loss: 138.4007\n",
      "Epoch 2825/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.8177 - val_loss: 346.6754\n",
      "Epoch 2826/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 199.7137 - val_loss: 155.1618\n",
      "Epoch 2827/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.9677 - val_loss: 178.9671\n",
      "Epoch 2828/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.7350 - val_loss: 147.1229\n",
      "Epoch 2829/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.2158 - val_loss: 138.0482\n",
      "Epoch 2830/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.8114 - val_loss: 150.8426\n",
      "Epoch 2831/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.0346 - val_loss: 151.3945\n",
      "Epoch 2832/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.0883 - val_loss: 177.1044\n",
      "Epoch 2833/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.1964 - val_loss: 152.1175\n",
      "Epoch 2834/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 152.6446 - val_loss: 162.4577\n",
      "Epoch 2835/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 166.2855 - val_loss: 145.2112\n",
      "Epoch 2836/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.2749 - val_loss: 152.1671\n",
      "Epoch 2837/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.2146 - val_loss: 242.6676\n",
      "Epoch 2838/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.8252 - val_loss: 146.8267\n",
      "Epoch 2839/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.4929 - val_loss: 145.5622\n",
      "Epoch 2840/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 169.6549 - val_loss: 192.9275\n",
      "Epoch 2841/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.7205 - val_loss: 172.2887\n",
      "Epoch 2842/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 149.4745 - val_loss: 148.0162\n",
      "Epoch 2843/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 159.9256 - val_loss: 241.8357\n",
      "Epoch 2844/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.8309 - val_loss: 152.6892\n",
      "Epoch 2845/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.0752 - val_loss: 250.8449\n",
      "Epoch 2846/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 152.3979 - val_loss: 144.0984\n",
      "Epoch 2847/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 142.9290 - val_loss: 174.5208\n",
      "Epoch 2848/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 175.2534 - val_loss: 163.5868\n",
      "Epoch 2849/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.4792 - val_loss: 157.9084\n",
      "Epoch 2850/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.0450 - val_loss: 142.4958\n",
      "Epoch 2851/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.1402 - val_loss: 140.3371\n",
      "Epoch 2852/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.6706 - val_loss: 146.7950\n",
      "Epoch 2853/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.1535 - val_loss: 152.8067\n",
      "Epoch 2854/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.1588 - val_loss: 153.1601\n",
      "Epoch 2855/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 140.2551 - val_loss: 160.7225\n",
      "Epoch 2856/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.7413 - val_loss: 155.7652\n",
      "Epoch 2857/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.5643 - val_loss: 148.8468\n",
      "Epoch 2858/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 129.8372 - val_loss: 150.0568\n",
      "Epoch 2859/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 127.0507 - val_loss: 148.1698\n",
      "Epoch 2860/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 134.2062 - val_loss: 137.3528\n",
      "Epoch 2861/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 135.4275 - val_loss: 147.0851\n",
      "Epoch 2862/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.7366 - val_loss: 247.1745\n",
      "Epoch 2863/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.9013 - val_loss: 153.0236\n",
      "Epoch 2864/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 168.4605 - val_loss: 154.4569\n",
      "Epoch 2865/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.0893 - val_loss: 152.9544\n",
      "Epoch 2866/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.1808 - val_loss: 140.5824\n",
      "Epoch 2867/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.5305 - val_loss: 149.1201\n",
      "Epoch 2868/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.6452 - val_loss: 142.9557\n",
      "Epoch 2869/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.8177 - val_loss: 155.4839\n",
      "Epoch 2870/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.0281 - val_loss: 149.0104\n",
      "Epoch 2871/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.4379 - val_loss: 151.0224\n",
      "Epoch 2872/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 159.6604 - val_loss: 184.7115\n",
      "Epoch 2873/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 149.9511 - val_loss: 152.0075\n",
      "Epoch 2874/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.3155 - val_loss: 138.1410\n",
      "Epoch 2875/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 170.5742 - val_loss: 175.3146\n",
      "Epoch 2876/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.4567 - val_loss: 145.8798\n",
      "Epoch 2877/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.7343 - val_loss: 166.5188\n",
      "Epoch 2878/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.4268 - val_loss: 205.9693\n",
      "Epoch 2879/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.3999 - val_loss: 172.1833\n",
      "Epoch 2880/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.7271 - val_loss: 171.1494\n",
      "Epoch 2881/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 147.7807 - val_loss: 282.3417\n",
      "Epoch 2882/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.7145 - val_loss: 141.1100\n",
      "Epoch 2883/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 126.315 - 0s 51us/step - loss: 126.5522 - val_loss: 160.9231\n",
      "Epoch 2884/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.0596 - val_loss: 140.1291\n",
      "Epoch 2885/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.2455 - val_loss: 140.0146\n",
      "Epoch 2886/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 143.7783 - val_loss: 149.2347\n",
      "Epoch 2887/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 148.2137 - val_loss: 259.5085\n",
      "Epoch 2888/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 148.7949 - val_loss: 147.9470\n",
      "Epoch 2889/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.8918 - val_loss: 215.8738\n",
      "Epoch 2890/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.2116 - val_loss: 141.1126\n",
      "Epoch 2891/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.3056 - val_loss: 153.5686\n",
      "Epoch 2892/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.3981 - val_loss: 202.2379\n",
      "Epoch 2893/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.7743 - val_loss: 141.7161\n",
      "Epoch 2894/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 138.9508 - val_loss: 175.1553\n",
      "Epoch 2895/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.4572 - val_loss: 224.1069\n",
      "Epoch 2896/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 140.6354 - val_loss: 153.6356\n",
      "Epoch 2897/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 146.5367 - val_loss: 159.1094\n",
      "Epoch 2898/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 142.1152 - val_loss: 176.0164\n",
      "Epoch 2899/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.6714 - val_loss: 184.2329\n",
      "Epoch 2900/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.6560 - val_loss: 202.4110\n",
      "Epoch 2901/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.6454 - val_loss: 150.3393\n",
      "Epoch 2902/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 131.4071 - val_loss: 143.1783\n",
      "Epoch 2903/10000\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 137.6919 - val_loss: 138.4563\n",
      "Epoch 2904/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 137.0716 - val_loss: 152.4852\n",
      "Epoch 2905/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 137.6138 - val_loss: 146.9917\n",
      "Epoch 2906/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 146.1732 - val_loss: 150.1545\n",
      "Epoch 2907/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 128.3777 - val_loss: 158.9271\n",
      "Epoch 2908/10000\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 143.2788 - val_loss: 161.9496\n",
      "Epoch 2909/10000\n",
      "8000/8000 [==============================] - 1s 126us/step - loss: 140.4226 - val_loss: 138.1479\n",
      "Epoch 2910/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 125.5619 - val_loss: 165.2615\n",
      "Epoch 2911/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 129.9629 - val_loss: 169.9898\n",
      "Epoch 2912/10000\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 136.2316 - val_loss: 174.0159\n",
      "Epoch 2913/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 126.5948 - val_loss: 142.7358\n",
      "Epoch 2914/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 134.4832 - val_loss: 155.4321\n",
      "Epoch 2915/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 137.9038 - val_loss: 145.6357\n",
      "Epoch 2916/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 143.3526 - val_loss: 246.6492\n",
      "Epoch 2917/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 148.5924 - val_loss: 153.1329\n",
      "Epoch 2918/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 152.6886 - val_loss: 230.2134\n",
      "Epoch 2919/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 165.3282 - val_loss: 192.9562\n",
      "Epoch 2920/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 149.1491 - val_loss: 181.3971\n",
      "Epoch 2921/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 126.4139 - val_loss: 153.7629\n",
      "Epoch 2922/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 130.9748 - val_loss: 145.1440\n",
      "Epoch 2923/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 144.9672 - val_loss: 211.0946\n",
      "Epoch 2924/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 144.3212 - val_loss: 169.0328\n",
      "Epoch 2925/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 133.0944 - val_loss: 158.2602\n",
      "Epoch 2926/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 131.3991 - val_loss: 170.0311\n",
      "Epoch 2927/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 136.2498 - val_loss: 165.0692\n",
      "Epoch 2928/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.2971 - val_loss: 164.2463\n",
      "Epoch 2929/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 136.6998 - val_loss: 144.6264\n",
      "Epoch 2930/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 156.6751 - val_loss: 189.1854\n",
      "Epoch 2931/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 126.4198 - val_loss: 145.1147\n",
      "Epoch 2932/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 134.7202 - val_loss: 166.5214\n",
      "Epoch 2933/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.8393 - val_loss: 193.7203\n",
      "Epoch 2934/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 165.4533 - val_loss: 139.5862\n",
      "Epoch 2935/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.5620 - val_loss: 142.4443\n",
      "Epoch 2936/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 124.8686 - val_loss: 160.8361\n",
      "Epoch 2937/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 138.0867 - val_loss: 188.9136\n",
      "Epoch 2938/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.3641 - val_loss: 169.1061\n",
      "Epoch 2939/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 140.3354 - val_loss: 175.2473\n",
      "Epoch 2940/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 142.2728 - val_loss: 147.4962\n",
      "Epoch 2941/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.3624 - val_loss: 287.3536\n",
      "Epoch 2942/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 148.1344 - val_loss: 140.9312\n",
      "Epoch 2943/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.4752 - val_loss: 139.7998\n",
      "Epoch 2944/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 133.1035 - val_loss: 141.9210\n",
      "Epoch 2945/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.4237 - val_loss: 165.5876\n",
      "Epoch 2946/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.2168 - val_loss: 179.9973\n",
      "Epoch 2947/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.7638 - val_loss: 158.8495\n",
      "Epoch 2948/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 156.9668 - val_loss: 181.1289\n",
      "Epoch 2949/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.8243 - val_loss: 159.9180\n",
      "Epoch 2950/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.6088 - val_loss: 142.3437\n",
      "Epoch 2951/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 158.3169 - val_loss: 172.8697\n",
      "Epoch 2952/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.5658 - val_loss: 145.9112\n",
      "Epoch 2953/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 135.0754 - val_loss: 143.1901\n",
      "Epoch 2954/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 151.4678 - val_loss: 146.5357\n",
      "Epoch 2955/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 173.0968 - val_loss: 215.6490\n",
      "Epoch 2956/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 141.7020 - val_loss: 143.2163\n",
      "Epoch 2957/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 136.8180 - val_loss: 153.9549\n",
      "Epoch 2958/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.7619 - val_loss: 153.7000\n",
      "Epoch 2959/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 139.9364 - val_loss: 150.3458\n",
      "Epoch 2960/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.6140 - val_loss: 139.6108\n",
      "Epoch 2961/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 129.7791 - val_loss: 148.8782\n",
      "Epoch 2962/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 124.8430 - val_loss: 161.2469\n",
      "Epoch 2963/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 124.3192 - val_loss: 153.8319\n",
      "Epoch 2964/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 123.8039 - val_loss: 174.0191\n",
      "Epoch 2965/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.2327 - val_loss: 205.4882\n",
      "Epoch 2966/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.3726 - val_loss: 158.6668\n",
      "Epoch 2967/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.4309 - val_loss: 180.1775\n",
      "Epoch 2968/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.8840 - val_loss: 160.8564\n",
      "Epoch 2969/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.4310 - val_loss: 183.1251\n",
      "Epoch 2970/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.9664 - val_loss: 168.5407\n",
      "Epoch 2971/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.5314 - val_loss: 141.0438\n",
      "Epoch 2972/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.3903 - val_loss: 159.5633\n",
      "Epoch 2973/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.5175 - val_loss: 153.6181\n",
      "Epoch 2974/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.8555 - val_loss: 142.8734\n",
      "Epoch 2975/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.0969 - val_loss: 146.5525\n",
      "Epoch 2976/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.3848 - val_loss: 164.6589\n",
      "Epoch 2977/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.7158 - val_loss: 152.4918\n",
      "Epoch 2978/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.8894 - val_loss: 157.4838\n",
      "Epoch 2979/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.4964 - val_loss: 141.6122\n",
      "Epoch 2980/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.2354 - val_loss: 156.2630\n",
      "Epoch 2981/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.5842 - val_loss: 195.4208\n",
      "Epoch 2982/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 133.0788 - val_loss: 153.2817\n",
      "Epoch 2983/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 123.8868 - val_loss: 151.0291\n",
      "Epoch 2984/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 151.0445 - val_loss: 141.4506\n",
      "Epoch 2985/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.5044 - val_loss: 146.0520\n",
      "Epoch 2986/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.8805 - val_loss: 140.0751\n",
      "Epoch 2987/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.7888 - val_loss: 147.2798\n",
      "Epoch 2988/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 128.4342 - val_loss: 220.5295\n",
      "Epoch 2989/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 156.8161 - val_loss: 185.4301\n",
      "Epoch 2990/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 146.7079 - val_loss: 143.1382\n",
      "Epoch 2991/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 123.9827 - val_loss: 140.2227\n",
      "Epoch 2992/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.9770 - val_loss: 151.2570\n",
      "Epoch 2993/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.7788 - val_loss: 162.8910\n",
      "Epoch 2994/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 163.5868 - val_loss: 229.1759\n",
      "Epoch 2995/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.1189 - val_loss: 160.4067\n",
      "Epoch 2996/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 143.3913 - val_loss: 152.4510\n",
      "Epoch 2997/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 129.4197 - val_loss: 187.8658\n",
      "Epoch 2998/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.7067 - val_loss: 174.2003\n",
      "Epoch 2999/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.2125 - val_loss: 142.8339\n",
      "Epoch 3000/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.9202 - val_loss: 149.1159\n",
      "Epoch 3001/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.5554 - val_loss: 144.8185\n",
      "Epoch 3002/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 133.9776 - val_loss: 140.3974\n",
      "Epoch 3003/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.2254 - val_loss: 148.0559\n",
      "Epoch 3004/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 132.2672 - val_loss: 138.0123\n",
      "Epoch 3005/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.4994 - val_loss: 144.1036\n",
      "Epoch 3006/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.5924 - val_loss: 145.0931\n",
      "Epoch 3007/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.6935 - val_loss: 178.6240\n",
      "Epoch 3008/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.4648 - val_loss: 140.9265\n",
      "Epoch 3009/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.9137 - val_loss: 161.2989\n",
      "Epoch 3010/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 149.4708 - val_loss: 307.9508\n",
      "Epoch 3011/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.1640 - val_loss: 163.4983\n",
      "Epoch 3012/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.2037 - val_loss: 243.3692\n",
      "Epoch 3013/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 131.2023 - val_loss: 176.8125\n",
      "Epoch 3014/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.7419 - val_loss: 145.2498\n",
      "Epoch 3015/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 124.5054 - val_loss: 145.4916\n",
      "Epoch 3016/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.3786 - val_loss: 168.6612\n",
      "Epoch 3017/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 233.6191 - val_loss: 192.7300\n",
      "Epoch 3018/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.8948 - val_loss: 149.7180\n",
      "Epoch 3019/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 123.8339 - val_loss: 168.8036\n",
      "Epoch 3020/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 151.3479 - val_loss: 144.4953\n",
      "Epoch 3021/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.9006 - val_loss: 143.2334\n",
      "Epoch 3022/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.8421 - val_loss: 204.0759\n",
      "Epoch 3023/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.1501 - val_loss: 163.0523\n",
      "Epoch 3024/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.6421 - val_loss: 237.4216\n",
      "Epoch 3025/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 144.1131 - val_loss: 159.1554\n",
      "Epoch 3026/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.6085 - val_loss: 141.2751\n",
      "Epoch 3027/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.0650 - val_loss: 210.1926\n",
      "Epoch 3028/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.4979 - val_loss: 180.2372\n",
      "Epoch 3029/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 147.0901 - val_loss: 190.4649\n",
      "Epoch 3030/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 153.9734 - val_loss: 141.3810\n",
      "Epoch 3031/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 148.2074 - val_loss: 206.1509\n",
      "Epoch 3032/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.2319 - val_loss: 144.9933\n",
      "Epoch 3033/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.9696 - val_loss: 151.5658\n",
      "Epoch 3034/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.5604 - val_loss: 139.0095\n",
      "Epoch 3035/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 169.7567 - val_loss: 147.9422\n",
      "Epoch 3036/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.2850 - val_loss: 139.4811\n",
      "Epoch 3037/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.5977 - val_loss: 142.1395\n",
      "Epoch 3038/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.4944 - val_loss: 144.9034\n",
      "Epoch 3039/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.5923 - val_loss: 190.6033\n",
      "Epoch 3040/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.3735 - val_loss: 173.8708\n",
      "Epoch 3041/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.8142 - val_loss: 153.4269\n",
      "Epoch 3042/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.2704 - val_loss: 148.6963\n",
      "Epoch 3043/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.8873 - val_loss: 161.9375\n",
      "Epoch 3044/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.1436 - val_loss: 142.0707\n",
      "Epoch 3045/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.7492 - val_loss: 169.9763\n",
      "Epoch 3046/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 190.2343 - val_loss: 162.0268\n",
      "Epoch 3047/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 130.9860 - val_loss: 162.3872\n",
      "Epoch 3048/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 146.0746 - val_loss: 145.9873\n",
      "Epoch 3049/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.0340 - val_loss: 144.2005\n",
      "Epoch 3050/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 154.4060 - val_loss: 147.7403\n",
      "Epoch 3051/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 134.1817 - val_loss: 306.7709\n",
      "Epoch 3052/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.7397 - val_loss: 158.2913\n",
      "Epoch 3053/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 125.4997 - val_loss: 139.4541\n",
      "Epoch 3054/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 129.2465 - val_loss: 171.0922\n",
      "Epoch 3055/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.7257 - val_loss: 144.1353\n",
      "Epoch 3056/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.1469 - val_loss: 163.4018\n",
      "Epoch 3057/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 170.8597 - val_loss: 149.9004\n",
      "Epoch 3058/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.4048 - val_loss: 146.1693\n",
      "Epoch 3059/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.8691 - val_loss: 140.3684\n",
      "Epoch 3060/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.8546 - val_loss: 159.9548\n",
      "Epoch 3061/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.1965 - val_loss: 146.9134\n",
      "Epoch 3062/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.3568 - val_loss: 156.8563\n",
      "Epoch 3063/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.7488 - val_loss: 188.0508\n",
      "Epoch 3064/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.6964 - val_loss: 142.9968\n",
      "Epoch 3065/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 128.8730 - val_loss: 138.6838\n",
      "Epoch 3066/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.3125 - val_loss: 166.0076\n",
      "Epoch 3067/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.8228 - val_loss: 145.3628\n",
      "Epoch 3068/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.3572 - val_loss: 146.3771\n",
      "Epoch 3069/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 216.1867 - val_loss: 162.0448\n",
      "Epoch 3070/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.2504 - val_loss: 142.0563\n",
      "Epoch 3071/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 128.5619 - val_loss: 144.6185\n",
      "Epoch 3072/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.5060 - val_loss: 151.7016\n",
      "Epoch 3073/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.8331 - val_loss: 150.4917\n",
      "Epoch 3074/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.8516 - val_loss: 164.2450\n",
      "Epoch 3075/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.9002 - val_loss: 153.9743\n",
      "Epoch 3076/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 133.4520 - val_loss: 139.8926\n",
      "Epoch 3077/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.5230 - val_loss: 160.8380\n",
      "Epoch 3078/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.2107 - val_loss: 154.1112\n",
      "Epoch 3079/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.8073 - val_loss: 163.3755\n",
      "Epoch 3080/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 201.8194 - val_loss: 173.4588\n",
      "Epoch 3081/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.4527 - val_loss: 169.2958\n",
      "Epoch 3082/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.3054 - val_loss: 181.6899\n",
      "Epoch 3083/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.7088 - val_loss: 192.3495\n",
      "Epoch 3084/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 177.7458 - val_loss: 155.8622\n",
      "Epoch 3085/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.6800 - val_loss: 143.0230\n",
      "Epoch 3086/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.6117 - val_loss: 147.3725\n",
      "Epoch 3087/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 126.3347 - val_loss: 148.0333\n",
      "Epoch 3088/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.9977 - val_loss: 196.5864\n",
      "Epoch 3089/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 143.3588 - val_loss: 145.1354\n",
      "Epoch 3090/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.9658 - val_loss: 147.5445\n",
      "Epoch 3091/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.7812 - val_loss: 202.7193\n",
      "Epoch 3092/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 155.4557 - val_loss: 147.1644\n",
      "Epoch 3093/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 131.4380 - val_loss: 147.1916\n",
      "Epoch 3094/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.4822 - val_loss: 157.5987\n",
      "Epoch 3095/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 133.9860 - val_loss: 186.6404\n",
      "Epoch 3096/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 130.6665 - val_loss: 173.4140\n",
      "Epoch 3097/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 129.1059 - val_loss: 170.3092\n",
      "Epoch 3098/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 135.0502 - val_loss: 166.1387\n",
      "Epoch 3099/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 129.5707 - val_loss: 201.3036\n",
      "Epoch 3100/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 172.1879 - val_loss: 146.4654\n",
      "Epoch 3101/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.4755 - val_loss: 165.5861\n",
      "Epoch 3102/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.4499 - val_loss: 154.6753\n",
      "Epoch 3103/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.7322 - val_loss: 159.6486\n",
      "Epoch 3104/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.4904 - val_loss: 143.0642\n",
      "Epoch 3105/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 124.0867 - val_loss: 154.9147\n",
      "Epoch 3106/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 133.9795 - val_loss: 150.0060\n",
      "Epoch 3107/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.6235 - val_loss: 143.5689\n",
      "Epoch 3108/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 127.0070 - val_loss: 157.8379\n",
      "Epoch 3109/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 125.1337 - val_loss: 158.0208\n",
      "Epoch 3110/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.2094 - val_loss: 180.6450\n",
      "Epoch 3111/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 143.6816 - val_loss: 145.1939\n",
      "Epoch 3112/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.5347 - val_loss: 149.6854\n",
      "Epoch 3113/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.2583 - val_loss: 159.0661\n",
      "Epoch 3114/10000\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 128.5344 - val_loss: 169.1335\n",
      "Epoch 3115/10000\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 130.2001 - val_loss: 149.0795\n",
      "Epoch 3116/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 126.0708 - val_loss: 183.1341\n",
      "Epoch 3117/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 130.2483 - val_loss: 150.7139\n",
      "Epoch 3118/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 216.1585 - val_loss: 163.0622\n",
      "Epoch 3119/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 175.0538 - val_loss: 145.2593\n",
      "Epoch 3120/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 123.4071 - val_loss: 205.1684\n",
      "Epoch 3121/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 125.2346 - val_loss: 290.9276\n",
      "Epoch 3122/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.2558 - val_loss: 231.5357\n",
      "Epoch 3123/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.3546 - val_loss: 144.3453\n",
      "Epoch 3124/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 162.9326 - val_loss: 161.2059\n",
      "Epoch 3125/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 155.685 - 0s 51us/step - loss: 155.1702 - val_loss: 137.6581\n",
      "Epoch 3126/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.0513 - val_loss: 161.4440\n",
      "Epoch 3127/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 128.5883 - val_loss: 137.2796\n",
      "Epoch 3128/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.1380 - val_loss: 166.4660\n",
      "Epoch 3129/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.3338 - val_loss: 151.1698\n",
      "Epoch 3130/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.8526 - val_loss: 163.4561\n",
      "Epoch 3131/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.7477 - val_loss: 150.7243\n",
      "Epoch 3132/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.6212 - val_loss: 174.5441\n",
      "Epoch 3133/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.7041 - val_loss: 156.6529\n",
      "Epoch 3134/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 230.2282 - val_loss: 151.3408\n",
      "Epoch 3135/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 173.1285 - val_loss: 142.1783\n",
      "Epoch 3136/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 131.8905 - val_loss: 143.8713\n",
      "Epoch 3137/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.8283 - val_loss: 139.7767\n",
      "Epoch 3138/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.0272 - val_loss: 148.4881\n",
      "Epoch 3139/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 120.1204 - val_loss: 142.2557\n",
      "Epoch 3140/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.8222 - val_loss: 145.9279\n",
      "Epoch 3141/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.0809 - val_loss: 167.0530\n",
      "Epoch 3142/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.6979 - val_loss: 168.8251\n",
      "Epoch 3143/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.2141 - val_loss: 139.3554\n",
      "Epoch 3144/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.0480 - val_loss: 152.4901\n",
      "Epoch 3145/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 125.7965 - val_loss: 151.8305\n",
      "Epoch 3146/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.3971 - val_loss: 166.4548\n",
      "Epoch 3147/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.7411 - val_loss: 159.2728\n",
      "Epoch 3148/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.5934 - val_loss: 147.4751\n",
      "Epoch 3149/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.8791 - val_loss: 142.6187\n",
      "Epoch 3150/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.1263 - val_loss: 153.2673\n",
      "Epoch 3151/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.9303 - val_loss: 141.0559\n",
      "Epoch 3152/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.1381 - val_loss: 140.5747\n",
      "Epoch 3153/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.9354 - val_loss: 160.6979\n",
      "Epoch 3154/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.1512 - val_loss: 139.0749\n",
      "Epoch 3155/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.8614 - val_loss: 177.7927\n",
      "Epoch 3156/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.2610 - val_loss: 172.8765\n",
      "Epoch 3157/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.9920 - val_loss: 160.9733\n",
      "Epoch 3158/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 161.4096 - val_loss: 173.9388\n",
      "Epoch 3159/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.4645 - val_loss: 159.5855\n",
      "Epoch 3160/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 165.5654 - val_loss: 145.6068\n",
      "Epoch 3161/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.8453 - val_loss: 152.6239\n",
      "Epoch 3162/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.9404 - val_loss: 145.7760\n",
      "Epoch 3163/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 185.6705 - val_loss: 180.2606\n",
      "Epoch 3164/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.8451 - val_loss: 156.7707\n",
      "Epoch 3165/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.9650 - val_loss: 151.7384\n",
      "Epoch 3166/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.4658 - val_loss: 158.1799\n",
      "Epoch 3167/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.0281 - val_loss: 152.7332\n",
      "Epoch 3168/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.9935 - val_loss: 223.7770\n",
      "Epoch 3169/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.9208 - val_loss: 139.7896\n",
      "Epoch 3170/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.0448 - val_loss: 191.0287\n",
      "Epoch 3171/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.3712 - val_loss: 190.3477\n",
      "Epoch 3172/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.2703 - val_loss: 153.1950\n",
      "Epoch 3173/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.8987 - val_loss: 143.0658\n",
      "Epoch 3174/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.0336 - val_loss: 147.1109\n",
      "Epoch 3175/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.6641 - val_loss: 143.9890\n",
      "Epoch 3176/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.6235 - val_loss: 144.2559\n",
      "Epoch 3177/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.8309 - val_loss: 160.2141\n",
      "Epoch 3178/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.2663 - val_loss: 160.9931\n",
      "Epoch 3179/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.9939 - val_loss: 262.9124\n",
      "Epoch 3180/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.2638 - val_loss: 150.1823\n",
      "Epoch 3181/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.5854 - val_loss: 165.7074\n",
      "Epoch 3182/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.4192 - val_loss: 156.9881\n",
      "Epoch 3183/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 140.1380 - val_loss: 148.1991\n",
      "Epoch 3184/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.7493 - val_loss: 162.2878\n",
      "Epoch 3185/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.4443 - val_loss: 199.9993\n",
      "Epoch 3186/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.2219 - val_loss: 176.1750\n",
      "Epoch 3187/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 195.4117 - val_loss: 157.1689\n",
      "Epoch 3188/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 130.3153 - val_loss: 150.9418\n",
      "Epoch 3189/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 135.4320 - val_loss: 150.2211\n",
      "Epoch 3190/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 129.2955 - val_loss: 143.2297\n",
      "Epoch 3191/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.5387 - val_loss: 189.2604\n",
      "Epoch 3192/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 181.8838 - val_loss: 272.4835\n",
      "Epoch 3193/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 226.3679 - val_loss: 159.3960\n",
      "Epoch 3194/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 208.6385 - val_loss: 153.0900\n",
      "Epoch 3195/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 178.6004 - val_loss: 158.5410\n",
      "Epoch 3196/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.7023 - val_loss: 177.8901\n",
      "Epoch 3197/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.8659 - val_loss: 171.3606\n",
      "Epoch 3198/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.5736 - val_loss: 139.5778\n",
      "Epoch 3199/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.4763 - val_loss: 157.7864\n",
      "Epoch 3200/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.2256 - val_loss: 212.8578\n",
      "Epoch 3201/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.5963 - val_loss: 147.8436\n",
      "Epoch 3202/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.8539 - val_loss: 148.0168\n",
      "Epoch 3203/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 148.2837 - val_loss: 176.2184\n",
      "Epoch 3204/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.7046 - val_loss: 161.3333\n",
      "Epoch 3205/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 135.238 - 0s 51us/step - loss: 133.8084 - val_loss: 146.2780\n",
      "Epoch 3206/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 190.8924 - val_loss: 164.0450\n",
      "Epoch 3207/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.0265 - val_loss: 171.1629\n",
      "Epoch 3208/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.1562 - val_loss: 139.4682\n",
      "Epoch 3209/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.9999 - val_loss: 206.4049\n",
      "Epoch 3210/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.2969 - val_loss: 164.9593\n",
      "Epoch 3211/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.0940 - val_loss: 142.1012\n",
      "Epoch 3212/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.2998 - val_loss: 159.4971\n",
      "Epoch 3213/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.6192 - val_loss: 150.8272\n",
      "Epoch 3214/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 140.0582 - val_loss: 140.3687\n",
      "Epoch 3215/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.2092 - val_loss: 147.3036\n",
      "Epoch 3216/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.0114 - val_loss: 160.2547\n",
      "Epoch 3217/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 125.5960 - val_loss: 157.2305\n",
      "Epoch 3218/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 155.9554 - val_loss: 146.2032\n",
      "Epoch 3219/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.1978 - val_loss: 144.8267\n",
      "Epoch 3220/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.6162 - val_loss: 141.9361\n",
      "Epoch 3221/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.4568 - val_loss: 183.4002\n",
      "Epoch 3222/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.5550 - val_loss: 273.9775\n",
      "Epoch 3223/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.9416 - val_loss: 139.6755\n",
      "Epoch 3224/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.9069 - val_loss: 193.2068\n",
      "Epoch 3225/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.2161 - val_loss: 230.1435\n",
      "Epoch 3226/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 169.4437 - val_loss: 186.4307\n",
      "Epoch 3227/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.1375 - val_loss: 142.4642\n",
      "Epoch 3228/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.4627 - val_loss: 179.0336\n",
      "Epoch 3229/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 146.7743 - val_loss: 222.6732\n",
      "Epoch 3230/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 145.0779 - val_loss: 148.9556\n",
      "Epoch 3231/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.3935 - val_loss: 144.2859\n",
      "Epoch 3232/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.7237 - val_loss: 164.9110\n",
      "Epoch 3233/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.4673 - val_loss: 148.8256\n",
      "Epoch 3234/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.7377 - val_loss: 139.3208\n",
      "Epoch 3235/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.1426 - val_loss: 147.1443\n",
      "Epoch 3236/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.5489 - val_loss: 162.5372\n",
      "Epoch 3237/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 165.7287 - val_loss: 285.4519\n",
      "Epoch 3238/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.3087 - val_loss: 181.4567\n",
      "Epoch 3239/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.1965 - val_loss: 142.5483\n",
      "Epoch 3240/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.2360 - val_loss: 167.9284\n",
      "Epoch 3241/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 148.6629 - val_loss: 160.0713\n",
      "Epoch 3242/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.8061 - val_loss: 143.9532\n",
      "Epoch 3243/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.3120 - val_loss: 145.7475\n",
      "Epoch 3244/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 162.3899 - val_loss: 335.3310\n",
      "Epoch 3245/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 163.4412 - val_loss: 166.2519\n",
      "Epoch 3246/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 145.2393 - val_loss: 142.3770\n",
      "Epoch 3247/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.5955 - val_loss: 149.7848\n",
      "Epoch 3248/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.1855 - val_loss: 156.3218\n",
      "Epoch 3249/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.3231 - val_loss: 161.2546\n",
      "Epoch 3250/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.6818 - val_loss: 142.7056\n",
      "Epoch 3251/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 142.2283 - val_loss: 140.8496\n",
      "Epoch 3252/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.8799 - val_loss: 150.9339\n",
      "Epoch 3253/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.4433 - val_loss: 148.4601\n",
      "Epoch 3254/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.8591 - val_loss: 142.3294\n",
      "Epoch 3255/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.4426 - val_loss: 146.2996\n",
      "Epoch 3256/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.7572 - val_loss: 154.5632\n",
      "Epoch 3257/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.8317 - val_loss: 178.0360\n",
      "Epoch 3258/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 229.7345 - val_loss: 159.2570\n",
      "Epoch 3259/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 165.5594 - val_loss: 145.9947\n",
      "Epoch 3260/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 125.4260 - val_loss: 141.9502\n",
      "Epoch 3261/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 127.8902 - val_loss: 153.0769\n",
      "Epoch 3262/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 119.9715 - val_loss: 181.8871\n",
      "Epoch 3263/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.5479 - val_loss: 196.2113\n",
      "Epoch 3264/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 139.4110 - val_loss: 166.5346\n",
      "Epoch 3265/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.4126 - val_loss: 144.6847\n",
      "Epoch 3266/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.5193 - val_loss: 159.5721\n",
      "Epoch 3267/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.0983 - val_loss: 177.4347\n",
      "Epoch 3268/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.3986 - val_loss: 147.7159\n",
      "Epoch 3269/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.7942 - val_loss: 145.4331\n",
      "Epoch 3270/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.4421 - val_loss: 143.9830\n",
      "Epoch 3271/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.7301 - val_loss: 148.5647\n",
      "Epoch 3272/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.6542 - val_loss: 266.7057\n",
      "Epoch 3273/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.8947 - val_loss: 184.3667\n",
      "Epoch 3274/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.6880 - val_loss: 201.5655\n",
      "Epoch 3275/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.0659 - val_loss: 145.7248\n",
      "Epoch 3276/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.6094 - val_loss: 144.3449\n",
      "Epoch 3277/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.8084 - val_loss: 172.1806\n",
      "Epoch 3278/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 148.9732 - val_loss: 192.8167\n",
      "Epoch 3279/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.6363 - val_loss: 153.8047\n",
      "Epoch 3280/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.0527 - val_loss: 159.4838\n",
      "Epoch 3281/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.0558 - val_loss: 146.7508\n",
      "Epoch 3282/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.5438 - val_loss: 142.7327\n",
      "Epoch 3283/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.0428 - val_loss: 153.0427\n",
      "Epoch 3284/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.5848 - val_loss: 145.6957\n",
      "Epoch 3285/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.1109 - val_loss: 148.5374\n",
      "Epoch 3286/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.8401 - val_loss: 165.6962\n",
      "Epoch 3287/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 142.3680 - val_loss: 148.7913\n",
      "Epoch 3288/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.2619 - val_loss: 168.2386\n",
      "Epoch 3289/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.9155 - val_loss: 157.1733\n",
      "Epoch 3290/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.7597 - val_loss: 154.5330\n",
      "Epoch 3291/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 184.4355 - val_loss: 354.7637\n",
      "Epoch 3292/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 157.7482 - val_loss: 139.1371\n",
      "Epoch 3293/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 125.254 - 0s 51us/step - loss: 124.6864 - val_loss: 161.6800\n",
      "Epoch 3294/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.7697 - val_loss: 164.8350\n",
      "Epoch 3295/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.0638 - val_loss: 190.1933\n",
      "Epoch 3296/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.9438 - val_loss: 161.9559\n",
      "Epoch 3297/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.0131 - val_loss: 142.4873\n",
      "Epoch 3298/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 171.2329 - val_loss: 167.6227\n",
      "Epoch 3299/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 177.9331 - val_loss: 157.5053\n",
      "Epoch 3300/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.4347 - val_loss: 157.1247\n",
      "Epoch 3301/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.8409 - val_loss: 138.5147\n",
      "Epoch 3302/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.2737 - val_loss: 140.2631\n",
      "Epoch 3303/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.5677 - val_loss: 158.7106\n",
      "Epoch 3304/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 147.0093 - val_loss: 154.6908\n",
      "Epoch 3305/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.1019 - val_loss: 336.3515\n",
      "Epoch 3306/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.8410 - val_loss: 143.9631\n",
      "Epoch 3307/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.8894 - val_loss: 157.0666\n",
      "Epoch 3308/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.7668 - val_loss: 169.5216\n",
      "Epoch 3309/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.2961 - val_loss: 151.3929\n",
      "Epoch 3310/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.1284 - val_loss: 149.6821\n",
      "Epoch 3311/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.7454 - val_loss: 158.4040\n",
      "Epoch 3312/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.6758 - val_loss: 158.0999\n",
      "Epoch 3313/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.2305 - val_loss: 206.7434\n",
      "Epoch 3314/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.0670 - val_loss: 212.0663\n",
      "Epoch 3315/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.8935 - val_loss: 140.8284\n",
      "Epoch 3316/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.3177 - val_loss: 163.5092\n",
      "Epoch 3317/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.6679 - val_loss: 151.0918\n",
      "Epoch 3318/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.0416 - val_loss: 155.6718\n",
      "Epoch 3319/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.6800 - val_loss: 194.0219\n",
      "Epoch 3320/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.8175 - val_loss: 167.6237\n",
      "Epoch 3321/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.4074 - val_loss: 155.1272\n",
      "Epoch 3322/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 153.5417 - val_loss: 149.9963\n",
      "Epoch 3323/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 234.8766 - val_loss: 215.5091\n",
      "Epoch 3324/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.6337 - val_loss: 150.8272\n",
      "Epoch 3325/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.9384 - val_loss: 146.1157\n",
      "Epoch 3326/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.7438 - val_loss: 157.7341\n",
      "Epoch 3327/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.7732 - val_loss: 147.7702\n",
      "Epoch 3328/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.4168 - val_loss: 145.3026\n",
      "Epoch 3329/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 146.1444 - val_loss: 149.5056\n",
      "Epoch 3330/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.8264 - val_loss: 142.1419\n",
      "Epoch 3331/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.6804 - val_loss: 144.2453\n",
      "Epoch 3332/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.8758 - val_loss: 176.6525\n",
      "Epoch 3333/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 130.9552 - val_loss: 232.9034\n",
      "Epoch 3334/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 131.4592 - val_loss: 159.7001\n",
      "Epoch 3335/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.2996 - val_loss: 154.1332\n",
      "Epoch 3336/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.9207 - val_loss: 161.7831\n",
      "Epoch 3337/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.4869 - val_loss: 162.7639\n",
      "Epoch 3338/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 128.9253 - val_loss: 143.0186\n",
      "Epoch 3339/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.9066 - val_loss: 142.6774\n",
      "Epoch 3340/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.0174 - val_loss: 170.5480\n",
      "Epoch 3341/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.6440 - val_loss: 161.1187\n",
      "Epoch 3342/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 158.6061 - val_loss: 184.6533\n",
      "Epoch 3343/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.6140 - val_loss: 142.2283\n",
      "Epoch 3344/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.5698 - val_loss: 145.2344\n",
      "Epoch 3345/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.4381 - val_loss: 157.8988\n",
      "Epoch 3346/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.7932 - val_loss: 141.9875\n",
      "Epoch 3347/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 136.4180 - val_loss: 168.7012\n",
      "Epoch 3348/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.2680 - val_loss: 157.3149\n",
      "Epoch 3349/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.3898 - val_loss: 137.0733\n",
      "Epoch 3350/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.1990 - val_loss: 139.3847\n",
      "Epoch 3351/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.2256 - val_loss: 463.3830\n",
      "Epoch 3352/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 218.5643 - val_loss: 182.3699\n",
      "Epoch 3353/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 138.4259 - val_loss: 144.9234\n",
      "Epoch 3354/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 128.4918 - val_loss: 146.7618\n",
      "Epoch 3355/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 124.2994 - val_loss: 150.1156\n",
      "Epoch 3356/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.8273 - val_loss: 151.1083\n",
      "Epoch 3357/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 135.9338 - val_loss: 182.0938\n",
      "Epoch 3358/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.0899 - val_loss: 145.4131- ETA: 0s - loss: 13\n",
      "Epoch 3359/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 129.0755 - val_loss: 140.3672\n",
      "Epoch 3360/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 135.1919 - val_loss: 166.5633\n",
      "Epoch 3361/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 142.3857 - val_loss: 159.2759\n",
      "Epoch 3362/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.9371 - val_loss: 162.9723\n",
      "Epoch 3363/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 147.6106 - val_loss: 236.3202\n",
      "Epoch 3364/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 159.7963 - val_loss: 145.1683\n",
      "Epoch 3365/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.5806 - val_loss: 147.3383\n",
      "Epoch 3366/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 168.6077 - val_loss: 150.0756\n",
      "Epoch 3367/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 185.0644 - val_loss: 183.7156\n",
      "Epoch 3368/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.6904 - val_loss: 146.3485\n",
      "Epoch 3369/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 181.2508 - val_loss: 162.4575\n",
      "Epoch 3370/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.2185 - val_loss: 149.5137\n",
      "Epoch 3371/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.3198 - val_loss: 172.3105\n",
      "Epoch 3372/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.2055 - val_loss: 147.6466\n",
      "Epoch 3373/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 136.8246 - val_loss: 147.6534\n",
      "Epoch 3374/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.0121 - val_loss: 154.5669\n",
      "Epoch 3375/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.1866 - val_loss: 143.4982\n",
      "Epoch 3376/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 132.7720 - val_loss: 148.3599\n",
      "Epoch 3377/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.3200 - val_loss: 146.2554\n",
      "Epoch 3378/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.8433 - val_loss: 137.4310\n",
      "Epoch 3379/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.4134 - val_loss: 143.2436\n",
      "Epoch 3380/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.6430 - val_loss: 140.1078\n",
      "Epoch 3381/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.2149 - val_loss: 210.3183\n",
      "Epoch 3382/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.2053 - val_loss: 139.1959\n",
      "Epoch 3383/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 125.3749 - val_loss: 147.9315\n",
      "Epoch 3384/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.3607 - val_loss: 140.7881\n",
      "Epoch 3385/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.6465 - val_loss: 148.3463\n",
      "Epoch 3386/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.0671 - val_loss: 163.1296\n",
      "Epoch 3387/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.7011 - val_loss: 151.8862\n",
      "Epoch 3388/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.3918 - val_loss: 169.3298\n",
      "Epoch 3389/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 145.9632 - val_loss: 152.4727\n",
      "Epoch 3390/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 164.7722 - val_loss: 152.8559\n",
      "Epoch 3391/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.8503 - val_loss: 171.2873\n",
      "Epoch 3392/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.5625 - val_loss: 167.2683\n",
      "Epoch 3393/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.7711 - val_loss: 183.1027\n",
      "Epoch 3394/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.0493 - val_loss: 147.3993\n",
      "Epoch 3395/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 128.832 - 0s 51us/step - loss: 128.4542 - val_loss: 146.7629\n",
      "Epoch 3396/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 121.8158 - val_loss: 140.3098\n",
      "Epoch 3397/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 141.2831 - val_loss: 159.4085\n",
      "Epoch 3398/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.2951 - val_loss: 155.7992\n",
      "Epoch 3399/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 163.8370 - val_loss: 141.7827\n",
      "Epoch 3400/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.6505 - val_loss: 148.4036\n",
      "Epoch 3401/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 130.0817 - val_loss: 170.3632\n",
      "Epoch 3402/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 129.0931 - val_loss: 171.3680\n",
      "Epoch 3403/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 128.0184 - val_loss: 175.5477\n",
      "Epoch 3404/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 182.2413 - val_loss: 187.2212\n",
      "Epoch 3405/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.3556 - val_loss: 140.9709\n",
      "Epoch 3406/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.1226 - val_loss: 187.9332\n",
      "Epoch 3407/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.9920 - val_loss: 167.1639\n",
      "Epoch 3408/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.2693 - val_loss: 232.6020\n",
      "Epoch 3409/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.7217 - val_loss: 144.9868\n",
      "Epoch 3410/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.3433 - val_loss: 144.9568\n",
      "Epoch 3411/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.8752 - val_loss: 181.1942\n",
      "Epoch 3412/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 172.6381 - val_loss: 147.4201\n",
      "Epoch 3413/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.7500 - val_loss: 149.1837\n",
      "Epoch 3414/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.5778 - val_loss: 139.4925\n",
      "Epoch 3415/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.6528 - val_loss: 172.3969\n",
      "Epoch 3416/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.6296 - val_loss: 186.6175\n",
      "Epoch 3417/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.0281 - val_loss: 246.1202\n",
      "Epoch 3418/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.6897 - val_loss: 155.9272\n",
      "Epoch 3419/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.1940 - val_loss: 152.3439\n",
      "Epoch 3420/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.4312 - val_loss: 150.2356\n",
      "Epoch 3421/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.6449 - val_loss: 140.1747\n",
      "Epoch 3422/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.4871 - val_loss: 152.1626\n",
      "Epoch 3423/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.8152 - val_loss: 146.7046\n",
      "Epoch 3424/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.4645 - val_loss: 148.0715\n",
      "Epoch 3425/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.9781 - val_loss: 158.5816\n",
      "Epoch 3426/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.3546 - val_loss: 170.7537\n",
      "Epoch 3427/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.2789 - val_loss: 148.3273\n",
      "Epoch 3428/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.1812 - val_loss: 154.2274\n",
      "Epoch 3429/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 151.2880 - val_loss: 142.8795\n",
      "Epoch 3430/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.0981 - val_loss: 169.9066\n",
      "Epoch 3431/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.1089 - val_loss: 143.5819\n",
      "Epoch 3432/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.4504 - val_loss: 174.2502\n",
      "Epoch 3433/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.7144 - val_loss: 147.2181\n",
      "Epoch 3434/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 141.4053 - val_loss: 153.1816\n",
      "Epoch 3435/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.8946 - val_loss: 146.8758\n",
      "Epoch 3436/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.3167 - val_loss: 143.4925\n",
      "Epoch 3437/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.8314 - val_loss: 146.2258\n",
      "Epoch 3438/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.7463 - val_loss: 175.3041\n",
      "Epoch 3439/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.5092 - val_loss: 168.1491\n",
      "Epoch 3440/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.6551 - val_loss: 143.8202\n",
      "Epoch 3441/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.2322 - val_loss: 141.0219\n",
      "Epoch 3442/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.0059 - val_loss: 144.3103\n",
      "Epoch 3443/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.1101 - val_loss: 144.7175\n",
      "Epoch 3444/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.1771 - val_loss: 148.5037\n",
      "Epoch 3445/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.0373 - val_loss: 151.7425\n",
      "Epoch 3446/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.8474 - val_loss: 175.6356\n",
      "Epoch 3447/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 158.3373 - val_loss: 258.6656\n",
      "Epoch 3448/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 142.6849 - val_loss: 150.8217\n",
      "Epoch 3449/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 190.4755 - val_loss: 177.0620\n",
      "Epoch 3450/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.9549 - val_loss: 146.8629\n",
      "Epoch 3451/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 128.8609 - val_loss: 139.7050\n",
      "Epoch 3452/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.1253 - val_loss: 140.0972\n",
      "Epoch 3453/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 136.2907 - val_loss: 152.1521\n",
      "Epoch 3454/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.9929 - val_loss: 144.0237\n",
      "Epoch 3455/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.4820 - val_loss: 149.2157\n",
      "Epoch 3456/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.0919 - val_loss: 152.1414\n",
      "Epoch 3457/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.1043 - val_loss: 153.9366\n",
      "Epoch 3458/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.6731 - val_loss: 150.5104\n",
      "Epoch 3459/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.7206 - val_loss: 157.8238\n",
      "Epoch 3460/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.9772 - val_loss: 198.8264\n",
      "Epoch 3461/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.3303 - val_loss: 166.7530\n",
      "Epoch 3462/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 162.3929 - val_loss: 167.0842\n",
      "Epoch 3463/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.6792 - val_loss: 163.2879\n",
      "Epoch 3464/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.7599 - val_loss: 164.5174\n",
      "Epoch 3465/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 120.7775 - val_loss: 149.6530\n",
      "Epoch 3466/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.2607 - val_loss: 150.5869\n",
      "Epoch 3467/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 155.8112 - val_loss: 145.9823\n",
      "Epoch 3468/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.9696 - val_loss: 160.1052\n",
      "Epoch 3469/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 158.8673 - val_loss: 165.8669\n",
      "Epoch 3470/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 155.3551 - val_loss: 141.4054\n",
      "Epoch 3471/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.0355 - val_loss: 173.3841\n",
      "Epoch 3472/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.4273 - val_loss: 140.9588\n",
      "Epoch 3473/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 131.5621 - val_loss: 150.6481\n",
      "Epoch 3474/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.7478 - val_loss: 223.7931\n",
      "Epoch 3475/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 179.3189 - val_loss: 158.3491\n",
      "Epoch 3476/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.8587 - val_loss: 152.1967\n",
      "Epoch 3477/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.1793 - val_loss: 141.6155\n",
      "Epoch 3478/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.3272 - val_loss: 142.9725\n",
      "Epoch 3479/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.9833 - val_loss: 158.1355\n",
      "Epoch 3480/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.7062 - val_loss: 143.8438\n",
      "Epoch 3481/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.3081 - val_loss: 144.7029\n",
      "Epoch 3482/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.4589 - val_loss: 242.6791\n",
      "Epoch 3483/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 155.7350 - val_loss: 155.6587\n",
      "Epoch 3484/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 128.0941 - val_loss: 148.7594\n",
      "Epoch 3485/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 251.0746 - val_loss: 177.4377\n",
      "Epoch 3486/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.5794 - val_loss: 153.1669\n",
      "Epoch 3487/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.8276 - val_loss: 216.3887\n",
      "Epoch 3488/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.2298 - val_loss: 147.1975\n",
      "Epoch 3489/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.2333 - val_loss: 141.7511\n",
      "Epoch 3490/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.2714 - val_loss: 165.3630\n",
      "Epoch 3491/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.6505 - val_loss: 150.1277\n",
      "Epoch 3492/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.1235 - val_loss: 153.9780\n",
      "Epoch 3493/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.8665 - val_loss: 141.4319\n",
      "Epoch 3494/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.4918 - val_loss: 186.2789\n",
      "Epoch 3495/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.5151 - val_loss: 152.2087\n",
      "Epoch 3496/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.4932 - val_loss: 145.5369\n",
      "Epoch 3497/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.1437 - val_loss: 156.2554\n",
      "Epoch 3498/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.8552 - val_loss: 159.3055\n",
      "Epoch 3499/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.1040 - val_loss: 145.7402\n",
      "Epoch 3500/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.8464 - val_loss: 140.4152\n",
      "Epoch 3501/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.9509 - val_loss: 227.6487\n",
      "Epoch 3502/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.2009 - val_loss: 143.2732\n",
      "Epoch 3503/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.0405 - val_loss: 145.0338\n",
      "Epoch 3504/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.0999 - val_loss: 144.5382\n",
      "Epoch 3505/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.5480 - val_loss: 145.4670\n",
      "Epoch 3506/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.7059 - val_loss: 154.4325\n",
      "Epoch 3507/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.4557 - val_loss: 151.7738\n",
      "Epoch 3508/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.6857 - val_loss: 165.3946\n",
      "Epoch 3509/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 127.8853 - val_loss: 141.1430\n",
      "Epoch 3510/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 179.1426 - val_loss: 259.7321\n",
      "Epoch 3511/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 208.7191 - val_loss: 147.2169\n",
      "Epoch 3512/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 127.8925 - val_loss: 142.9580\n",
      "Epoch 3513/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.9847 - val_loss: 230.4869\n",
      "Epoch 3514/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.7818 - val_loss: 170.1073\n",
      "Epoch 3515/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.5023 - val_loss: 152.8077\n",
      "Epoch 3516/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.8531 - val_loss: 144.8648\n",
      "Epoch 3517/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.2002 - val_loss: 146.4819\n",
      "Epoch 3518/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.0980 - val_loss: 149.7916\n",
      "Epoch 3519/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.2022 - val_loss: 143.8808\n",
      "Epoch 3520/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.0103 - val_loss: 156.6694\n",
      "Epoch 3521/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.2099 - val_loss: 146.5062\n",
      "Epoch 3522/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.9298 - val_loss: 171.6475\n",
      "Epoch 3523/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.7928 - val_loss: 148.1846\n",
      "Epoch 3524/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.0842 - val_loss: 150.5002\n",
      "Epoch 3525/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.4146 - val_loss: 146.5772\n",
      "Epoch 3526/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.8057 - val_loss: 163.7959\n",
      "Epoch 3527/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 146.7485 - val_loss: 142.7717\n",
      "Epoch 3528/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.3839 - val_loss: 151.8084\n",
      "Epoch 3529/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 164.4145 - val_loss: 158.6099\n",
      "Epoch 3530/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.1293 - val_loss: 179.5045\n",
      "Epoch 3531/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.3840 - val_loss: 164.7114\n",
      "Epoch 3532/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.8534 - val_loss: 209.1199\n",
      "Epoch 3533/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.7580 - val_loss: 137.9516\n",
      "Epoch 3534/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.6619 - val_loss: 158.3805\n",
      "Epoch 3535/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.5265 - val_loss: 146.5531\n",
      "Epoch 3536/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.1302 - val_loss: 142.1765\n",
      "Epoch 3537/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.7032 - val_loss: 199.9833\n",
      "Epoch 3538/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.0311 - val_loss: 162.5100\n",
      "Epoch 3539/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 149.9925 - val_loss: 167.7592\n",
      "Epoch 3540/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.5202 - val_loss: 152.2671\n",
      "Epoch 3541/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.5959 - val_loss: 174.8911\n",
      "Epoch 3542/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.0895 - val_loss: 208.1441\n",
      "Epoch 3543/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.0894 - val_loss: 160.2006\n",
      "Epoch 3544/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.6007 - val_loss: 159.6157\n",
      "Epoch 3545/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.3507 - val_loss: 152.6127\n",
      "Epoch 3546/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 129.2844 - val_loss: 162.9904\n",
      "Epoch 3547/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 125.5540 - val_loss: 167.2416\n",
      "Epoch 3548/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 123.5914 - val_loss: 150.8020\n",
      "Epoch 3549/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.5167 - val_loss: 138.8000\n",
      "Epoch 3550/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 161.6505 - val_loss: 162.5049\n",
      "Epoch 3551/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 158.5402 - val_loss: 155.4826\n",
      "Epoch 3552/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.8858 - val_loss: 179.5238\n",
      "Epoch 3553/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 157.3614 - val_loss: 141.0172\n",
      "Epoch 3554/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.9451 - val_loss: 143.0189\n",
      "Epoch 3555/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.7491 - val_loss: 145.9883\n",
      "Epoch 3556/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.5607 - val_loss: 143.8528\n",
      "Epoch 3557/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.6609 - val_loss: 142.8217\n",
      "Epoch 3558/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.8367 - val_loss: 165.4561\n",
      "Epoch 3559/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.4456 - val_loss: 139.9511\n",
      "Epoch 3560/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.8495 - val_loss: 150.4790\n",
      "Epoch 3561/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.9189 - val_loss: 144.1208\n",
      "Epoch 3562/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.8177 - val_loss: 172.5122\n",
      "Epoch 3563/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.1152 - val_loss: 166.7750\n",
      "Epoch 3564/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.2940 - val_loss: 145.8997\n",
      "Epoch 3565/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.5193 - val_loss: 155.0991\n",
      "Epoch 3566/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.6450 - val_loss: 145.5234\n",
      "Epoch 3567/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 133.919 - 0s 51us/step - loss: 134.4434 - val_loss: 144.0919\n",
      "Epoch 3568/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.8720 - val_loss: 164.9494\n",
      "Epoch 3569/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.7690 - val_loss: 164.7822\n",
      "Epoch 3570/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.5402 - val_loss: 164.5166\n",
      "Epoch 3571/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 182.2117 - val_loss: 164.9888\n",
      "Epoch 3572/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.6143 - val_loss: 148.6854\n",
      "Epoch 3573/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.4993 - val_loss: 147.5696\n",
      "Epoch 3574/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.8949 - val_loss: 186.5721\n",
      "Epoch 3575/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 126.4894 - val_loss: 164.2495\n",
      "Epoch 3576/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 141.7761 - val_loss: 193.2358\n",
      "Epoch 3577/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.2630 - val_loss: 143.4059\n",
      "Epoch 3578/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.5537 - val_loss: 150.9728\n",
      "Epoch 3579/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 165.1658 - val_loss: 261.7627\n",
      "Epoch 3580/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 221.8935 - val_loss: 165.8251\n",
      "Epoch 3581/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 133.6031 - val_loss: 141.8782\n",
      "Epoch 3582/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.8797 - val_loss: 144.7811\n",
      "Epoch 3583/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.2389 - val_loss: 141.1543\n",
      "Epoch 3584/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 122.6291 - val_loss: 147.3007\n",
      "Epoch 3585/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 134.2925 - val_loss: 149.3047\n",
      "Epoch 3586/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 128.7152 - val_loss: 146.1930\n",
      "Epoch 3587/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 147.0329 - val_loss: 157.5794\n",
      "Epoch 3588/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 127.6886 - val_loss: 147.6416\n",
      "Epoch 3589/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 124.1974 - val_loss: 148.5657\n",
      "Epoch 3590/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 130.7191 - val_loss: 233.2536\n",
      "Epoch 3591/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 128.4071 - val_loss: 176.7145\n",
      "Epoch 3592/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.7597 - val_loss: 142.5345\n",
      "Epoch 3593/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.7003 - val_loss: 150.3736\n",
      "Epoch 3594/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.8370 - val_loss: 148.9621\n",
      "Epoch 3595/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.6113 - val_loss: 147.0896\n",
      "Epoch 3596/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 131.2482 - val_loss: 162.5227\n",
      "Epoch 3597/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.3621 - val_loss: 152.4345\n",
      "Epoch 3598/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.4974 - val_loss: 151.2193\n",
      "Epoch 3599/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.8511 - val_loss: 145.7850\n",
      "Epoch 3600/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.1215 - val_loss: 160.6643\n",
      "Epoch 3601/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 165.3467 - val_loss: 167.9060\n",
      "Epoch 3602/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.1845 - val_loss: 165.6942\n",
      "Epoch 03602: early stopping\n",
      "Fold score (RMSE): 12.635539054870605\n",
      "Fold #3\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 8489.3264 - val_loss: 8034.1042\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 5159.1117 - val_loss: 4368.0654\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 4554.5820 - val_loss: 4414.4568\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 4471.6901 - val_loss: 4130.9264\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 4216.1460 - val_loss: 4053.6808\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 4163.8525 - val_loss: 4486.7732\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 3939.1268 - val_loss: 4750.2568\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 4004.8608 - val_loss: 3844.3595\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 3771.3485 - val_loss: 3792.8030\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 3469.6314 - val_loss: 3420.5357\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 3617.2384 - val_loss: 3254.7382\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 3199.1414 - val_loss: 3000.2180\n",
      "Epoch 13/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 3389.6165 - val_loss: 3299.7264\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 3087.8015 - val_loss: 3670.8164\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 2748.5436 - val_loss: 2330.4550\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 2557.1586 - val_loss: 2191.1363\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 2365.9014 - val_loss: 2519.3777\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 2116.3635 - val_loss: 1718.9250\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 1850.1166 - val_loss: 2302.3796\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 1636.6166 - val_loss: 1341.6860\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 1158.0014 - val_loss: 915.6222\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 1283.9139 - val_loss: 691.2011\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 940.6901 - val_loss: 619.3569\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 790.3125 - val_loss: 864.0292\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 761.3914 - val_loss: 1017.7713\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 666.9505 - val_loss: 580.6293\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 628.1535 - val_loss: 538.2256\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 771.4271 - val_loss: 1750.5947\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 722.1934 - val_loss: 539.6849\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 577.8821 - val_loss: 533.7512\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 591.8906 - val_loss: 521.7596\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 529.2475 - val_loss: 397.2163\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 630.5450 - val_loss: 411.0990\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 678.2427 - val_loss: 766.2997\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 745.3103 - val_loss: 547.6382\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 609.9208 - val_loss: 393.6039\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 493.7202 - val_loss: 376.6949\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 637.4190 - val_loss: 459.2092\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 478.0235 - val_loss: 652.3277\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 499.9337 - val_loss: 346.1181\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 445.0975 - val_loss: 438.0652\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 621.9779 - val_loss: 462.2788\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 505.8809 - val_loss: 360.5432\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 479.7016 - val_loss: 401.7448\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 572.9518 - val_loss: 405.2697\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 483.2804 - val_loss: 402.4484\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 433.8869 - val_loss: 300.9568\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 402.2756 - val_loss: 457.0522\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 399.1896 - val_loss: 324.9910\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 460.2749 - val_loss: 767.9239\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 471.3755 - val_loss: 616.4158\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 394.7669 - val_loss: 319.7934\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 380.3663 - val_loss: 285.1359\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 405.0325 - val_loss: 514.5096\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 508.1433 - val_loss: 392.1685\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 483.7572 - val_loss: 537.4546\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 449.1204 - val_loss: 436.0119\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 527.6718 - val_loss: 400.4764\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 413.8261 - val_loss: 383.3578\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 404.1537 - val_loss: 340.6452\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 353.5648 - val_loss: 308.4484\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 412.8699 - val_loss: 288.5194\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 345.8497 - val_loss: 357.6954\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 416.9101 - val_loss: 271.9083\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 415.3329 - val_loss: 2276.6697\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 428.5679 - val_loss: 309.2295\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 316.3906 - val_loss: 255.4944\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 435.1136 - val_loss: 969.4630\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 352.4715 - val_loss: 255.5798\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 347.7576 - val_loss: 1561.8300\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 382.5932 - val_loss: 284.4139\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 348.9974 - val_loss: 284.3272\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 364.7395 - val_loss: 551.2843\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 381.4204 - val_loss: 402.2908\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 386.6579 - val_loss: 269.0135\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 342.6054 - val_loss: 280.8873\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 331.3319 - val_loss: 237.3737\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 267.6899 - val_loss: 322.0386\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 364.2455 - val_loss: 284.2958\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 426.2003 - val_loss: 344.1362\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 332.5709 - val_loss: 235.2079\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 451.7176 - val_loss: 256.0542\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 317.6102 - val_loss: 289.4445\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 353.2309 - val_loss: 238.0087\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 315.4972 - val_loss: 282.5756\n",
      "Epoch 86/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 307.9757 - val_loss: 239.8633\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 347.3467 - val_loss: 517.2294\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 345.8289 - val_loss: 293.5649\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 379.7392 - val_loss: 356.7208\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 345.6348 - val_loss: 266.7515\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 328.1947 - val_loss: 282.8056\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 1s 151us/step - loss: 426.5589 - val_loss: 301.5036\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 258.9248 - val_loss: 608.4917\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 299.0721 - val_loss: 479.6271\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 296.3037 - val_loss: 468.5646\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 302.5715 - val_loss: 224.2353\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 327.7955 - val_loss: 353.5902\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 289.3856 - val_loss: 231.2970\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 334.0689 - val_loss: 272.5046\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 277.6059 - val_loss: 261.1093\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 282.1440 - val_loss: 253.1751\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 318.8638 - val_loss: 240.4557\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 305.3985 - val_loss: 296.2778\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 325.2006 - val_loss: 297.4941\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 324.3681 - val_loss: 240.9533\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 336.6052 - val_loss: 297.8163\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 346.3836 - val_loss: 271.1373\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 259.8555 - val_loss: 324.8282\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 308.1411 - val_loss: 208.0407\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 280.5375 - val_loss: 222.2323\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 331.2052 - val_loss: 203.0669\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 293.0223 - val_loss: 253.6219\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 356.2572 - val_loss: 327.1945\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 276.3344 - val_loss: 473.5583\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 258.6658 - val_loss: 205.5947\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 269.3849 - val_loss: 422.8409\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 366.3914 - val_loss: 362.0960\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 296.9233 - val_loss: 212.4903\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 273.5402 - val_loss: 197.8833\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 386.9110 - val_loss: 393.0621\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 1s 130us/step - loss: 252.7954 - val_loss: 211.6067\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 259.3831 - val_loss: 223.1963\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 1s 145us/step - loss: 293.9177 - val_loss: 240.6956\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 268.2854 - val_loss: 382.5125\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 303.4203 - val_loss: 327.4450\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 268.2764 - val_loss: 287.6377\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 252.7648 - val_loss: 266.1397\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 280.8070 - val_loss: 196.0081\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 1s 126us/step - loss: 264.8498 - val_loss: 261.8274\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 429.1850 - val_loss: 203.8595\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 263.4093 - val_loss: 258.5913\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 269.5613 - val_loss: 233.5064\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 248.8655 - val_loss: 234.6840\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 268.7592 - val_loss: 203.8339\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 288.7891 - val_loss: 257.8270\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 375.1937 - val_loss: 195.4294\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 256.7936 - val_loss: 312.3990\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 241.1043 - val_loss: 194.6614\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 263.8028 - val_loss: 354.6673\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 344.4679 - val_loss: 267.3762\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 260.7782 - val_loss: 486.3154\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 270.3123 - val_loss: 186.5384\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 337.7700 - val_loss: 445.7163\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 371.8420 - val_loss: 230.9536\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 240.1255 - val_loss: 274.9989\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 307.9551 - val_loss: 443.3659\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 249.3167 - val_loss: 361.1098\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 272.9115 - val_loss: 259.3749\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 365.8654 - val_loss: 248.0119\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 234.1883 - val_loss: 208.6124\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 248.3063 - val_loss: 442.5100\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 256.8920 - val_loss: 191.1257\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 267.3759 - val_loss: 291.8321\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 237.7681 - val_loss: 237.7738\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 227.9803 - val_loss: 297.2975\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 331.9257 - val_loss: 203.7413\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 279.6917 - val_loss: 254.0147\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 272.5591 - val_loss: 188.6495\n",
      "Epoch 159/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 201.1418 - val_loss: 194.3589\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 234.2213 - val_loss: 179.6660\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 312.3279 - val_loss: 191.3684\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 238.4222 - val_loss: 344.3064\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 252.5145 - val_loss: 609.7966\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 247.6491 - val_loss: 552.9512\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 274.8466 - val_loss: 218.8840\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 320.1896 - val_loss: 606.0091\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 258.7777 - val_loss: 253.7245\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 299.3946 - val_loss: 257.1519\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 346.5711 - val_loss: 237.1134\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 255.0709 - val_loss: 1125.0672\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 275.2960 - val_loss: 185.8435\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 251.0776 - val_loss: 241.4236\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 280.3963 - val_loss: 445.0824\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 275.9375 - val_loss: 178.6677\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 267.4907 - val_loss: 195.0092\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 238.3901 - val_loss: 239.4410\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 246.4509 - val_loss: 186.1507\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 230.6771 - val_loss: 200.5547\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 217.3072 - val_loss: 272.4502\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 242.0725 - val_loss: 384.6576\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 274.7151 - val_loss: 215.9733\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 202.0095 - val_loss: 186.0527\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 214.1963 - val_loss: 178.1337\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 297.1800 - val_loss: 251.1598\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 360.4488 - val_loss: 953.4382\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 270.7925 - val_loss: 264.9241\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 209.6755 - val_loss: 210.4818\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 275.8824 - val_loss: 392.5378\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 267.6581 - val_loss: 182.8163\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 194.7218 - val_loss: 243.1445\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 264.5446 - val_loss: 239.0503\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 237.8250 - val_loss: 195.0595\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 244.5041 - val_loss: 189.3656\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 230.9198 - val_loss: 419.6630\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 268.1880 - val_loss: 197.6487\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 330.9524 - val_loss: 230.7378\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 226.4160 - val_loss: 173.4502\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 277.9865 - val_loss: 670.2107\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 262.0169 - val_loss: 198.9248\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 202.6767 - val_loss: 218.5118\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 295.5109 - val_loss: 357.5070\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 242.5797 - val_loss: 173.4674\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 211.4974 - val_loss: 366.1173\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 220.2324 - val_loss: 174.9578\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 251.7464 - val_loss: 179.6727\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 225.5622 - val_loss: 169.2396\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 234.8886 - val_loss: 159.5108\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 265.2002 - val_loss: 173.3712\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 210.7381 - val_loss: 182.4445\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 290.0504 - val_loss: 167.0293\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 212.4928 - val_loss: 178.4318\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 192.6549 - val_loss: 438.7553\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 217.6101 - val_loss: 165.7483\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 485.5941 - val_loss: 341.4311\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 247.6162 - val_loss: 206.7324\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 241.1146 - val_loss: 342.9979\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 240.2267 - val_loss: 174.3951\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 221.9352 - val_loss: 172.2286\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 213.3131 - val_loss: 294.1838\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 307.7508 - val_loss: 174.4716\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 266.3239 - val_loss: 404.8748\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 237.2163 - val_loss: 215.5438\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 230.1049 - val_loss: 449.3892\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 282.3715 - val_loss: 185.8992\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 247.6978 - val_loss: 747.8682\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 322.5628 - val_loss: 241.4746\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 236.0917 - val_loss: 168.5528\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 219.1522 - val_loss: 182.1467\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 198.4271 - val_loss: 203.2613\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 199.8009 - val_loss: 158.0853\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 233.1270 - val_loss: 332.9762\n",
      "Epoch 232/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 56us/step - loss: 252.1015 - val_loss: 202.6768\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 233.5232 - val_loss: 191.0659\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 209.1969 - val_loss: 241.7855\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 200.6056 - val_loss: 176.1518\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 198.3656 - val_loss: 155.1761\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 180.1549 - val_loss: 178.4275\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 223.4903 - val_loss: 215.3043\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 231.4960 - val_loss: 327.4701\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 427.0858 - val_loss: 515.7140\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 405.3879 - val_loss: 235.6341- ETA: 0s - loss: 49\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 261.7246 - val_loss: 211.5587\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 226.2924 - val_loss: 161.8107\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 308.8912 - val_loss: 192.9814\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 195.7656 - val_loss: 174.6302\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 272.5951 - val_loss: 184.3110\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 223.0662 - val_loss: 295.1913\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 209.8739 - val_loss: 173.7196\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 197.6001 - val_loss: 237.1905\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 235.4204 - val_loss: 190.8867\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 182.3460 - val_loss: 200.9097\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 268.0630 - val_loss: 170.1390\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 244.2123 - val_loss: 210.0572\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 192.9174 - val_loss: 182.7139\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 205.5211 - val_loss: 158.7913\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 329.5489 - val_loss: 212.0631\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 249.9540 - val_loss: 219.5642\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 212.3797 - val_loss: 168.1462\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 225.0275 - val_loss: 165.8927\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 267.8969 - val_loss: 213.6395\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 206.2476 - val_loss: 184.7337\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 350.3702 - val_loss: 660.3541\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 215.6162 - val_loss: 181.9790\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 184.8603 - val_loss: 160.9248\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 216.8985 - val_loss: 231.0054\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 186.3145 - val_loss: 215.1377\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 389.4904 - val_loss: 325.7586\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 271.2398 - val_loss: 219.1542\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 176.9767 - val_loss: 153.5756\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 271.8422 - val_loss: 186.3086\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 194.7893 - val_loss: 157.4619\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 208.4910 - val_loss: 160.9318\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 191.2811 - val_loss: 165.5204\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 220.4997 - val_loss: 262.2696\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 348.0723 - val_loss: 435.2409\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 247.2002 - val_loss: 522.4521\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 255.7788 - val_loss: 163.2781\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 360.7822 - val_loss: 249.5031\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 232.6168 - val_loss: 158.9653\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 192.5738 - val_loss: 291.2482\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 215.9210 - val_loss: 243.1144\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 181.4356 - val_loss: 153.4044\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 201.1689 - val_loss: 159.2196\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 206.7037 - val_loss: 299.7415\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 207.9300 - val_loss: 184.4513\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 207.6485 - val_loss: 1080.9107\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 235.8379 - val_loss: 170.5027\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 237.0541 - val_loss: 166.0224\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 197.8306 - val_loss: 165.8972\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 217.3883 - val_loss: 245.9703\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.9004 - val_loss: 147.9863\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 264.6130 - val_loss: 201.5659\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 319.5149 - val_loss: 184.4386\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 205.4974 - val_loss: 157.3255\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 188.5147 - val_loss: 199.6892\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 245.9533 - val_loss: 178.7291\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 173.4012 - val_loss: 174.2025\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 250.6547 - val_loss: 538.9988\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 201.9014 - val_loss: 174.6149\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 194.6142 - val_loss: 356.7589\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 296.8952 - val_loss: 205.3386\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 186.3405 - val_loss: 176.8788\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 196.5061 - val_loss: 166.6553\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 217.9863 - val_loss: 155.1007\n",
      "Epoch 305/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 75us/step - loss: 211.4812 - val_loss: 375.7176\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 358.2307 - val_loss: 270.6836\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 189.8803 - val_loss: 157.3390\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 189.9946 - val_loss: 152.0392\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 199.9592 - val_loss: 232.6072\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 199.1060 - val_loss: 261.7618\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 226.3344 - val_loss: 174.9738\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 187.2052 - val_loss: 151.6509\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 183.9997 - val_loss: 202.0261\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 195.4802 - val_loss: 242.9905\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 173.2522 - val_loss: 156.3147\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 486.1978 - val_loss: 365.4967\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 342.7826 - val_loss: 226.8902\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 280.6185 - val_loss: 203.6765\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 327.2756 - val_loss: 205.2881\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 279.2416 - val_loss: 223.7920\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 226.4999 - val_loss: 403.0493\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 233.2233 - val_loss: 335.0587\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 249.7787 - val_loss: 405.9326\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 221.3577 - val_loss: 330.0966\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 291.0992 - val_loss: 244.6552\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 209.6439 - val_loss: 314.8113\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 231.4752 - val_loss: 166.4472\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 221.3360 - val_loss: 177.8818\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 222.8140 - val_loss: 283.5630\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 198.9442 - val_loss: 244.7518\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 192.8787 - val_loss: 216.1509\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 203.1249 - val_loss: 189.6922\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 214.8922 - val_loss: 784.6460\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 221.3484 - val_loss: 192.0383\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 246.0141 - val_loss: 228.0650\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 224.3917 - val_loss: 218.5299\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 203.5291 - val_loss: 205.9441\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 211.0475 - val_loss: 156.7566\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 185.7873 - val_loss: 160.1699\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 231.1815 - val_loss: 204.7193\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 225.0838 - val_loss: 265.9448\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 195.0652 - val_loss: 243.4468\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 189.5805 - val_loss: 151.8246\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 185.2855 - val_loss: 153.3733\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 211.2082 - val_loss: 211.0889\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 186.0128 - val_loss: 186.2263\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 208.6144 - val_loss: 231.5799\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 191.0763 - val_loss: 226.5809\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 238.3631 - val_loss: 176.7261\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 182.7165 - val_loss: 167.2398\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 177.6701 - val_loss: 258.0691\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 198.2834 - val_loss: 246.4796\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 184.0475 - val_loss: 168.6436\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 201.5898 - val_loss: 231.9909\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 184.8225 - val_loss: 188.3823\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 189.3041 - val_loss: 164.6537\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 193.5849 - val_loss: 185.2927\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 182.8126 - val_loss: 254.6434\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 241.1683 - val_loss: 167.4423\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 181.0500 - val_loss: 167.6762\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 199.9484 - val_loss: 159.9604\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 187.8192 - val_loss: 211.1084\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 188.1847 - val_loss: 187.9662\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 305.3144 - val_loss: 174.7593\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 267.9352 - val_loss: 1170.4966\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 252.2809 - val_loss: 272.6299\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 181.6763 - val_loss: 159.0605\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 206.8477 - val_loss: 165.9022\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 192.1917 - val_loss: 170.4403\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 215.8369 - val_loss: 443.6983\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 192.1854 - val_loss: 181.5553\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 184.7860 - val_loss: 253.4744\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 207.7060 - val_loss: 167.2428\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 181.3770 - val_loss: 249.3877\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 222.0457 - val_loss: 444.9633\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 194.2221 - val_loss: 150.0938\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 177.6905 - val_loss: 181.5397\n",
      "Epoch 378/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 55us/step - loss: 223.9776 - val_loss: 312.5183\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 173.7211 - val_loss: 149.3903\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 215.1045 - val_loss: 237.6312\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 210.7100 - val_loss: 186.5925\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 205.5354 - val_loss: 188.4781\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 178.2543 - val_loss: 211.8766\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 178.2525 - val_loss: 171.1527\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 203.5701 - val_loss: 150.1442\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 176.3961 - val_loss: 149.2395\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 281.0837 - val_loss: 204.9069\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 252.9259 - val_loss: 211.3564\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 242.6542 - val_loss: 230.8690\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 184.8942 - val_loss: 169.2612\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 170.7562 - val_loss: 161.5053\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 239.1950 - val_loss: 317.2722\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 214.0041 - val_loss: 216.9610\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 175.0211 - val_loss: 156.4041\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 167.5788 - val_loss: 148.4358\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 194.3359 - val_loss: 334.3038\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 183.4773 - val_loss: 551.4432\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 248.6325 - val_loss: 188.5883\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 189.2253 - val_loss: 151.7647\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.8790 - val_loss: 217.1770\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 198.7447 - val_loss: 145.1186\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 192.4030 - val_loss: 302.9225\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 183.2559 - val_loss: 173.4739\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 212.0289 - val_loss: 164.3493\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 183.4556 - val_loss: 163.9377\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 176.7464 - val_loss: 203.4930\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 390.9990 - val_loss: 208.9171\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 234.3945 - val_loss: 298.7390\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 234.0619 - val_loss: 220.9559\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 209.8930 - val_loss: 165.6843\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 224.8411 - val_loss: 360.3769\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 221.3877 - val_loss: 153.7375\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 206.6874 - val_loss: 219.2519\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 242.7326 - val_loss: 153.3795\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 192.7143 - val_loss: 170.8903\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 186.4095 - val_loss: 183.4366\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 172.7021 - val_loss: 184.0462\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 233.4753 - val_loss: 184.7796\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 187.2919 - val_loss: 151.0245\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 175.8587 - val_loss: 159.0299\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 190.8602 - val_loss: 225.0177\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 183.7517 - val_loss: 187.0116\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 201.7037 - val_loss: 155.3195\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 199.8378 - val_loss: 183.7202\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 196.7316 - val_loss: 180.3444\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 256.7591 - val_loss: 151.3944\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 171.5753 - val_loss: 175.3258\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 163.3622 - val_loss: 220.0118\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 184.6394 - val_loss: 161.8195\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 169.9099 - val_loss: 165.6215\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 179.6656 - val_loss: 159.2729\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 164.5627 - val_loss: 168.2846\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 192.3592 - val_loss: 226.8582\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 219.9299 - val_loss: 160.7855\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.4539 - val_loss: 223.9041\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 223.8623 - val_loss: 210.6297\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 202.0756 - val_loss: 283.1759\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 192.9090 - val_loss: 182.0985\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 186.6788 - val_loss: 148.7787\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 184.9901 - val_loss: 162.4825\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 200.1863 - val_loss: 158.7967\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 180.0700 - val_loss: 158.8137\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 188.8445 - val_loss: 161.3193\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 209.2588 - val_loss: 143.7703\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 201.6284 - val_loss: 145.3639\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 165.0410 - val_loss: 156.6967\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.5170 - val_loss: 174.6363\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 179.2265 - val_loss: 147.8325\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 180.5519 - val_loss: 143.9258\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 157.4531 - val_loss: 162.7248\n",
      "Epoch 451/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 69us/step - loss: 209.2186 - val_loss: 162.5092\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 192.4923 - val_loss: 194.6197\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 200.0534 - val_loss: 160.2928\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 212.8356 - val_loss: 243.0372\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 178.6381 - val_loss: 183.3516\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 186.8211 - val_loss: 229.6858\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 171.4890 - val_loss: 155.3246\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 195.2192 - val_loss: 176.7515\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 178.7496 - val_loss: 142.8622\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 175.3579 - val_loss: 288.8144\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 174.3646 - val_loss: 157.2424\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 187.5467 - val_loss: 198.0823\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 200.5339 - val_loss: 188.8681\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 184.6063 - val_loss: 161.1312\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 183.6338 - val_loss: 186.2182\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 180.2553 - val_loss: 205.7220\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 190.6861 - val_loss: 316.4532\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 181.3222 - val_loss: 147.8834\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 194.3044 - val_loss: 241.4814\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 186.8840 - val_loss: 192.8203\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 169.8868 - val_loss: 160.1124\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 166.1738 - val_loss: 166.3218\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 190.6060 - val_loss: 166.1921\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 187.6196 - val_loss: 139.2297\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 162.9002 - val_loss: 169.5076\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 247.8876 - val_loss: 220.5313\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 204.5411 - val_loss: 143.7378\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 176.5663 - val_loss: 150.8595\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 190.8245 - val_loss: 177.4022\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 174.0485 - val_loss: 181.5849\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 186.9000 - val_loss: 156.4429\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 196.7856 - val_loss: 216.4102\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 175.7039 - val_loss: 156.6323\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 196.7093 - val_loss: 263.6991\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 167.2539 - val_loss: 210.9834\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 186.7843 - val_loss: 191.6694\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 199.3704 - val_loss: 173.6061\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 186.3292 - val_loss: 158.1204\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 178.9980 - val_loss: 192.4788\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 169.7691 - val_loss: 205.2220\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 188.9222 - val_loss: 216.1510\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 180.8304 - val_loss: 199.7585\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 188.6368 - val_loss: 181.5833\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 199.5347 - val_loss: 161.6402\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 182.3876 - val_loss: 148.4770\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 173.5738 - val_loss: 212.3344\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 169.5492 - val_loss: 350.7468\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 201.4368 - val_loss: 263.9184\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 171.7137 - val_loss: 492.6760\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.8590 - val_loss: 227.2216\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 293.5642 - val_loss: 189.7733\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 166.1931 - val_loss: 161.0139\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 160.8670 - val_loss: 158.4268\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 162.2633 - val_loss: 426.5490\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 181.8659 - val_loss: 219.4333\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 204.1581 - val_loss: 145.4655\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 167.4793 - val_loss: 251.1759\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 169.1275 - val_loss: 148.5561\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 161.4583 - val_loss: 154.6963\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 166.2254 - val_loss: 310.2210\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 226.0945 - val_loss: 195.8175\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 185.9590 - val_loss: 187.2575\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 438.5506 - val_loss: 190.7648\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 182.0182 - val_loss: 198.4983\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 164.8045 - val_loss: 160.4070\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 151.4344 - val_loss: 163.5989\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 167.1466 - val_loss: 142.2756\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.2169 - val_loss: 144.1765\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 196.9758 - val_loss: 145.0410\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 160.0074 - val_loss: 139.8234\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.6441 - val_loss: 342.4724\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 219.2668 - val_loss: 154.3242\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.6403 - val_loss: 154.6282\n",
      "Epoch 524/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.2546 - val_loss: 149.2346\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 159.4715 - val_loss: 140.1505\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 196.0802 - val_loss: 199.3034\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 167.6565 - val_loss: 168.0760\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 195.0557 - val_loss: 161.6178\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 227.3438 - val_loss: 174.1175\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 164.1502 - val_loss: 182.5777\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.0975 - val_loss: 138.5830\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 158.1125 - val_loss: 178.1845\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 170.8260 - val_loss: 161.6275\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 229.0758 - val_loss: 169.4103\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.8407 - val_loss: 168.8649\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 179.4977 - val_loss: 242.0621\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 165.8767 - val_loss: 142.8012\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 202.8506 - val_loss: 174.0007\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 180.0102 - val_loss: 196.0728\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 225.4216 - val_loss: 149.9925\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 179.0165 - val_loss: 266.6632\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.4005 - val_loss: 154.3990\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.8157 - val_loss: 161.3090\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 205.2942 - val_loss: 207.5698\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.8653 - val_loss: 136.1774\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 156.9800 - val_loss: 185.9070\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.7141 - val_loss: 202.4201\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 297.5457 - val_loss: 156.4122\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 186.1706 - val_loss: 153.7874\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.9084 - val_loss: 216.5669\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 163.4520 - val_loss: 141.9151\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.6111 - val_loss: 165.5895\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 172.1810 - val_loss: 168.9448\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 178.3455 - val_loss: 161.4562\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 169.7200 - val_loss: 166.3920\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 185.7386 - val_loss: 145.6569\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 171.3151 - val_loss: 182.4243\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 152.2818 - val_loss: 139.0646\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 161.1149 - val_loss: 138.2357\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 184.6424 - val_loss: 216.4300\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 176.9546 - val_loss: 200.5110\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 196.1017 - val_loss: 152.5675\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 223.7947 - val_loss: 192.1357\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 187.6128 - val_loss: 152.4380\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 167.9010 - val_loss: 146.8058\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 149.4586 - val_loss: 188.4213\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 173.8162 - val_loss: 201.7517\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 182.8687 - val_loss: 138.8549\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 202.6054 - val_loss: 153.1906\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 218.8261 - val_loss: 164.6982\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 203.6157 - val_loss: 187.3203\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 203.9148 - val_loss: 159.0135\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.7139 - val_loss: 165.6556\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 165.3338 - val_loss: 162.3748\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 201.6059 - val_loss: 165.6054\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 171.5900 - val_loss: 169.8335\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 145.4106 - val_loss: 192.4970\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 165.2711 - val_loss: 235.3640\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 169.4864 - val_loss: 148.9437\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 169.5760 - val_loss: 163.9330\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 169.3751 - val_loss: 197.9017\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 230.0307 - val_loss: 165.3608\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 197.4222 - val_loss: 145.3399\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 170.5384 - val_loss: 142.7388\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 183.1128 - val_loss: 166.5703\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.9455 - val_loss: 144.7055\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 179.9700 - val_loss: 155.9396\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 174.3945 - val_loss: 303.3349\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 173.8013 - val_loss: 142.6818\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 173.4138 - val_loss: 149.2188\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.9952 - val_loss: 206.1603\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 223.5623 - val_loss: 1717.6339\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 304.7858 - val_loss: 153.3787\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 178.8091 - val_loss: 174.9480\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 194.4772 - val_loss: 358.5728\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.9414 - val_loss: 147.2050\n",
      "Epoch 597/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 193.0276 - val_loss: 609.7459\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 172.7467 - val_loss: 163.8528\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 181.1793 - val_loss: 144.8809\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 170.5466 - val_loss: 195.8657\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 171.0065 - val_loss: 173.0581\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 178.8242 - val_loss: 157.1524\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.4023 - val_loss: 163.8794\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 188.3137 - val_loss: 147.3948\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.3795 - val_loss: 173.6627\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 177.7782 - val_loss: 163.7034\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 162.8538 - val_loss: 145.7182\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 160.5057 - val_loss: 145.4767\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.8284 - val_loss: 171.0910\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 249.0423 - val_loss: 190.1027\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 165.8235 - val_loss: 162.0019\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 158.6920 - val_loss: 139.2614\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 158.6166 - val_loss: 134.9961\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 158.6242 - val_loss: 168.1304\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 155.5487 - val_loss: 134.9869\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 157.4910 - val_loss: 419.2374\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 175.5005 - val_loss: 140.7433\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 158.5913 - val_loss: 145.6592\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 174.9414 - val_loss: 186.6579\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.6745 - val_loss: 137.0577\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 210.9806 - val_loss: 146.3584\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 153.6528 - val_loss: 143.1627\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 166.5463 - val_loss: 164.6509\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 167.8960 - val_loss: 200.4080\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 185.3202 - val_loss: 259.5758\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 195.2631 - val_loss: 228.5071\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 169.1293 - val_loss: 154.6757\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 183.6441 - val_loss: 312.0906\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 274.1488 - val_loss: 254.6252\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 193.1727 - val_loss: 262.7342\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 162.8641 - val_loss: 333.9574\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 178.5104 - val_loss: 149.7211- ETA: 0s - loss: 18\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 157.5582 - val_loss: 141.4330\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 178.3748 - val_loss: 150.8423\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.0085 - val_loss: 192.9833\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 161.6597 - val_loss: 162.4017\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 161.6178 - val_loss: 198.3064\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 181.6918 - val_loss: 311.2580\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 208.0618 - val_loss: 270.8411\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 181.5625 - val_loss: 141.1533\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 165.2859 - val_loss: 143.6816\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 276.0703 - val_loss: 152.3435\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 159.2439 - val_loss: 187.3084\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 152.4537 - val_loss: 156.1528\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 153.3895 - val_loss: 140.6167\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 164.9583 - val_loss: 158.5058\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 217.3805 - val_loss: 161.5244\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 168.6322 - val_loss: 210.3292\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 175.9859 - val_loss: 153.0004\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 140.543 - 0s 57us/step - loss: 153.4257 - val_loss: 160.0354\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 168.4655 - val_loss: 160.6474\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 158.8882 - val_loss: 134.7577\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 165.9323 - val_loss: 174.9885\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 154.5131 - val_loss: 141.1581\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 182.3428 - val_loss: 152.4926\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 164.8963 - val_loss: 137.6184\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.2273 - val_loss: 148.8420\n",
      "Epoch 658/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 185.8587 - val_loss: 153.3166\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 196.7152 - val_loss: 212.5604\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.6730 - val_loss: 174.3483\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 181.6321 - val_loss: 151.3359\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.9593 - val_loss: 149.5318\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 185.7857 - val_loss: 155.0138\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 195.3560 - val_loss: 180.8584\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.0627 - val_loss: 135.5586\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.5612 - val_loss: 155.0085\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 164.2918 - val_loss: 145.1971\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 163.8790 - val_loss: 153.4148\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 173.3842 - val_loss: 189.8047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 188.2005 - val_loss: 153.6211\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.8400 - val_loss: 149.1818\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 244.5178 - val_loss: 204.8298\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 164.4414 - val_loss: 132.4918\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 163.5907 - val_loss: 269.8949\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 188.7906 - val_loss: 328.3892\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 185.8646 - val_loss: 145.4140\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 154.3553 - val_loss: 145.8379\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 238.1984 - val_loss: 139.8182\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.9236 - val_loss: 179.1615\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 158.605 - 1s 70us/step - loss: 167.8176 - val_loss: 144.3365\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.4460 - val_loss: 164.4912\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 161.4546 - val_loss: 138.5499\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.3553 - val_loss: 165.7896\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.3398 - val_loss: 147.6423\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.1658 - val_loss: 157.6468\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 180.1274 - val_loss: 202.9929\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 162.0179 - val_loss: 141.5791\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 166.2982 - val_loss: 141.3033\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 162.9093 - val_loss: 156.9840\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 164.3743 - val_loss: 157.0631\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 170.0464 - val_loss: 142.0206\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.3462 - val_loss: 137.3045\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 156.8993 - val_loss: 171.8424\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 199.6834 - val_loss: 148.9739\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 177.1968 - val_loss: 150.0133\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 165.5664 - val_loss: 131.1316\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 152.7345 - val_loss: 153.0686\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 158.3489 - val_loss: 224.1613\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 161.5149 - val_loss: 172.2100\n",
      "Epoch 700/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 176.5405 - val_loss: 182.6572\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 175.9858 - val_loss: 161.0639\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 184.9216 - val_loss: 310.3511\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.9438 - val_loss: 145.1475\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.1879 - val_loss: 206.6123\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 154.5494 - val_loss: 176.7593\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.2772 - val_loss: 173.3122\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 186.9106 - val_loss: 141.3823\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 149.1602 - val_loss: 180.8861\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 166.3568 - val_loss: 135.2520\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 250.3816 - val_loss: 190.2023\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.6472 - val_loss: 202.2247\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.3037 - val_loss: 139.8584\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 186.8138 - val_loss: 142.3359\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 154.4387 - val_loss: 233.8098\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 182.0191 - val_loss: 141.3281\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 147.2497 - val_loss: 139.9226\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 176.1593 - val_loss: 216.5045\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 149.9151 - val_loss: 133.2298\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 178.3894 - val_loss: 228.6862\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 171.1722 - val_loss: 147.0508\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 160.7621 - val_loss: 200.9984\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 175.6541 - val_loss: 170.2626\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 170.7827 - val_loss: 142.8717\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 163.5707 - val_loss: 136.7675\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 148.7782 - val_loss: 148.6065\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 212.3927 - val_loss: 173.0201\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 153.1791 - val_loss: 137.7961\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 149.5497 - val_loss: 141.5954\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 150.0662 - val_loss: 140.5675\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 168.9622 - val_loss: 153.5448\n",
      "Epoch 731/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.8789 - val_loss: 152.3619\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 179.5568 - val_loss: 175.9988\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 163.9257 - val_loss: 163.3041\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 170.3789 - val_loss: 143.8535\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 151.5986 - val_loss: 136.9583\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 147.2199 - val_loss: 142.7147\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.8799 - val_loss: 138.0286\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 191.0250 - val_loss: 230.7530\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 217.1404 - val_loss: 214.8018\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 173.9924 - val_loss: 179.6469\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.2680 - val_loss: 150.7128\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 182.2572 - val_loss: 152.2323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 184.8990 - val_loss: 173.5317\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 155.5603 - val_loss: 160.9919\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 236.7799 - val_loss: 148.1510\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 160.9043 - val_loss: 152.8498\n",
      "Epoch 747/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 148.3211 - val_loss: 231.6761\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 151.9297 - val_loss: 152.9090\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 147.1609 - val_loss: 148.8694\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 183.4425 - val_loss: 147.7290\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 160.1576 - val_loss: 152.5419\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 199.5057 - val_loss: 143.2765\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 151.8691 - val_loss: 133.2322\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 188.4903 - val_loss: 188.2191\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 160.1662 - val_loss: 136.4269\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 151.9649 - val_loss: 390.2489\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 152.6145 - val_loss: 146.1195\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 154.7069 - val_loss: 138.6340\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 167.4152 - val_loss: 190.2537\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.9862 - val_loss: 153.2596\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 166.3625 - val_loss: 236.8177\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 183.6637 - val_loss: 208.2736\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.8381 - val_loss: 142.3969\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 150.7715 - val_loss: 203.4679\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 162.6614 - val_loss: 174.6145\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.8149 - val_loss: 159.0958\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 207.2814 - val_loss: 312.0362\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 185.8675 - val_loss: 150.8814\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 171.3111 - val_loss: 142.7901\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.0590 - val_loss: 148.4907\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 157.3500 - val_loss: 169.7020\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 171.8331 - val_loss: 202.1616\n",
      "Epoch 773/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.2688 - val_loss: 140.6664\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 168.9105 - val_loss: 170.1806\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.5216 - val_loss: 158.8073\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 173.9330 - val_loss: 139.4236\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 153.6447 - val_loss: 171.0166\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 2s 247us/step - loss: 156.7972 - val_loss: 214.6085\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 1s 163us/step - loss: 146.0873 - val_loss: 167.0558\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 165.7141 - val_loss: 147.8497\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 160.2187 - val_loss: 137.8040\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.9386 - val_loss: 160.8255\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 153.0592 - val_loss: 201.8071- ETA: 0s - loss:\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 184.8186 - val_loss: 177.4412\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 162.0370 - val_loss: 217.2120\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 143.9771 - val_loss: 199.2741\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.5821 - val_loss: 193.6173\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 151.5604 - val_loss: 167.4119\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 150.7146 - val_loss: 188.3880\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 154.3247 - val_loss: 275.0249\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 181.9078 - val_loss: 165.4987\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 204.0495 - val_loss: 141.9804\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 148.9624 - val_loss: 255.2959\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.2719 - val_loss: 141.8236\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 152.4614 - val_loss: 185.9147\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.7369 - val_loss: 172.6962\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 147.1410 - val_loss: 137.3855\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 156.6889 - val_loss: 177.1891\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 187.4498 - val_loss: 221.7472\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 281.4723 - val_loss: 234.5492\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 213.9616 - val_loss: 162.2842\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 142.8022 - val_loss: 138.3961\n",
      "Epoch 803/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 245.7979 - val_loss: 425.9112\n",
      "Epoch 804/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 310.2150 - val_loss: 170.3003\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 192.155 - 1s 65us/step - loss: 191.8737 - val_loss: 163.0665\n",
      "Epoch 806/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 189.5459 - val_loss: 163.1677\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 167.0462 - val_loss: 216.4674\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 182.8456 - val_loss: 170.4302\n",
      "Epoch 809/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 201.9271 - val_loss: 186.7430\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 161.5763 - val_loss: 154.5712\n",
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 175.9990 - val_loss: 154.6902\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 179.9627 - val_loss: 209.7921\n",
      "Epoch 813/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 162.5926 - val_loss: 140.1772\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 165.6629 - val_loss: 212.7960\n",
      "Epoch 815/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 79us/step - loss: 162.9873 - val_loss: 137.4737\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 175.2100 - val_loss: 153.2990\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.3035 - val_loss: 152.6633\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 158.0299 - val_loss: 259.9279\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.1136 - val_loss: 139.9655\n",
      "Epoch 820/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 170.8742 - val_loss: 143.9373\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.8402 - val_loss: 197.2846\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 194.8019 - val_loss: 175.0054\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 149.9635 - val_loss: 136.3059\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 149.4396 - val_loss: 151.7143\n",
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.0483 - val_loss: 201.1728\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 168.8174 - val_loss: 139.7015\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 160.8563 - val_loss: 151.3351\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.2765 - val_loss: 140.0512\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.6131 - val_loss: 136.9846\n",
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 173.4376 - val_loss: 161.8387\n",
      "Epoch 831/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 155.8811 - val_loss: 139.1440\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 156.6958 - val_loss: 175.9419\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.3678 - val_loss: 156.1504\n",
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 150.4118 - val_loss: 198.0817\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 192.2000 - val_loss: 184.7640\n",
      "Epoch 836/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 190.0190 - val_loss: 169.4241\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 147.5709 - val_loss: 139.5442\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 155.4985 - val_loss: 156.9684\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 200.4478 - val_loss: 169.0531\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 157.5316 - val_loss: 150.1763\n",
      "Epoch 841/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 151.9302 - val_loss: 150.6161\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 148.8237 - val_loss: 133.3331\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 154.4382 - val_loss: 165.1567\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.5611 - val_loss: 196.0418\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 177.7886 - val_loss: 155.9451\n",
      "Epoch 846/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 151.4794 - val_loss: 150.2950\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 182.8646 - val_loss: 289.4263\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 160.5644 - val_loss: 172.1381\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 148.2164 - val_loss: 135.9898\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 165.2396 - val_loss: 168.5646\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 159.7627 - val_loss: 143.6709\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 149.3914 - val_loss: 136.1135\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 172.069 - 1s 83us/step - loss: 173.9647 - val_loss: 139.2242\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.8831 - val_loss: 171.4244\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.1153 - val_loss: 149.6960\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 184.3530 - val_loss: 137.4319\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.9427 - val_loss: 216.4411\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 155.4627 - val_loss: 152.4467\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 160.7519 - val_loss: 145.8667\n",
      "Epoch 860/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.1631 - val_loss: 136.9165\n",
      "Epoch 861/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 148.5555 - val_loss: 145.9320\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 228.7001 - val_loss: 175.3593\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 145.9699 - val_loss: 149.9794\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 153.7893 - val_loss: 138.3809\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 161.6436 - val_loss: 167.9968\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 155.4507 - val_loss: 152.7282\n",
      "Epoch 867/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 154.9203 - val_loss: 157.3120\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.9806 - val_loss: 143.7234\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 144.7381 - val_loss: 182.6966\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 229.6854 - val_loss: 170.4473\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 198.8141 - val_loss: 157.0971\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 177.0991 - val_loss: 170.7185\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 247.2162 - val_loss: 160.5193\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.3853 - val_loss: 160.1664\n",
      "Epoch 875/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.9394 - val_loss: 155.3709\n",
      "Epoch 876/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 236.1754 - val_loss: 154.8195\n",
      "Epoch 877/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 159.0509 - val_loss: 157.0907\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 184.8417 - val_loss: 147.9507\n",
      "Epoch 879/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 151.3950 - val_loss: 143.4359\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 167.1133 - val_loss: 183.9663\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.7245 - val_loss: 137.4770\n",
      "Epoch 882/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 149.0568 - val_loss: 161.6743\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 160.6224 - val_loss: 138.1931\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 170.7755 - val_loss: 173.5571\n",
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.8427 - val_loss: 150.8931\n",
      "Epoch 886/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 153.7937 - val_loss: 145.0366\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 154.6338 - val_loss: 170.7078\n",
      "Epoch 888/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.2797 - val_loss: 241.8753\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 179.2479 - val_loss: 244.4722\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 146.9156 - val_loss: 144.0704\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 167.8104 - val_loss: 184.0499\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 161.4897 - val_loss: 189.9250\n",
      "Epoch 893/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.9865 - val_loss: 154.0715\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 257.3533 - val_loss: 159.6106\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 147.8310 - val_loss: 189.7017\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.9635 - val_loss: 216.5977\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 153.8592 - val_loss: 138.0144\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 201.8318 - val_loss: 200.8796\n",
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 204.1400 - val_loss: 164.7976\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.5922 - val_loss: 150.1879\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 186.9688 - val_loss: 248.7773\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 212.9393 - val_loss: 186.4991\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 192.8310 - val_loss: 146.8097\n",
      "Epoch 904/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.7887 - val_loss: 178.9997\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 277.3780 - val_loss: 212.3594\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 177.7580 - val_loss: 145.2543\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 179.2380 - val_loss: 157.6690\n",
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 174.0108 - val_loss: 169.1422\n",
      "Epoch 909/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 163.8998 - val_loss: 150.6753\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 162.7688 - val_loss: 151.6353\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 167.4203 - val_loss: 221.5594\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 196.8004 - val_loss: 269.9384\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 166.3935 - val_loss: 170.6745\n",
      "Epoch 914/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.5620 - val_loss: 267.3693\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 196.1100 - val_loss: 150.0870\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 231.4835 - val_loss: 149.1297\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.9949 - val_loss: 172.6473\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.6667 - val_loss: 192.2710\n",
      "Epoch 919/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 163.9904 - val_loss: 140.5654\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 197.1428 - val_loss: 138.2675\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 156.7760 - val_loss: 184.6259\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 164.9862 - val_loss: 144.6145\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 171.6589 - val_loss: 165.6152\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 156.2121 - val_loss: 195.1417\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 166.6394 - val_loss: 223.2493\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 194.7541 - val_loss: 165.2359\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 165.3967 - val_loss: 173.6879\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 164.3255 - val_loss: 200.6729\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 169.0126 - val_loss: 147.0917\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 194.0195 - val_loss: 154.4594\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 158.8506 - val_loss: 151.1326\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.1111 - val_loss: 141.6042\n",
      "Epoch 933/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 169.2906 - val_loss: 155.2270\n",
      "Epoch 934/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 172.7765 - val_loss: 136.5675\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 186.2349 - val_loss: 149.6300\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 182.7639 - val_loss: 271.5233\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 295.7873 - val_loss: 142.7935\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 152.1208 - val_loss: 157.7916\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 177.2038 - val_loss: 172.6260\n",
      "Epoch 940/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 203.7688 - val_loss: 139.3328\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.7140 - val_loss: 143.1985\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.0064 - val_loss: 219.6335\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 156.6353 - val_loss: 241.0445\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 161.8051 - val_loss: 203.6090\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 202.6623 - val_loss: 153.7929\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 146.2580 - val_loss: 178.7743\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 158.6597 - val_loss: 150.1656\n",
      "Epoch 948/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 159.7483 - val_loss: 137.2772\n",
      "Epoch 949/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 165.5686 - val_loss: 209.1258\n",
      "Epoch 950/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 243.5441 - val_loss: 253.5070\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 204.0822 - val_loss: 183.0455\n",
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 145.2158 - val_loss: 161.3382\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 178.9457 - val_loss: 219.3103\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 167.4506 - val_loss: 145.9101\n",
      "Epoch 955/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 180.1160 - val_loss: 154.8521\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 175.1315 - val_loss: 308.6073\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 160.7656 - val_loss: 140.5274\n",
      "Epoch 958/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.5700 - val_loss: 227.4318\n",
      "Epoch 959/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.1851 - val_loss: 204.3743\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 183.0699 - val_loss: 168.7945\n",
      "Epoch 961/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 154.5325 - val_loss: 170.8248\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.8174 - val_loss: 149.9670\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 233.6344 - val_loss: 234.4373\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 202.5667 - val_loss: 209.7586\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 168.2577 - val_loss: 175.0696\n",
      "Epoch 966/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 159.0639 - val_loss: 160.0102\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 180.2812 - val_loss: 153.7629\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 151.2065 - val_loss: 191.2685\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 172.8208 - val_loss: 181.7599\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.9636 - val_loss: 143.0910\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 156.3714 - val_loss: 175.4480\n",
      "Epoch 972/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.1768 - val_loss: 150.8263\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.7696 - val_loss: 186.0651\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 169.7283 - val_loss: 153.3787\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 149.8275 - val_loss: 226.6965\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 169.7083 - val_loss: 136.5307\n",
      "Epoch 977/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 197.7379 - val_loss: 135.6212\n",
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 147.3337 - val_loss: 136.5741\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 165.2201 - val_loss: 153.1099\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.7728 - val_loss: 148.1150\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 177.2531 - val_loss: 136.6033\n",
      "Epoch 982/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 251.8794 - val_loss: 270.1263\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 161.6609 - val_loss: 135.9614\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 187.8206 - val_loss: 149.1865\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 155.9128 - val_loss: 165.9757\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 160.7239 - val_loss: 192.5576\n",
      "Epoch 987/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 182.1167 - val_loss: 171.0371\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.6589 - val_loss: 129.7808\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 160.1399 - val_loss: 145.7415\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 151.2267 - val_loss: 137.1553\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.2661 - val_loss: 139.6380\n",
      "Epoch 992/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.8276 - val_loss: 420.4184\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 180.9077 - val_loss: 182.7740\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 167.9734 - val_loss: 142.4275\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 192.6419 - val_loss: 162.1496\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 173.7344 - val_loss: 222.5616\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 159.7137 - val_loss: 147.8349\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.4334 - val_loss: 199.9935\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 197.0968 - val_loss: 227.5954\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 212.4944 - val_loss: 315.5845\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 219.7616 - val_loss: 139.0541\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 170.912 - 0s 57us/step - loss: 170.4834 - val_loss: 145.1114\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 156.1497 - val_loss: 169.2560\n",
      "Epoch 1004/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 163.1287 - val_loss: 148.6743\n",
      "Epoch 1005/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 167.1379 - val_loss: 165.4305\n",
      "Epoch 1006/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 147.2835 - val_loss: 271.4298\n",
      "Epoch 1007/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 153.6753 - val_loss: 166.3700\n",
      "Epoch 1008/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 141.1525 - val_loss: 137.5758\n",
      "Epoch 1009/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.2942 - val_loss: 203.3521\n",
      "Epoch 1010/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 183.8973 - val_loss: 148.3725\n",
      "Epoch 1011/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 186.7788 - val_loss: 221.5414\n",
      "Epoch 1012/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 166.7010 - val_loss: 254.2603\n",
      "Epoch 1013/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 156.6052 - val_loss: 167.8102\n",
      "Epoch 1014/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 193.8748 - val_loss: 149.7206\n",
      "Epoch 1015/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 169.8825 - val_loss: 153.3694\n",
      "Epoch 1016/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 155.3504 - val_loss: 166.7848\n",
      "Epoch 1017/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 158.1748 - val_loss: 147.7323\n",
      "Epoch 1018/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 169.5408 - val_loss: 153.4008\n",
      "Epoch 1019/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 150.1958 - val_loss: 150.2877\n",
      "Epoch 1020/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.9612 - val_loss: 175.6273\n",
      "Epoch 1021/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.8225 - val_loss: 137.1068\n",
      "Epoch 1022/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 157.3373 - val_loss: 175.8077\n",
      "Epoch 1023/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 193.8719 - val_loss: 228.7481\n",
      "Epoch 1024/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 156.1918 - val_loss: 136.7831\n",
      "Epoch 1025/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 142.5354 - val_loss: 135.0352\n",
      "Epoch 1026/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 158.4679 - val_loss: 172.5212\n",
      "Epoch 1027/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 174.7278 - val_loss: 177.7369\n",
      "Epoch 1028/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 170.7188 - val_loss: 162.3465\n",
      "Epoch 1029/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 150.3710 - val_loss: 145.3897\n",
      "Epoch 1030/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 176.4910 - val_loss: 417.0178\n",
      "Epoch 1031/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 177.3970 - val_loss: 167.1985\n",
      "Epoch 1032/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 142.2486 - val_loss: 165.3130\n",
      "Epoch 1033/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 209.6428 - val_loss: 159.9001\n",
      "Epoch 1034/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 182.1887 - val_loss: 153.8521\n",
      "Epoch 1035/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 156.0689 - val_loss: 168.0527\n",
      "Epoch 1036/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 144.4796 - val_loss: 161.8149\n",
      "Epoch 1037/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 197.5184 - val_loss: 143.4615\n",
      "Epoch 1038/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 230.2917 - val_loss: 186.1737\n",
      "Epoch 1039/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.5941 - val_loss: 156.6761\n",
      "Epoch 1040/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.9096 - val_loss: 171.9500\n",
      "Epoch 1041/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 150.7305 - val_loss: 151.0918\n",
      "Epoch 1042/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 151.2960 - val_loss: 146.7694\n",
      "Epoch 1043/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.6360 - val_loss: 138.8377\n",
      "Epoch 1044/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 165.2044 - val_loss: 175.7784\n",
      "Epoch 1045/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 161.5328 - val_loss: 177.0183\n",
      "Epoch 1046/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 148.6759 - val_loss: 144.3453\n",
      "Epoch 1047/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 161.5432 - val_loss: 136.1425\n",
      "Epoch 1048/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 368.3774 - val_loss: 195.4685\n",
      "Epoch 1049/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 169.6499 - val_loss: 139.4435\n",
      "Epoch 1050/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 141.9562 - val_loss: 143.3287\n",
      "Epoch 1051/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 149.5229 - val_loss: 172.4689\n",
      "Epoch 1052/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 184.1908 - val_loss: 187.8364\n",
      "Epoch 1053/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 169.9335 - val_loss: 177.2553\n",
      "Epoch 1054/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 164.1809 - val_loss: 151.5534\n",
      "Epoch 1055/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 153.6378 - val_loss: 173.2948\n",
      "Epoch 1056/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 153.4624 - val_loss: 146.3497\n",
      "Epoch 1057/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 147.8163 - val_loss: 159.1075\n",
      "Epoch 1058/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 163.6440 - val_loss: 239.9348\n",
      "Epoch 1059/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 329.5551 - val_loss: 154.7792\n",
      "Epoch 1060/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 204.2321 - val_loss: 168.0516\n",
      "Epoch 1061/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 179.4363 - val_loss: 150.0545\n",
      "Epoch 1062/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 183.5288 - val_loss: 142.0219\n",
      "Epoch 1063/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 197.8250 - val_loss: 168.9319\n",
      "Epoch 1064/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.4640 - val_loss: 138.0437\n",
      "Epoch 1065/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.8684 - val_loss: 138.8871\n",
      "Epoch 1066/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.3016 - val_loss: 151.1775\n",
      "Epoch 1067/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 145.5009 - val_loss: 134.5689\n",
      "Epoch 1068/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 149.1023 - val_loss: 192.4840\n",
      "Epoch 1069/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 223.3330 - val_loss: 171.6172\n",
      "Epoch 1070/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 167.2833 - val_loss: 154.4684\n",
      "Epoch 1071/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 150.3352 - val_loss: 146.3780\n",
      "Epoch 1072/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.3297 - val_loss: 141.3011\n",
      "Epoch 1073/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.8949 - val_loss: 208.0932\n",
      "Epoch 1074/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 189.1830 - val_loss: 157.4023\n",
      "Epoch 1075/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 157.2352 - val_loss: 146.2863\n",
      "Epoch 1076/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.4168 - val_loss: 134.8639\n",
      "Epoch 1077/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.0082 - val_loss: 136.6935\n",
      "Epoch 1078/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.6617 - val_loss: 147.5685\n",
      "Epoch 1079/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 184.8276 - val_loss: 210.1743\n",
      "Epoch 1080/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.5571 - val_loss: 139.6235\n",
      "Epoch 1081/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.3038 - val_loss: 160.4117\n",
      "Epoch 1082/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 151.5454 - val_loss: 162.1066\n",
      "Epoch 1083/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 159.6926 - val_loss: 150.4982\n",
      "Epoch 1084/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 164.8127 - val_loss: 183.3007\n",
      "Epoch 1085/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.7637 - val_loss: 156.5757\n",
      "Epoch 1086/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 144.8766 - val_loss: 146.5801\n",
      "Epoch 1087/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 141.1403 - val_loss: 154.3187\n",
      "Epoch 1088/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 151.3299 - val_loss: 194.0278\n",
      "Epoch 1089/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 200.8251 - val_loss: 198.5543\n",
      "Epoch 1090/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 296.1762 - val_loss: 261.5185\n",
      "Epoch 1091/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 205.6183 - val_loss: 150.2173\n",
      "Epoch 1092/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 181.0623 - val_loss: 201.5241\n",
      "Epoch 1093/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 179.6306 - val_loss: 153.7961\n",
      "Epoch 1094/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 158.2880 - val_loss: 162.7605\n",
      "Epoch 1095/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 152.1879 - val_loss: 186.7556\n",
      "Epoch 1096/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.1581 - val_loss: 141.6166\n",
      "Epoch 1097/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 142.4557 - val_loss: 156.4602\n",
      "Epoch 1098/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 291.1538 - val_loss: 164.4564\n",
      "Epoch 1099/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 174.0535 - val_loss: 167.5427\n",
      "Epoch 1100/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.6404 - val_loss: 170.7895\n",
      "Epoch 1101/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 175.3108 - val_loss: 156.9171\n",
      "Epoch 1102/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 165.0176 - val_loss: 154.3460\n",
      "Epoch 1103/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 171.7197 - val_loss: 144.4881\n",
      "Epoch 1104/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 236.5322 - val_loss: 159.6234\n",
      "Epoch 1105/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 161.2129 - val_loss: 146.4526\n",
      "Epoch 1106/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 158.4921 - val_loss: 231.7901\n",
      "Epoch 1107/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 188.3486 - val_loss: 149.6711\n",
      "Epoch 1108/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 182.6815 - val_loss: 336.8757\n",
      "Epoch 1109/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 221.7966 - val_loss: 171.6993\n",
      "Epoch 1110/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 142.2859 - val_loss: 153.5783\n",
      "Epoch 1111/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 139.3133 - val_loss: 136.7471\n",
      "Epoch 1112/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 177.1352 - val_loss: 131.0854\n",
      "Epoch 1113/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 152.5815 - val_loss: 134.6321\n",
      "Epoch 1114/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 157.3354 - val_loss: 139.8906\n",
      "Epoch 1115/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 148.6208 - val_loss: 148.0033\n",
      "Epoch 1116/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 158.5121 - val_loss: 146.3171\n",
      "Epoch 1117/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 174.7629 - val_loss: 147.8980\n",
      "Epoch 1118/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 203.4321 - val_loss: 139.8013\n",
      "Epoch 1119/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 185.2348 - val_loss: 167.3808\n",
      "Epoch 1120/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 155.3256 - val_loss: 186.7517\n",
      "Epoch 1121/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.9130 - val_loss: 154.3903\n",
      "Epoch 1122/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 193.1127 - val_loss: 140.6657\n",
      "Epoch 1123/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 154.0543 - val_loss: 143.4287\n",
      "Epoch 1124/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 160.1361 - val_loss: 146.2934\n",
      "Epoch 1125/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 149.9798 - val_loss: 135.1071\n",
      "Epoch 1126/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 153.5625 - val_loss: 201.0833\n",
      "Epoch 1127/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 157.7439 - val_loss: 217.9847\n",
      "Epoch 1128/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 215.8285 - val_loss: 141.4893\n",
      "Epoch 1129/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.8769 - val_loss: 134.5213\n",
      "Epoch 1130/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 138.9577 - val_loss: 168.5117\n",
      "Epoch 1131/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.0369 - val_loss: 175.8934\n",
      "Epoch 1132/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 158.1049 - val_loss: 188.2811\n",
      "Epoch 1133/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 179.9907 - val_loss: 139.0384\n",
      "Epoch 1134/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 171.0166 - val_loss: 146.7896\n",
      "Epoch 1135/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 156.0227 - val_loss: 147.9471\n",
      "Epoch 1136/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 148.6546 - val_loss: 175.7357\n",
      "Epoch 1137/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.6816 - val_loss: 173.1438\n",
      "Epoch 1138/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 184.0604 - val_loss: 149.6133\n",
      "Epoch 1139/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 162.3121 - val_loss: 236.3295\n",
      "Epoch 1140/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.4938 - val_loss: 154.0092\n",
      "Epoch 1141/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 174.6940 - val_loss: 145.1077\n",
      "Epoch 1142/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 163.0768 - val_loss: 288.0484\n",
      "Epoch 1143/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 196.1170 - val_loss: 215.6158\n",
      "Epoch 1144/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 167.8893 - val_loss: 142.5304\n",
      "Epoch 1145/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 152.0223 - val_loss: 156.3877\n",
      "Epoch 1146/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 146.2368 - val_loss: 186.4315\n",
      "Epoch 1147/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.9788 - val_loss: 172.2045\n",
      "Epoch 1148/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 143.1763 - val_loss: 155.4630\n",
      "Epoch 1149/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.5443 - val_loss: 206.7584\n",
      "Epoch 1150/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 191.3893 - val_loss: 150.4392\n",
      "Epoch 1151/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 174.4762 - val_loss: 206.7771\n",
      "Epoch 1152/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.8448 - val_loss: 150.4791\n",
      "Epoch 1153/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 154.3759 - val_loss: 173.4447\n",
      "Epoch 1154/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 151.6248 - val_loss: 156.0209\n",
      "Epoch 1155/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.1606 - val_loss: 209.4808\n",
      "Epoch 1156/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.7686 - val_loss: 153.7675\n",
      "Epoch 1157/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 145.3401 - val_loss: 135.3812\n",
      "Epoch 1158/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 344.1218 - val_loss: 265.3222\n",
      "Epoch 1159/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 309.7407 - val_loss: 301.6922\n",
      "Epoch 1160/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 233.9947 - val_loss: 195.3794\n",
      "Epoch 1161/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 193.4834 - val_loss: 175.0363\n",
      "Epoch 1162/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 190.4946 - val_loss: 175.4733\n",
      "Epoch 1163/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 207.8566 - val_loss: 163.6083\n",
      "Epoch 1164/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 167.8417 - val_loss: 161.5410\n",
      "Epoch 1165/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 189.6724 - val_loss: 188.3153\n",
      "Epoch 1166/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 156.1726 - val_loss: 164.9993\n",
      "Epoch 1167/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 188.5489 - val_loss: 143.7058\n",
      "Epoch 1168/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 151.0460 - val_loss: 140.9676\n",
      "Epoch 1169/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.5623 - val_loss: 151.8506\n",
      "Epoch 1170/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.1933 - val_loss: 139.0426\n",
      "Epoch 1171/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 159.8037 - val_loss: 156.4808\n",
      "Epoch 1172/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 197.1155 - val_loss: 197.5271\n",
      "Epoch 1173/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 210.7741 - val_loss: 145.7854\n",
      "Epoch 1174/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.0188 - val_loss: 171.7183\n",
      "Epoch 1175/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.2123 - val_loss: 142.1102\n",
      "Epoch 1176/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.5767 - val_loss: 244.8267\n",
      "Epoch 1177/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 162.7546 - val_loss: 147.0902\n",
      "Epoch 1178/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 147.6065 - val_loss: 131.5563\n",
      "Epoch 1179/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.8638 - val_loss: 160.0852\n",
      "Epoch 1180/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 165.8525 - val_loss: 151.4704\n",
      "Epoch 1181/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 155.3664 - val_loss: 136.4921\n",
      "Epoch 1182/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.1007 - val_loss: 147.2191\n",
      "Epoch 1183/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.7001 - val_loss: 169.0440\n",
      "Epoch 1184/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 190.4760 - val_loss: 168.9018\n",
      "Epoch 1185/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 150.3263 - val_loss: 147.5642\n",
      "Epoch 1186/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.0163 - val_loss: 156.0488\n",
      "Epoch 1187/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 152.5781 - val_loss: 140.1309\n",
      "Epoch 1188/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 160.7686 - val_loss: 141.4824\n",
      "Epoch 1189/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 147.1766 - val_loss: 206.8248\n",
      "Epoch 1190/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 264.4012 - val_loss: 187.0831\n",
      "Epoch 1191/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 164.4612 - val_loss: 200.4243\n",
      "Epoch 1192/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.0959 - val_loss: 230.7485\n",
      "Epoch 1193/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 209.3619 - val_loss: 136.5761\n",
      "Epoch 1194/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.3168 - val_loss: 138.6050\n",
      "Epoch 1195/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 157.3347 - val_loss: 142.3104\n",
      "Epoch 1196/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 138.5467 - val_loss: 147.7554\n",
      "Epoch 1197/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.6538 - val_loss: 255.4719\n",
      "Epoch 1198/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.6369 - val_loss: 142.4030\n",
      "Epoch 1199/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 183.9722 - val_loss: 169.3433\n",
      "Epoch 1200/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 170.1341 - val_loss: 158.7821\n",
      "Epoch 1201/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 145.0634 - val_loss: 139.8183\n",
      "Epoch 1202/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.3916 - val_loss: 150.7499\n",
      "Epoch 1203/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.5641 - val_loss: 160.5586\n",
      "Epoch 1204/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 157.6879 - val_loss: 159.1122\n",
      "Epoch 1205/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 217.0054 - val_loss: 144.4848\n",
      "Epoch 1206/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 161.8259 - val_loss: 286.1386\n",
      "Epoch 1207/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 149.4570 - val_loss: 156.2620\n",
      "Epoch 1208/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.1458 - val_loss: 168.4797\n",
      "Epoch 1209/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 169.8456 - val_loss: 141.6273\n",
      "Epoch 1210/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 150.7951 - val_loss: 157.5192\n",
      "Epoch 1211/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 152.1233 - val_loss: 157.1455\n",
      "Epoch 1212/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 172.6233 - val_loss: 185.5051\n",
      "Epoch 1213/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.8891 - val_loss: 154.0353\n",
      "Epoch 1214/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 153.2383 - val_loss: 147.9050\n",
      "Epoch 1215/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 171.1521 - val_loss: 302.7198\n",
      "Epoch 1216/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.5917 - val_loss: 145.6047\n",
      "Epoch 1217/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 154.2012 - val_loss: 154.5661\n",
      "Epoch 1218/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.4101 - val_loss: 162.8193\n",
      "Epoch 1219/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 141.1386 - val_loss: 268.1591\n",
      "Epoch 1220/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.3054 - val_loss: 138.5771\n",
      "Epoch 1221/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 167.1276 - val_loss: 144.3233\n",
      "Epoch 1222/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 144.1251 - val_loss: 195.5289\n",
      "Epoch 1223/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 150.6104 - val_loss: 134.0974\n",
      "Epoch 1224/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 157.0034 - val_loss: 134.3199\n",
      "Epoch 1225/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 169.1348 - val_loss: 169.0981\n",
      "Epoch 1226/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 152.5032 - val_loss: 136.8683\n",
      "Epoch 1227/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 216.6169 - val_loss: 167.5594\n",
      "Epoch 1228/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 163.2001 - val_loss: 171.1555\n",
      "Epoch 1229/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 176.8347 - val_loss: 149.8333\n",
      "Epoch 1230/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 214.5657 - val_loss: 160.7505\n",
      "Epoch 1231/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.7094 - val_loss: 158.7613\n",
      "Epoch 1232/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 170.1600 - val_loss: 149.2370\n",
      "Epoch 1233/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 146.2488 - val_loss: 160.8480\n",
      "Epoch 1234/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 167.4053 - val_loss: 342.3652\n",
      "Epoch 1235/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 171.0172 - val_loss: 148.1225\n",
      "Epoch 1236/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 150.6382 - val_loss: 140.6998\n",
      "Epoch 1237/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 149.7510 - val_loss: 135.1366\n",
      "Epoch 1238/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 184.5574 - val_loss: 147.4668\n",
      "Epoch 1239/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 151.8974 - val_loss: 152.5236\n",
      "Epoch 1240/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 184.5555 - val_loss: 136.6878\n",
      "Epoch 1241/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.0555 - val_loss: 139.3236\n",
      "Epoch 1242/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.3008 - val_loss: 160.6108\n",
      "Epoch 1243/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 148.8499 - val_loss: 187.7794\n",
      "Epoch 1244/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 189.2970 - val_loss: 142.9110\n",
      "Epoch 1245/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 158.2571 - val_loss: 163.4457\n",
      "Epoch 1246/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 174.7554 - val_loss: 152.1972\n",
      "Epoch 1247/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 174.7484 - val_loss: 302.2644\n",
      "Epoch 1248/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 164.0696 - val_loss: 157.5029\n",
      "Epoch 1249/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.4437 - val_loss: 166.2795\n",
      "Epoch 1250/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 177.3353 - val_loss: 170.0468\n",
      "Epoch 1251/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 142.6331 - val_loss: 147.1426\n",
      "Epoch 1252/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.6013 - val_loss: 151.9244\n",
      "Epoch 1253/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 162.8773 - val_loss: 176.3356\n",
      "Epoch 1254/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 178.7540 - val_loss: 142.0934\n",
      "Epoch 1255/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 148.7097 - val_loss: 150.7978\n",
      "Epoch 1256/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 194.3991 - val_loss: 134.0542\n",
      "Epoch 1257/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 149.9181 - val_loss: 180.6557\n",
      "Epoch 1258/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.9807 - val_loss: 149.2399\n",
      "Epoch 1259/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 149.4768 - val_loss: 142.9909\n",
      "Epoch 1260/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 139.3247 - val_loss: 233.9493\n",
      "Epoch 1261/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 180.1666 - val_loss: 154.8100\n",
      "Epoch 1262/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 155.9359 - val_loss: 166.7705\n",
      "Epoch 1263/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 179.4911 - val_loss: 319.5771\n",
      "Epoch 1264/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 171.6637 - val_loss: 144.6870\n",
      "Epoch 1265/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 146.9315 - val_loss: 198.6475\n",
      "Epoch 1266/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 162.1632 - val_loss: 134.3902\n",
      "Epoch 1267/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.5117 - val_loss: 139.2456\n",
      "Epoch 1268/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 187.1544 - val_loss: 139.4301\n",
      "Epoch 1269/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 162.7906 - val_loss: 148.2172\n",
      "Epoch 1270/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 140.1200 - val_loss: 132.5836\n",
      "Epoch 1271/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 154.9825 - val_loss: 200.0437\n",
      "Epoch 1272/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 165.4273 - val_loss: 179.2562\n",
      "Epoch 1273/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 139.7259 - val_loss: 130.4196\n",
      "Epoch 1274/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 207.2528 - val_loss: 137.7253\n",
      "Epoch 1275/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.4158 - val_loss: 143.5207\n",
      "Epoch 1276/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 138.9923 - val_loss: 138.9909\n",
      "Epoch 1277/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.0884 - val_loss: 134.2832\n",
      "Epoch 1278/10000\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 150.5098 - val_loss: 138.4181\n",
      "Epoch 1279/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 147.5906 - val_loss: 157.7009\n",
      "Epoch 1280/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 155.4022 - val_loss: 154.5481\n",
      "Epoch 1281/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.7602 - val_loss: 149.5810\n",
      "Epoch 1282/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.7840 - val_loss: 153.1537\n",
      "Epoch 1283/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 148.5686 - val_loss: 134.1942\n",
      "Epoch 1284/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 148.7961 - val_loss: 133.1110\n",
      "Epoch 1285/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 171.9255 - val_loss: 142.6490\n",
      "Epoch 1286/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 145.8389 - val_loss: 156.1883\n",
      "Epoch 1287/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 148.8469 - val_loss: 146.2655\n",
      "Epoch 1288/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 154.6766 - val_loss: 147.8276\n",
      "Epoch 1289/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.8190 - val_loss: 139.0982\n",
      "Epoch 1290/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 149.9408 - val_loss: 140.0960\n",
      "Epoch 1291/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 162.8678 - val_loss: 194.9826\n",
      "Epoch 1292/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 150.4596 - val_loss: 223.5475\n",
      "Epoch 1293/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 145.9043 - val_loss: 150.1316\n",
      "Epoch 1294/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.9350 - val_loss: 136.8576\n",
      "Epoch 1295/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 171.2929 - val_loss: 133.7899\n",
      "Epoch 1296/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.2967 - val_loss: 139.2130\n",
      "Epoch 1297/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 157.2285 - val_loss: 258.6917\n",
      "Epoch 1298/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 140.7499 - val_loss: 136.1142\n",
      "Epoch 1299/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 148.4460 - val_loss: 145.1658\n",
      "Epoch 1300/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.4798 - val_loss: 157.6255\n",
      "Epoch 1301/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 193.9597 - val_loss: 137.8698\n",
      "Epoch 1302/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 185.7058 - val_loss: 194.5132\n",
      "Epoch 1303/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 150.3700 - val_loss: 147.9117\n",
      "Epoch 1304/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 170.5941 - val_loss: 174.7390\n",
      "Epoch 1305/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 200.8624 - val_loss: 215.5445\n",
      "Epoch 1306/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 175.8000 - val_loss: 162.8602\n",
      "Epoch 1307/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 145.0837 - val_loss: 139.9035\n",
      "Epoch 1308/10000\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 149.5473 - val_loss: 143.2946\n",
      "Epoch 1309/10000\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 153.4546 - val_loss: 133.2985\n",
      "Epoch 1310/10000\n",
      "8000/8000 [==============================] - 1s 127us/step - loss: 172.1779 - val_loss: 173.0840\n",
      "Epoch 1311/10000\n",
      "8000/8000 [==============================] - 1s 130us/step - loss: 174.2926 - val_loss: 145.4057\n",
      "Epoch 1312/10000\n",
      "8000/8000 [==============================] - 1s 143us/step - loss: 145.9544 - val_loss: 169.8891\n",
      "Epoch 1313/10000\n",
      "8000/8000 [==============================] - 1s 144us/step - loss: 150.5009 - val_loss: 146.7238\n",
      "Epoch 1314/10000\n",
      "8000/8000 [==============================] - 1s 143us/step - loss: 150.2284 - val_loss: 149.1764\n",
      "Epoch 1315/10000\n",
      "8000/8000 [==============================] - 1s 141us/step - loss: 152.7388 - val_loss: 135.7470\n",
      "Epoch 1316/10000\n",
      "8000/8000 [==============================] - 1s 146us/step - loss: 141.8406 - val_loss: 167.9546\n",
      "Epoch 1317/10000\n",
      "8000/8000 [==============================] - 1s 140us/step - loss: 157.2554 - val_loss: 138.7270\n",
      "Epoch 1318/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 153.3530 - val_loss: 171.9753\n",
      "Epoch 1319/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 148.8485 - val_loss: 177.7570\n",
      "Epoch 1320/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.1375 - val_loss: 138.1621\n",
      "Epoch 1321/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 219.6586 - val_loss: 199.2102\n",
      "Epoch 1322/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 185.2334 - val_loss: 143.1693\n",
      "Epoch 1323/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.7150 - val_loss: 171.1279\n",
      "Epoch 1324/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.8974 - val_loss: 153.7973\n",
      "Epoch 1325/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 145.2091 - val_loss: 132.2126\n",
      "Epoch 1326/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 135.7728 - val_loss: 206.9933\n",
      "Epoch 1327/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 147.3038 - val_loss: 155.2350\n",
      "Epoch 1328/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.0437 - val_loss: 135.7051\n",
      "Epoch 1329/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.5428 - val_loss: 147.2102\n",
      "Epoch 1330/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 141.2568 - val_loss: 139.3127\n",
      "Epoch 1331/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 153.3391 - val_loss: 176.4020\n",
      "Epoch 1332/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 148.1398 - val_loss: 155.4134\n",
      "Epoch 1333/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 157.3623 - val_loss: 180.8721\n",
      "Epoch 1334/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 139.9805 - val_loss: 131.0769\n",
      "Epoch 1335/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 176.4124 - val_loss: 157.3898\n",
      "Epoch 1336/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 172.7755 - val_loss: 166.8601\n",
      "Epoch 1337/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 161.6343 - val_loss: 139.9213\n",
      "Epoch 1338/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 142.8822 - val_loss: 158.1982\n",
      "Epoch 1339/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 154.6091 - val_loss: 142.6166\n",
      "Epoch 1340/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 153.5410 - val_loss: 165.3779\n",
      "Epoch 1341/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 148.9693 - val_loss: 170.6194\n",
      "Epoch 1342/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 154.4047 - val_loss: 257.6594\n",
      "Epoch 1343/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 158.5646 - val_loss: 145.9228\n",
      "Epoch 1344/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 147.2866 - val_loss: 165.0479\n",
      "Epoch 1345/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 156.7840 - val_loss: 189.8173\n",
      "Epoch 1346/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 140.0421 - val_loss: 137.3896\n",
      "Epoch 1347/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 175.9136 - val_loss: 161.9749\n",
      "Epoch 1348/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.5493 - val_loss: 158.9114\n",
      "Epoch 1349/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 153.6080 - val_loss: 152.3971\n",
      "Epoch 1350/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.4647 - val_loss: 161.5634\n",
      "Epoch 1351/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.1338 - val_loss: 146.6402\n",
      "Epoch 1352/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.7120 - val_loss: 137.6354\n",
      "Epoch 1353/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 208.7739 - val_loss: 151.1523\n",
      "Epoch 1354/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 166.4694 - val_loss: 195.1395\n",
      "Epoch 1355/10000\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 137.7295 - val_loss: 140.7538\n",
      "Epoch 1356/10000\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 144.8033 - val_loss: 142.1406\n",
      "Epoch 1357/10000\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 138.4937 - val_loss: 135.9944\n",
      "Epoch 1358/10000\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 144.2753 - val_loss: 163.2202\n",
      "Epoch 1359/10000\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 150.0215 - val_loss: 210.0305\n",
      "Epoch 1360/10000\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 189.0729 - val_loss: 173.4788\n",
      "Epoch 1361/10000\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 164.3260 - val_loss: 135.0830\n",
      "Epoch 1362/10000\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 150.1692 - val_loss: 133.3745\n",
      "Epoch 1363/10000\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 145.7276 - val_loss: 133.2458\n",
      "Epoch 1364/10000\n",
      "8000/8000 [==============================] - 1s 162us/step - loss: 167.9882 - val_loss: 154.1106\n",
      "Epoch 1365/10000\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 137.6315 - val_loss: 135.2252\n",
      "Epoch 1366/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 147.121 - 2s 195us/step - loss: 147.2256 - val_loss: 134.7121\n",
      "Epoch 1367/10000\n",
      "8000/8000 [==============================] - 1s 152us/step - loss: 146.1318 - val_loss: 154.5064\n",
      "Epoch 1368/10000\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 184.0651 - val_loss: 214.9005\n",
      "Epoch 1369/10000\n",
      "8000/8000 [==============================] - 1s 148us/step - loss: 147.0292 - val_loss: 143.9344\n",
      "Epoch 1370/10000\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 154.0250 - val_loss: 136.9306\n",
      "Epoch 1371/10000\n",
      "8000/8000 [==============================] - 1s 152us/step - loss: 169.7939 - val_loss: 146.7474\n",
      "Epoch 1372/10000\n",
      "8000/8000 [==============================] - 1s 151us/step - loss: 141.2954 - val_loss: 141.7787\n",
      "Epoch 1373/10000\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 143.5542 - val_loss: 160.4160\n",
      "Epoch 1374/10000\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 140.3382 - val_loss: 143.1406\n",
      "Epoch 1375/10000\n",
      "8000/8000 [==============================] - 1s 148us/step - loss: 155.1116 - val_loss: 142.8465\n",
      "Epoch 1376/10000\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 148.9729 - val_loss: 147.7243\n",
      "Epoch 1377/10000\n",
      "8000/8000 [==============================] - 1s 141us/step - loss: 154.4612 - val_loss: 146.5848\n",
      "Epoch 1378/10000\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 147.3388 - val_loss: 134.1538\n",
      "Epoch 1379/10000\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 157.0045 - val_loss: 236.3658\n",
      "Epoch 1380/10000\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 168.7170 - val_loss: 151.1999\n",
      "Epoch 1381/10000\n",
      "8000/8000 [==============================] - 1s 151us/step - loss: 156.6521 - val_loss: 164.4761\n",
      "Epoch 1382/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 137.4643 - val_loss: 150.4555\n",
      "Epoch 1383/10000\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 159.4445 - val_loss: 135.8305\n",
      "Epoch 1384/10000\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 147.8942 - val_loss: 162.5647\n",
      "Epoch 1385/10000\n",
      "8000/8000 [==============================] - 1s 184us/step - loss: 148.9583 - val_loss: 161.7125\n",
      "Epoch 1386/10000\n",
      "8000/8000 [==============================] - 1s 163us/step - loss: 164.9378 - val_loss: 168.6263\n",
      "Epoch 1387/10000\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 149.9705 - val_loss: 135.0470\n",
      "Epoch 1388/10000\n",
      "8000/8000 [==============================] - 1s 130us/step - loss: 136.7318 - val_loss: 136.5977\n",
      "Epoch 1389/10000\n",
      "8000/8000 [==============================] - 1s 146us/step - loss: 141.8558 - val_loss: 138.1022\n",
      "Epoch 1390/10000\n",
      "8000/8000 [==============================] - 1s 152us/step - loss: 154.9176 - val_loss: 138.1342\n",
      "Epoch 1391/10000\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 153.5634 - val_loss: 149.9953\n",
      "Epoch 1392/10000\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 144.7876 - val_loss: 168.1895\n",
      "Epoch 1393/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 120us/step - loss: 147.9974 - val_loss: 142.5629\n",
      "Epoch 1394/10000\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 146.8512 - val_loss: 139.3921\n",
      "Epoch 1395/10000\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 159.2364 - val_loss: 205.8602\n",
      "Epoch 1396/10000\n",
      "8000/8000 [==============================] - 1s 146us/step - loss: 147.1605 - val_loss: 142.4240\n",
      "Epoch 1397/10000\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 189.9663 - val_loss: 176.1373\n",
      "Epoch 1398/10000\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 147.5846 - val_loss: 155.1084\n",
      "Epoch 1399/10000\n",
      "8000/8000 [==============================] - 1s 129us/step - loss: 157.4201 - val_loss: 212.1507\n",
      "Epoch 1400/10000\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 210.9235 - val_loss: 161.0741\n",
      "Epoch 1401/10000\n",
      "8000/8000 [==============================] - 1s 140us/step - loss: 152.3017 - val_loss: 156.3924\n",
      "Epoch 1402/10000\n",
      "8000/8000 [==============================] - 1s 153us/step - loss: 146.7208 - val_loss: 158.6344\n",
      "Epoch 1403/10000\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 144.8784 - val_loss: 158.5686\n",
      "Epoch 1404/10000\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 158.5229 - val_loss: 147.1672\n",
      "Epoch 1405/10000\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 146.8317 - val_loss: 134.5427\n",
      "Epoch 1406/10000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 148.9042 - val_loss: 146.5452\n",
      "Epoch 1407/10000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 162.3875 - val_loss: 145.6794\n",
      "Epoch 1408/10000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 146.9124 - val_loss: 146.2324\n",
      "Epoch 1409/10000\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 181.3204 - val_loss: 170.8568\n",
      "Epoch 1410/10000\n",
      "8000/8000 [==============================] - 1s 145us/step - loss: 139.6203 - val_loss: 164.7649\n",
      "Epoch 1411/10000\n",
      "8000/8000 [==============================] - 1s 126us/step - loss: 134.1927 - val_loss: 184.8401\n",
      "Epoch 1412/10000\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 160.4182 - val_loss: 135.9515\n",
      "Epoch 1413/10000\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 160.9973 - val_loss: 188.4010\n",
      "Epoch 1414/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 170.706 - 1s 157us/step - loss: 170.9229 - val_loss: 137.5891\n",
      "Epoch 1415/10000\n",
      "8000/8000 [==============================] - 1s 148us/step - loss: 151.4588 - val_loss: 136.2359\n",
      "Epoch 1416/10000\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 142.0448 - val_loss: 135.3387\n",
      "Epoch 1417/10000\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 162.0026 - val_loss: 167.5044\n",
      "Epoch 1418/10000\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 149.6816 - val_loss: 131.3549\n",
      "Epoch 1419/10000\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 161.9454 - val_loss: 290.4233\n",
      "Epoch 1420/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 149.0997 - val_loss: 188.1200\n",
      "Epoch 1421/10000\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 136.8824 - val_loss: 177.2660\n",
      "Epoch 1422/10000\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 155.4828 - val_loss: 191.8289\n",
      "Epoch 1423/10000\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 146.0397 - val_loss: 175.5191\n",
      "Epoch 1424/10000\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 143.0678 - val_loss: 158.9099\n",
      "Epoch 1425/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 144.8271 - val_loss: 139.3586\n",
      "Epoch 1426/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.3235 - val_loss: 138.2473\n",
      "Epoch 1427/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 173.8261 - val_loss: 140.0029\n",
      "Epoch 1428/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 150.2156 - val_loss: 162.9284\n",
      "Epoch 1429/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 196.0388 - val_loss: 159.0874\n",
      "Epoch 1430/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 154.2351 - val_loss: 183.9736\n",
      "Epoch 1431/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.2064 - val_loss: 153.1580\n",
      "Epoch 1432/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.2826 - val_loss: 171.0243\n",
      "Epoch 1433/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.1737 - val_loss: 155.2027\n",
      "Epoch 1434/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.9764 - val_loss: 207.5796\n",
      "Epoch 1435/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.0999 - val_loss: 171.9208\n",
      "Epoch 1436/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.4714 - val_loss: 133.8128\n",
      "Epoch 1437/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.9574 - val_loss: 175.0141\n",
      "Epoch 1438/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.8180 - val_loss: 141.8846\n",
      "Epoch 1439/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.9617 - val_loss: 138.2693\n",
      "Epoch 1440/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 142.4683 - val_loss: 165.4267\n",
      "Epoch 1441/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.8246 - val_loss: 135.8029\n",
      "Epoch 1442/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 146.7278 - val_loss: 144.2351\n",
      "Epoch 1443/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.8874 - val_loss: 137.9352\n",
      "Epoch 1444/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.6777 - val_loss: 174.8292\n",
      "Epoch 1445/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.2301 - val_loss: 238.3875\n",
      "Epoch 1446/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 187.8714 - val_loss: 138.0271\n",
      "Epoch 1447/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.5711 - val_loss: 135.6897\n",
      "Epoch 1448/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.8807 - val_loss: 161.8414\n",
      "Epoch 1449/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.8992 - val_loss: 138.8324\n",
      "Epoch 1450/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 150.2854 - val_loss: 138.5563\n",
      "Epoch 1451/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 164.0843 - val_loss: 137.6047\n",
      "Epoch 1452/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 174.2879 - val_loss: 144.5129\n",
      "Epoch 1453/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.4707 - val_loss: 136.4345\n",
      "Epoch 1454/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 134.6708 - val_loss: 144.4639\n",
      "Epoch 1455/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 176.6980 - val_loss: 248.7202\n",
      "Epoch 1456/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.9299 - val_loss: 137.7156\n",
      "Epoch 1457/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.6257 - val_loss: 149.1317\n",
      "Epoch 1458/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.4409 - val_loss: 138.3453\n",
      "Epoch 1459/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 142.8036 - val_loss: 137.3271\n",
      "Epoch 1460/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 141.0134 - val_loss: 150.9920\n",
      "Epoch 1461/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 198.7158 - val_loss: 177.8962\n",
      "Epoch 1462/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 167.1962 - val_loss: 146.5266\n",
      "Epoch 1463/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 148.2086 - val_loss: 138.2773\n",
      "Epoch 1464/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 146.2850 - val_loss: 140.8494\n",
      "Epoch 1465/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 72us/step - loss: 148.2425 - val_loss: 156.1227\n",
      "Epoch 1466/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.1942 - val_loss: 138.7311\n",
      "Epoch 1467/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.5063 - val_loss: 135.7283\n",
      "Epoch 1468/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 236.3542 - val_loss: 146.4875\n",
      "Epoch 1469/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 142.9779 - val_loss: 134.3066\n",
      "Epoch 1470/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.2078 - val_loss: 143.1783\n",
      "Epoch 1471/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.9683 - val_loss: 132.9366\n",
      "Epoch 1472/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.5006 - val_loss: 142.8888\n",
      "Epoch 1473/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.2949 - val_loss: 145.8130\n",
      "Epoch 1474/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.5485 - val_loss: 143.4243\n",
      "Epoch 1475/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.0917 - val_loss: 136.0587\n",
      "Epoch 1476/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 172.3003 - val_loss: 173.2335\n",
      "Epoch 1477/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.5374 - val_loss: 130.8235\n",
      "Epoch 1478/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 140.7463 - val_loss: 161.9053\n",
      "Epoch 1479/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.8785 - val_loss: 140.7289\n",
      "Epoch 1480/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.5223 - val_loss: 313.6661\n",
      "Epoch 1481/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 148.6151 - val_loss: 130.3702\n",
      "Epoch 1482/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.8153 - val_loss: 135.8592\n",
      "Epoch 1483/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.0567 - val_loss: 144.9879\n",
      "Epoch 1484/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.6591 - val_loss: 140.2882\n",
      "Epoch 1485/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.9338 - val_loss: 141.4933\n",
      "Epoch 1486/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 165.9873 - val_loss: 168.0465\n",
      "Epoch 1487/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.9520 - val_loss: 132.7439\n",
      "Epoch 1488/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.5026 - val_loss: 177.9882\n",
      "Epoch 1489/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.7139 - val_loss: 136.0760\n",
      "Epoch 1490/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 142.5919 - val_loss: 209.0119\n",
      "Epoch 1491/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 165.6529 - val_loss: 195.7173\n",
      "Epoch 1492/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 153.2997 - val_loss: 149.1196\n",
      "Epoch 1493/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.4473 - val_loss: 147.6064\n",
      "Epoch 1494/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.2083 - val_loss: 151.0088\n",
      "Epoch 1495/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 232.4856 - val_loss: 138.5487\n",
      "Epoch 1496/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 153.3091 - val_loss: 183.7760\n",
      "Epoch 1497/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.7888 - val_loss: 155.2030\n",
      "Epoch 1498/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.1764 - val_loss: 136.6266\n",
      "Epoch 1499/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 139.8419 - val_loss: 136.4631\n",
      "Epoch 1500/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 148.7849 - val_loss: 199.0247\n",
      "Epoch 1501/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 159.6935 - val_loss: 150.5284\n",
      "Epoch 1502/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.3047 - val_loss: 148.0325\n",
      "Epoch 1503/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 194.9761 - val_loss: 153.1865\n",
      "Epoch 1504/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 176.4521 - val_loss: 148.4995\n",
      "Epoch 1505/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.8670 - val_loss: 142.9634\n",
      "Epoch 1506/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 198.4809 - val_loss: 164.5213\n",
      "Epoch 1507/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.9457 - val_loss: 146.9203\n",
      "Epoch 1508/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.6267 - val_loss: 153.4544\n",
      "Epoch 1509/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.9509 - val_loss: 161.0995\n",
      "Epoch 1510/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 155.9656 - val_loss: 133.8362\n",
      "Epoch 1511/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.0468 - val_loss: 136.9365\n",
      "Epoch 1512/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.7645 - val_loss: 146.9532\n",
      "Epoch 1513/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.7187 - val_loss: 138.1841\n",
      "Epoch 1514/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.0731 - val_loss: 149.1801\n",
      "Epoch 1515/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.9436 - val_loss: 207.9343\n",
      "Epoch 1516/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 173.3517 - val_loss: 239.7831\n",
      "Epoch 1517/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 166.1125 - val_loss: 147.6333\n",
      "Epoch 1518/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 136.1163 - val_loss: 132.9170\n",
      "Epoch 1519/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 140.7588 - val_loss: 184.1021\n",
      "Epoch 1520/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 136.5632 - val_loss: 138.9222\n",
      "Epoch 1521/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.4627 - val_loss: 147.0079\n",
      "Epoch 1522/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.3140 - val_loss: 151.5626\n",
      "Epoch 1523/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 169.9899 - val_loss: 181.8591\n",
      "Epoch 1524/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.4659 - val_loss: 175.4031\n",
      "Epoch 1525/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 240.9733 - val_loss: 218.7215\n",
      "Epoch 1526/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 185.3131 - val_loss: 161.3489\n",
      "Epoch 1527/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.1476 - val_loss: 220.2769\n",
      "Epoch 1528/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.4032 - val_loss: 161.6297\n",
      "Epoch 1529/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.9416 - val_loss: 137.7821\n",
      "Epoch 1530/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.2421 - val_loss: 171.8731\n",
      "Epoch 1531/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 187.4621 - val_loss: 135.1229\n",
      "Epoch 1532/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.4239 - val_loss: 140.8523\n",
      "Epoch 1533/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 135.8207 - val_loss: 138.4612\n",
      "Epoch 1534/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.6397 - val_loss: 138.0598\n",
      "Epoch 1535/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.4694 - val_loss: 131.1438\n",
      "Epoch 1536/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 161.2223 - val_loss: 164.0164\n",
      "Epoch 1537/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.6104 - val_loss: 152.9111\n",
      "Epoch 1538/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 169.0818 - val_loss: 171.9414\n",
      "Epoch 1539/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.4921 - val_loss: 135.2095\n",
      "Epoch 1540/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.2229 - val_loss: 189.8315\n",
      "Epoch 1541/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.2658 - val_loss: 146.1371\n",
      "Epoch 1542/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.1570 - val_loss: 155.5706\n",
      "Epoch 1543/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.8759 - val_loss: 149.4786\n",
      "Epoch 1544/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.2507 - val_loss: 158.8929\n",
      "Epoch 1545/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.0640 - val_loss: 142.8226\n",
      "Epoch 1546/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.9036 - val_loss: 145.0552\n",
      "Epoch 1547/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.6972 - val_loss: 137.1271\n",
      "Epoch 1548/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 288.1808 - val_loss: 137.6015\n",
      "Epoch 1549/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.2114 - val_loss: 140.7629\n",
      "Epoch 1550/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 142.7539 - val_loss: 138.5438\n",
      "Epoch 1551/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.8797 - val_loss: 151.7322\n",
      "Epoch 1552/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.4233 - val_loss: 152.8058\n",
      "Epoch 1553/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 157.6705 - val_loss: 150.7024\n",
      "Epoch 1554/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.5567 - val_loss: 157.7276\n",
      "Epoch 1555/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.0693 - val_loss: 142.1618\n",
      "Epoch 1556/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.4816 - val_loss: 133.9613\n",
      "Epoch 1557/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.3007 - val_loss: 155.0300\n",
      "Epoch 1558/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.6107 - val_loss: 147.6936\n",
      "Epoch 1559/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.2680 - val_loss: 176.5691\n",
      "Epoch 1560/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.1117 - val_loss: 150.4066\n",
      "Epoch 1561/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.6410 - val_loss: 140.3212\n",
      "Epoch 1562/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.8079 - val_loss: 140.5840\n",
      "Epoch 1563/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.0284 - val_loss: 137.0901\n",
      "Epoch 1564/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.6537 - val_loss: 161.3454\n",
      "Epoch 1565/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 190.8597 - val_loss: 151.2905\n",
      "Epoch 1566/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.5818 - val_loss: 170.9580\n",
      "Epoch 1567/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 155.7506 - val_loss: 148.1059\n",
      "Epoch 1568/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 149.7886 - val_loss: 161.2849\n",
      "Epoch 1569/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.3199 - val_loss: 134.9416\n",
      "Epoch 1570/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.6633 - val_loss: 148.3910\n",
      "Epoch 1571/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.3498 - val_loss: 136.1049\n",
      "Epoch 1572/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 162.3964 - val_loss: 142.9250\n",
      "Epoch 1573/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.3168 - val_loss: 145.4464\n",
      "Epoch 1574/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.1893 - val_loss: 165.6470\n",
      "Epoch 1575/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.8380 - val_loss: 138.6795\n",
      "Epoch 1576/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.5119 - val_loss: 155.7967\n",
      "Epoch 1577/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.7765 - val_loss: 136.2595\n",
      "Epoch 1578/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.4735 - val_loss: 134.3060\n",
      "Epoch 1579/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 157.8790 - val_loss: 144.6457\n",
      "Epoch 1580/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.9721 - val_loss: 212.9885\n",
      "Epoch 1581/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.3884 - val_loss: 144.6620\n",
      "Epoch 1582/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.7924 - val_loss: 133.3631\n",
      "Epoch 1583/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.6071 - val_loss: 180.4768\n",
      "Epoch 1584/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 155.1019 - val_loss: 149.6919\n",
      "Epoch 1585/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.3473 - val_loss: 180.1358\n",
      "Epoch 1586/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 168.9568 - val_loss: 144.1667\n",
      "Epoch 1587/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 148.4570 - val_loss: 132.9744\n",
      "Epoch 1588/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 187.3151 - val_loss: 146.6125\n",
      "Epoch 1589/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 148.2869 - val_loss: 135.5297\n",
      "Epoch 1590/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 226.6595 - val_loss: 134.6044\n",
      "Epoch 1591/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.2080 - val_loss: 139.1996\n",
      "Epoch 1592/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.7451 - val_loss: 161.9961\n",
      "Epoch 1593/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.4047 - val_loss: 150.5482\n",
      "Epoch 1594/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.1575 - val_loss: 153.8488\n",
      "Epoch 1595/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.5270 - val_loss: 131.9904\n",
      "Epoch 1596/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.1647 - val_loss: 144.2243\n",
      "Epoch 1597/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.0153 - val_loss: 143.0083\n",
      "Epoch 1598/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 216.5033 - val_loss: 142.7412\n",
      "Epoch 1599/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 147.1776 - val_loss: 136.2091\n",
      "Epoch 1600/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.5938 - val_loss: 139.6797\n",
      "Epoch 1601/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.3847 - val_loss: 137.9786\n",
      "Epoch 1602/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 149.3398 - val_loss: 139.4233\n",
      "Epoch 1603/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.7147 - val_loss: 170.3437\n",
      "Epoch 1604/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.9536 - val_loss: 150.8384\n",
      "Epoch 1605/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.8471 - val_loss: 223.4959\n",
      "Epoch 1606/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 175.3244 - val_loss: 164.4327\n",
      "Epoch 1607/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.7810 - val_loss: 148.0490\n",
      "Epoch 1608/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.9766 - val_loss: 158.5826\n",
      "Epoch 1609/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 169.6656 - val_loss: 200.0119\n",
      "Epoch 1610/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.6800 - val_loss: 171.1732\n",
      "Epoch 1611/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.5467 - val_loss: 139.4898\n",
      "Epoch 1612/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.2345 - val_loss: 134.1763\n",
      "Epoch 1613/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.2220 - val_loss: 143.7525\n",
      "Epoch 1614/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 153.4250 - val_loss: 204.2790\n",
      "Epoch 1615/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.3104 - val_loss: 132.6934\n",
      "Epoch 1616/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.3331 - val_loss: 131.0159\n",
      "Epoch 1617/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.1567 - val_loss: 155.3636\n",
      "Epoch 1618/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 174.0804 - val_loss: 135.6349\n",
      "Epoch 1619/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 193.1740 - val_loss: 145.0276\n",
      "Epoch 1620/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.6998 - val_loss: 135.1931\n",
      "Epoch 1621/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.9949 - val_loss: 136.7468\n",
      "Epoch 1622/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.9387 - val_loss: 132.4944\n",
      "Epoch 1623/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.7679 - val_loss: 147.6519\n",
      "Epoch 1624/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.9160 - val_loss: 161.1844\n",
      "Epoch 1625/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.3630 - val_loss: 149.9828\n",
      "Epoch 1626/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.2270 - val_loss: 149.3889\n",
      "Epoch 1627/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.9538 - val_loss: 155.7271\n",
      "Epoch 1628/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.7315 - val_loss: 139.9451\n",
      "Epoch 1629/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 140.3901 - val_loss: 134.9224\n",
      "Epoch 1630/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 177.8598 - val_loss: 199.7267\n",
      "Epoch 1631/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.1814 - val_loss: 144.6351\n",
      "Epoch 1632/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.0155 - val_loss: 223.7586\n",
      "Epoch 1633/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.0607 - val_loss: 162.3429\n",
      "Epoch 1634/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.2299 - val_loss: 140.3947\n",
      "Epoch 1635/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.6834 - val_loss: 153.7849\n",
      "Epoch 1636/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.1966 - val_loss: 153.7792\n",
      "Epoch 1637/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.9582 - val_loss: 154.4323\n",
      "Epoch 1638/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.8432 - val_loss: 137.3984\n",
      "Epoch 1639/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 144.0203 - val_loss: 261.8143\n",
      "Epoch 1640/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 187.2737 - val_loss: 992.3212\n",
      "Epoch 1641/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 190.4362 - val_loss: 165.1663\n",
      "Epoch 1642/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 197.9651 - val_loss: 147.0323\n",
      "Epoch 1643/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.1814 - val_loss: 162.4827\n",
      "Epoch 1644/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.6271 - val_loss: 136.3596\n",
      "Epoch 1645/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.6630 - val_loss: 145.7908\n",
      "Epoch 1646/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.7203 - val_loss: 164.0879\n",
      "Epoch 1647/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 155.0595 - val_loss: 147.2439\n",
      "Epoch 1648/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.7178 - val_loss: 139.4174\n",
      "Epoch 1649/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.9442 - val_loss: 159.8094\n",
      "Epoch 1650/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 169.9548 - val_loss: 156.1250\n",
      "Epoch 1651/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.4382 - val_loss: 136.0872\n",
      "Epoch 1652/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.5028 - val_loss: 151.9281\n",
      "Epoch 1653/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 242.4775 - val_loss: 145.8103\n",
      "Epoch 1654/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.2623 - val_loss: 165.0358\n",
      "Epoch 1655/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 148.3670 - val_loss: 135.8027\n",
      "Epoch 1656/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 138.7905 - val_loss: 143.6832\n",
      "Epoch 1657/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.0365 - val_loss: 135.7029\n",
      "Epoch 1658/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.1583 - val_loss: 232.9493\n",
      "Epoch 1659/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.9445 - val_loss: 156.0755\n",
      "Epoch 1660/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.1456 - val_loss: 145.4440\n",
      "Epoch 1661/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.3326 - val_loss: 134.0614\n",
      "Epoch 1662/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.8613 - val_loss: 164.3510\n",
      "Epoch 1663/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.1382 - val_loss: 250.2722\n",
      "Epoch 1664/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.2380 - val_loss: 137.4681\n",
      "Epoch 1665/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.3694 - val_loss: 166.2571\n",
      "Epoch 1666/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.2514 - val_loss: 157.7269\n",
      "Epoch 1667/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.9699 - val_loss: 148.2123\n",
      "Epoch 1668/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 207.4755 - val_loss: 145.2785\n",
      "Epoch 1669/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.3860 - val_loss: 135.3966\n",
      "Epoch 1670/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.2701 - val_loss: 154.5516\n",
      "Epoch 1671/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.6344 - val_loss: 150.6038\n",
      "Epoch 1672/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.7765 - val_loss: 156.2212\n",
      "Epoch 1673/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.6098 - val_loss: 148.4460\n",
      "Epoch 1674/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.1354 - val_loss: 166.0760\n",
      "Epoch 1675/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 149.8473 - val_loss: 152.1782\n",
      "Epoch 1676/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 159.6626 - val_loss: 142.5519\n",
      "Epoch 1677/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.5869 - val_loss: 140.3197\n",
      "Epoch 1678/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 175.9725 - val_loss: 170.4493\n",
      "Epoch 1679/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 150.1284 - val_loss: 145.7959\n",
      "Epoch 1680/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.5500 - val_loss: 166.2924\n",
      "Epoch 1681/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 142.4841 - val_loss: 162.6813\n",
      "Epoch 1682/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.1627 - val_loss: 138.8168\n",
      "Epoch 1683/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 160.4503 - val_loss: 216.1410\n",
      "Epoch 1684/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.4297 - val_loss: 150.7956\n",
      "Epoch 1685/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.9968 - val_loss: 180.7230\n",
      "Epoch 1686/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.0745 - val_loss: 135.7662\n",
      "Epoch 1687/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.0423 - val_loss: 162.5149\n",
      "Epoch 1688/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 165.5913 - val_loss: 386.2178\n",
      "Epoch 1689/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 192.0922 - val_loss: 131.9674\n",
      "Epoch 1690/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.2384 - val_loss: 161.6577\n",
      "Epoch 1691/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.0125 - val_loss: 146.1899\n",
      "Epoch 1692/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 148.0945 - val_loss: 145.5599\n",
      "Epoch 1693/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 180.4517 - val_loss: 131.9183\n",
      "Epoch 1694/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.3772 - val_loss: 304.8753\n",
      "Epoch 1695/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.0381 - val_loss: 137.1691\n",
      "Epoch 1696/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.3384 - val_loss: 141.0577\n",
      "Epoch 1697/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 146.9004 - val_loss: 201.7731\n",
      "Epoch 1698/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.6131 - val_loss: 179.5219\n",
      "Epoch 1699/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.3982 - val_loss: 154.0203\n",
      "Epoch 1700/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 180.5010 - val_loss: 143.9334\n",
      "Epoch 1701/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 144.1334 - val_loss: 141.6905\n",
      "Epoch 1702/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 148.3019 - val_loss: 237.4050\n",
      "Epoch 1703/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.2075 - val_loss: 141.5960\n",
      "Epoch 1704/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.8721 - val_loss: 145.5029\n",
      "Epoch 1705/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.6713 - val_loss: 160.2175\n",
      "Epoch 1706/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.3880 - val_loss: 157.7959\n",
      "Epoch 1707/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.4677 - val_loss: 183.7754\n",
      "Epoch 1708/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.8912 - val_loss: 130.1609\n",
      "Epoch 1709/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.2763 - val_loss: 128.4727\n",
      "Epoch 1710/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.6515 - val_loss: 137.6764\n",
      "Epoch 1711/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.9405 - val_loss: 151.3167\n",
      "Epoch 1712/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 137.8053 - val_loss: 142.3336\n",
      "Epoch 1713/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.7530 - val_loss: 145.6882\n",
      "Epoch 1714/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.2003 - val_loss: 139.5044\n",
      "Epoch 1715/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 177.9524 - val_loss: 155.5826\n",
      "Epoch 1716/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.4638 - val_loss: 169.1869\n",
      "Epoch 1717/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 164.1244 - val_loss: 133.0708\n",
      "Epoch 1718/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.1152 - val_loss: 145.7589\n",
      "Epoch 1719/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.5815 - val_loss: 131.2836\n",
      "Epoch 1720/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.9678 - val_loss: 154.9226\n",
      "Epoch 1721/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.1002 - val_loss: 152.3056\n",
      "Epoch 1722/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 141.3180 - val_loss: 156.0333\n",
      "Epoch 1723/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 154.3617 - val_loss: 196.7942\n",
      "Epoch 1724/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 150.2640 - val_loss: 198.7086\n",
      "Epoch 1725/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.5938 - val_loss: 192.4286\n",
      "Epoch 1726/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 185.0492 - val_loss: 152.1753\n",
      "Epoch 1727/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 189.8809 - val_loss: 160.4095\n",
      "Epoch 1728/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.6961 - val_loss: 141.7578\n",
      "Epoch 1729/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.3901 - val_loss: 139.9784\n",
      "Epoch 1730/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 138.1052 - val_loss: 145.6520\n",
      "Epoch 1731/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 146.4781 - val_loss: 137.0350\n",
      "Epoch 1732/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.3197 - val_loss: 192.0159\n",
      "Epoch 1733/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.7481 - val_loss: 147.4871\n",
      "Epoch 1734/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 152.4650 - val_loss: 208.2147\n",
      "Epoch 1735/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 169.0112 - val_loss: 196.8167\n",
      "Epoch 1736/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.5375 - val_loss: 138.6798\n",
      "Epoch 1737/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.5679 - val_loss: 133.4935\n",
      "Epoch 1738/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.3776 - val_loss: 203.3875\n",
      "Epoch 1739/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 139.6632 - val_loss: 139.8012\n",
      "Epoch 1740/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 148.9650 - val_loss: 140.4922\n",
      "Epoch 1741/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 145.3171 - val_loss: 138.4689\n",
      "Epoch 1742/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 174.8291 - val_loss: 179.3477\n",
      "Epoch 1743/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 203.7398 - val_loss: 149.4760\n",
      "Epoch 1744/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 151.2901 - val_loss: 147.9794\n",
      "Epoch 1745/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 172.9855 - val_loss: 149.2740\n",
      "Epoch 1746/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 149.6961 - val_loss: 132.3968\n",
      "Epoch 1747/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 143.6832 - val_loss: 130.0795\n",
      "Epoch 1748/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 132.5077 - val_loss: 151.1836\n",
      "Epoch 1749/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 135.3408 - val_loss: 140.1878\n",
      "Epoch 1750/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 154.1501 - val_loss: 167.9308\n",
      "Epoch 1751/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 177.8547 - val_loss: 185.4867\n",
      "Epoch 1752/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 140.2645 - val_loss: 136.3891\n",
      "Epoch 1753/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 65us/step - loss: 148.4995 - val_loss: 215.8052\n",
      "Epoch 1754/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.5838 - val_loss: 163.7063\n",
      "Epoch 1755/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 142.4932 - val_loss: 182.3444\n",
      "Epoch 1756/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 188.6437 - val_loss: 144.5006\n",
      "Epoch 1757/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 177.5776 - val_loss: 156.8973\n",
      "Epoch 1758/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 138.0037 - val_loss: 152.1730\n",
      "Epoch 1759/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 136.4769 - val_loss: 227.4158\n",
      "Epoch 1760/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.2938 - val_loss: 154.3465\n",
      "Epoch 1761/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 167.8104 - val_loss: 147.6207\n",
      "Epoch 1762/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.9004 - val_loss: 143.1665\n",
      "Epoch 1763/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.7253 - val_loss: 131.3412\n",
      "Epoch 1764/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.4643 - val_loss: 321.5674\n",
      "Epoch 1765/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 149.3415 - val_loss: 230.8855\n",
      "Epoch 1766/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 135.3773 - val_loss: 142.3542\n",
      "Epoch 1767/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 175.5487 - val_loss: 151.9893\n",
      "Epoch 1768/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.8574 - val_loss: 145.7164\n",
      "Epoch 1769/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.8113 - val_loss: 261.7705\n",
      "Epoch 1770/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 154.7281 - val_loss: 143.8841\n",
      "Epoch 1771/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.8498 - val_loss: 148.0417\n",
      "Epoch 1772/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.2524 - val_loss: 205.4902\n",
      "Epoch 1773/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.5943 - val_loss: 147.0827\n",
      "Epoch 1774/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.2294 - val_loss: 138.9829\n",
      "Epoch 1775/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 137.8383 - val_loss: 145.7145\n",
      "Epoch 1776/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.7327 - val_loss: 131.4476\n",
      "Epoch 1777/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.5904 - val_loss: 142.6461\n",
      "Epoch 1778/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 143.9850 - val_loss: 145.0458\n",
      "Epoch 1779/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.9830 - val_loss: 158.3350\n",
      "Epoch 1780/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.0358 - val_loss: 164.6513\n",
      "Epoch 1781/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.3892 - val_loss: 166.0835\n",
      "Epoch 1782/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 155.2584 - val_loss: 153.3133\n",
      "Epoch 1783/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 137.8056 - val_loss: 131.4984\n",
      "Epoch 1784/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 146.8406 - val_loss: 154.4961\n",
      "Epoch 1785/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 158.0031 - val_loss: 133.4033\n",
      "Epoch 1786/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.0484 - val_loss: 149.3229\n",
      "Epoch 1787/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.3910 - val_loss: 194.6256\n",
      "Epoch 1788/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 141.9464 - val_loss: 152.3927\n",
      "Epoch 1789/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 139.7243 - val_loss: 133.4802\n",
      "Epoch 1790/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 163.6619 - val_loss: 182.6777\n",
      "Epoch 1791/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 139.4685 - val_loss: 136.7579\n",
      "Epoch 1792/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 210.3455 - val_loss: 144.0756\n",
      "Epoch 1793/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.0019 - val_loss: 133.8477\n",
      "Epoch 1794/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.0145 - val_loss: 144.0995\n",
      "Epoch 1795/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.4961 - val_loss: 142.1805\n",
      "Epoch 1796/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 137.2660 - val_loss: 205.2174\n",
      "Epoch 1797/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 140.4966 - val_loss: 145.1951\n",
      "Epoch 1798/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 140.3360 - val_loss: 137.2957\n",
      "Epoch 1799/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 136.9513 - val_loss: 150.5375\n",
      "Epoch 1800/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 147.8512 - val_loss: 152.4230\n",
      "Epoch 1801/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 146.8137 - val_loss: 133.8702\n",
      "Epoch 1802/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 139.5855 - val_loss: 279.2887\n",
      "Epoch 1803/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 187.8971 - val_loss: 149.8244\n",
      "Epoch 1804/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 134.8197 - val_loss: 140.2024\n",
      "Epoch 1805/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 184.9482 - val_loss: 151.8228\n",
      "Epoch 1806/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 147.3842 - val_loss: 131.2659\n",
      "Epoch 1807/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 135.7921 - val_loss: 133.1067\n",
      "Epoch 1808/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 138.7462 - val_loss: 141.0965\n",
      "Epoch 1809/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 146.0250 - val_loss: 147.4154\n",
      "Epoch 1810/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 141.6512 - val_loss: 139.4503\n",
      "Epoch 1811/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.2777 - val_loss: 182.3871\n",
      "Epoch 1812/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 131.5512 - val_loss: 143.0881\n",
      "Epoch 1813/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 153.7114 - val_loss: 136.0125\n",
      "Epoch 1814/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 156.2182 - val_loss: 185.1655\n",
      "Epoch 1815/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 162.2354 - val_loss: 158.8161\n",
      "Epoch 1816/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.6132 - val_loss: 179.0148\n",
      "Epoch 1817/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 138.5389 - val_loss: 186.5529\n",
      "Epoch 1818/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.4926 - val_loss: 146.3948\n",
      "Epoch 1819/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.6802 - val_loss: 182.3510\n",
      "Epoch 1820/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.7715 - val_loss: 133.9017\n",
      "Epoch 1821/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.3571 - val_loss: 186.2602\n",
      "Epoch 1822/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 149.2019 - val_loss: 155.3662\n",
      "Epoch 1823/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.4838 - val_loss: 157.4843\n",
      "Epoch 1824/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.1770 - val_loss: 250.8024\n",
      "Epoch 1825/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.8760 - val_loss: 132.8930\n",
      "Epoch 1826/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.4286 - val_loss: 138.3036\n",
      "Epoch 1827/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 144.1674 - val_loss: 135.2053\n",
      "Epoch 1828/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.6810 - val_loss: 179.6241\n",
      "Epoch 1829/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.4237 - val_loss: 135.4971\n",
      "Epoch 1830/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.0655 - val_loss: 158.1839\n",
      "Epoch 1831/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.0404 - val_loss: 147.0727\n",
      "Epoch 1832/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.3880 - val_loss: 195.7260\n",
      "Epoch 1833/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 193.9802 - val_loss: 163.1889\n",
      "Epoch 1834/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.3678 - val_loss: 170.5372\n",
      "Epoch 1835/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.2934 - val_loss: 160.6542\n",
      "Epoch 1836/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 143.8030 - val_loss: 138.9772\n",
      "Epoch 1837/10000\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 137.5852 - val_loss: 178.7028\n",
      "Epoch 1838/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 144.5273 - val_loss: 182.8530\n",
      "Epoch 1839/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 150.4878 - val_loss: 157.3388\n",
      "Epoch 1840/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.4619 - val_loss: 162.2794\n",
      "Epoch 1841/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 138.1863 - val_loss: 144.2536\n",
      "Epoch 1842/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.3669 - val_loss: 134.2078\n",
      "Epoch 1843/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 139.8026 - val_loss: 135.2982\n",
      "Epoch 1844/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.8061 - val_loss: 154.2455\n",
      "Epoch 1845/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.0662 - val_loss: 160.2187\n",
      "Epoch 1846/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.3406 - val_loss: 149.3847\n",
      "Epoch 1847/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 159.9260 - val_loss: 134.9107\n",
      "Epoch 1848/10000\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 135.6049 - val_loss: 217.9612\n",
      "Epoch 1849/10000\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 140.1833 - val_loss: 164.2550\n",
      "Epoch 1850/10000\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 135.0597 - val_loss: 132.9546\n",
      "Epoch 1851/10000\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 140.6374 - val_loss: 209.8425\n",
      "Epoch 1852/10000\n",
      "8000/8000 [==============================] - 2s 278us/step - loss: 155.4310 - val_loss: 150.5496\n",
      "Epoch 1853/10000\n",
      "8000/8000 [==============================] - 2s 251us/step - loss: 134.3645 - val_loss: 152.1629\n",
      "Epoch 1854/10000\n",
      "8000/8000 [==============================] - 2s 309us/step - loss: 138.4433 - val_loss: 191.8676\n",
      "Epoch 1855/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 169.0404 - val_loss: 185.7465\n",
      "Epoch 1856/10000\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 146.8099 - val_loss: 172.3642\n",
      "Epoch 1857/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 147.4128 - val_loss: 136.2773\n",
      "Epoch 1858/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.3214 - val_loss: 137.5542\n",
      "Epoch 1859/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 157.4865 - val_loss: 142.1220\n",
      "Epoch 1860/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 142.0150 - val_loss: 160.2746\n",
      "Epoch 1861/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 127.4758 - val_loss: 162.2472\n",
      "Epoch 1862/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 140.3106 - val_loss: 138.3829\n",
      "Epoch 1863/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 165.0790 - val_loss: 143.8583\n",
      "Epoch 1864/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.2587 - val_loss: 143.9105\n",
      "Epoch 1865/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 138.1828 - val_loss: 139.3206\n",
      "Epoch 1866/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 225.2238 - val_loss: 136.6396\n",
      "Epoch 1867/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.0847 - val_loss: 138.8335\n",
      "Epoch 1868/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 137.0276 - val_loss: 163.1708\n",
      "Epoch 1869/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 145.7846 - val_loss: 157.9161\n",
      "Epoch 1870/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 138.2415 - val_loss: 135.2542\n",
      "Epoch 1871/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 139.4667 - val_loss: 180.5160\n",
      "Epoch 1872/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 150.1904 - val_loss: 139.5461\n",
      "Epoch 1873/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 139.0319 - val_loss: 139.4070\n",
      "Epoch 1874/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 160.6635 - val_loss: 139.6927\n",
      "Epoch 1875/10000\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 145.3682 - val_loss: 141.2546\n",
      "Epoch 1876/10000\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 134.1336 - val_loss: 183.4806\n",
      "Epoch 1877/10000\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 144.8682 - val_loss: 145.5429\n",
      "Epoch 1878/10000\n",
      "8000/8000 [==============================] - 1s 163us/step - loss: 172.1664 - val_loss: 174.2273\n",
      "Epoch 1879/10000\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 146.1021 - val_loss: 215.6829\n",
      "Epoch 1880/10000\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 157.0597 - val_loss: 135.7973\n",
      "Epoch 1881/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 135.720 - 1s 142us/step - loss: 138.2615 - val_loss: 138.4711\n",
      "Epoch 1882/10000\n",
      "8000/8000 [==============================] - 1s 142us/step - loss: 162.5878 - val_loss: 353.6698\n",
      "Epoch 1883/10000\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 192.4969 - val_loss: 136.4579\n",
      "Epoch 1884/10000\n",
      "8000/8000 [==============================] - 1s 172us/step - loss: 131.8645 - val_loss: 144.4347\n",
      "Epoch 1885/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 135.4354 - val_loss: 141.0872\n",
      "Epoch 1886/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.6514 - val_loss: 136.7816\n",
      "Epoch 1887/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.0728 - val_loss: 167.1791\n",
      "Epoch 1888/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 149.2746 - val_loss: 138.4585\n",
      "Epoch 1889/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 154.8605 - val_loss: 170.5219\n",
      "Epoch 1890/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 169.2683 - val_loss: 158.0429\n",
      "Epoch 1891/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 153.2239 - val_loss: 150.5097\n",
      "Epoch 1892/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 152.0797 - val_loss: 163.2634\n",
      "Epoch 1893/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 137.9721 - val_loss: 146.3177\n",
      "Epoch 1894/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 139.6205 - val_loss: 154.2647\n",
      "Epoch 1895/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.1255 - val_loss: 165.9285\n",
      "Epoch 1896/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 133.1707 - val_loss: 140.3696\n",
      "Epoch 1897/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.7086 - val_loss: 150.6460\n",
      "Epoch 1898/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 146.8558 - val_loss: 136.9531\n",
      "Epoch 1899/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.4659 - val_loss: 195.4314\n",
      "Epoch 1900/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.2843 - val_loss: 152.2639\n",
      "Epoch 1901/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 161.8026 - val_loss: 137.4357\n",
      "Epoch 1902/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 159.7326 - val_loss: 137.2207\n",
      "Epoch 1903/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 136.3716 - val_loss: 132.5326\n",
      "Epoch 1904/10000\n",
      "8000/8000 [==============================] - 1s 143us/step - loss: 149.7106 - val_loss: 285.9763\n",
      "Epoch 1905/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 142.8987 - val_loss: 136.7776\n",
      "Epoch 1906/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.1135 - val_loss: 147.3081\n",
      "Epoch 1907/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.4017 - val_loss: 156.7201\n",
      "Epoch 1908/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 156.6876 - val_loss: 171.7250\n",
      "Epoch 1909/10000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 165.6413 - val_loss: 175.6936\n",
      "Epoch 1910/10000\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 154.5763 - val_loss: 137.4068\n",
      "Epoch 1911/10000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 149.2389 - val_loss: 142.0083\n",
      "Epoch 1912/10000\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 139.4100 - val_loss: 135.0623\n",
      "Epoch 1913/10000\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 141.7645 - val_loss: 151.4482\n",
      "Epoch 1914/10000\n",
      "8000/8000 [==============================] - 1s 140us/step - loss: 144.5975 - val_loss: 189.3065\n",
      "Epoch 1915/10000\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 149.6755 - val_loss: 131.5944\n",
      "Epoch 1916/10000\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 133.7986 - val_loss: 146.7840\n",
      "Epoch 1917/10000\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 145.9652 - val_loss: 145.2733\n",
      "Epoch 1918/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.5830 - val_loss: 185.1916\n",
      "Epoch 1919/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.2517 - val_loss: 157.3646\n",
      "Epoch 1920/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.5882 - val_loss: 141.1628\n",
      "Epoch 1921/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.0043 - val_loss: 133.7681\n",
      "Epoch 1922/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 133.3617 - val_loss: 154.7631\n",
      "Epoch 1923/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 139.8551 - val_loss: 138.4040\n",
      "Epoch 1924/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 132.7451 - val_loss: 137.9833\n",
      "Epoch 1925/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 150.1081 - val_loss: 152.8788\n",
      "Epoch 1926/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.1152 - val_loss: 146.9223\n",
      "Epoch 1927/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.3820 - val_loss: 137.2859\n",
      "Epoch 1928/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.0185 - val_loss: 250.8506\n",
      "Epoch 1929/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.4652 - val_loss: 156.6376\n",
      "Epoch 1930/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 163.2072 - val_loss: 136.8555\n",
      "Epoch 1931/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 138.8973 - val_loss: 135.2923\n",
      "Epoch 1932/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 139.7072 - val_loss: 147.8012\n",
      "Epoch 1933/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 142.1097 - val_loss: 136.9586\n",
      "Epoch 1934/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 144.0811 - val_loss: 139.2573\n",
      "Epoch 1935/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 153.3300 - val_loss: 152.0983\n",
      "Epoch 1936/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.7879 - val_loss: 175.7782\n",
      "Epoch 1937/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.8468 - val_loss: 133.6743\n",
      "Epoch 1938/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.2269 - val_loss: 137.1277\n",
      "Epoch 1939/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.3844 - val_loss: 157.9021\n",
      "Epoch 1940/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.4145 - val_loss: 145.9809\n",
      "Epoch 1941/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 186.0796 - val_loss: 150.3607\n",
      "Epoch 1942/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 145.3007 - val_loss: 137.0753\n",
      "Epoch 1943/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.3748 - val_loss: 182.5853\n",
      "Epoch 1944/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.5541 - val_loss: 144.8010\n",
      "Epoch 1945/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 132.1695 - val_loss: 208.3140\n",
      "Epoch 1946/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.6556 - val_loss: 169.9812\n",
      "Epoch 1947/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.6164 - val_loss: 219.3774\n",
      "Epoch 1948/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 231.3040 - val_loss: 136.6058\n",
      "Epoch 1949/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 188.6888 - val_loss: 136.1294\n",
      "Epoch 1950/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.1997 - val_loss: 144.6193\n",
      "Epoch 1951/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.1661 - val_loss: 135.5932\n",
      "Epoch 1952/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 148.2075 - val_loss: 143.7193\n",
      "Epoch 1953/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 135.5142 - val_loss: 148.9224\n",
      "Epoch 1954/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 148.4590 - val_loss: 136.1192\n",
      "Epoch 1955/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.7956 - val_loss: 135.4051\n",
      "Epoch 1956/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.2953 - val_loss: 142.0290\n",
      "Epoch 1957/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 158.9431 - val_loss: 139.6083\n",
      "Epoch 1958/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 158.4172 - val_loss: 137.2193\n",
      "Epoch 1959/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 151.8106 - val_loss: 152.7962\n",
      "Epoch 1960/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 412.5932 - val_loss: 269.5343\n",
      "Epoch 1961/10000\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 239.8296 - val_loss: 162.8568\n",
      "Epoch 1962/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 199.5764 - val_loss: 154.8724\n",
      "Epoch 1963/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 199.9244 - val_loss: 156.9256\n",
      "Epoch 1964/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 187.9208 - val_loss: 265.1980\n",
      "Epoch 1965/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 190.1032 - val_loss: 160.2921\n",
      "Epoch 1966/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 182.1699 - val_loss: 215.1386\n",
      "Epoch 1967/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 175.3965 - val_loss: 146.9293\n",
      "Epoch 1968/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 168.5661 - val_loss: 197.6704\n",
      "Epoch 1969/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 66us/step - loss: 179.3720 - val_loss: 215.0749\n",
      "Epoch 1970/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.5380 - val_loss: 225.9561\n",
      "Epoch 1971/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.3819 - val_loss: 176.8733\n",
      "Epoch 1972/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 153.6627 - val_loss: 149.0654\n",
      "Epoch 1973/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 175.7175 - val_loss: 148.2489\n",
      "Epoch 1974/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 171.0322 - val_loss: 184.9844\n",
      "Epoch 1975/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.8610 - val_loss: 185.0253\n",
      "Epoch 1976/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 190.2360 - val_loss: 156.4913\n",
      "Epoch 1977/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 165.7517 - val_loss: 220.1986\n",
      "Epoch 1978/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.7483 - val_loss: 145.5220\n",
      "Epoch 1979/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.5761 - val_loss: 146.1207\n",
      "Epoch 1980/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 168.5379 - val_loss: 154.5124\n",
      "Epoch 1981/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 172.7162 - val_loss: 157.0644\n",
      "Epoch 1982/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 161.6932 - val_loss: 174.7079\n",
      "Epoch 1983/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.0047 - val_loss: 181.7547\n",
      "Epoch 1984/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 166.9309 - val_loss: 150.7777\n",
      "Epoch 1985/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 158.7507 - val_loss: 137.6155\n",
      "Epoch 1986/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 170.8776 - val_loss: 260.2294\n",
      "Epoch 1987/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 168.0839 - val_loss: 189.5604\n",
      "Epoch 1988/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 177.7681 - val_loss: 156.1397\n",
      "Epoch 1989/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.0995 - val_loss: 173.7512\n",
      "Epoch 1990/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 157.9683 - val_loss: 260.0179\n",
      "Epoch 1991/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 166.8611 - val_loss: 188.5731\n",
      "Epoch 1992/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.1114 - val_loss: 152.5110\n",
      "Epoch 1993/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.9196 - val_loss: 154.8150\n",
      "Epoch 1994/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 171.3047 - val_loss: 143.3717\n",
      "Epoch 1995/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 177.7975 - val_loss: 150.3033\n",
      "Epoch 1996/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 149.9397 - val_loss: 142.2913\n",
      "Epoch 1997/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 151.8411 - val_loss: 141.4872\n",
      "Epoch 1998/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 145.5703 - val_loss: 167.3182\n",
      "Epoch 1999/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 156.1033 - val_loss: 140.4745\n",
      "Epoch 2000/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 192.9835 - val_loss: 153.7894\n",
      "Epoch 2001/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 146.7575 - val_loss: 137.5541\n",
      "Epoch 2002/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 197.1413 - val_loss: 140.3926\n",
      "Epoch 2003/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 256.2075 - val_loss: 422.9621\n",
      "Epoch 2004/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 371.3354 - val_loss: 355.7077\n",
      "Epoch 2005/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 233.8573 - val_loss: 210.6266\n",
      "Epoch 2006/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 197.4089 - val_loss: 170.4242\n",
      "Epoch 2007/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 196.2917 - val_loss: 167.8456\n",
      "Epoch 2008/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 186.4285 - val_loss: 162.1705\n",
      "Epoch 2009/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.9224 - val_loss: 216.7080\n",
      "Epoch 2010/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 175.6230 - val_loss: 148.1849\n",
      "Epoch 2011/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 173.5354 - val_loss: 172.2492\n",
      "Epoch 2012/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 176.0696 - val_loss: 167.5359\n",
      "Epoch 2013/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 172.5620 - val_loss: 141.8847\n",
      "Epoch 2014/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 172.1851 - val_loss: 158.7345\n",
      "Epoch 2015/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.8175 - val_loss: 217.8950\n",
      "Epoch 2016/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 185.7925 - val_loss: 143.1045\n",
      "Epoch 2017/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 173.8332 - val_loss: 169.7292\n",
      "Epoch 2018/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 157.7779 - val_loss: 198.0405\n",
      "Epoch 2019/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 174.0646 - val_loss: 256.4050\n",
      "Epoch 2020/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 172.1842 - val_loss: 145.2828\n",
      "Epoch 2021/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 194.5534 - val_loss: 173.5962\n",
      "Epoch 2022/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 170.5973 - val_loss: 159.3214\n",
      "Epoch 2023/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 162.7786 - val_loss: 183.7799\n",
      "Epoch 2024/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 151.6519 - val_loss: 156.0726\n",
      "Epoch 2025/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 165.9508 - val_loss: 152.4380\n",
      "Epoch 2026/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 164.7113 - val_loss: 153.3619\n",
      "Epoch 2027/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 155.3212 - val_loss: 187.3970\n",
      "Epoch 2028/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 164.4503 - val_loss: 140.9062\n",
      "Epoch 2029/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 159.6560 - val_loss: 142.9717\n",
      "Epoch 2030/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 170.2830 - val_loss: 156.8419\n",
      "Epoch 2031/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 159.3917 - val_loss: 150.0165\n",
      "Epoch 2032/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 155.7049 - val_loss: 161.9840\n",
      "Epoch 2033/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 157.4304 - val_loss: 202.5766\n",
      "Epoch 2034/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 161.1856 - val_loss: 199.6057\n",
      "Epoch 2035/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 162.693 - 1s 70us/step - loss: 161.0880 - val_loss: 232.2164\n",
      "Epoch 2036/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 168.5975 - val_loss: 139.9381\n",
      "Epoch 2037/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 150.6178 - val_loss: 144.8399\n",
      "Epoch 2038/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 159.5319 - val_loss: 141.6997\n",
      "Epoch 2039/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 161.3808 - val_loss: 157.6734\n",
      "Epoch 2040/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 163.5877 - val_loss: 141.0523\n",
      "Epoch 2041/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 88us/step - loss: 167.6635 - val_loss: 152.1583\n",
      "Epoch 2042/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 155.4857 - val_loss: 154.1296\n",
      "Epoch 2043/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 159.1090 - val_loss: 144.5275\n",
      "Epoch 2044/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 163.1270 - val_loss: 193.4662\n",
      "Epoch 2045/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 157.2900 - val_loss: 155.5520\n",
      "Epoch 2046/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 151.8781 - val_loss: 230.0399\n",
      "Epoch 2047/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 184.4111 - val_loss: 141.6387\n",
      "Epoch 2048/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 166.4344 - val_loss: 151.9416\n",
      "Epoch 2049/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 166.5342 - val_loss: 159.7480\n",
      "Epoch 2050/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 187.9454 - val_loss: 152.6466\n",
      "Epoch 2051/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 150.5193 - val_loss: 141.4013\n",
      "Epoch 2052/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.2145 - val_loss: 164.3039\n",
      "Epoch 2053/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 152.5792 - val_loss: 156.8191\n",
      "Epoch 2054/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 153.8210 - val_loss: 168.0765\n",
      "Epoch 2055/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.2413 - val_loss: 181.5063\n",
      "Epoch 2056/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 149.1833 - val_loss: 147.7386\n",
      "Epoch 2057/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 164.9006 - val_loss: 151.3018\n",
      "Epoch 2058/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.9660 - val_loss: 169.2148\n",
      "Epoch 2059/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.8382 - val_loss: 140.2061\n",
      "Epoch 2060/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 146.4001 - val_loss: 168.7153\n",
      "Epoch 2061/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 160.4287 - val_loss: 132.8230\n",
      "Epoch 2062/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 173.6400 - val_loss: 146.5456\n",
      "Epoch 2063/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 158.0790 - val_loss: 149.5673\n",
      "Epoch 2064/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 154.3685 - val_loss: 148.6928\n",
      "Epoch 2065/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.5213 - val_loss: 151.9120\n",
      "Epoch 2066/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.7617 - val_loss: 158.3288\n",
      "Epoch 2067/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 163.1554 - val_loss: 162.5756\n",
      "Epoch 2068/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 161.6870 - val_loss: 207.0021\n",
      "Epoch 2069/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.9510 - val_loss: 195.3610\n",
      "Epoch 2070/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 167.0712 - val_loss: 149.9901\n",
      "Epoch 2071/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 165.1731 - val_loss: 152.8302\n",
      "Epoch 2072/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.6153 - val_loss: 182.8959\n",
      "Epoch 2073/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.9327 - val_loss: 142.8737\n",
      "Epoch 2074/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 166.2670 - val_loss: 156.1552\n",
      "Epoch 2075/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 162.3202 - val_loss: 169.8015\n",
      "Epoch 2076/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 182.1945 - val_loss: 141.1443\n",
      "Epoch 2077/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 164.2145 - val_loss: 221.4414\n",
      "Epoch 2078/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 160.5818 - val_loss: 654.6287\n",
      "Epoch 2079/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 179.5489 - val_loss: 150.0671\n",
      "Epoch 2080/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 153.6188 - val_loss: 157.3097\n",
      "Epoch 2081/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 145.1914 - val_loss: 133.6159\n",
      "Epoch 2082/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 160.7704 - val_loss: 146.7415\n",
      "Epoch 2083/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.9523 - val_loss: 169.7714\n",
      "Epoch 2084/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 159.6392 - val_loss: 178.6092\n",
      "Epoch 2085/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 166.4357 - val_loss: 247.3811\n",
      "Epoch 2086/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 159.0852 - val_loss: 141.3564\n",
      "Epoch 2087/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.2563 - val_loss: 133.1547\n",
      "Epoch 2088/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 154.4258 - val_loss: 139.7752\n",
      "Epoch 2089/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.0055 - val_loss: 184.4948\n",
      "Epoch 2090/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 168.2583 - val_loss: 157.7321\n",
      "Epoch 2091/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 155.5723 - val_loss: 166.6335\n",
      "Epoch 2092/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 151.3696 - val_loss: 135.8095\n",
      "Epoch 2093/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 148.5628 - val_loss: 148.9212\n",
      "Epoch 2094/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 143.2745 - val_loss: 202.7914\n",
      "Epoch 2095/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 157.9574 - val_loss: 131.1373\n",
      "Epoch 2096/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 152.7141 - val_loss: 164.4319\n",
      "Epoch 2097/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 160.8730 - val_loss: 141.7132\n",
      "Epoch 2098/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 155.9797 - val_loss: 152.6542\n",
      "Epoch 2099/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 145.6625 - val_loss: 140.2501\n",
      "Epoch 2100/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 146.9764 - val_loss: 138.8514\n",
      "Epoch 2101/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.5647 - val_loss: 178.6195\n",
      "Epoch 2102/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.9376 - val_loss: 153.5927\n",
      "Epoch 2103/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 143.7921 - val_loss: 139.8832\n",
      "Epoch 2104/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 153.0508 - val_loss: 149.0069\n",
      "Epoch 2105/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 153.1163 - val_loss: 130.3516\n",
      "Epoch 2106/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 157.4300 - val_loss: 174.5645\n",
      "Epoch 2107/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.2956 - val_loss: 168.7630\n",
      "Epoch 2108/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.8521 - val_loss: 164.4828\n",
      "Epoch 2109/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 157.4522 - val_loss: 136.4534\n",
      "Epoch 2110/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 157.4590 - val_loss: 179.8125\n",
      "Epoch 2111/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 161.1736 - val_loss: 159.4611\n",
      "Epoch 2112/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.3319 - val_loss: 189.3818\n",
      "Epoch 2113/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 66us/step - loss: 150.4040 - val_loss: 169.9063\n",
      "Epoch 2114/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.2330 - val_loss: 145.3128\n",
      "Epoch 2115/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.8922 - val_loss: 150.9303\n",
      "Epoch 2116/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 150.2117 - val_loss: 164.5516\n",
      "Epoch 2117/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 168.8387 - val_loss: 139.0628\n",
      "Epoch 2118/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.9048 - val_loss: 188.7644\n",
      "Epoch 2119/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 165.6148 - val_loss: 173.6212\n",
      "Epoch 2120/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 157.4503 - val_loss: 138.0687\n",
      "Epoch 2121/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 151.1935 - val_loss: 137.5149\n",
      "Epoch 2122/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.9890 - val_loss: 152.7203\n",
      "Epoch 2123/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.2780 - val_loss: 135.3391\n",
      "Epoch 2124/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 153.8344 - val_loss: 191.5266\n",
      "Epoch 2125/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 146.2118 - val_loss: 182.5150\n",
      "Epoch 2126/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 185.3430 - val_loss: 162.5329\n",
      "Epoch 2127/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 159.3763 - val_loss: 144.4590\n",
      "Epoch 2128/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 144.2008 - val_loss: 186.8776\n",
      "Epoch 2129/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.6815 - val_loss: 165.0204\n",
      "Epoch 2130/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.4275 - val_loss: 252.8414\n",
      "Epoch 2131/10000\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 147.8842 - val_loss: 169.7576\n",
      "Epoch 2132/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 149.5193 - val_loss: 129.7285\n",
      "Epoch 2133/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.5623 - val_loss: 150.9937\n",
      "Epoch 2134/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.1669 - val_loss: 175.8908\n",
      "Epoch 2135/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 156.9405 - val_loss: 142.3373\n",
      "Epoch 2136/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.2905 - val_loss: 134.2501\n",
      "Epoch 2137/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.0259 - val_loss: 145.7367\n",
      "Epoch 2138/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.2194 - val_loss: 133.7017\n",
      "Epoch 2139/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 162.5235 - val_loss: 135.8980\n",
      "Epoch 2140/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.8848 - val_loss: 159.5491\n",
      "Epoch 2141/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.1828 - val_loss: 157.2543\n",
      "Epoch 2142/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.7861 - val_loss: 137.9163\n",
      "Epoch 2143/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 161.3162 - val_loss: 138.4963\n",
      "Epoch 2144/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 158.3658 - val_loss: 136.7834\n",
      "Epoch 2145/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.4124 - val_loss: 151.2621\n",
      "Epoch 2146/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.3712 - val_loss: 150.6276\n",
      "Epoch 2147/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 155.8119 - val_loss: 142.1209\n",
      "Epoch 2148/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.6092 - val_loss: 155.9920\n",
      "Epoch 2149/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.3830 - val_loss: 198.3150\n",
      "Epoch 2150/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.5468 - val_loss: 306.9214\n",
      "Epoch 2151/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 167.5635 - val_loss: 199.8006\n",
      "Epoch 2152/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 139.7076 - val_loss: 162.4037\n",
      "Epoch 2153/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 133.7109 - val_loss: 286.0819\n",
      "Epoch 2154/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 151.0734 - val_loss: 163.5325\n",
      "Epoch 2155/10000\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 167.5626 - val_loss: 329.7670\n",
      "Epoch 2156/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 171.8829 - val_loss: 141.1495\n",
      "Epoch 2157/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 138.9209 - val_loss: 154.1853\n",
      "Epoch 2158/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 144.9735 - val_loss: 134.9331\n",
      "Epoch 2159/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.5266 - val_loss: 133.7681\n",
      "Epoch 2160/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.0669 - val_loss: 140.9186\n",
      "Epoch 2161/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 208.9481 - val_loss: 167.3060\n",
      "Epoch 2162/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 156.6207 - val_loss: 143.9009\n",
      "Epoch 2163/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 159.5944 - val_loss: 142.8570\n",
      "Epoch 2164/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 161.0271 - val_loss: 139.7773\n",
      "Epoch 2165/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.6679 - val_loss: 139.9660\n",
      "Epoch 2166/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.0779 - val_loss: 138.8841\n",
      "Epoch 2167/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.9566 - val_loss: 149.7786\n",
      "Epoch 2168/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 146.2314 - val_loss: 173.9159\n",
      "Epoch 2169/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 148.7336 - val_loss: 149.0402\n",
      "Epoch 2170/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.5826 - val_loss: 131.4075\n",
      "Epoch 2171/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.5357 - val_loss: 192.6481\n",
      "Epoch 2172/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 169.9060 - val_loss: 161.3614\n",
      "Epoch 2173/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.5663 - val_loss: 193.2317\n",
      "Epoch 2174/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.8367 - val_loss: 152.4562\n",
      "Epoch 2175/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.1337 - val_loss: 256.6413\n",
      "Epoch 2176/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 155.9687 - val_loss: 165.0298\n",
      "Epoch 2177/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.2130 - val_loss: 154.2424\n",
      "Epoch 2178/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.4966 - val_loss: 167.1266\n",
      "Epoch 2179/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.4432 - val_loss: 266.9627\n",
      "Epoch 2180/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 151.8821 - val_loss: 129.6523\n",
      "Epoch 2181/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 150.8778 - val_loss: 165.5023\n",
      "Epoch 2182/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 183.4481 - val_loss: 258.3956\n",
      "Epoch 2183/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 161.3051 - val_loss: 164.6533\n",
      "Epoch 2184/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 175.4479 - val_loss: 212.4798\n",
      "Epoch 2185/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 66us/step - loss: 156.8766 - val_loss: 167.1683\n",
      "Epoch 2186/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 147.4637 - val_loss: 143.2863\n",
      "Epoch 2187/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.7939 - val_loss: 173.2185\n",
      "Epoch 2188/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 141.4507 - val_loss: 139.7183\n",
      "Epoch 2189/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 162.1115 - val_loss: 162.4522\n",
      "Epoch 2190/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 147.4267 - val_loss: 160.8838\n",
      "Epoch 2191/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.4205 - val_loss: 146.2688\n",
      "Epoch 2192/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.5533 - val_loss: 144.0932\n",
      "Epoch 2193/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.7379 - val_loss: 181.3387\n",
      "Epoch 2194/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.8552 - val_loss: 187.6399\n",
      "Epoch 2195/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 155.3676 - val_loss: 151.2048\n",
      "Epoch 2196/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 165.1649 - val_loss: 147.3982\n",
      "Epoch 2197/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.7198 - val_loss: 152.0035\n",
      "Epoch 2198/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 147.7889 - val_loss: 173.3147\n",
      "Epoch 2199/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.4161 - val_loss: 141.3900\n",
      "Epoch 2200/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 139.7494 - val_loss: 136.3334\n",
      "Epoch 2201/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.6370 - val_loss: 164.9260\n",
      "Epoch 2202/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 266.6914 - val_loss: 140.3795\n",
      "Epoch 2203/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 178.6914 - val_loss: 144.3297\n",
      "Epoch 2204/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 161.1755 - val_loss: 183.4897\n",
      "Epoch 2205/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.1877 - val_loss: 137.9588\n",
      "Epoch 2206/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 151.9010 - val_loss: 249.1762\n",
      "Epoch 2207/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 139.3854 - val_loss: 134.4632\n",
      "Epoch 2208/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 154.0442 - val_loss: 148.2241\n",
      "Epoch 2209/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.1473 - val_loss: 142.6152\n",
      "Epoch 2210/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.4925 - val_loss: 137.1326\n",
      "Epoch 2211/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.4684 - val_loss: 143.3294\n",
      "Epoch 2212/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.8278 - val_loss: 152.0342\n",
      "Epoch 2213/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 141.9260 - val_loss: 173.1404\n",
      "Epoch 2214/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.3232 - val_loss: 153.0655\n",
      "Epoch 2215/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.0348 - val_loss: 133.0117\n",
      "Epoch 2216/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 143.3980 - val_loss: 147.4389\n",
      "Epoch 2217/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 155.5822 - val_loss: 164.2762\n",
      "Epoch 2218/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 137.8892 - val_loss: 141.0115\n",
      "Epoch 2219/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.7451 - val_loss: 157.6377\n",
      "Epoch 2220/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.6773 - val_loss: 142.7888\n",
      "Epoch 2221/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 148.9452 - val_loss: 212.5928\n",
      "Epoch 2222/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 161.0552 - val_loss: 138.6152\n",
      "Epoch 2223/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 169.6496 - val_loss: 172.5139\n",
      "Epoch 2224/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 159.7298 - val_loss: 187.2851\n",
      "Epoch 2225/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 153.1173 - val_loss: 139.3646\n",
      "Epoch 2226/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 137.5023 - val_loss: 137.3565\n",
      "Epoch 2227/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.1651 - val_loss: 162.4459\n",
      "Epoch 2228/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.1677 - val_loss: 203.5835\n",
      "Epoch 2229/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 156.5672 - val_loss: 183.2795\n",
      "Epoch 2230/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.4796 - val_loss: 146.9988\n",
      "Epoch 2231/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.3168 - val_loss: 233.7972\n",
      "Epoch 2232/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.5628 - val_loss: 161.8689\n",
      "Epoch 2233/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 153.5911 - val_loss: 145.6907\n",
      "Epoch 2234/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 146.2403 - val_loss: 171.6638\n",
      "Epoch 2235/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.8234 - val_loss: 133.1792\n",
      "Epoch 2236/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.4279 - val_loss: 139.0969\n",
      "Epoch 2237/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.4077 - val_loss: 136.1325\n",
      "Epoch 2238/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.9601 - val_loss: 140.6111\n",
      "Epoch 2239/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 171.7589 - val_loss: 145.2553\n",
      "Epoch 2240/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.1083 - val_loss: 160.4184\n",
      "Epoch 2241/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.6558 - val_loss: 186.9936\n",
      "Epoch 2242/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.5487 - val_loss: 144.6385\n",
      "Epoch 2243/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.6763 - val_loss: 137.7063\n",
      "Epoch 2244/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.4111 - val_loss: 152.5337\n",
      "Epoch 2245/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 183.3872 - val_loss: 147.8292\n",
      "Epoch 2246/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.0153 - val_loss: 192.6004\n",
      "Epoch 2247/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.6340 - val_loss: 132.8138\n",
      "Epoch 2248/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.1851 - val_loss: 268.2130\n",
      "Epoch 2249/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 179.5451 - val_loss: 144.7573\n",
      "Epoch 2250/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.7235 - val_loss: 160.2090\n",
      "Epoch 2251/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.4216 - val_loss: 138.4261\n",
      "Epoch 2252/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.9841 - val_loss: 138.5814\n",
      "Epoch 2253/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.0664 - val_loss: 169.9284\n",
      "Epoch 2254/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.4351 - val_loss: 149.2435\n",
      "Epoch 2255/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.4741 - val_loss: 147.6245\n",
      "Epoch 2256/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.9843 - val_loss: 168.0408\n",
      "Epoch 2257/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.3747 - val_loss: 152.6437\n",
      "Epoch 2258/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.1528 - val_loss: 443.8998\n",
      "Epoch 2259/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.4892 - val_loss: 134.4069\n",
      "Epoch 2260/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.2074 - val_loss: 131.1200\n",
      "Epoch 2261/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 161.4512 - val_loss: 135.6797\n",
      "Epoch 2262/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.6132 - val_loss: 186.2890\n",
      "Epoch 2263/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.1832 - val_loss: 198.9291\n",
      "Epoch 2264/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 146.2025 - val_loss: 134.6016\n",
      "Epoch 2265/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 152.2002 - val_loss: 155.8931\n",
      "Epoch 2266/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 141.3636 - val_loss: 130.3744\n",
      "Epoch 2267/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.5683 - val_loss: 149.6620\n",
      "Epoch 2268/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.6968 - val_loss: 137.9592\n",
      "Epoch 2269/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.3319 - val_loss: 269.7610\n",
      "Epoch 2270/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.7886 - val_loss: 197.4107\n",
      "Epoch 2271/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.5290 - val_loss: 179.0342\n",
      "Epoch 2272/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 153.9858 - val_loss: 250.1978\n",
      "Epoch 2273/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.1819 - val_loss: 141.8593\n",
      "Epoch 2274/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.8454 - val_loss: 150.0932\n",
      "Epoch 2275/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.1629 - val_loss: 133.8990\n",
      "Epoch 2276/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.5116 - val_loss: 190.3002\n",
      "Epoch 2277/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.6969 - val_loss: 130.9500\n",
      "Epoch 2278/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.1407 - val_loss: 182.9714\n",
      "Epoch 2279/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.1943 - val_loss: 154.6439\n",
      "Epoch 2280/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.2721 - val_loss: 206.2493\n",
      "Epoch 2281/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.6738 - val_loss: 149.4729\n",
      "Epoch 2282/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.4008 - val_loss: 139.9384\n",
      "Epoch 2283/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.0429 - val_loss: 140.0079\n",
      "Epoch 2284/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 176.3322 - val_loss: 149.2024\n",
      "Epoch 2285/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.6450 - val_loss: 165.2105\n",
      "Epoch 2286/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.2577 - val_loss: 176.7629\n",
      "Epoch 2287/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 159.4154 - val_loss: 134.0602\n",
      "Epoch 2288/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 139.6708 - val_loss: 152.3131\n",
      "Epoch 2289/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.4029 - val_loss: 170.0424\n",
      "Epoch 2290/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.5712 - val_loss: 193.2469\n",
      "Epoch 2291/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.0379 - val_loss: 143.5554\n",
      "Epoch 2292/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.4574 - val_loss: 184.9192\n",
      "Epoch 2293/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.9775 - val_loss: 171.3788\n",
      "Epoch 2294/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.7719 - val_loss: 162.1827\n",
      "Epoch 2295/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.3989 - val_loss: 139.4359\n",
      "Epoch 2296/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.1920 - val_loss: 152.0478\n",
      "Epoch 2297/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.8337 - val_loss: 152.0782\n",
      "Epoch 2298/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.9387 - val_loss: 142.0191\n",
      "Epoch 2299/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.3011 - val_loss: 134.0921\n",
      "Epoch 2300/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.4692 - val_loss: 138.0212\n",
      "Epoch 2301/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.9583 - val_loss: 233.3500\n",
      "Epoch 2302/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.2118 - val_loss: 134.9660\n",
      "Epoch 2303/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.4388 - val_loss: 136.9702\n",
      "Epoch 2304/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 146.2358 - val_loss: 131.1061\n",
      "Epoch 2305/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.7285 - val_loss: 130.4252\n",
      "Epoch 2306/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.0931 - val_loss: 291.6553\n",
      "Epoch 2307/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.7435 - val_loss: 128.5820\n",
      "Epoch 2308/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 142.7671 - val_loss: 156.6775\n",
      "Epoch 2309/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.7004 - val_loss: 149.1720\n",
      "Epoch 2310/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.6260 - val_loss: 139.8075\n",
      "Epoch 2311/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.6192 - val_loss: 137.1552\n",
      "Epoch 2312/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.4478 - val_loss: 156.0097\n",
      "Epoch 2313/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.3671 - val_loss: 237.4810\n",
      "Epoch 2314/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.5098 - val_loss: 189.3465\n",
      "Epoch 2315/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.8997 - val_loss: 189.9179\n",
      "Epoch 2316/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 148.8014 - val_loss: 207.3774\n",
      "Epoch 2317/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 163.0186 - val_loss: 168.3854\n",
      "Epoch 2318/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.5979 - val_loss: 132.8893\n",
      "Epoch 2319/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.0385 - val_loss: 155.1384\n",
      "Epoch 2320/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 139.1617 - val_loss: 152.8708\n",
      "Epoch 2321/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.7280 - val_loss: 157.2589\n",
      "Epoch 2322/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 150.7935 - val_loss: 159.4294\n",
      "Epoch 2323/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 154.3887 - val_loss: 131.9229\n",
      "Epoch 2324/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 163.8793 - val_loss: 302.9899\n",
      "Epoch 2325/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 144.8836 - val_loss: 140.4412\n",
      "Epoch 2326/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.4907 - val_loss: 139.6594\n",
      "Epoch 2327/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 134.6657 - val_loss: 180.9732\n",
      "Epoch 2328/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.7331 - val_loss: 131.7464\n",
      "Epoch 2329/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.9388 - val_loss: 138.5277\n",
      "Epoch 2330/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.3476 - val_loss: 133.6225\n",
      "Epoch 2331/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 149.2754 - val_loss: 133.9852\n",
      "Epoch 2332/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.9936 - val_loss: 172.1549\n",
      "Epoch 2333/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.3018 - val_loss: 253.0520\n",
      "Epoch 2334/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 149.9926 - val_loss: 169.9331\n",
      "Epoch 2335/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 142.5431 - val_loss: 159.8986\n",
      "Epoch 2336/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.6304 - val_loss: 152.1391\n",
      "Epoch 2337/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.4662 - val_loss: 156.5781\n",
      "Epoch 2338/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 156.6440 - val_loss: 143.3532\n",
      "Epoch 2339/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 175.1268 - val_loss: 190.9698\n",
      "Epoch 2340/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.2738 - val_loss: 243.0648\n",
      "Epoch 2341/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.4665 - val_loss: 128.7727\n",
      "Epoch 2342/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.9382 - val_loss: 156.7897\n",
      "Epoch 2343/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 138.8290 - val_loss: 165.5216\n",
      "Epoch 2344/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.2289 - val_loss: 138.5770\n",
      "Epoch 2345/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 143.2454 - val_loss: 140.9280\n",
      "Epoch 2346/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 135.2843 - val_loss: 134.5807\n",
      "Epoch 2347/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.7459 - val_loss: 157.0582\n",
      "Epoch 2348/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 157.9727 - val_loss: 132.6604\n",
      "Epoch 2349/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.6374 - val_loss: 193.1312\n",
      "Epoch 2350/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 151.4341 - val_loss: 145.1659\n",
      "Epoch 2351/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 143.4952 - val_loss: 160.5976\n",
      "Epoch 2352/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 162.8844 - val_loss: 136.2272\n",
      "Epoch 2353/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 147.3941 - val_loss: 170.7468\n",
      "Epoch 2354/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.2141 - val_loss: 158.8574\n",
      "Epoch 2355/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 147.1022 - val_loss: 160.1751\n",
      "Epoch 2356/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.2849 - val_loss: 135.5775\n",
      "Epoch 2357/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.7814 - val_loss: 146.8019\n",
      "Epoch 2358/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.0797 - val_loss: 145.9363\n",
      "Epoch 2359/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.0897 - val_loss: 155.9965\n",
      "Epoch 2360/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.9232 - val_loss: 160.9698\n",
      "Epoch 2361/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 156.6147 - val_loss: 147.4615\n",
      "Epoch 2362/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.5580 - val_loss: 246.0374\n",
      "Epoch 2363/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.9187 - val_loss: 135.1907\n",
      "Epoch 2364/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.6374 - val_loss: 132.9025\n",
      "Epoch 2365/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 143.3720 - val_loss: 209.9567\n",
      "Epoch 2366/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.3176 - val_loss: 144.0355\n",
      "Epoch 2367/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 154.0832 - val_loss: 152.6989\n",
      "Epoch 2368/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.0915 - val_loss: 135.9768\n",
      "Epoch 2369/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 165.5257 - val_loss: 135.4545\n",
      "Epoch 2370/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.1095 - val_loss: 166.5087\n",
      "Epoch 2371/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.2382 - val_loss: 142.9885\n",
      "Epoch 2372/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.8231 - val_loss: 136.2834\n",
      "Epoch 2373/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.3875 - val_loss: 149.8241\n",
      "Epoch 2374/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.9321 - val_loss: 190.4386\n",
      "Epoch 2375/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.9225 - val_loss: 176.8256\n",
      "Epoch 2376/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.7831 - val_loss: 134.3567\n",
      "Epoch 2377/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.5482 - val_loss: 169.8191\n",
      "Epoch 2378/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.7734 - val_loss: 156.9190\n",
      "Epoch 2379/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 138.9330 - val_loss: 161.1376\n",
      "Epoch 2380/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 143.5207 - val_loss: 135.9017\n",
      "Epoch 2381/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 136.2441 - val_loss: 155.3312\n",
      "Epoch 2382/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 141.0376 - val_loss: 140.2739\n",
      "Epoch 2383/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.7043 - val_loss: 266.3811\n",
      "Epoch 2384/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.3496 - val_loss: 146.6565\n",
      "Epoch 2385/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.0025 - val_loss: 201.2724\n",
      "Epoch 2386/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 147.1177 - val_loss: 198.1740\n",
      "Epoch 2387/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.0117 - val_loss: 139.2632\n",
      "Epoch 2388/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.0096 - val_loss: 166.9300\n",
      "Epoch 2389/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.2206 - val_loss: 142.3555\n",
      "Epoch 2390/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.2614 - val_loss: 140.3729\n",
      "Epoch 2391/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.7134 - val_loss: 136.6985\n",
      "Epoch 2392/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.2655 - val_loss: 133.5973\n",
      "Epoch 2393/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.4948 - val_loss: 133.9067\n",
      "Epoch 2394/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 155.8702 - val_loss: 150.2652\n",
      "Epoch 2395/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.7603 - val_loss: 162.2078\n",
      "Epoch 2396/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.6876 - val_loss: 154.7183\n",
      "Epoch 2397/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.0806 - val_loss: 132.4556\n",
      "Epoch 2398/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 150.5581 - val_loss: 131.4651\n",
      "Epoch 2399/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.9131 - val_loss: 146.6911\n",
      "Epoch 2400/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.7939 - val_loss: 136.2923\n",
      "Epoch 2401/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.7995 - val_loss: 147.6708\n",
      "Epoch 2402/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 148.4557 - val_loss: 126.1815TA: 0s - loss: 144\n",
      "Epoch 2403/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 158.7848 - val_loss: 177.8186\n",
      "Epoch 2404/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 138.1657 - val_loss: 133.9166\n",
      "Epoch 2405/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 135.5284 - val_loss: 128.8690\n",
      "Epoch 2406/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 153.6231 - val_loss: 178.2695\n",
      "Epoch 2407/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.0086 - val_loss: 148.2087\n",
      "Epoch 2408/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.3189 - val_loss: 133.3889\n",
      "Epoch 2409/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 160.7647 - val_loss: 205.0830\n",
      "Epoch 2410/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 147.5952 - val_loss: 143.0617\n",
      "Epoch 2411/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.5743 - val_loss: 132.5183\n",
      "Epoch 2412/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.9519 - val_loss: 130.0022\n",
      "Epoch 2413/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 145.1761 - val_loss: 160.5612\n",
      "Epoch 2414/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 150.0232 - val_loss: 128.8199\n",
      "Epoch 2415/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.2788 - val_loss: 133.3777\n",
      "Epoch 2416/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 140.7970 - val_loss: 143.4966\n",
      "Epoch 2417/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 144.5408 - val_loss: 144.6591\n",
      "Epoch 2418/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 142.707 - 1s 63us/step - loss: 144.7525 - val_loss: 173.7105\n",
      "Epoch 2419/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 146.8439 - val_loss: 141.8668\n",
      "Epoch 2420/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.2201 - val_loss: 152.5869\n",
      "Epoch 2421/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.4472 - val_loss: 217.8171\n",
      "Epoch 2422/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.8047 - val_loss: 154.4634\n",
      "Epoch 2423/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.6285 - val_loss: 129.7998\n",
      "Epoch 2424/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.4177 - val_loss: 171.4967\n",
      "Epoch 2425/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 146.7227 - val_loss: 136.4253\n",
      "Epoch 2426/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 173.4986 - val_loss: 139.1727\n",
      "Epoch 2427/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.0449 - val_loss: 128.6267\n",
      "Epoch 2428/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 151.0878 - val_loss: 138.2726\n",
      "Epoch 2429/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 161.9159 - val_loss: 147.1536\n",
      "Epoch 2430/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.5992 - val_loss: 182.2613\n",
      "Epoch 2431/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.9842 - val_loss: 135.6139\n",
      "Epoch 2432/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 152.9480 - val_loss: 141.7393\n",
      "Epoch 2433/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.7594 - val_loss: 134.4240\n",
      "Epoch 2434/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.6018 - val_loss: 140.1429\n",
      "Epoch 2435/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 158.3554 - val_loss: 150.9819\n",
      "Epoch 2436/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 142.6509 - val_loss: 195.5407\n",
      "Epoch 2437/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 151.0272 - val_loss: 127.9107\n",
      "Epoch 2438/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 152.7250 - val_loss: 137.7815\n",
      "Epoch 2439/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 135.7305 - val_loss: 165.3465\n",
      "Epoch 2440/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 138.2686 - val_loss: 140.1918\n",
      "Epoch 2441/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 143.1753 - val_loss: 137.8731\n",
      "Epoch 2442/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.4935 - val_loss: 135.4897\n",
      "Epoch 2443/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.3831 - val_loss: 135.0761\n",
      "Epoch 2444/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.5259 - val_loss: 143.2188\n",
      "Epoch 2445/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 154.0232 - val_loss: 387.1452\n",
      "Epoch 2446/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.7073 - val_loss: 148.5418\n",
      "Epoch 2447/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.0321 - val_loss: 156.3520\n",
      "Epoch 2448/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.8267 - val_loss: 145.4730\n",
      "Epoch 2449/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 136.2438 - val_loss: 152.7228\n",
      "Epoch 2450/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.9741 - val_loss: 176.9365\n",
      "Epoch 2451/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.1027 - val_loss: 140.0738\n",
      "Epoch 2452/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 157.5163 - val_loss: 207.5942\n",
      "Epoch 2453/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.3002 - val_loss: 148.1581\n",
      "Epoch 2454/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 142.8269 - val_loss: 219.0840\n",
      "Epoch 2455/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.6331 - val_loss: 143.7120\n",
      "Epoch 2456/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.3582 - val_loss: 146.5902\n",
      "Epoch 2457/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.3893 - val_loss: 146.8269\n",
      "Epoch 2458/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 148.8688 - val_loss: 192.5050\n",
      "Epoch 2459/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.9497 - val_loss: 172.5569\n",
      "Epoch 2460/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 133.0311 - val_loss: 139.1165\n",
      "Epoch 2461/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 135.7389 - val_loss: 135.3006\n",
      "Epoch 2462/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 135.7036 - val_loss: 144.1435\n",
      "Epoch 2463/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 156.0347 - val_loss: 156.9443\n",
      "Epoch 2464/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 145.3049 - val_loss: 155.3808\n",
      "Epoch 2465/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.0675 - val_loss: 139.6343\n",
      "Epoch 2466/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 134.1264 - val_loss: 136.8807\n",
      "Epoch 2467/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.5555 - val_loss: 134.1042\n",
      "Epoch 2468/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 134.7453 - val_loss: 149.8808\n",
      "Epoch 2469/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.2833 - val_loss: 176.9809\n",
      "Epoch 2470/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 138.1819 - val_loss: 167.8506\n",
      "Epoch 2471/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.6948 - val_loss: 136.6604\n",
      "Epoch 2472/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.5152 - val_loss: 139.7307\n",
      "Epoch 2473/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.3053 - val_loss: 138.7738\n",
      "Epoch 2474/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.7735 - val_loss: 186.8700\n",
      "Epoch 2475/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.8265 - val_loss: 129.6959\n",
      "Epoch 2476/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.9983 - val_loss: 140.4910\n",
      "Epoch 2477/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.3813 - val_loss: 160.9112\n",
      "Epoch 2478/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.1227 - val_loss: 133.3256\n",
      "Epoch 2479/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.6340 - val_loss: 170.7487\n",
      "Epoch 2480/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 157.8872 - val_loss: 153.3961\n",
      "Epoch 2481/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 154.0950 - val_loss: 142.2072\n",
      "Epoch 2482/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 133.0396 - val_loss: 166.6349\n",
      "Epoch 2483/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 139.4413 - val_loss: 136.1093\n",
      "Epoch 2484/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 148.5068 - val_loss: 142.0665\n",
      "Epoch 2485/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.6350 - val_loss: 131.6854\n",
      "Epoch 2486/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.9106 - val_loss: 132.2338\n",
      "Epoch 2487/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 138.7542 - val_loss: 379.5724\n",
      "Epoch 2488/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.8907 - val_loss: 174.6025\n",
      "Epoch 2489/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.0319 - val_loss: 154.0251\n",
      "Epoch 2490/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.2318 - val_loss: 194.0997\n",
      "Epoch 2491/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 151.7137 - val_loss: 138.8255\n",
      "Epoch 2492/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.7699 - val_loss: 141.2267\n",
      "Epoch 2493/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 154.2103 - val_loss: 147.6673\n",
      "Epoch 2494/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 144.3050 - val_loss: 184.3147\n",
      "Epoch 2495/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 139.4339 - val_loss: 155.8324\n",
      "Epoch 2496/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 151.6193 - val_loss: 139.5187\n",
      "Epoch 2497/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 135.3486 - val_loss: 142.4936\n",
      "Epoch 2498/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 151.5494 - val_loss: 206.7032\n",
      "Epoch 2499/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.0438 - val_loss: 175.9413\n",
      "Epoch 2500/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 133.7279 - val_loss: 133.6404\n",
      "Epoch 2501/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.8437 - val_loss: 139.8903\n",
      "Epoch 2502/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 142.6951 - val_loss: 140.1755\n",
      "Epoch 2503/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 144.6858 - val_loss: 137.9753\n",
      "Epoch 2504/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.0632 - val_loss: 159.8558\n",
      "Epoch 2505/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.8065 - val_loss: 129.6670\n",
      "Epoch 2506/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.8856 - val_loss: 136.9931\n",
      "Epoch 2507/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 156.0404 - val_loss: 138.2070\n",
      "Epoch 2508/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.8025 - val_loss: 186.9604\n",
      "Epoch 2509/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.9996 - val_loss: 150.1379\n",
      "Epoch 2510/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 137.2135 - val_loss: 153.3595\n",
      "Epoch 2511/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 182.6644 - val_loss: 138.8812\n",
      "Epoch 2512/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.0320 - val_loss: 133.4901\n",
      "Epoch 2513/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.4694 - val_loss: 147.4347\n",
      "Epoch 2514/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 147.4656 - val_loss: 225.9145\n",
      "Epoch 2515/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.6056 - val_loss: 146.0961\n",
      "Epoch 2516/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 157.5599 - val_loss: 156.2824\n",
      "Epoch 2517/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 144.2629 - val_loss: 143.5721\n",
      "Epoch 2518/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.7802 - val_loss: 148.1455\n",
      "Epoch 2519/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 143.4121 - val_loss: 132.3340\n",
      "Epoch 2520/10000\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 132.4878 - val_loss: 142.0869\n",
      "Epoch 2521/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 136.8474 - val_loss: 132.7524\n",
      "Epoch 2522/10000\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 144.3063 - val_loss: 159.6755\n",
      "Epoch 2523/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 141.2226 - val_loss: 144.4917\n",
      "Epoch 2524/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.8682 - val_loss: 179.9166\n",
      "Epoch 2525/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.8995 - val_loss: 161.3923\n",
      "Epoch 2526/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.8122 - val_loss: 131.6641\n",
      "Epoch 2527/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.9751 - val_loss: 139.5051\n",
      "Epoch 2528/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.3635 - val_loss: 153.2712\n",
      "Epoch 2529/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.0125 - val_loss: 152.5727\n",
      "Epoch 2530/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.7496 - val_loss: 131.9402\n",
      "Epoch 2531/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.9600 - val_loss: 136.1046\n",
      "Epoch 2532/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.1187 - val_loss: 147.5711\n",
      "Epoch 2533/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.4668 - val_loss: 138.9230\n",
      "Epoch 2534/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.6543 - val_loss: 158.5053\n",
      "Epoch 2535/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 140.6221 - val_loss: 159.7321\n",
      "Epoch 2536/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.4796 - val_loss: 132.8885\n",
      "Epoch 2537/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.4981 - val_loss: 132.1842\n",
      "Epoch 2538/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 135.0548 - val_loss: 171.8834\n",
      "Epoch 2539/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 156.6207 - val_loss: 139.2920\n",
      "Epoch 2540/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 143.4315 - val_loss: 132.1960\n",
      "Epoch 2541/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 132.7056 - val_loss: 141.9370\n",
      "Epoch 2542/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.4758 - val_loss: 177.2378\n",
      "Epoch 2543/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 134.2256 - val_loss: 138.0031\n",
      "Epoch 2544/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.7387 - val_loss: 143.7716\n",
      "Epoch 2545/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.6151 - val_loss: 168.8206\n",
      "Epoch 2546/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.3565 - val_loss: 159.2364\n",
      "Epoch 2547/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.2223 - val_loss: 139.3501\n",
      "Epoch 2548/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.7171 - val_loss: 133.8726\n",
      "Epoch 2549/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 138.1079 - val_loss: 155.7481\n",
      "Epoch 2550/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 137.5057 - val_loss: 137.5913\n",
      "Epoch 2551/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 141.2009 - val_loss: 137.5242\n",
      "Epoch 2552/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.0605 - val_loss: 165.9266\n",
      "Epoch 2553/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.1827 - val_loss: 140.2011\n",
      "Epoch 2554/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 156.3459 - val_loss: 148.4051\n",
      "Epoch 2555/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.9056 - val_loss: 147.8118\n",
      "Epoch 2556/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.3429 - val_loss: 150.8012\n",
      "Epoch 2557/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.1844 - val_loss: 135.6678\n",
      "Epoch 2558/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 153.1447 - val_loss: 187.2880\n",
      "Epoch 2559/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.6131 - val_loss: 143.5366\n",
      "Epoch 2560/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.1772 - val_loss: 157.0336\n",
      "Epoch 2561/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.0112 - val_loss: 134.2193\n",
      "Epoch 2562/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 149.4095 - val_loss: 211.6492\n",
      "Epoch 2563/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.8164 - val_loss: 254.2215\n",
      "Epoch 2564/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.9204 - val_loss: 131.3554\n",
      "Epoch 2565/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.5507 - val_loss: 140.0362\n",
      "Epoch 2566/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 134.0615 - val_loss: 193.9157\n",
      "Epoch 2567/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.3347 - val_loss: 128.9097\n",
      "Epoch 2568/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.7828 - val_loss: 195.3972\n",
      "Epoch 2569/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.4552 - val_loss: 171.3077\n",
      "Epoch 2570/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 150.5993 - val_loss: 131.8048\n",
      "Epoch 2571/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.8207 - val_loss: 176.0399\n",
      "Epoch 2572/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.7525 - val_loss: 173.4135\n",
      "Epoch 2573/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.7679 - val_loss: 131.5406\n",
      "Epoch 2574/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 135.1175 - val_loss: 151.1246\n",
      "Epoch 2575/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.0807 - val_loss: 186.2389\n",
      "Epoch 2576/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.1994 - val_loss: 150.6761\n",
      "Epoch 2577/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.7314 - val_loss: 152.1406\n",
      "Epoch 2578/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.0364 - val_loss: 129.9543\n",
      "Epoch 2579/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.5178 - val_loss: 159.0942\n",
      "Epoch 2580/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 133.9751 - val_loss: 142.1061\n",
      "Epoch 2581/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.5477 - val_loss: 146.5679\n",
      "Epoch 2582/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 136.0522 - val_loss: 150.9733\n",
      "Epoch 2583/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.1258 - val_loss: 210.0132\n",
      "Epoch 2584/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.4892 - val_loss: 139.0064\n",
      "Epoch 2585/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.2143 - val_loss: 154.7053\n",
      "Epoch 2586/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.9387 - val_loss: 138.0508\n",
      "Epoch 2587/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.1157 - val_loss: 128.5423\n",
      "Epoch 2588/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.3219 - val_loss: 142.5316\n",
      "Epoch 2589/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.2200 - val_loss: 134.8248\n",
      "Epoch 2590/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.9956 - val_loss: 144.6888\n",
      "Epoch 2591/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.0907 - val_loss: 126.6207\n",
      "Epoch 2592/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.7661 - val_loss: 133.6474\n",
      "Epoch 2593/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.5168 - val_loss: 129.8927\n",
      "Epoch 2594/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.7684 - val_loss: 206.1692\n",
      "Epoch 2595/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 134.2954 - val_loss: 164.3338\n",
      "Epoch 2596/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.7588 - val_loss: 131.9020\n",
      "Epoch 2597/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.1311 - val_loss: 157.7337\n",
      "Epoch 2598/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.8112 - val_loss: 144.8883\n",
      "Epoch 2599/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.0382 - val_loss: 131.0015\n",
      "Epoch 2600/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.7725 - val_loss: 149.0278\n",
      "Epoch 2601/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.2674 - val_loss: 129.6059\n",
      "Epoch 2602/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 139.8401 - val_loss: 138.7006\n",
      "Epoch 2603/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.7323 - val_loss: 165.7478\n",
      "Epoch 2604/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.2324 - val_loss: 131.3034\n",
      "Epoch 2605/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 163.8050 - val_loss: 245.1020\n",
      "Epoch 2606/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.3056 - val_loss: 151.7051\n",
      "Epoch 2607/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.7312 - val_loss: 150.9569\n",
      "Epoch 2608/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 137.4672 - val_loss: 139.5543\n",
      "Epoch 2609/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 151.2408 - val_loss: 131.3639\n",
      "Epoch 2610/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 142.7110 - val_loss: 132.2419\n",
      "Epoch 2611/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.9962 - val_loss: 144.4447\n",
      "Epoch 2612/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.6560 - val_loss: 167.1886\n",
      "Epoch 2613/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 163.0805 - val_loss: 188.3216\n",
      "Epoch 2614/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.7796 - val_loss: 170.0776\n",
      "Epoch 2615/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.1339 - val_loss: 142.4421\n",
      "Epoch 2616/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.7536 - val_loss: 136.9363\n",
      "Epoch 2617/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 148.6257 - val_loss: 131.7012\n",
      "Epoch 2618/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.2786 - val_loss: 135.8343\n",
      "Epoch 2619/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.9327 - val_loss: 164.1289\n",
      "Epoch 2620/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.0220 - val_loss: 194.3922\n",
      "Epoch 2621/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 144.2880 - val_loss: 165.4690\n",
      "Epoch 2622/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 134.2077 - val_loss: 177.3790\n",
      "Epoch 2623/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.1166 - val_loss: 139.3640\n",
      "Epoch 2624/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.1241 - val_loss: 157.7858\n",
      "Epoch 2625/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.6652 - val_loss: 127.6524\n",
      "Epoch 2626/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.2984 - val_loss: 131.6998\n",
      "Epoch 2627/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.7097 - val_loss: 132.8041\n",
      "Epoch 2628/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.8115 - val_loss: 247.2318\n",
      "Epoch 2629/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 144.4331 - val_loss: 146.1497\n",
      "Epoch 2630/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.8140 - val_loss: 167.6399\n",
      "Epoch 2631/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 132.3216 - val_loss: 124.6206\n",
      "Epoch 2632/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.0713 - val_loss: 144.4345\n",
      "Epoch 2633/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 137.1737 - val_loss: 141.9555\n",
      "Epoch 2634/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 142.2771 - val_loss: 160.6249\n",
      "Epoch 2635/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.3049 - val_loss: 188.2478\n",
      "Epoch 2636/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.1344 - val_loss: 134.5164\n",
      "Epoch 2637/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.1983 - val_loss: 141.6274\n",
      "Epoch 2638/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.9560 - val_loss: 134.7358\n",
      "Epoch 2639/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.8605 - val_loss: 151.1218\n",
      "Epoch 2640/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.8416 - val_loss: 146.1797\n",
      "Epoch 2641/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 148.4113 - val_loss: 132.2413\n",
      "Epoch 2642/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.5772 - val_loss: 144.2718\n",
      "Epoch 2643/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 160.5848 - val_loss: 128.3816\n",
      "Epoch 2644/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 137.4895 - val_loss: 158.2801\n",
      "Epoch 2645/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.1424 - val_loss: 135.3747\n",
      "Epoch 2646/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.0479 - val_loss: 138.0889\n",
      "Epoch 2647/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.4671 - val_loss: 142.1356\n",
      "Epoch 2648/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 150.5355 - val_loss: 143.8861\n",
      "Epoch 2649/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 170.6865 - val_loss: 135.6796\n",
      "Epoch 2650/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.1580 - val_loss: 134.0534\n",
      "Epoch 2651/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.0755 - val_loss: 132.0128\n",
      "Epoch 2652/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 140.6202 - val_loss: 134.3831\n",
      "Epoch 2653/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 161.7311 - val_loss: 150.5224\n",
      "Epoch 2654/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.2287 - val_loss: 141.1936\n",
      "Epoch 2655/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.4378 - val_loss: 155.4979\n",
      "Epoch 2656/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 139.9901 - val_loss: 129.1588\n",
      "Epoch 2657/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.9482 - val_loss: 185.8699\n",
      "Epoch 2658/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 159.1906 - val_loss: 152.7902\n",
      "Epoch 2659/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.1538 - val_loss: 148.0554\n",
      "Epoch 2660/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 132.0149 - val_loss: 154.3327\n",
      "Epoch 2661/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.8876 - val_loss: 130.7942\n",
      "Epoch 2662/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.9507 - val_loss: 132.4279\n",
      "Epoch 2663/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.0010 - val_loss: 168.0116\n",
      "Epoch 2664/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 141.5594 - val_loss: 145.1611\n",
      "Epoch 2665/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 139.4572 - val_loss: 149.0579\n",
      "Epoch 2666/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.1183 - val_loss: 155.1085\n",
      "Epoch 2667/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.7858 - val_loss: 136.4415\n",
      "Epoch 2668/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 158.7097 - val_loss: 131.2416\n",
      "Epoch 2669/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.2452 - val_loss: 127.7170\n",
      "Epoch 2670/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.3407 - val_loss: 132.7494\n",
      "Epoch 2671/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.5232 - val_loss: 163.8658\n",
      "Epoch 2672/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 177.4770 - val_loss: 135.5434\n",
      "Epoch 2673/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 139.2232 - val_loss: 128.1104\n",
      "Epoch 2674/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 139.4620 - val_loss: 133.3161\n",
      "Epoch 2675/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 136.5552 - val_loss: 201.2281\n",
      "Epoch 2676/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.3317 - val_loss: 151.6491\n",
      "Epoch 2677/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.4986 - val_loss: 161.7623\n",
      "Epoch 2678/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 156.9106 - val_loss: 148.7615\n",
      "Epoch 2679/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.7948 - val_loss: 135.2708\n",
      "Epoch 2680/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 140.8038 - val_loss: 178.8445\n",
      "Epoch 2681/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.3148 - val_loss: 172.4318\n",
      "Epoch 2682/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.0435 - val_loss: 143.8178\n",
      "Epoch 2683/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.0002 - val_loss: 160.2943\n",
      "Epoch 2684/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.9284 - val_loss: 155.1685\n",
      "Epoch 2685/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.0141 - val_loss: 150.1908\n",
      "Epoch 2686/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.2128 - val_loss: 133.6839\n",
      "Epoch 2687/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 150.9426 - val_loss: 137.5805\n",
      "Epoch 2688/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.4864 - val_loss: 162.7666\n",
      "Epoch 2689/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 64us/step - loss: 142.1008 - val_loss: 151.6178\n",
      "Epoch 2690/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.4111 - val_loss: 153.2851\n",
      "Epoch 2691/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.0223 - val_loss: 143.1169\n",
      "Epoch 2692/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.3892 - val_loss: 139.8457\n",
      "Epoch 2693/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 147.6544 - val_loss: 159.8995\n",
      "Epoch 2694/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 142.9036 - val_loss: 143.0713\n",
      "Epoch 2695/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.9803 - val_loss: 137.3980\n",
      "Epoch 2696/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.6701 - val_loss: 157.0410\n",
      "Epoch 2697/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.8362 - val_loss: 143.8892\n",
      "Epoch 2698/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.6203 - val_loss: 144.3856\n",
      "Epoch 2699/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.7041 - val_loss: 135.6532\n",
      "Epoch 2700/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.9593 - val_loss: 140.6641\n",
      "Epoch 2701/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.5098 - val_loss: 208.2058\n",
      "Epoch 2702/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.4806 - val_loss: 142.8971\n",
      "Epoch 2703/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.0545 - val_loss: 135.9832\n",
      "Epoch 2704/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.7406 - val_loss: 135.2912\n",
      "Epoch 2705/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.8646 - val_loss: 141.8993\n",
      "Epoch 2706/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.3162 - val_loss: 137.5690\n",
      "Epoch 2707/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 146.1694 - val_loss: 129.3448\n",
      "Epoch 2708/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.0356 - val_loss: 137.6688\n",
      "Epoch 2709/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.5275 - val_loss: 136.5406\n",
      "Epoch 2710/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.6159 - val_loss: 167.8833\n",
      "Epoch 2711/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.1308 - val_loss: 127.5543\n",
      "Epoch 2712/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.0405 - val_loss: 131.1884\n",
      "Epoch 2713/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.1895 - val_loss: 135.8100\n",
      "Epoch 2714/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.9297 - val_loss: 132.2492\n",
      "Epoch 2715/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.0590 - val_loss: 142.4909\n",
      "Epoch 2716/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.6577 - val_loss: 138.2917\n",
      "Epoch 2717/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.0852 - val_loss: 178.2647\n",
      "Epoch 2718/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 136.4721 - val_loss: 177.4868\n",
      "Epoch 2719/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.0704 - val_loss: 134.9631\n",
      "Epoch 2720/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 133.4742 - val_loss: 173.8494\n",
      "Epoch 2721/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.5708 - val_loss: 134.9903\n",
      "Epoch 2722/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 147.5029 - val_loss: 143.6963\n",
      "Epoch 2723/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 140.0316 - val_loss: 132.1637\n",
      "Epoch 2724/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 162.6978 - val_loss: 150.0671\n",
      "Epoch 2725/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 143.7035 - val_loss: 134.8381\n",
      "Epoch 2726/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.8646 - val_loss: 127.8744\n",
      "Epoch 2727/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.1244 - val_loss: 139.7939\n",
      "Epoch 2728/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 163.4078 - val_loss: 131.4930\n",
      "Epoch 2729/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 132.9228 - val_loss: 148.2352\n",
      "Epoch 2730/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.6002 - val_loss: 132.7313\n",
      "Epoch 2731/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.8159 - val_loss: 132.0287\n",
      "Epoch 2732/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.1618 - val_loss: 156.5225\n",
      "Epoch 2733/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 137.0292 - val_loss: 139.1228\n",
      "Epoch 2734/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.4917 - val_loss: 151.6413\n",
      "Epoch 2735/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.8277 - val_loss: 136.8901\n",
      "Epoch 2736/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.9351 - val_loss: 130.9471\n",
      "Epoch 2737/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 144.6107 - val_loss: 131.8763\n",
      "Epoch 2738/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.9806 - val_loss: 137.5599\n",
      "Epoch 2739/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.9823 - val_loss: 135.7235\n",
      "Epoch 2740/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.2112 - val_loss: 150.6490\n",
      "Epoch 2741/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 150.4863 - val_loss: 180.4689\n",
      "Epoch 2742/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.8982 - val_loss: 186.1448\n",
      "Epoch 2743/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.3155 - val_loss: 142.3472\n",
      "Epoch 2744/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.8210 - val_loss: 142.3686\n",
      "Epoch 2745/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.3306 - val_loss: 125.8069\n",
      "Epoch 2746/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 144.1461 - val_loss: 147.7308\n",
      "Epoch 2747/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.8950 - val_loss: 157.3006\n",
      "Epoch 2748/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 143.3409 - val_loss: 159.6181\n",
      "Epoch 2749/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.3313 - val_loss: 136.0034\n",
      "Epoch 2750/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.5256 - val_loss: 163.0453\n",
      "Epoch 2751/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.2423 - val_loss: 134.2701\n",
      "Epoch 2752/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.7231 - val_loss: 140.5830\n",
      "Epoch 2753/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.3967 - val_loss: 141.5044\n",
      "Epoch 2754/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.0763 - val_loss: 137.6127\n",
      "Epoch 2755/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.9953 - val_loss: 145.6379\n",
      "Epoch 2756/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 142.8301 - val_loss: 148.3047\n",
      "Epoch 2757/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 139.1890 - val_loss: 144.3749\n",
      "Epoch 2758/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 148.9417 - val_loss: 140.7789\n",
      "Epoch 2759/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 142.5595 - val_loss: 148.3315\n",
      "Epoch 2760/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 141.6739 - val_loss: 138.9369\n",
      "Epoch 2761/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 81us/step - loss: 135.1486 - val_loss: 148.4266\n",
      "Epoch 2762/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 135.1926 - val_loss: 128.7225\n",
      "Epoch 2763/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 132.9935 - val_loss: 128.5090\n",
      "Epoch 2764/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.5669 - val_loss: 188.0078\n",
      "Epoch 2765/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 137.4334 - val_loss: 144.1486\n",
      "Epoch 2766/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.7681 - val_loss: 140.9956\n",
      "Epoch 2767/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.1610 - val_loss: 139.0221\n",
      "Epoch 2768/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 133.0448 - val_loss: 144.8583\n",
      "Epoch 2769/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.3820 - val_loss: 162.6201\n",
      "Epoch 2770/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 136.4340 - val_loss: 152.9560\n",
      "Epoch 2771/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 151.5602 - val_loss: 185.9297\n",
      "Epoch 2772/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 160.5697 - val_loss: 147.0378\n",
      "Epoch 2773/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.3728 - val_loss: 126.7113\n",
      "Epoch 2774/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.3744 - val_loss: 136.0690\n",
      "Epoch 2775/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 136.6318 - val_loss: 128.5082\n",
      "Epoch 2776/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.5159 - val_loss: 132.0537\n",
      "Epoch 2777/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.6923 - val_loss: 138.0736\n",
      "Epoch 2778/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.0969 - val_loss: 133.6330\n",
      "Epoch 2779/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.4351 - val_loss: 168.0103\n",
      "Epoch 2780/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 134.2207 - val_loss: 130.8957\n",
      "Epoch 2781/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 133.9673 - val_loss: 139.9838\n",
      "Epoch 2782/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 140.6802 - val_loss: 131.6746\n",
      "Epoch 2783/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.1415 - val_loss: 183.5935\n",
      "Epoch 2784/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.0865 - val_loss: 199.8544\n",
      "Epoch 2785/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 149.5188 - val_loss: 133.8175\n",
      "Epoch 2786/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 135.3272 - val_loss: 137.3914\n",
      "Epoch 2787/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.2845 - val_loss: 183.6210\n",
      "Epoch 2788/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.9573 - val_loss: 155.7220\n",
      "Epoch 2789/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.6804 - val_loss: 177.1368\n",
      "Epoch 2790/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 146.5649 - val_loss: 131.9569\n",
      "Epoch 2791/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.1468 - val_loss: 132.8246\n",
      "Epoch 2792/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.8190 - val_loss: 126.1517\n",
      "Epoch 2793/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.2624 - val_loss: 182.2157\n",
      "Epoch 2794/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 188.0605 - val_loss: 137.1849\n",
      "Epoch 2795/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 163.9763 - val_loss: 146.6813\n",
      "Epoch 2796/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 205.9644 - val_loss: 141.4374\n",
      "Epoch 2797/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.6245 - val_loss: 155.2047\n",
      "Epoch 2798/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 178.1846 - val_loss: 137.3642\n",
      "Epoch 2799/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 173.2782 - val_loss: 152.2683\n",
      "Epoch 2800/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.3046 - val_loss: 143.8143\n",
      "Epoch 2801/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 151.7405 - val_loss: 152.1319\n",
      "Epoch 2802/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 156.7676 - val_loss: 146.7019\n",
      "Epoch 2803/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 155.8811 - val_loss: 134.8590\n",
      "Epoch 2804/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 193.3957 - val_loss: 141.4193\n",
      "Epoch 2805/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 157.8313 - val_loss: 139.4569\n",
      "Epoch 2806/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 159.7898 - val_loss: 142.0431\n",
      "Epoch 2807/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.5515 - val_loss: 131.8434\n",
      "Epoch 2808/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 177.6991 - val_loss: 142.0660\n",
      "Epoch 2809/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 168.3780 - val_loss: 157.9861\n",
      "Epoch 2810/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 170.8853 - val_loss: 155.9260\n",
      "Epoch 2811/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 173.2504 - val_loss: 140.7442\n",
      "Epoch 2812/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.5388 - val_loss: 134.0122\n",
      "Epoch 2813/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 150.1370 - val_loss: 155.4363\n",
      "Epoch 2814/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 133.0342 - val_loss: 160.3786\n",
      "Epoch 2815/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.3887 - val_loss: 164.3425\n",
      "Epoch 2816/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.3643 - val_loss: 128.8410\n",
      "Epoch 2817/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 165.9435 - val_loss: 147.2550\n",
      "Epoch 2818/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.3189 - val_loss: 196.5722\n",
      "Epoch 2819/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.6179 - val_loss: 134.2339\n",
      "Epoch 2820/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.1664 - val_loss: 190.5409\n",
      "Epoch 2821/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.0863 - val_loss: 162.0610\n",
      "Epoch 2822/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.6651 - val_loss: 151.0372\n",
      "Epoch 2823/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.1526 - val_loss: 134.5742\n",
      "Epoch 2824/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.1469 - val_loss: 162.7776\n",
      "Epoch 2825/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.1363 - val_loss: 131.0936\n",
      "Epoch 2826/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.3536 - val_loss: 132.5207\n",
      "Epoch 2827/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.2680 - val_loss: 151.0531\n",
      "Epoch 2828/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.1781 - val_loss: 153.7091\n",
      "Epoch 2829/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 148.7738 - val_loss: 126.7512\n",
      "Epoch 2830/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.3676 - val_loss: 193.9208\n",
      "Epoch 2831/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 147.6432 - val_loss: 134.5340\n",
      "Epoch 2832/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.4902 - val_loss: 146.5418\n",
      "Epoch 2833/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.3092 - val_loss: 148.7810\n",
      "Epoch 2834/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.6162 - val_loss: 133.1276\n",
      "Epoch 2835/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.2607 - val_loss: 124.9195\n",
      "Epoch 2836/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 135.9919 - val_loss: 136.0975\n",
      "Epoch 2837/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 142.4943 - val_loss: 130.4118\n",
      "Epoch 2838/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 141.8670 - val_loss: 156.9967\n",
      "Epoch 2839/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 160.9911 - val_loss: 149.3282\n",
      "Epoch 2840/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 140.9349 - val_loss: 130.7990\n",
      "Epoch 2841/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 138.5153 - val_loss: 132.3097\n",
      "Epoch 2842/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.6146 - val_loss: 141.7093\n",
      "Epoch 2843/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.0197 - val_loss: 134.8339\n",
      "Epoch 2844/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.0117 - val_loss: 128.7728\n",
      "Epoch 2845/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 167.3105 - val_loss: 161.4371\n",
      "Epoch 2846/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.5007 - val_loss: 131.7171\n",
      "Epoch 2847/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.6585 - val_loss: 243.5380\n",
      "Epoch 2848/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 146.4420 - val_loss: 145.1327\n",
      "Epoch 2849/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 153.3838 - val_loss: 169.0334\n",
      "Epoch 2850/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 137.4578 - val_loss: 137.8927\n",
      "Epoch 2851/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 154.4314 - val_loss: 132.4466\n",
      "Epoch 2852/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 135.6713 - val_loss: 143.1910\n",
      "Epoch 2853/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.5528 - val_loss: 182.7867\n",
      "Epoch 2854/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.7002 - val_loss: 149.2590\n",
      "Epoch 2855/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 144.2893 - val_loss: 228.8354\n",
      "Epoch 2856/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 144.3954 - val_loss: 137.6732\n",
      "Epoch 2857/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.9666 - val_loss: 134.2677\n",
      "Epoch 2858/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.0045 - val_loss: 128.7123\n",
      "Epoch 2859/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.4807 - val_loss: 177.3769\n",
      "Epoch 2860/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.1702 - val_loss: 156.0964\n",
      "Epoch 2861/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 137.5866 - val_loss: 126.1254\n",
      "Epoch 2862/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.2427 - val_loss: 139.1186\n",
      "Epoch 2863/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 142.9133 - val_loss: 146.8192\n",
      "Epoch 2864/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 142.8520 - val_loss: 197.9827\n",
      "Epoch 2865/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.6521 - val_loss: 142.4910\n",
      "Epoch 2866/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.0300 - val_loss: 133.7457\n",
      "Epoch 2867/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 129.7237 - val_loss: 153.9454\n",
      "Epoch 2868/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.4956 - val_loss: 146.1219\n",
      "Epoch 2869/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.8532 - val_loss: 125.6380\n",
      "Epoch 2870/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.8996 - val_loss: 129.3512\n",
      "Epoch 2871/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 135.3146 - val_loss: 149.8724\n",
      "Epoch 2872/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.6916 - val_loss: 241.5856\n",
      "Epoch 2873/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.2151 - val_loss: 134.3018\n",
      "Epoch 2874/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 139.5384 - val_loss: 136.3897\n",
      "Epoch 2875/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 135.6744 - val_loss: 144.2887\n",
      "Epoch 2876/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.2361 - val_loss: 201.1890\n",
      "Epoch 2877/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.9907 - val_loss: 185.0991\n",
      "Epoch 2878/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 132.2080 - val_loss: 141.0889\n",
      "Epoch 2879/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 129.9784 - val_loss: 127.5139\n",
      "Epoch 2880/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 132.6503 - val_loss: 143.4402\n",
      "Epoch 2881/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.8326 - val_loss: 132.5586\n",
      "Epoch 2882/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 135.6385 - val_loss: 149.9649\n",
      "Epoch 2883/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.2677 - val_loss: 156.3933\n",
      "Epoch 2884/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.3269 - val_loss: 165.1168\n",
      "Epoch 2885/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.9714 - val_loss: 200.8934\n",
      "Epoch 2886/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 135.9466 - val_loss: 154.0050\n",
      "Epoch 2887/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.3067 - val_loss: 131.5328\n",
      "Epoch 2888/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.8716 - val_loss: 150.1862\n",
      "Epoch 2889/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 136.3876 - val_loss: 189.2068\n",
      "Epoch 2890/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 134.0141 - val_loss: 220.0596\n",
      "Epoch 2891/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 154.8152 - val_loss: 131.9621\n",
      "Epoch 2892/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 133.0786 - val_loss: 138.2231\n",
      "Epoch 2893/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 148.1318 - val_loss: 155.4364\n",
      "Epoch 2894/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.2208 - val_loss: 167.4706\n",
      "Epoch 2895/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.8658 - val_loss: 143.2735\n",
      "Epoch 2896/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 135.8514 - val_loss: 139.1197\n",
      "Epoch 2897/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 133.7296 - val_loss: 153.9844\n",
      "Epoch 2898/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 132.5012 - val_loss: 135.5659\n",
      "Epoch 2899/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 136.6379 - val_loss: 138.8677\n",
      "Epoch 2900/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 132.4054 - val_loss: 205.3190\n",
      "Epoch 2901/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 163.9454 - val_loss: 139.1066\n",
      "Epoch 2902/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.1946 - val_loss: 201.8621\n",
      "Epoch 2903/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 152.1464 - val_loss: 138.4588\n",
      "Epoch 2904/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 131.8219 - val_loss: 143.5470\n",
      "Epoch 2905/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.7811 - val_loss: 148.6614\n",
      "Epoch 2906/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.2283 - val_loss: 148.2241\n",
      "Epoch 2907/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 135.9087 - val_loss: 174.3500\n",
      "Epoch 2908/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 144.6536 - val_loss: 136.4746\n",
      "Epoch 2909/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.4595 - val_loss: 132.8066\n",
      "Epoch 2910/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.8571 - val_loss: 184.3727\n",
      "Epoch 2911/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.2165 - val_loss: 141.1994\n",
      "Epoch 2912/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 131.8543 - val_loss: 131.6411\n",
      "Epoch 2913/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.5371 - val_loss: 160.2622\n",
      "Epoch 2914/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.6236 - val_loss: 165.7021\n",
      "Epoch 2915/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 137.2167 - val_loss: 137.7509\n",
      "Epoch 2916/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 136.8168 - val_loss: 155.8625\n",
      "Epoch 2917/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 140.5237 - val_loss: 171.9709\n",
      "Epoch 2918/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 155.7328 - val_loss: 215.2542\n",
      "Epoch 2919/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.4671 - val_loss: 137.4090\n",
      "Epoch 2920/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.3967 - val_loss: 203.9623\n",
      "Epoch 2921/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.1675 - val_loss: 146.1132\n",
      "Epoch 2922/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.0231 - val_loss: 180.7457\n",
      "Epoch 2923/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 148.3911 - val_loss: 134.9338\n",
      "Epoch 2924/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.8431 - val_loss: 142.3273\n",
      "Epoch 2925/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.6581 - val_loss: 146.2243\n",
      "Epoch 2926/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.7788 - val_loss: 144.7148\n",
      "Epoch 2927/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 155.5294 - val_loss: 128.8685\n",
      "Epoch 2928/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.4867 - val_loss: 127.7266\n",
      "Epoch 2929/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.4352 - val_loss: 137.4719\n",
      "Epoch 2930/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.1677 - val_loss: 154.6256\n",
      "Epoch 2931/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 140.4443 - val_loss: 181.1914\n",
      "Epoch 2932/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.4790 - val_loss: 192.2873\n",
      "Epoch 2933/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 135.1308 - val_loss: 188.4645\n",
      "Epoch 2934/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 146.9895 - val_loss: 182.5409\n",
      "Epoch 2935/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 133.3578 - val_loss: 145.4020\n",
      "Epoch 2936/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 156.2998 - val_loss: 151.5130\n",
      "Epoch 2937/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.7921 - val_loss: 139.9854\n",
      "Epoch 2938/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.6721 - val_loss: 183.9765\n",
      "Epoch 2939/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.1519 - val_loss: 163.5268\n",
      "Epoch 2940/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.3771 - val_loss: 131.1911\n",
      "Epoch 2941/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.4164 - val_loss: 127.2431\n",
      "Epoch 2942/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 128.8667 - val_loss: 148.1885\n",
      "Epoch 2943/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 150.3967 - val_loss: 155.2582\n",
      "Epoch 2944/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.0384 - val_loss: 136.0089\n",
      "Epoch 2945/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.2560 - val_loss: 155.6654\n",
      "Epoch 2946/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.9381 - val_loss: 178.4078\n",
      "Epoch 2947/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.1943 - val_loss: 194.9727\n",
      "Epoch 2948/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.6729 - val_loss: 135.1685\n",
      "Epoch 2949/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.4878 - val_loss: 151.0354\n",
      "Epoch 2950/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.4330 - val_loss: 134.9219\n",
      "Epoch 2951/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.1884 - val_loss: 147.1211\n",
      "Epoch 2952/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.8933 - val_loss: 130.2134\n",
      "Epoch 2953/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 145.5827 - val_loss: 141.9268\n",
      "Epoch 2954/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 144.2926 - val_loss: 155.1805\n",
      "Epoch 2955/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 136.4872 - val_loss: 134.4400\n",
      "Epoch 2956/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 175.4961 - val_loss: 143.1799\n",
      "Epoch 2957/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.0778 - val_loss: 127.7918\n",
      "Epoch 2958/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 128.7567 - val_loss: 150.3742\n",
      "Epoch 2959/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.6068 - val_loss: 135.4589\n",
      "Epoch 2960/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.1870 - val_loss: 128.9151- ETA: 0s - loss: 1\n",
      "Epoch 2961/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.2487 - val_loss: 129.7495\n",
      "Epoch 2962/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.9845 - val_loss: 138.0279\n",
      "Epoch 2963/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.1760 - val_loss: 187.5679\n",
      "Epoch 2964/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.8999 - val_loss: 135.4529\n",
      "Epoch 2965/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.8754 - val_loss: 142.6617\n",
      "Epoch 2966/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.6947 - val_loss: 142.8685\n",
      "Epoch 2967/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.2102 - val_loss: 146.0458\n",
      "Epoch 2968/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.7342 - val_loss: 133.9675\n",
      "Epoch 2969/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.5223 - val_loss: 146.1654\n",
      "Epoch 2970/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 138.8253 - val_loss: 142.6774\n",
      "Epoch 2971/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.1018 - val_loss: 134.0672\n",
      "Epoch 2972/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.2105 - val_loss: 129.0038\n",
      "Epoch 2973/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.0785 - val_loss: 174.1081\n",
      "Epoch 2974/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.9664 - val_loss: 131.4540\n",
      "Epoch 2975/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.7848 - val_loss: 133.0368\n",
      "Epoch 2976/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.0735 - val_loss: 143.4422\n",
      "Epoch 2977/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.7554 - val_loss: 135.1989\n",
      "Epoch 2978/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.9613 - val_loss: 154.3296\n",
      "Epoch 2979/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 131.6131 - val_loss: 151.0349\n",
      "Epoch 2980/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.8437 - val_loss: 126.3272\n",
      "Epoch 2981/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.1537 - val_loss: 132.7539\n",
      "Epoch 2982/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.1022 - val_loss: 154.4911\n",
      "Epoch 2983/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.7057 - val_loss: 138.7938\n",
      "Epoch 2984/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 134.5208 - val_loss: 131.8258\n",
      "Epoch 2985/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.4280 - val_loss: 143.8761\n",
      "Epoch 2986/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.0869 - val_loss: 154.2184\n",
      "Epoch 2987/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.1183 - val_loss: 169.3427\n",
      "Epoch 2988/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.9761 - val_loss: 149.6928\n",
      "Epoch 2989/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.7162 - val_loss: 185.7658\n",
      "Epoch 2990/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.9452 - val_loss: 133.3868\n",
      "Epoch 2991/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.7497 - val_loss: 129.7130\n",
      "Epoch 2992/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.9952 - val_loss: 128.8926\n",
      "Epoch 2993/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.6121 - val_loss: 143.0493\n",
      "Epoch 2994/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.5096 - val_loss: 134.2261\n",
      "Epoch 2995/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.5425 - val_loss: 131.3592\n",
      "Epoch 2996/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.7969 - val_loss: 131.2612\n",
      "Epoch 2997/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.9036 - val_loss: 144.8000\n",
      "Epoch 2998/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 135.8247 - val_loss: 164.0597\n",
      "Epoch 2999/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.2138 - val_loss: 134.5360\n",
      "Epoch 3000/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.1425 - val_loss: 222.1705\n",
      "Epoch 3001/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.1570 - val_loss: 154.2435\n",
      "Epoch 3002/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.4068 - val_loss: 127.2491\n",
      "Epoch 3003/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.8100 - val_loss: 130.7753\n",
      "Epoch 3004/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.3453 - val_loss: 141.9498\n",
      "Epoch 3005/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 134.5857 - val_loss: 192.4711\n",
      "Epoch 3006/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.9165 - val_loss: 142.2413\n",
      "Epoch 3007/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 138.4525 - val_loss: 135.8547\n",
      "Epoch 3008/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.0029 - val_loss: 172.8951\n",
      "Epoch 3009/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.1337 - val_loss: 133.4847\n",
      "Epoch 3010/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.1268 - val_loss: 179.8862\n",
      "Epoch 3011/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 151.9122 - val_loss: 178.6449\n",
      "Epoch 3012/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.1241 - val_loss: 165.8524\n",
      "Epoch 3013/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 134.7352 - val_loss: 136.7341\n",
      "Epoch 3014/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 129.5919 - val_loss: 127.7108\n",
      "Epoch 3015/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 130.0222 - val_loss: 132.6296\n",
      "Epoch 3016/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.3533 - val_loss: 149.1721\n",
      "Epoch 3017/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.5731 - val_loss: 152.4717\n",
      "Epoch 3018/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 156.1841 - val_loss: 143.9057\n",
      "Epoch 3019/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.3386 - val_loss: 143.7114\n",
      "Epoch 3020/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.9188 - val_loss: 137.1489\n",
      "Epoch 3021/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.8331 - val_loss: 131.3592\n",
      "Epoch 3022/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 136.4913 - val_loss: 149.5359\n",
      "Epoch 3023/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.8038 - val_loss: 131.8150\n",
      "Epoch 3024/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.6107 - val_loss: 148.3308\n",
      "Epoch 3025/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.8643 - val_loss: 129.7120\n",
      "Epoch 3026/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 141.9506 - val_loss: 147.5016\n",
      "Epoch 3027/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.1268 - val_loss: 133.6176\n",
      "Epoch 3028/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.2017 - val_loss: 149.5550\n",
      "Epoch 3029/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 141.5918 - val_loss: 143.5019\n",
      "Epoch 3030/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 136.0550 - val_loss: 139.9992\n",
      "Epoch 3031/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.3315 - val_loss: 151.3265\n",
      "Epoch 3032/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.7632 - val_loss: 168.0962\n",
      "Epoch 3033/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.3721 - val_loss: 164.7220\n",
      "Epoch 3034/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 147.6414 - val_loss: 151.2584\n",
      "Epoch 3035/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.1185 - val_loss: 143.3748\n",
      "Epoch 3036/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 127.7213 - val_loss: 140.5010\n",
      "Epoch 3037/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 143.3676 - val_loss: 134.7149\n",
      "Epoch 3038/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.2050 - val_loss: 134.0527\n",
      "Epoch 3039/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.2352 - val_loss: 158.8123\n",
      "Epoch 3040/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.2512 - val_loss: 184.2670\n",
      "Epoch 3041/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.1559 - val_loss: 129.7481\n",
      "Epoch 3042/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.0684 - val_loss: 137.7026\n",
      "Epoch 3043/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.9710 - val_loss: 154.3539\n",
      "Epoch 3044/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 181.6632 - val_loss: 156.7222\n",
      "Epoch 3045/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 155.9653 - val_loss: 134.2423\n",
      "Epoch 3046/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 169.7975 - val_loss: 138.4510\n",
      "Epoch 3047/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.0391 - val_loss: 137.9061\n",
      "Epoch 3048/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.0271 - val_loss: 180.3054\n",
      "Epoch 3049/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.7126 - val_loss: 148.7403\n",
      "Epoch 3050/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 155.8796 - val_loss: 148.9810\n",
      "Epoch 3051/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 165.2042 - val_loss: 155.2492\n",
      "Epoch 3052/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 157.4216 - val_loss: 175.3508\n",
      "Epoch 3053/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.9099 - val_loss: 136.6951\n",
      "Epoch 3054/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.1958 - val_loss: 170.8486\n",
      "Epoch 3055/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.9088 - val_loss: 138.2599\n",
      "Epoch 3056/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.4560 - val_loss: 167.3084\n",
      "Epoch 3057/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.1406 - val_loss: 147.8154\n",
      "Epoch 3058/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 130.4897 - val_loss: 135.1802\n",
      "Epoch 3059/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 179.3639 - val_loss: 140.7787\n",
      "Epoch 3060/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.5264 - val_loss: 157.7668\n",
      "Epoch 3061/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.9961 - val_loss: 157.2566\n",
      "Epoch 3062/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 146.7354 - val_loss: 144.3398\n",
      "Epoch 3063/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.6197 - val_loss: 152.4700\n",
      "Epoch 3064/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 179.4302 - val_loss: 139.9965\n",
      "Epoch 3065/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 140.3499 - val_loss: 123.6457\n",
      "Epoch 3066/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 134.6366 - val_loss: 133.5886\n",
      "Epoch 3067/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.4358 - val_loss: 167.1438\n",
      "Epoch 3068/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.5322 - val_loss: 163.1801\n",
      "Epoch 3069/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.1257 - val_loss: 130.9042\n",
      "Epoch 3070/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.5269 - val_loss: 148.8005\n",
      "Epoch 3071/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 178.8290 - val_loss: 147.6408\n",
      "Epoch 3072/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 166.2960 - val_loss: 134.3485\n",
      "Epoch 3073/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 145.891 - 1s 83us/step - loss: 146.6072 - val_loss: 153.6368\n",
      "Epoch 3074/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 157.0052 - val_loss: 197.2941\n",
      "Epoch 3075/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 140.4904 - val_loss: 139.2734\n",
      "Epoch 3076/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.5574 - val_loss: 131.5116\n",
      "Epoch 3077/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 175.7417 - val_loss: 230.5923\n",
      "Epoch 3078/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.3823 - val_loss: 160.4422\n",
      "Epoch 3079/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.9942 - val_loss: 131.6036\n",
      "Epoch 3080/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.3995 - val_loss: 139.7530\n",
      "Epoch 3081/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.8164 - val_loss: 125.4647\n",
      "Epoch 3082/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.7727 - val_loss: 156.4056\n",
      "Epoch 3083/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 147.5965 - val_loss: 177.1267\n",
      "Epoch 3084/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.3689 - val_loss: 158.3019\n",
      "Epoch 3085/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 136.9287 - val_loss: 166.3308\n",
      "Epoch 3086/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.5905 - val_loss: 129.4590\n",
      "Epoch 3087/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.8111 - val_loss: 129.5694\n",
      "Epoch 3088/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.8179 - val_loss: 159.9893\n",
      "Epoch 3089/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 156.4535 - val_loss: 144.4764\n",
      "Epoch 3090/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.5970 - val_loss: 195.4440\n",
      "Epoch 3091/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.4535 - val_loss: 158.8851\n",
      "Epoch 3092/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 155.9775 - val_loss: 147.2366\n",
      "Epoch 3093/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.0283 - val_loss: 198.5913\n",
      "Epoch 3094/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.0904 - val_loss: 148.0670\n",
      "Epoch 3095/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.9695 - val_loss: 155.0586\n",
      "Epoch 3096/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.2012 - val_loss: 141.1511\n",
      "Epoch 3097/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 165.8853 - val_loss: 152.5394\n",
      "Epoch 3098/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.4068 - val_loss: 140.7857\n",
      "Epoch 3099/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.3046 - val_loss: 163.2146\n",
      "Epoch 3100/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.7999 - val_loss: 151.0661\n",
      "Epoch 3101/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.0023 - val_loss: 145.4165\n",
      "Epoch 3102/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.6824 - val_loss: 129.8661\n",
      "Epoch 3103/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 218.5596 - val_loss: 143.0285\n",
      "Epoch 3104/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.6620 - val_loss: 176.7876\n",
      "Epoch 3105/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 131.6875 - val_loss: 136.3174\n",
      "Epoch 3106/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.6158 - val_loss: 138.6146\n",
      "Epoch 3107/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.0149 - val_loss: 142.4882\n",
      "Epoch 3108/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.3127 - val_loss: 160.2572\n",
      "Epoch 3109/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 134.5443 - val_loss: 137.3583\n",
      "Epoch 3110/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.1033 - val_loss: 132.4934\n",
      "Epoch 3111/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.5460 - val_loss: 139.7129\n",
      "Epoch 3112/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 189.407 - 1s 63us/step - loss: 187.2256 - val_loss: 154.8298\n",
      "Epoch 3113/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 150.4040 - val_loss: 155.7263\n",
      "Epoch 3114/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 132.5570 - val_loss: 226.1474\n",
      "Epoch 3115/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 128.1424 - val_loss: 133.3181\n",
      "Epoch 3116/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 130.0324 - val_loss: 144.3138\n",
      "Epoch 3117/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 134.9812 - val_loss: 173.7384\n",
      "Epoch 3118/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.7757 - val_loss: 159.4294\n",
      "Epoch 3119/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.0358 - val_loss: 136.0661\n",
      "Epoch 3120/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 173.6441 - val_loss: 217.6287\n",
      "Epoch 3121/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 66us/step - loss: 150.5233 - val_loss: 139.8405\n",
      "Epoch 3122/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.5964 - val_loss: 139.0897\n",
      "Epoch 3123/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.8389 - val_loss: 131.2547\n",
      "Epoch 3124/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 127.3721 - val_loss: 139.4277\n",
      "Epoch 3125/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.4928 - val_loss: 129.4859\n",
      "Epoch 3126/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.4837 - val_loss: 163.1987\n",
      "Epoch 3127/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.8217 - val_loss: 133.6154\n",
      "Epoch 3128/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.1560 - val_loss: 142.5039\n",
      "Epoch 3129/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.7345 - val_loss: 142.9718\n",
      "Epoch 3130/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 138.0905 - val_loss: 134.9027\n",
      "Epoch 3131/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 155.4929 - val_loss: 152.9984\n",
      "Epoch 3132/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 156.7257 - val_loss: 222.4494\n",
      "Epoch 3133/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.6659 - val_loss: 137.7782\n",
      "Epoch 3134/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.3317 - val_loss: 147.7094\n",
      "Epoch 3135/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 189.5069 - val_loss: 138.7961\n",
      "Epoch 3136/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.7761 - val_loss: 150.0725\n",
      "Epoch 3137/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 137.3300 - val_loss: 142.1757\n",
      "Epoch 3138/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 129.4848 - val_loss: 136.6412\n",
      "Epoch 3139/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.9267 - val_loss: 190.3533\n",
      "Epoch 3140/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 161.8171 - val_loss: 134.7850\n",
      "Epoch 3141/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.8376 - val_loss: 138.6701\n",
      "Epoch 3142/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.0176 - val_loss: 184.3602\n",
      "Epoch 3143/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.1501 - val_loss: 148.7901\n",
      "Epoch 3144/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.8730 - val_loss: 163.0237\n",
      "Epoch 3145/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.1998 - val_loss: 128.7652\n",
      "Epoch 3146/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.5734 - val_loss: 205.9830\n",
      "Epoch 3147/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.1422 - val_loss: 158.0946\n",
      "Epoch 3148/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.0524 - val_loss: 134.7430\n",
      "Epoch 3149/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.4380 - val_loss: 159.7425\n",
      "Epoch 3150/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 144.2379 - val_loss: 140.9191\n",
      "Epoch 3151/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.9614 - val_loss: 132.0039\n",
      "Epoch 3152/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.7799 - val_loss: 144.3666\n",
      "Epoch 3153/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.0300 - val_loss: 177.8232\n",
      "Epoch 3154/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.6156 - val_loss: 152.5925\n",
      "Epoch 3155/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.7578 - val_loss: 131.9817\n",
      "Epoch 3156/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.8219 - val_loss: 142.0846\n",
      "Epoch 3157/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.7197 - val_loss: 130.1696\n",
      "Epoch 3158/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 133.2267 - val_loss: 149.3779\n",
      "Epoch 3159/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 149.6711 - val_loss: 146.9623\n",
      "Epoch 3160/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 136.5043 - val_loss: 133.2659\n",
      "Epoch 3161/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.8189 - val_loss: 141.9524\n",
      "Epoch 3162/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.6750 - val_loss: 142.8413\n",
      "Epoch 3163/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.1105 - val_loss: 150.1249\n",
      "Epoch 3164/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 133.1281 - val_loss: 128.7639\n",
      "Epoch 3165/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 151.6435 - val_loss: 159.0309\n",
      "Epoch 3166/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 146.8635 - val_loss: 138.2812\n",
      "Epoch 3167/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 131.4130 - val_loss: 177.7922\n",
      "Epoch 3168/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.3979 - val_loss: 142.8466\n",
      "Epoch 3169/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.1761 - val_loss: 160.3256\n",
      "Epoch 3170/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 133.4138 - val_loss: 140.9053\n",
      "Epoch 3171/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 131.3212 - val_loss: 143.6760\n",
      "Epoch 3172/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.4938 - val_loss: 330.6010\n",
      "Epoch 3173/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 205.1276 - val_loss: 133.1056\n",
      "Epoch 3174/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.0371 - val_loss: 137.6151\n",
      "Epoch 3175/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.8908 - val_loss: 144.7513\n",
      "Epoch 3176/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.1845 - val_loss: 138.5876\n",
      "Epoch 3177/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 156.1855 - val_loss: 168.7417\n",
      "Epoch 3178/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 159.0054 - val_loss: 148.8066\n",
      "Epoch 3179/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.4252 - val_loss: 195.5343\n",
      "Epoch 3180/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.0176 - val_loss: 166.5152\n",
      "Epoch 3181/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.9924 - val_loss: 134.0444\n",
      "Epoch 3182/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 137.7895 - val_loss: 134.7204\n",
      "Epoch 3183/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.0119 - val_loss: 150.3030\n",
      "Epoch 3184/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.8952 - val_loss: 174.1707\n",
      "Epoch 3185/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.0546 - val_loss: 127.6104\n",
      "Epoch 3186/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.8255 - val_loss: 154.3741\n",
      "Epoch 3187/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.0095 - val_loss: 235.0228\n",
      "Epoch 3188/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 169.0624 - val_loss: 136.0946\n",
      "Epoch 3189/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 136.3593 - val_loss: 141.4661\n",
      "Epoch 3190/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 146.7265 - val_loss: 147.1874\n",
      "Epoch 3191/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 143.3873 - val_loss: 227.8103\n",
      "Epoch 3192/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.9751 - val_loss: 161.6797\n",
      "Epoch 3193/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 69us/step - loss: 136.1749 - val_loss: 135.7612\n",
      "Epoch 3194/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 131.9547 - val_loss: 142.5603\n",
      "Epoch 3195/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 141.8673 - val_loss: 141.9713\n",
      "Epoch 3196/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 198.0272 - val_loss: 140.5842\n",
      "Epoch 3197/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 142.8800 - val_loss: 139.3833\n",
      "Epoch 3198/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 140.0798 - val_loss: 197.5158\n",
      "Epoch 3199/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.9756 - val_loss: 138.9809\n",
      "Epoch 3200/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.7641 - val_loss: 133.8484\n",
      "Epoch 3201/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.6428 - val_loss: 142.4057\n",
      "Epoch 3202/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.2155 - val_loss: 168.1254\n",
      "Epoch 3203/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.0741 - val_loss: 129.5112\n",
      "Epoch 3204/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.2792 - val_loss: 131.3875\n",
      "Epoch 3205/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.3698 - val_loss: 154.9235\n",
      "Epoch 3206/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.0750 - val_loss: 214.3892\n",
      "Epoch 3207/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.3623 - val_loss: 440.7895\n",
      "Epoch 3208/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.1423 - val_loss: 128.2161\n",
      "Epoch 3209/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 136.4710 - val_loss: 169.0887\n",
      "Epoch 3210/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.7865 - val_loss: 135.9421\n",
      "Epoch 3211/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.6499 - val_loss: 128.1721\n",
      "Epoch 3212/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 138.3269 - val_loss: 161.6011\n",
      "Epoch 3213/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.2216 - val_loss: 130.1166\n",
      "Epoch 3214/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.5835 - val_loss: 138.6057\n",
      "Epoch 3215/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 129.5714 - val_loss: 146.6478\n",
      "Epoch 3216/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.3452 - val_loss: 213.5261\n",
      "Epoch 3217/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 135.9843 - val_loss: 130.2025\n",
      "Epoch 3218/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.1872 - val_loss: 137.9154\n",
      "Epoch 3219/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.2052 - val_loss: 128.7896\n",
      "Epoch 3220/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 135.0875 - val_loss: 179.4189\n",
      "Epoch 3221/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.1428 - val_loss: 179.2861\n",
      "Epoch 3222/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.2751 - val_loss: 150.5300\n",
      "Epoch 3223/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.6104 - val_loss: 138.7936\n",
      "Epoch 3224/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 150.6863 - val_loss: 134.8678\n",
      "Epoch 3225/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 146.0921 - val_loss: 184.3117\n",
      "Epoch 3226/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 150.8277 - val_loss: 125.5353\n",
      "Epoch 3227/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 153.1388 - val_loss: 146.7333\n",
      "Epoch 3228/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 152.8187 - val_loss: 141.8943\n",
      "Epoch 3229/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 132.9308 - val_loss: 135.9200\n",
      "Epoch 3230/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 149.7659 - val_loss: 147.0678\n",
      "Epoch 3231/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 134.8400 - val_loss: 141.9072\n",
      "Epoch 3232/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 136.8997 - val_loss: 239.8343\n",
      "Epoch 3233/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 140.2842 - val_loss: 141.4327\n",
      "Epoch 3234/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 147.2702 - val_loss: 132.9078\n",
      "Epoch 3235/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 154.9070 - val_loss: 155.0479\n",
      "Epoch 3236/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 149.7985 - val_loss: 136.9693\n",
      "Epoch 3237/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 131.9120 - val_loss: 126.9089\n",
      "Epoch 3238/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 135.4771 - val_loss: 135.1109\n",
      "Epoch 3239/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 137.5170 - val_loss: 124.3007\n",
      "Epoch 3240/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 137.6380 - val_loss: 136.3565\n",
      "Epoch 3241/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 138.7941 - val_loss: 177.2293\n",
      "Epoch 3242/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 161.1823 - val_loss: 195.1555\n",
      "Epoch 3243/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 178.2443 - val_loss: 158.1262\n",
      "Epoch 3244/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 173.5458 - val_loss: 156.9788\n",
      "Epoch 3245/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 133.6977 - val_loss: 141.5660\n",
      "Epoch 3246/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 132.5170 - val_loss: 130.2842\n",
      "Epoch 3247/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.4716 - val_loss: 139.7058\n",
      "Epoch 3248/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 126.5142 - val_loss: 175.5208\n",
      "Epoch 3249/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.9060 - val_loss: 145.2795\n",
      "Epoch 3250/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 135.9945 - val_loss: 130.9071\n",
      "Epoch 3251/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.4238 - val_loss: 157.8196\n",
      "Epoch 3252/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.2909 - val_loss: 171.9209\n",
      "Epoch 3253/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 142.2645 - val_loss: 141.7687\n",
      "Epoch 3254/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 142.8596 - val_loss: 148.3053\n",
      "Epoch 3255/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 140.7481 - val_loss: 135.7383\n",
      "Epoch 3256/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 145.9968 - val_loss: 135.0645\n",
      "Epoch 3257/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 146.6833 - val_loss: 133.8148\n",
      "Epoch 3258/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.3893 - val_loss: 184.1408\n",
      "Epoch 3259/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.0079 - val_loss: 146.8790\n",
      "Epoch 3260/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 171.9225 - val_loss: 150.6761\n",
      "Epoch 3261/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 136.7502 - val_loss: 151.2181\n",
      "Epoch 3262/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 143.3839 - val_loss: 167.9404\n",
      "Epoch 3263/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.1945 - val_loss: 195.9809\n",
      "Epoch 3264/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.5548 - val_loss: 134.0065\n",
      "Epoch 3265/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.2556 - val_loss: 128.6889\n",
      "Epoch 3266/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 138.3874 - val_loss: 143.1884\n",
      "Epoch 3267/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 128.0812 - val_loss: 133.8671\n",
      "Epoch 3268/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 142.3700 - val_loss: 183.5997\n",
      "Epoch 3269/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 153.8830 - val_loss: 127.1832\n",
      "Epoch 3270/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.5483 - val_loss: 131.2909\n",
      "Epoch 3271/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.6365 - val_loss: 145.1668\n",
      "Epoch 3272/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.0288 - val_loss: 141.4292\n",
      "Epoch 3273/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 149.2372 - val_loss: 150.8719\n",
      "Epoch 3274/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 140.1524 - val_loss: 134.5387\n",
      "Epoch 3275/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.4172 - val_loss: 129.1739\n",
      "Epoch 3276/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.9835 - val_loss: 129.3092\n",
      "Epoch 3277/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.9054 - val_loss: 134.9012\n",
      "Epoch 3278/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.5998 - val_loss: 207.8971\n",
      "Epoch 3279/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 144.4792 - val_loss: 170.0782\n",
      "Epoch 3280/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.8424 - val_loss: 132.3073\n",
      "Epoch 3281/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 142.2158 - val_loss: 144.4000\n",
      "Epoch 3282/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.7980 - val_loss: 132.3580\n",
      "Epoch 3283/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.1534 - val_loss: 157.9496\n",
      "Epoch 3284/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 131.7557 - val_loss: 157.2246\n",
      "Epoch 3285/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 168.4363 - val_loss: 133.3672\n",
      "Epoch 3286/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.3555 - val_loss: 137.6217\n",
      "Epoch 3287/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 134.3813 - val_loss: 198.8783\n",
      "Epoch 3288/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 134.2071 - val_loss: 141.6710\n",
      "Epoch 3289/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.3404 - val_loss: 146.3985\n",
      "Epoch 3290/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 129.6646 - val_loss: 133.0107\n",
      "Epoch 3291/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.5715 - val_loss: 140.7730\n",
      "Epoch 3292/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.2903 - val_loss: 131.9282\n",
      "Epoch 3293/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.1041 - val_loss: 175.8768\n",
      "Epoch 3294/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.5344 - val_loss: 138.0711\n",
      "Epoch 3295/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.8783 - val_loss: 144.0045\n",
      "Epoch 3296/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 135.9145 - val_loss: 142.8710\n",
      "Epoch 3297/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.7363 - val_loss: 135.5575\n",
      "Epoch 3298/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.5686 - val_loss: 197.7477\n",
      "Epoch 3299/10000\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 150.3888 - val_loss: 131.9639\n",
      "Epoch 3300/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 148.3284 - val_loss: 191.9603\n",
      "Epoch 3301/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 163.1180 - val_loss: 134.4320\n",
      "Epoch 3302/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 142.5336 - val_loss: 130.1290\n",
      "Epoch 3303/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 135.7595 - val_loss: 139.1474\n",
      "Epoch 3304/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 137.4776 - val_loss: 136.8281\n",
      "Epoch 3305/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.9398 - val_loss: 143.8351\n",
      "Epoch 3306/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 159.4483 - val_loss: 138.3137\n",
      "Epoch 3307/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.8857 - val_loss: 130.2426\n",
      "Epoch 3308/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 157.5866 - val_loss: 180.1322\n",
      "Epoch 3309/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 144.1715 - val_loss: 152.4732\n",
      "Epoch 3310/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 151.9901 - val_loss: 147.9075\n",
      "Epoch 3311/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 178.7052 - val_loss: 148.1526\n",
      "Epoch 3312/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 153.2017 - val_loss: 146.4549\n",
      "Epoch 3313/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.1557 - val_loss: 154.2129\n",
      "Epoch 3314/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 128.0612 - val_loss: 159.9486\n",
      "Epoch 3315/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 127.4549 - val_loss: 128.2309\n",
      "Epoch 3316/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 134.3805 - val_loss: 170.8447\n",
      "Epoch 3317/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 125.4576 - val_loss: 129.9936\n",
      "Epoch 3318/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 147.2177 - val_loss: 139.1348\n",
      "Epoch 3319/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.3797 - val_loss: 137.6030\n",
      "Epoch 3320/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.8154 - val_loss: 186.9509\n",
      "Epoch 3321/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 179.6656 - val_loss: 165.5107\n",
      "Epoch 3322/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.7520 - val_loss: 162.5714\n",
      "Epoch 3323/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 131.9968 - val_loss: 150.1198\n",
      "Epoch 3324/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.2765 - val_loss: 161.4050\n",
      "Epoch 3325/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.2548 - val_loss: 153.8971\n",
      "Epoch 3326/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 144.7290 - val_loss: 134.9174\n",
      "Epoch 3327/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.2806 - val_loss: 126.6184\n",
      "Epoch 3328/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.3625 - val_loss: 147.2046\n",
      "Epoch 3329/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.4833 - val_loss: 140.1696\n",
      "Epoch 3330/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.5530 - val_loss: 143.5109\n",
      "Epoch 3331/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.4652 - val_loss: 136.9107\n",
      "Epoch 3332/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.2700 - val_loss: 142.2367\n",
      "Epoch 3333/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.7226 - val_loss: 146.2450\n",
      "Epoch 3334/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 130.7016 - val_loss: 145.7915\n",
      "Epoch 3335/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 128.7424 - val_loss: 133.2851\n",
      "Epoch 3336/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 143.8853 - val_loss: 142.4904\n",
      "Epoch 3337/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 65us/step - loss: 142.0551 - val_loss: 147.5589\n",
      "Epoch 3338/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 151.5955 - val_loss: 139.5668\n",
      "Epoch 3339/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.1065 - val_loss: 213.3878\n",
      "Epoch 3340/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.0906 - val_loss: 168.6277\n",
      "Epoch 3341/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.1400 - val_loss: 144.0441\n",
      "Epoch 3342/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 160.5679 - val_loss: 156.3664\n",
      "Epoch 3343/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.2574 - val_loss: 152.2091\n",
      "Epoch 3344/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.0581 - val_loss: 152.1878\n",
      "Epoch 3345/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 128.7082 - val_loss: 132.9695\n",
      "Epoch 3346/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 139.1641 - val_loss: 130.7823\n",
      "Epoch 3347/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.1356 - val_loss: 132.5345\n",
      "Epoch 3348/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 137.9751 - val_loss: 128.8912\n",
      "Epoch 3349/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.2686 - val_loss: 147.3806\n",
      "Epoch 3350/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.8222 - val_loss: 136.4730\n",
      "Epoch 3351/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.5498 - val_loss: 191.0761\n",
      "Epoch 3352/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 139.2669 - val_loss: 144.8466\n",
      "Epoch 3353/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.4380 - val_loss: 143.8901\n",
      "Epoch 3354/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 154.7945 - val_loss: 229.7083\n",
      "Epoch 3355/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 173.7143 - val_loss: 136.7398\n",
      "Epoch 3356/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.3116 - val_loss: 147.5417\n",
      "Epoch 3357/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 133.4904 - val_loss: 139.2367\n",
      "Epoch 3358/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 137.8712 - val_loss: 188.2692\n",
      "Epoch 3359/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 146.6996 - val_loss: 130.7958\n",
      "Epoch 3360/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 141.3172 - val_loss: 135.6974\n",
      "Epoch 3361/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 149.6100 - val_loss: 140.2362\n",
      "Epoch 3362/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 135.4292 - val_loss: 179.7574\n",
      "Epoch 3363/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.0087 - val_loss: 137.3334\n",
      "Epoch 3364/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 161.4345 - val_loss: 166.2627\n",
      "Epoch 3365/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.8980 - val_loss: 128.9749\n",
      "Epoch 3366/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.9230 - val_loss: 153.2880\n",
      "Epoch 3367/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.6784 - val_loss: 145.5421\n",
      "Epoch 3368/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 123.6668 - val_loss: 161.1871\n",
      "Epoch 3369/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 124.8166 - val_loss: 150.7260\n",
      "Epoch 3370/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.1610 - val_loss: 137.8271\n",
      "Epoch 3371/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 131.7589 - val_loss: 129.6561\n",
      "Epoch 3372/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 133.3685 - val_loss: 173.0203\n",
      "Epoch 3373/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.1297 - val_loss: 132.7471\n",
      "Epoch 3374/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.7293 - val_loss: 160.7635\n",
      "Epoch 3375/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.8554 - val_loss: 182.3075\n",
      "Epoch 3376/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 157.6192 - val_loss: 226.9723\n",
      "Epoch 3377/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 150.7270 - val_loss: 158.2560\n",
      "Epoch 3378/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.5153 - val_loss: 129.5968\n",
      "Epoch 3379/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.5314 - val_loss: 137.2171\n",
      "Epoch 3380/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 128.6520 - val_loss: 140.0301\n",
      "Epoch 3381/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.9754 - val_loss: 134.0534\n",
      "Epoch 3382/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.5198 - val_loss: 142.2167\n",
      "Epoch 3383/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 130.2752 - val_loss: 139.4093\n",
      "Epoch 3384/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 131.2561 - val_loss: 140.3063\n",
      "Epoch 3385/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.7205 - val_loss: 136.5469\n",
      "Epoch 3386/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.9534 - val_loss: 175.8451\n",
      "Epoch 3387/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.6030 - val_loss: 128.4445\n",
      "Epoch 3388/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 129.1148 - val_loss: 165.7593\n",
      "Epoch 3389/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.6873 - val_loss: 133.9617\n",
      "Epoch 3390/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.6961 - val_loss: 134.0556\n",
      "Epoch 3391/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.7255 - val_loss: 161.0926\n",
      "Epoch 3392/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 134.2597 - val_loss: 177.7762\n",
      "Epoch 3393/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.5808 - val_loss: 132.3962\n",
      "Epoch 3394/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 137.3021 - val_loss: 142.5645\n",
      "Epoch 3395/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 157.4669 - val_loss: 244.1839\n",
      "Epoch 3396/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 154.9482 - val_loss: 163.5053\n",
      "Epoch 3397/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.9768 - val_loss: 150.9874\n",
      "Epoch 3398/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.4218 - val_loss: 128.7051\n",
      "Epoch 3399/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.7199 - val_loss: 146.2434\n",
      "Epoch 3400/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.7570 - val_loss: 148.6960\n",
      "Epoch 3401/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.8074 - val_loss: 165.7049\n",
      "Epoch 3402/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.4019 - val_loss: 129.2309\n",
      "Epoch 3403/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.9909 - val_loss: 134.8580\n",
      "Epoch 3404/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 130.1359 - val_loss: 195.2768\n",
      "Epoch 3405/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 159.8368 - val_loss: 152.6595\n",
      "Epoch 3406/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.8784 - val_loss: 148.4676\n",
      "Epoch 3407/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 147.1448 - val_loss: 172.4075\n",
      "Epoch 3408/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 158.3023 - val_loss: 131.1570\n",
      "Epoch 3409/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 71us/step - loss: 129.8773 - val_loss: 165.2190\n",
      "Epoch 3410/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 134.4092 - val_loss: 161.1038\n",
      "Epoch 3411/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 137.7556 - val_loss: 139.9710\n",
      "Epoch 3412/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 141.2829 - val_loss: 169.1944\n",
      "Epoch 3413/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 137.4378 - val_loss: 151.2969\n",
      "Epoch 3414/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.2882 - val_loss: 174.1054\n",
      "Epoch 3415/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 138.5933 - val_loss: 162.7842\n",
      "Epoch 3416/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 136.2238 - val_loss: 173.0649\n",
      "Epoch 3417/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 143.1082 - val_loss: 142.2662\n",
      "Epoch 3418/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 129.6572 - val_loss: 137.9592\n",
      "Epoch 3419/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 133.9269 - val_loss: 132.4653\n",
      "Epoch 3420/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 162.1519 - val_loss: 149.1608\n",
      "Epoch 3421/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.6596 - val_loss: 133.6919\n",
      "Epoch 3422/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 166.8800 - val_loss: 154.6444\n",
      "Epoch 3423/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 142.7214 - val_loss: 147.0288\n",
      "Epoch 3424/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.5012 - val_loss: 162.5067\n",
      "Epoch 3425/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.6074 - val_loss: 138.5174\n",
      "Epoch 3426/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 159.5636 - val_loss: 161.6481\n",
      "Epoch 3427/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 165.7327 - val_loss: 145.6474\n",
      "Epoch 3428/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 133.6824 - val_loss: 150.4280\n",
      "Epoch 3429/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 133.3455 - val_loss: 134.1854\n",
      "Epoch 3430/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 133.3221 - val_loss: 158.6928\n",
      "Epoch 3431/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.1325 - val_loss: 166.9072\n",
      "Epoch 3432/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 126.0626 - val_loss: 143.0608\n",
      "Epoch 3433/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.9665 - val_loss: 168.5892\n",
      "Epoch 3434/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 127.1273 - val_loss: 136.3534\n",
      "Epoch 3435/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 130.9947 - val_loss: 137.2264\n",
      "Epoch 3436/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.0983 - val_loss: 140.2050\n",
      "Epoch 3437/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.2861 - val_loss: 144.0906\n",
      "Epoch 3438/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 194.8928 - val_loss: 204.6381\n",
      "Epoch 3439/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.2601 - val_loss: 175.7734\n",
      "Epoch 3440/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 135.5451 - val_loss: 137.9392\n",
      "Epoch 3441/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 131.2631 - val_loss: 155.3792\n",
      "Epoch 3442/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 132.8286 - val_loss: 134.8453\n",
      "Epoch 3443/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.9037 - val_loss: 146.9950\n",
      "Epoch 3444/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 133.6866 - val_loss: 138.6454\n",
      "Epoch 3445/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.9611 - val_loss: 184.3513\n",
      "Epoch 3446/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 136.6327 - val_loss: 144.7681\n",
      "Epoch 3447/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.4255 - val_loss: 265.6721\n",
      "Epoch 3448/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.7321 - val_loss: 163.4485\n",
      "Epoch 3449/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 136.8171 - val_loss: 132.9564\n",
      "Epoch 3450/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 157.8076 - val_loss: 135.6405\n",
      "Epoch 3451/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 132.4451 - val_loss: 136.2208\n",
      "Epoch 3452/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.4191 - val_loss: 177.3429\n",
      "Epoch 3453/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 151.9815 - val_loss: 134.4383\n",
      "Epoch 3454/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.5435 - val_loss: 133.8221\n",
      "Epoch 3455/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 181.8804 - val_loss: 147.3892\n",
      "Epoch 3456/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 140.9481 - val_loss: 138.4348\n",
      "Epoch 3457/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.8748 - val_loss: 126.8156\n",
      "Epoch 3458/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.1553 - val_loss: 153.1954\n",
      "Epoch 3459/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 150.7225 - val_loss: 154.6514\n",
      "Epoch 3460/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.7458 - val_loss: 167.0518\n",
      "Epoch 3461/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 135.6908 - val_loss: 126.9116\n",
      "Epoch 3462/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 129.4441 - val_loss: 134.6905\n",
      "Epoch 3463/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 131.0115 - val_loss: 133.4009\n",
      "Epoch 3464/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.5137 - val_loss: 153.3031\n",
      "Epoch 3465/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 167.2955 - val_loss: 153.4726\n",
      "Epoch 3466/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.2037 - val_loss: 138.8353\n",
      "Epoch 3467/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.0191 - val_loss: 173.4357\n",
      "Epoch 3468/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.0302 - val_loss: 129.7716\n",
      "Epoch 3469/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 128.7861 - val_loss: 134.0100\n",
      "Epoch 3470/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 127.8179 - val_loss: 139.3844\n",
      "Epoch 3471/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.2460 - val_loss: 130.3780\n",
      "Epoch 3472/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 195.5622 - val_loss: 161.4286\n",
      "Epoch 3473/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 139.6288 - val_loss: 156.7087\n",
      "Epoch 3474/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 139.3339 - val_loss: 129.5563\n",
      "Epoch 3475/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 129.1195 - val_loss: 133.1175\n",
      "Epoch 3476/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 161.9899 - val_loss: 130.6788\n",
      "Epoch 3477/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.0199 - val_loss: 139.2953\n",
      "Epoch 3478/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.6528 - val_loss: 133.3747\n",
      "Epoch 3479/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 147.7987 - val_loss: 180.5266\n",
      "Epoch 3480/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 128.5717 - val_loss: 147.4625\n",
      "Epoch 3481/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.9124 - val_loss: 136.3398\n",
      "Epoch 3482/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.2912 - val_loss: 161.3844\n",
      "Epoch 3483/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.8961 - val_loss: 154.9977\n",
      "Epoch 3484/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 138.0133 - val_loss: 135.2370\n",
      "Epoch 3485/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 129.4053 - val_loss: 145.3498\n",
      "Epoch 3486/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.7261 - val_loss: 151.6167\n",
      "Epoch 3487/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 166.1624 - val_loss: 130.4183\n",
      "Epoch 3488/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 133.3640 - val_loss: 136.9319\n",
      "Epoch 3489/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.3462 - val_loss: 134.2972\n",
      "Epoch 3490/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 131.2488 - val_loss: 157.1804\n",
      "Epoch 3491/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 127.4080 - val_loss: 158.1872\n",
      "Epoch 3492/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 135.8724 - val_loss: 164.7685\n",
      "Epoch 3493/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 127.0991 - val_loss: 138.3079\n",
      "Epoch 3494/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 189.2982 - val_loss: 141.5381\n",
      "Epoch 3495/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 152.9977 - val_loss: 128.6673\n",
      "Epoch 3496/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 154.8562 - val_loss: 235.2034\n",
      "Epoch 3497/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.1738 - val_loss: 128.3932\n",
      "Epoch 3498/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 139.1777 - val_loss: 132.3488\n",
      "Epoch 3499/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 127.8350 - val_loss: 143.2991\n",
      "Epoch 3500/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 144.3363 - val_loss: 215.7327\n",
      "Epoch 3501/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.7389 - val_loss: 164.1645\n",
      "Epoch 3502/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.3382 - val_loss: 147.4545\n",
      "Epoch 3503/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 128.1139 - val_loss: 130.6419\n",
      "Epoch 3504/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 135.9832 - val_loss: 133.1261\n",
      "Epoch 3505/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 131.5046 - val_loss: 134.6563\n",
      "Epoch 3506/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 160.6786 - val_loss: 158.5784\n",
      "Epoch 3507/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.0126 - val_loss: 167.4271\n",
      "Epoch 3508/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 130.6405 - val_loss: 128.8926\n",
      "Epoch 3509/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 127.3266 - val_loss: 132.2684\n",
      "Epoch 3510/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 134.8874 - val_loss: 126.4612\n",
      "Epoch 3511/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.0693 - val_loss: 130.9769\n",
      "Epoch 3512/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.4649 - val_loss: 136.6902\n",
      "Epoch 3513/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.3205 - val_loss: 146.5189\n",
      "Epoch 3514/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 146.5947 - val_loss: 183.4811\n",
      "Epoch 3515/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 156.6502 - val_loss: 128.7712\n",
      "Epoch 3516/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.4763 - val_loss: 146.4592\n",
      "Epoch 3517/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.1193 - val_loss: 144.2629\n",
      "Epoch 3518/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.9766 - val_loss: 160.1082\n",
      "Epoch 3519/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.0929 - val_loss: 134.9642\n",
      "Epoch 3520/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 132.6491 - val_loss: 171.7007\n",
      "Epoch 3521/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 138.1977 - val_loss: 185.4607\n",
      "Epoch 3522/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 175.6275 - val_loss: 185.1449\n",
      "Epoch 3523/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 147.0715 - val_loss: 152.5014\n",
      "Epoch 3524/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.9372 - val_loss: 137.1001\n",
      "Epoch 3525/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 149.9974 - val_loss: 145.3655\n",
      "Epoch 3526/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 125.5427 - val_loss: 138.8441\n",
      "Epoch 3527/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 128.5180 - val_loss: 132.6073\n",
      "Epoch 3528/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 129.2466 - val_loss: 127.1839\n",
      "Epoch 3529/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 135.5747 - val_loss: 154.2662\n",
      "Epoch 3530/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 145.6654 - val_loss: 137.2955\n",
      "Epoch 3531/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 136.0900 - val_loss: 138.2562\n",
      "Epoch 3532/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 144.9881 - val_loss: 145.3533\n",
      "Epoch 3533/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 134.0075 - val_loss: 142.7846\n",
      "Epoch 3534/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 135.788 - 1s 63us/step - loss: 135.8125 - val_loss: 135.4580\n",
      "Epoch 3535/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.6087 - val_loss: 126.4799\n",
      "Epoch 3536/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 138.0103 - val_loss: 144.4532\n",
      "Epoch 3537/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.8317 - val_loss: 135.6777\n",
      "Epoch 3538/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.1311 - val_loss: 132.1383\n",
      "Epoch 3539/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.8215 - val_loss: 134.4887\n",
      "Epoch 3540/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 142.1273 - val_loss: 144.5048\n",
      "Epoch 3541/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 142.2587 - val_loss: 221.3820\n",
      "Epoch 3542/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 133.8802 - val_loss: 139.8858\n",
      "Epoch 3543/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 133.7489 - val_loss: 133.5136\n",
      "Epoch 3544/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 130.6888 - val_loss: 127.1363\n",
      "Epoch 3545/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.0041 - val_loss: 151.6359\n",
      "Epoch 3546/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 129.8882 - val_loss: 127.3231\n",
      "Epoch 3547/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.4619 - val_loss: 143.9940\n",
      "Epoch 3548/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 127.5507 - val_loss: 302.3388\n",
      "Epoch 3549/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.8366 - val_loss: 153.0542\n",
      "Epoch 3550/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 129.9598 - val_loss: 139.3368\n",
      "Epoch 3551/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.1960 - val_loss: 144.1536\n",
      "Epoch 3552/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 164.2234 - val_loss: 138.3704\n",
      "Epoch 3553/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.3338 - val_loss: 134.8980\n",
      "Epoch 3554/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.8047 - val_loss: 136.0198\n",
      "Epoch 3555/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 133.3989 - val_loss: 131.7270\n",
      "Epoch 3556/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 129.8477 - val_loss: 132.8433\n",
      "Epoch 3557/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.6842 - val_loss: 175.3079\n",
      "Epoch 3558/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 153.7184 - val_loss: 134.2174\n",
      "Epoch 3559/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.6183 - val_loss: 139.6197\n",
      "Epoch 3560/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 160.7595 - val_loss: 142.1870\n",
      "Epoch 3561/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.5751 - val_loss: 128.6509\n",
      "Epoch 3562/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 149.6692 - val_loss: 142.4606\n",
      "Epoch 3563/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 131.4846 - val_loss: 132.9327\n",
      "Epoch 3564/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 128.3618 - val_loss: 136.5575\n",
      "Epoch 3565/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 134.5969 - val_loss: 137.0104\n",
      "Epoch 3566/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.1741 - val_loss: 139.6487\n",
      "Epoch 3567/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.7457 - val_loss: 138.9352\n",
      "Epoch 3568/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 127.1977 - val_loss: 135.5990\n",
      "Epoch 3569/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 147.9858 - val_loss: 140.2776\n",
      "Epoch 3570/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 169.9264 - val_loss: 158.4675\n",
      "Epoch 3571/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 135.5271 - val_loss: 134.4514\n",
      "Epoch 3572/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.1067 - val_loss: 147.9366\n",
      "Epoch 3573/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 129.9800 - val_loss: 131.3057\n",
      "Epoch 3574/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.8007 - val_loss: 139.9303\n",
      "Epoch 3575/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 140.7898 - val_loss: 167.6372\n",
      "Epoch 3576/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.7696 - val_loss: 154.1945\n",
      "Epoch 3577/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.9927 - val_loss: 126.6057\n",
      "Epoch 3578/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 192.7451 - val_loss: 275.0751\n",
      "Epoch 3579/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 168.1718 - val_loss: 238.3165\n",
      "Epoch 3580/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.0463 - val_loss: 137.2759\n",
      "Epoch 3581/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 123.2977 - val_loss: 136.0616\n",
      "Epoch 3582/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 136.9677 - val_loss: 140.3011\n",
      "Epoch 3583/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 129.4475 - val_loss: 130.6447\n",
      "Epoch 3584/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 130.7228 - val_loss: 133.9113\n",
      "Epoch 3585/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 130.0689 - val_loss: 150.0868\n",
      "Epoch 3586/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 132.9066 - val_loss: 134.7645\n",
      "Epoch 3587/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 126.930 - 1s 83us/step - loss: 129.0017 - val_loss: 140.3052\n",
      "Epoch 3588/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 137.6023 - val_loss: 142.9957\n",
      "Epoch 3589/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 153.8867 - val_loss: 130.6312\n",
      "Epoch 3590/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 128.3436 - val_loss: 128.3049\n",
      "Epoch 3591/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.6610 - val_loss: 182.3777\n",
      "Epoch 3592/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 158.7025 - val_loss: 161.1997\n",
      "Epoch 3593/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 131.0825 - val_loss: 159.0037\n",
      "Epoch 3594/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 126.1066 - val_loss: 170.0315\n",
      "Epoch 3595/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.6742 - val_loss: 135.2090\n",
      "Epoch 3596/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 177.9820 - val_loss: 153.6811\n",
      "Epoch 3597/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 146.7909 - val_loss: 154.5941\n",
      "Epoch 3598/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.3135 - val_loss: 135.5148\n",
      "Epoch 3599/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 128.2123 - val_loss: 128.4486\n",
      "Epoch 3600/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.1862 - val_loss: 158.0821\n",
      "Epoch 3601/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 131.0167 - val_loss: 186.1058\n",
      "Epoch 3602/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 133.9149 - val_loss: 143.6845\n",
      "Epoch 3603/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 136.2969 - val_loss: 127.9220\n",
      "Epoch 3604/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 128.1339 - val_loss: 143.9486\n",
      "Epoch 3605/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 124.6382 - val_loss: 136.9504\n",
      "Epoch 3606/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.3079 - val_loss: 129.1323\n",
      "Epoch 3607/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 186.7296 - val_loss: 154.7176\n",
      "Epoch 3608/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 134.5072 - val_loss: 138.7729\n",
      "Epoch 3609/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 138.8416 - val_loss: 134.5679\n",
      "Epoch 3610/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 132.4808 - val_loss: 134.6882\n",
      "Epoch 3611/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 127.8146 - val_loss: 135.1573\n",
      "Epoch 3612/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 123.8282 - val_loss: 146.1380\n",
      "Epoch 3613/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 128.9509 - val_loss: 132.2192\n",
      "Epoch 3614/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 128.9540 - val_loss: 129.7454\n",
      "Epoch 3615/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.7301 - val_loss: 127.9616\n",
      "Epoch 3616/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.7414 - val_loss: 160.6635\n",
      "Epoch 3617/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.3069 - val_loss: 149.1425\n",
      "Epoch 3618/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 126.7970 - val_loss: 128.1621\n",
      "Epoch 3619/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 129.7190 - val_loss: 147.5604\n",
      "Epoch 3620/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 133.6946 - val_loss: 138.5512\n",
      "Epoch 3621/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 133.0037 - val_loss: 165.2016\n",
      "Epoch 3622/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.0161 - val_loss: 151.7913\n",
      "Epoch 3623/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 171.2150 - val_loss: 153.3334\n",
      "Epoch 3624/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 127.6096 - val_loss: 251.8997\n",
      "Epoch 3625/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.1810 - val_loss: 177.3327\n",
      "Epoch 3626/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.2323 - val_loss: 156.5633\n",
      "Epoch 3627/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 134.9790 - val_loss: 153.0097\n",
      "Epoch 3628/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 128.4752 - val_loss: 141.2859\n",
      "Epoch 3629/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 133.4798 - val_loss: 139.3267\n",
      "Epoch 3630/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 124.8790 - val_loss: 129.5841\n",
      "Epoch 3631/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.7926 - val_loss: 134.3622\n",
      "Epoch 3632/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 137.4099 - val_loss: 135.1889\n",
      "Epoch 3633/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 133.7631 - val_loss: 133.0102\n",
      "Epoch 3634/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.0116 - val_loss: 143.7270\n",
      "Epoch 3635/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.3025 - val_loss: 142.9301\n",
      "Epoch 3636/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 131.2962 - val_loss: 130.9743\n",
      "Epoch 3637/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 138.5120 - val_loss: 155.2715\n",
      "Epoch 3638/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.3660 - val_loss: 162.9233\n",
      "Epoch 3639/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 132.7811 - val_loss: 133.3866\n",
      "Epoch 3640/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 123.8988 - val_loss: 140.2843\n",
      "Epoch 3641/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 140.5438 - val_loss: 164.8293\n",
      "Epoch 3642/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.3117 - val_loss: 127.8631\n",
      "Epoch 3643/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 133.8555 - val_loss: 152.8683\n",
      "Epoch 3644/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 129.9892 - val_loss: 136.2018\n",
      "Epoch 3645/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 130.1844 - val_loss: 184.6798\n",
      "Epoch 3646/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 131.7879 - val_loss: 145.4489\n",
      "Epoch 3647/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 140.7024 - val_loss: 129.9606\n",
      "Epoch 3648/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.8477 - val_loss: 145.0632\n",
      "Epoch 3649/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.3332 - val_loss: 138.4152\n",
      "Epoch 3650/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 132.4119 - val_loss: 143.2380\n",
      "Epoch 3651/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.0084 - val_loss: 137.8736\n",
      "Epoch 3652/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 157.2600 - val_loss: 141.1556\n",
      "Epoch 3653/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.9197 - val_loss: 143.6700\n",
      "Epoch 3654/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.5974 - val_loss: 136.4264\n",
      "Epoch 3655/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 125.1891 - val_loss: 138.2198\n",
      "Epoch 3656/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 131.5462 - val_loss: 133.7535\n",
      "Epoch 3657/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 136.5400 - val_loss: 142.0947\n",
      "Epoch 3658/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.2811 - val_loss: 196.5919\n",
      "Epoch 3659/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 147.9801 - val_loss: 126.3209\n",
      "Epoch 3660/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 132.1876 - val_loss: 155.3761\n",
      "Epoch 3661/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 132.1883 - val_loss: 136.3222\n",
      "Epoch 3662/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.4059 - val_loss: 138.5753\n",
      "Epoch 3663/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 130.3312 - val_loss: 137.8985\n",
      "Epoch 3664/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 129.6445 - val_loss: 191.1692\n",
      "Epoch 3665/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.4025 - val_loss: 140.2073\n",
      "Epoch 3666/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 125.2582 - val_loss: 149.5736\n",
      "Epoch 3667/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 132.6424 - val_loss: 142.9258\n",
      "Epoch 3668/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 132.1022 - val_loss: 129.6820\n",
      "Epoch 3669/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.5511 - val_loss: 157.0372\n",
      "Epoch 3670/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.1370 - val_loss: 158.5797\n",
      "Epoch 3671/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 142.5376 - val_loss: 188.8429\n",
      "Epoch 3672/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 132.6739 - val_loss: 154.6108\n",
      "Epoch 3673/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 128.1392 - val_loss: 137.8098\n",
      "Epoch 3674/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 179.5560 - val_loss: 152.9450\n",
      "Epoch 3675/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 154.8329 - val_loss: 155.8385\n",
      "Epoch 3676/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.3827 - val_loss: 144.2035\n",
      "Epoch 3677/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.6904 - val_loss: 154.4548\n",
      "Epoch 3678/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 144.1786 - val_loss: 139.9516\n",
      "Epoch 3679/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 177.1119 - val_loss: 142.4164\n",
      "Epoch 3680/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.8608 - val_loss: 135.4957\n",
      "Epoch 3681/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 135.2854 - val_loss: 166.1338\n",
      "Epoch 3682/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.2110 - val_loss: 145.6132\n",
      "Epoch 3683/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 142.0008 - val_loss: 160.6269\n",
      "Epoch 3684/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 126.2026 - val_loss: 134.3905\n",
      "Epoch 3685/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 136.2267 - val_loss: 133.5886\n",
      "Epoch 3686/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 128.0695 - val_loss: 132.2476\n",
      "Epoch 3687/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.9722 - val_loss: 152.8794\n",
      "Epoch 3688/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 130.3354 - val_loss: 132.4081\n",
      "Epoch 3689/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 126.3245 - val_loss: 143.2092\n",
      "Epoch 3690/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 128.7013 - val_loss: 138.6352\n",
      "Epoch 3691/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 142.1168 - val_loss: 136.4266\n",
      "Epoch 3692/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 131.9677 - val_loss: 149.3663\n",
      "Epoch 3693/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 137.5763 - val_loss: 154.9145\n",
      "Epoch 3694/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 135.7566 - val_loss: 133.4954\n",
      "Epoch 3695/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.0157 - val_loss: 179.2138\n",
      "Epoch 3696/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.8864 - val_loss: 143.4124\n",
      "Epoch 3697/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 65us/step - loss: 133.4880 - val_loss: 132.0308\n",
      "Epoch 3698/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.3707 - val_loss: 206.8312\n",
      "Epoch 3699/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 136.6424 - val_loss: 138.8134\n",
      "Epoch 3700/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.0953 - val_loss: 131.5030\n",
      "Epoch 3701/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 125.9047 - val_loss: 140.0035\n",
      "Epoch 3702/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 134.3833 - val_loss: 130.7759\n",
      "Epoch 3703/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 156.2140 - val_loss: 141.9827\n",
      "Epoch 3704/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 130.5672 - val_loss: 134.0020\n",
      "Epoch 3705/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 136.9684 - val_loss: 151.8219\n",
      "Epoch 3706/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 129.2607 - val_loss: 154.2183\n",
      "Epoch 3707/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.1621 - val_loss: 181.0850\n",
      "Epoch 3708/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 126.4441 - val_loss: 182.6826\n",
      "Epoch 3709/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 129.4952 - val_loss: 154.8759\n",
      "Epoch 3710/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.0398 - val_loss: 147.2777\n",
      "Epoch 3711/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 131.1028 - val_loss: 143.3486\n",
      "Epoch 3712/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.3461 - val_loss: 136.1685\n",
      "Epoch 3713/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 128.7759 - val_loss: 134.2960\n",
      "Epoch 3714/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 147.9865 - val_loss: 138.3837\n",
      "Epoch 3715/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.9509 - val_loss: 188.3386\n",
      "Epoch 3716/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 140.3232 - val_loss: 137.4445\n",
      "Epoch 3717/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 128.3907 - val_loss: 148.1764\n",
      "Epoch 3718/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.2337 - val_loss: 161.6348\n",
      "Epoch 3719/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 127.2993 - val_loss: 141.2932\n",
      "Epoch 3720/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.8654 - val_loss: 128.6381\n",
      "Epoch 3721/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 132.4774 - val_loss: 192.7339\n",
      "Epoch 3722/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 132.9415 - val_loss: 136.7719\n",
      "Epoch 3723/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.8596 - val_loss: 147.6267\n",
      "Epoch 3724/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 137.8848 - val_loss: 129.5575\n",
      "Epoch 3725/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 128.2555 - val_loss: 128.2464\n",
      "Epoch 3726/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 127.7552 - val_loss: 135.4012\n",
      "Epoch 3727/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.5065 - val_loss: 139.5935\n",
      "Epoch 3728/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 148.8382 - val_loss: 145.4134\n",
      "Epoch 3729/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 161.9823 - val_loss: 163.1762\n",
      "Epoch 3730/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.9492 - val_loss: 262.9411\n",
      "Epoch 3731/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 164.6313 - val_loss: 156.9254\n",
      "Epoch 3732/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 147.6111 - val_loss: 149.7609\n",
      "Epoch 3733/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 139.5654 - val_loss: 141.3845\n",
      "Epoch 3734/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.0969 - val_loss: 130.5170\n",
      "Epoch 3735/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.7771 - val_loss: 159.9561\n",
      "Epoch 3736/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 161.7302 - val_loss: 149.2631\n",
      "Epoch 3737/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.0349 - val_loss: 128.5646\n",
      "Epoch 3738/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.7041 - val_loss: 137.6307\n",
      "Epoch 3739/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.7991 - val_loss: 148.7634\n",
      "Epoch 3740/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 128.1338 - val_loss: 177.6863\n",
      "Epoch 3741/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 131.2254 - val_loss: 143.4796\n",
      "Epoch 3742/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 128.7512 - val_loss: 144.4611\n",
      "Epoch 3743/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 131.0520 - val_loss: 144.2412\n",
      "Epoch 3744/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 154.4113 - val_loss: 183.0229\n",
      "Epoch 3745/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 221.4473 - val_loss: 154.6984\n",
      "Epoch 3746/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.4259 - val_loss: 144.9032\n",
      "Epoch 3747/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.0795 - val_loss: 158.9412\n",
      "Epoch 3748/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 141.6826 - val_loss: 133.8146\n",
      "Epoch 3749/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 136.8359 - val_loss: 182.6232\n",
      "Epoch 3750/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 165.4616 - val_loss: 133.0479\n",
      "Epoch 3751/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 131.9642 - val_loss: 129.5276\n",
      "Epoch 3752/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 150.9385 - val_loss: 137.8345\n",
      "Epoch 3753/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.0280 - val_loss: 163.8371\n",
      "Epoch 3754/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 158.6355 - val_loss: 163.5215\n",
      "Epoch 3755/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 149.9206 - val_loss: 138.3051\n",
      "Epoch 3756/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 140.9560 - val_loss: 202.8368\n",
      "Epoch 3757/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 130.7962 - val_loss: 169.5776\n",
      "Epoch 3758/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 124.2238 - val_loss: 171.6343\n",
      "Epoch 3759/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 150.7902 - val_loss: 133.4493\n",
      "Epoch 3760/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 137.1514 - val_loss: 196.8102\n",
      "Epoch 3761/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 131.5481 - val_loss: 131.2962\n",
      "Epoch 3762/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 135.9252 - val_loss: 148.9423\n",
      "Epoch 3763/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 130.5906 - val_loss: 127.2537\n",
      "Epoch 3764/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 136.9493 - val_loss: 201.7685\n",
      "Epoch 3765/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 126.4385 - val_loss: 142.3181\n",
      "Epoch 3766/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 138.8160 - val_loss: 168.5306\n",
      "Epoch 3767/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 147.9137 - val_loss: 155.8062\n",
      "Epoch 3768/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.6251 - val_loss: 135.7230\n",
      "Epoch 3769/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 71us/step - loss: 144.9863 - val_loss: 143.2407\n",
      "Epoch 3770/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 164.6144 - val_loss: 140.7184\n",
      "Epoch 3771/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.4237 - val_loss: 171.6538\n",
      "Epoch 3772/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.3357 - val_loss: 136.9463\n",
      "Epoch 3773/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 131.6676 - val_loss: 139.5843\n",
      "Epoch 3774/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 125.7890 - val_loss: 166.4982\n",
      "Epoch 3775/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.4862 - val_loss: 169.7839\n",
      "Epoch 3776/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 128.8114 - val_loss: 134.1082\n",
      "Epoch 3777/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 124.3837 - val_loss: 149.7639\n",
      "Epoch 3778/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.5149 - val_loss: 190.8889\n",
      "Epoch 3779/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.4484 - val_loss: 131.9103\n",
      "Epoch 3780/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 187.1421 - val_loss: 130.3426\n",
      "Epoch 3781/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 158.3333 - val_loss: 228.2308\n",
      "Epoch 3782/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.2656 - val_loss: 157.2827\n",
      "Epoch 3783/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 131.0707 - val_loss: 128.7409\n",
      "Epoch 3784/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 126.6925 - val_loss: 159.0059\n",
      "Epoch 3785/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 126.9141 - val_loss: 165.2702\n",
      "Epoch 3786/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.2089 - val_loss: 132.8080\n",
      "Epoch 3787/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 123.8126 - val_loss: 136.5545\n",
      "Epoch 3788/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 147.4653 - val_loss: 138.8588\n",
      "Epoch 3789/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.9670 - val_loss: 133.0296\n",
      "Epoch 3790/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 129.2254 - val_loss: 151.0832\n",
      "Epoch 3791/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.4424 - val_loss: 154.0862\n",
      "Epoch 3792/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 177.2964 - val_loss: 137.9391\n",
      "Epoch 3793/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.6511 - val_loss: 154.0044\n",
      "Epoch 3794/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.7743 - val_loss: 148.8825\n",
      "Epoch 3795/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.9367 - val_loss: 149.7722\n",
      "Epoch 3796/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 129.5099 - val_loss: 132.6798\n",
      "Epoch 3797/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 127.3091 - val_loss: 135.2779\n",
      "Epoch 3798/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.4895 - val_loss: 130.9990\n",
      "Epoch 3799/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.6046 - val_loss: 184.6902\n",
      "Epoch 3800/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 126.1158 - val_loss: 130.0536\n",
      "Epoch 3801/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 130.5424 - val_loss: 179.4089\n",
      "Epoch 3802/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.1816 - val_loss: 148.8325\n",
      "Epoch 3803/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.8555 - val_loss: 129.9784\n",
      "Epoch 3804/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 126.7873 - val_loss: 131.3654\n",
      "Epoch 3805/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 129.5454 - val_loss: 139.2154\n",
      "Epoch 3806/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.7686 - val_loss: 152.3926\n",
      "Epoch 3807/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 136.8489 - val_loss: 141.2854\n",
      "Epoch 3808/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 130.3651 - val_loss: 183.2463\n",
      "Epoch 3809/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 132.8693 - val_loss: 147.8582\n",
      "Epoch 3810/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.1937 - val_loss: 130.3953\n",
      "Epoch 3811/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.1123 - val_loss: 212.3712\n",
      "Epoch 3812/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 128.9412 - val_loss: 137.3450\n",
      "Epoch 3813/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 127.5010 - val_loss: 183.5662\n",
      "Epoch 3814/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 172.8074 - val_loss: 156.4396\n",
      "Epoch 3815/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 150.4994 - val_loss: 133.0780\n",
      "Epoch 3816/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 131.7900 - val_loss: 132.6912\n",
      "Epoch 3817/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.7863 - val_loss: 135.2149\n",
      "Epoch 3818/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.3363 - val_loss: 131.9788\n",
      "Epoch 3819/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 125.5647 - val_loss: 133.6732\n",
      "Epoch 3820/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 146.7197 - val_loss: 146.1142\n",
      "Epoch 3821/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.6112 - val_loss: 150.8299\n",
      "Epoch 3822/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 127.2639 - val_loss: 131.0176\n",
      "Epoch 3823/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 129.5713 - val_loss: 139.4509\n",
      "Epoch 3824/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 129.9218 - val_loss: 132.2922\n",
      "Epoch 3825/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 128.9948 - val_loss: 219.7450\n",
      "Epoch 3826/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 124.9162 - val_loss: 160.0500\n",
      "Epoch 3827/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 128.0821 - val_loss: 179.9790\n",
      "Epoch 3828/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.8313 - val_loss: 147.6755\n",
      "Epoch 3829/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 126.5216 - val_loss: 138.1548\n",
      "Epoch 3830/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.2478 - val_loss: 127.3932\n",
      "Epoch 3831/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.5107 - val_loss: 158.3724\n",
      "Epoch 3832/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.8432 - val_loss: 160.7390\n",
      "Epoch 3833/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.8540 - val_loss: 161.3740\n",
      "Epoch 3834/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 134.6911 - val_loss: 154.1226\n",
      "Epoch 3835/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 127.8809 - val_loss: 135.0181\n",
      "Epoch 3836/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 126.5060 - val_loss: 246.5973\n",
      "Epoch 3837/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 131.6154 - val_loss: 136.6990\n",
      "Epoch 3838/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 158.6566 - val_loss: 183.2774\n",
      "Epoch 3839/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.5189 - val_loss: 141.2100\n",
      "Epoch 3840/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 146.6286 - val_loss: 131.6944\n",
      "Epoch 3841/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.8394 - val_loss: 158.4558\n",
      "Epoch 3842/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 172.7369 - val_loss: 140.9492\n",
      "Epoch 3843/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 161.0663 - val_loss: 151.7411\n",
      "Epoch 3844/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 129.0602 - val_loss: 147.3895\n",
      "Epoch 3845/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.7479 - val_loss: 133.1725\n",
      "Epoch 3846/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 133.1985 - val_loss: 151.7087\n",
      "Epoch 3847/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.2575 - val_loss: 140.4906\n",
      "Epoch 3848/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 125.4340 - val_loss: 131.3823\n",
      "Epoch 3849/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 127.8779 - val_loss: 135.4903\n",
      "Epoch 3850/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 145.4136 - val_loss: 153.9109\n",
      "Epoch 3851/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 127.7092 - val_loss: 156.0696\n",
      "Epoch 3852/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.3803 - val_loss: 136.8155\n",
      "Epoch 3853/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 139.4861 - val_loss: 131.6191\n",
      "Epoch 3854/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 133.7770 - val_loss: 137.2017\n",
      "Epoch 3855/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.1858 - val_loss: 145.2257\n",
      "Epoch 3856/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.1712 - val_loss: 148.1279\n",
      "Epoch 3857/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 126.3174 - val_loss: 149.0970\n",
      "Epoch 3858/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 135.0112 - val_loss: 147.8834\n",
      "Epoch 3859/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 127.0832 - val_loss: 155.1208\n",
      "Epoch 3860/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.2919 - val_loss: 171.9684\n",
      "Epoch 3861/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.6700 - val_loss: 191.9140\n",
      "Epoch 3862/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 133.9057 - val_loss: 131.8179\n",
      "Epoch 3863/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.8776 - val_loss: 125.5871\n",
      "Epoch 3864/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.6007 - val_loss: 133.8642\n",
      "Epoch 3865/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 125.2337 - val_loss: 126.0742\n",
      "Epoch 3866/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 136.5874 - val_loss: 145.6191\n",
      "Epoch 3867/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 126.9155 - val_loss: 135.0366\n",
      "Epoch 3868/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 127.7220 - val_loss: 129.8818\n",
      "Epoch 3869/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 126.2025 - val_loss: 127.8898\n",
      "Epoch 3870/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 130.4617 - val_loss: 135.9103\n",
      "Epoch 3871/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 128.1159 - val_loss: 139.7211\n",
      "Epoch 3872/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 127.6031 - val_loss: 130.6040\n",
      "Epoch 3873/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 145.1805 - val_loss: 184.8298\n",
      "Epoch 3874/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 157.9800 - val_loss: 142.4313\n",
      "Epoch 3875/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 127.6058 - val_loss: 131.3357\n",
      "Epoch 3876/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.9659 - val_loss: 154.4122\n",
      "Epoch 3877/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 131.0746 - val_loss: 202.7139\n",
      "Epoch 3878/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 139.2604 - val_loss: 129.9147\n",
      "Epoch 3879/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.0454 - val_loss: 137.6562\n",
      "Epoch 3880/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.8046 - val_loss: 168.3001\n",
      "Epoch 3881/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 133.0426 - val_loss: 163.2619\n",
      "Epoch 3882/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.6888 - val_loss: 129.8610\n",
      "Epoch 3883/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 123.8088 - val_loss: 130.5411\n",
      "Epoch 3884/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.1527 - val_loss: 130.7820\n",
      "Epoch 3885/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 129.4453 - val_loss: 166.4814\n",
      "Epoch 3886/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 129.1801 - val_loss: 162.4473\n",
      "Epoch 3887/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.3822 - val_loss: 153.3993\n",
      "Epoch 3888/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.9989 - val_loss: 157.2287\n",
      "Epoch 3889/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 128.0945 - val_loss: 141.3899\n",
      "Epoch 3890/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.0223 - val_loss: 133.8713\n",
      "Epoch 3891/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.5233 - val_loss: 131.9965\n",
      "Epoch 3892/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 127.1093 - val_loss: 131.7574\n",
      "Epoch 3893/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 149.1891 - val_loss: 160.9740\n",
      "Epoch 3894/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 127.7848 - val_loss: 157.7725\n",
      "Epoch 3895/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 135.6816 - val_loss: 139.8094\n",
      "Epoch 3896/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 131.0451 - val_loss: 138.8157\n",
      "Epoch 3897/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 124.1203 - val_loss: 160.6595\n",
      "Epoch 3898/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 129.7777 - val_loss: 142.3148\n",
      "Epoch 3899/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 132.3945 - val_loss: 143.3064\n",
      "Epoch 3900/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.0766 - val_loss: 200.8400\n",
      "Epoch 3901/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.2227 - val_loss: 139.4266\n",
      "Epoch 3902/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 129.3419 - val_loss: 134.4289\n",
      "Epoch 3903/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 127.4213 - val_loss: 139.1969\n",
      "Epoch 3904/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 138.1010 - val_loss: 155.5868\n",
      "Epoch 3905/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.7958 - val_loss: 163.3190\n",
      "Epoch 3906/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 137.3403 - val_loss: 146.9383\n",
      "Epoch 3907/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 130.3948 - val_loss: 129.3424\n",
      "Epoch 3908/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 126.7175 - val_loss: 154.0359\n",
      "Epoch 3909/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.2163 - val_loss: 135.3916\n",
      "Epoch 3910/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 131.8135 - val_loss: 143.9521\n",
      "Epoch 3911/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 135.5596 - val_loss: 137.8583\n",
      "Epoch 3912/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 127.7030 - val_loss: 174.4110\n",
      "Epoch 3913/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 63us/step - loss: 132.5064 - val_loss: 151.8722\n",
      "Epoch 3914/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.7378 - val_loss: 190.2222\n",
      "Epoch 3915/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 141.5781 - val_loss: 222.8801\n",
      "Epoch 3916/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.8159 - val_loss: 147.8435\n",
      "Epoch 3917/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 123.8367 - val_loss: 133.8476\n",
      "Epoch 3918/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 136.6771 - val_loss: 179.8673\n",
      "Epoch 3919/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 144.1473 - val_loss: 187.2961\n",
      "Epoch 3920/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.6915 - val_loss: 135.8783\n",
      "Epoch 3921/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 132.7754 - val_loss: 135.7954\n",
      "Epoch 3922/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.2949 - val_loss: 150.2904\n",
      "Epoch 3923/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.1314 - val_loss: 136.4924\n",
      "Epoch 3924/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.8853 - val_loss: 152.3582\n",
      "Epoch 3925/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 127.8646 - val_loss: 141.3892\n",
      "Epoch 3926/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 125.9274 - val_loss: 132.0246\n",
      "Epoch 3927/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 130.9719 - val_loss: 146.3658\n",
      "Epoch 3928/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 127.9349 - val_loss: 181.3979\n",
      "Epoch 3929/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 135.4704 - val_loss: 188.8364\n",
      "Epoch 3930/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 141.4043 - val_loss: 131.2087\n",
      "Epoch 3931/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 172.2463 - val_loss: 134.5739\n",
      "Epoch 3932/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 151.8889 - val_loss: 135.0650\n",
      "Epoch 3933/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 181.1921 - val_loss: 136.8850\n",
      "Epoch 3934/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.3944 - val_loss: 135.6944\n",
      "Epoch 3935/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 130.3234 - val_loss: 134.1904\n",
      "Epoch 3936/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 124.5117 - val_loss: 135.2664\n",
      "Epoch 3937/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 126.5954 - val_loss: 135.9750\n",
      "Epoch 3938/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 124.2764 - val_loss: 131.7126\n",
      "Epoch 3939/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 124.1937 - val_loss: 153.0483\n",
      "Epoch 3940/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.5250 - val_loss: 178.4871\n",
      "Epoch 3941/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.6057 - val_loss: 272.9592\n",
      "Epoch 3942/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 135.8044 - val_loss: 146.4705\n",
      "Epoch 3943/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 128.0634 - val_loss: 160.9474\n",
      "Epoch 3944/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 147.3371 - val_loss: 183.9993\n",
      "Epoch 3945/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 164.1800 - val_loss: 136.6450\n",
      "Epoch 3946/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.3300 - val_loss: 226.6083\n",
      "Epoch 3947/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 135.8086 - val_loss: 169.8155\n",
      "Epoch 3948/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 126.2318 - val_loss: 151.4293\n",
      "Epoch 3949/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 132.6938 - val_loss: 144.4386\n",
      "Epoch 3950/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 126.5312 - val_loss: 129.8985\n",
      "Epoch 3951/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 129.2460 - val_loss: 129.7567\n",
      "Epoch 3952/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 135.4423 - val_loss: 170.8555\n",
      "Epoch 3953/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 134.754 - 1s 64us/step - loss: 134.5114 - val_loss: 149.2688\n",
      "Epoch 3954/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.0712 - val_loss: 131.6661\n",
      "Epoch 3955/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 136.4809 - val_loss: 133.7650\n",
      "Epoch 3956/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.8679 - val_loss: 130.5886\n",
      "Epoch 3957/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 131.6705 - val_loss: 154.0991\n",
      "Epoch 3958/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 131.2156 - val_loss: 142.7258\n",
      "Epoch 3959/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 147.4265 - val_loss: 129.5858\n",
      "Epoch 3960/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.7277 - val_loss: 153.2742\n",
      "Epoch 3961/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.6460 - val_loss: 198.4520\n",
      "Epoch 3962/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 123.6494 - val_loss: 142.1942\n",
      "Epoch 3963/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 149.7911 - val_loss: 199.3502\n",
      "Epoch 3964/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 142.6666 - val_loss: 144.5339\n",
      "Epoch 3965/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 140.5875 - val_loss: 139.9537\n",
      "Epoch 3966/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 124.9039 - val_loss: 142.9157\n",
      "Epoch 3967/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 124.6897 - val_loss: 131.8976\n",
      "Epoch 3968/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.5395 - val_loss: 143.8457\n",
      "Epoch 3969/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.6153 - val_loss: 135.7242\n",
      "Epoch 3970/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 126.4729 - val_loss: 136.5526\n",
      "Epoch 3971/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 129.1899 - val_loss: 137.9850\n",
      "Epoch 3972/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.0766 - val_loss: 146.6713\n",
      "Epoch 3973/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 128.5902 - val_loss: 128.8699\n",
      "Epoch 3974/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 127.3527 - val_loss: 140.1867\n",
      "Epoch 3975/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 127.7650 - val_loss: 131.7988\n",
      "Epoch 3976/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.6836 - val_loss: 140.0694\n",
      "Epoch 3977/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 131.8121 - val_loss: 133.3153\n",
      "Epoch 3978/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 128.0572 - val_loss: 129.8774\n",
      "Epoch 3979/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.7196 - val_loss: 130.9283\n",
      "Epoch 3980/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 130.5267 - val_loss: 148.9024\n",
      "Epoch 3981/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.4638 - val_loss: 148.4786\n",
      "Epoch 3982/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.0459 - val_loss: 254.0338\n",
      "Epoch 3983/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 146.8348 - val_loss: 140.8080\n",
      "Epoch 3984/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 127.5548 - val_loss: 194.7558\n",
      "Epoch 3985/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 128.2867 - val_loss: 130.0327\n",
      "Epoch 3986/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 130.9566 - val_loss: 141.3376\n",
      "Epoch 3987/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 138.1760 - val_loss: 145.7451\n",
      "Epoch 3988/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 134.5871 - val_loss: 136.9253\n",
      "Epoch 3989/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 126.8729 - val_loss: 139.8574\n",
      "Epoch 3990/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 131.9019 - val_loss: 144.1777\n",
      "Epoch 3991/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 133.9527 - val_loss: 156.3150\n",
      "Epoch 3992/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.5593 - val_loss: 193.3540\n",
      "Epoch 3993/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 152.7816 - val_loss: 178.8397\n",
      "Epoch 3994/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 155.9063 - val_loss: 165.3664\n",
      "Epoch 3995/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 169.3444 - val_loss: 158.9464\n",
      "Epoch 3996/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.5703 - val_loss: 152.2432\n",
      "Epoch 3997/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 139.1154 - val_loss: 133.6368\n",
      "Epoch 3998/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.1006 - val_loss: 148.8106\n",
      "Epoch 3999/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 125.1517 - val_loss: 152.5637\n",
      "Epoch 4000/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 134.5513 - val_loss: 148.7845\n",
      "Epoch 4001/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.3024 - val_loss: 136.6388\n",
      "Epoch 4002/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.2561 - val_loss: 132.2798\n",
      "Epoch 4003/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 131.8601 - val_loss: 149.5168\n",
      "Epoch 4004/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 182.9226 - val_loss: 146.3874\n",
      "Epoch 4005/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 148.4670 - val_loss: 146.9422\n",
      "Epoch 4006/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.6869 - val_loss: 141.9212\n",
      "Epoch 4007/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.2603 - val_loss: 147.3357\n",
      "Epoch 4008/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 124.4466 - val_loss: 131.8034\n",
      "Epoch 4009/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 124.9352 - val_loss: 142.0360\n",
      "Epoch 4010/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.4927 - val_loss: 342.4974\n",
      "Epoch 4011/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.8290 - val_loss: 142.9699\n",
      "Epoch 4012/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.9143 - val_loss: 131.4411\n",
      "Epoch 4013/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 130.6331 - val_loss: 145.1012\n",
      "Epoch 4014/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.4624 - val_loss: 130.2449\n",
      "Epoch 4015/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 126.1533 - val_loss: 131.2606\n",
      "Epoch 4016/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 137.3350 - val_loss: 152.3865\n",
      "Epoch 4017/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 129.3190 - val_loss: 181.6159\n",
      "Epoch 4018/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.9110 - val_loss: 141.9783\n",
      "Epoch 4019/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 128.1930 - val_loss: 140.1249\n",
      "Epoch 4020/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.0368 - val_loss: 130.8925\n",
      "Epoch 4021/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 147.8960 - val_loss: 136.0414\n",
      "Epoch 4022/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 132.4108 - val_loss: 133.3303\n",
      "Epoch 4023/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 131.3291 - val_loss: 138.7940\n",
      "Epoch 4024/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.5536 - val_loss: 150.2095\n",
      "Epoch 4025/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.2577 - val_loss: 137.4787\n",
      "Epoch 4026/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.2623 - val_loss: 141.8130\n",
      "Epoch 4027/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 127.5518 - val_loss: 140.0997\n",
      "Epoch 4028/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.2541 - val_loss: 134.3429\n",
      "Epoch 4029/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 131.3961 - val_loss: 132.2810\n",
      "Epoch 4030/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 134.0580 - val_loss: 133.5748\n",
      "Epoch 4031/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 136.9219 - val_loss: 166.0049\n",
      "Epoch 4032/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 133.7784 - val_loss: 137.6744\n",
      "Epoch 4033/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 128.1860 - val_loss: 137.0238\n",
      "Epoch 4034/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 133.1519 - val_loss: 134.6551\n",
      "Epoch 4035/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 127.7837 - val_loss: 129.3694\n",
      "Epoch 4036/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.4349 - val_loss: 163.5530\n",
      "Epoch 4037/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 129.5217 - val_loss: 136.2870\n",
      "Epoch 4038/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 139.6613 - val_loss: 165.4552\n",
      "Epoch 4039/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.8676 - val_loss: 142.5199\n",
      "Epoch 4040/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 133.0342 - val_loss: 150.5318\n",
      "Epoch 4041/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 142.8416 - val_loss: 146.5714\n",
      "Epoch 4042/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 127.1545 - val_loss: 144.0606\n",
      "Epoch 4043/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 124.8676 - val_loss: 161.6264\n",
      "Epoch 4044/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 131.662 - 0s 62us/step - loss: 128.9878 - val_loss: 129.3251\n",
      "Epoch 4045/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 130.5477 - val_loss: 259.9500\n",
      "Epoch 4046/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 136.2050 - val_loss: 149.0664\n",
      "Epoch 4047/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 155.0530 - val_loss: 148.5557\n",
      "Epoch 4048/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.7519 - val_loss: 190.8958\n",
      "Epoch 4049/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.5657 - val_loss: 140.1950\n",
      "Epoch 4050/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.9987 - val_loss: 146.8769\n",
      "Epoch 4051/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 151.0686 - val_loss: 153.1033\n",
      "Epoch 4052/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 136.7354 - val_loss: 148.0548\n",
      "Epoch 4053/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.1600 - val_loss: 135.2874\n",
      "Epoch 4054/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 128.9804 - val_loss: 147.0876\n",
      "Epoch 4055/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 129.6154 - val_loss: 147.1207\n",
      "Epoch 4056/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.1623 - val_loss: 133.0275\n",
      "Epoch 4057/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 128.6538 - val_loss: 164.5497\n",
      "Epoch 4058/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 129.4035 - val_loss: 170.6552\n",
      "Epoch 4059/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 142.2685 - val_loss: 136.0598\n",
      "Epoch 4060/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 179.3229 - val_loss: 132.1012\n",
      "Epoch 4061/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 138.0708 - val_loss: 189.8391\n",
      "Epoch 4062/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 126.3078 - val_loss: 140.2554\n",
      "Epoch 4063/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 127.1281 - val_loss: 131.4474\n",
      "Epoch 4064/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.4838 - val_loss: 147.8941\n",
      "Epoch 4065/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 125.4714 - val_loss: 128.0328\n",
      "Epoch 04065: early stopping\n",
      "Fold score (RMSE): 11.073962211608887\n",
      "Fold #4\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 5001.1817 - val_loss: 4619.2209\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 4453.2312 - val_loss: 4424.0572\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 4339.8316 - val_loss: 4349.7813\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 4296.9321 - val_loss: 4353.7585\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 4319.1476 - val_loss: 4339.6890\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 4162.0165 - val_loss: 4201.5990\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 4063.3449 - val_loss: 4133.1954\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 4008.7885 - val_loss: 3966.7245\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 3934.9671 - val_loss: 3835.8038\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 3813.4361 - val_loss: 4083.0817\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 3607.1141 - val_loss: 3894.3678\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 3594.2878 - val_loss: 3343.2801\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 3357.8218 - val_loss: 3185.3844\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 3228.1774 - val_loss: 2806.1280\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 3054.8423 - val_loss: 2736.0993\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 2448.0498 - val_loss: 1842.1420\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 2393.5563 - val_loss: 1638.8280\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 1856.1659 - val_loss: 2852.6279\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 1390.2903 - val_loss: 935.1105\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 1157.0726 - val_loss: 701.7132\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 1149.2038 - val_loss: 626.9197\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 851.1796 - val_loss: 826.2965\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 807.4437 - val_loss: 498.4694\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 757.3863 - val_loss: 462.6954\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 660.4403 - val_loss: 440.3073\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 768.9259 - val_loss: 445.5043\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 661.6636 - val_loss: 485.4373\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 591.1637 - val_loss: 395.3596\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 826.9080 - val_loss: 434.0142\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 537.7045 - val_loss: 426.2108\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 692.4877 - val_loss: 385.5929\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 629.8146 - val_loss: 605.5277\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 557.9269 - val_loss: 501.6379\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 536.6581 - val_loss: 363.9676\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 677.3032 - val_loss: 383.5641\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 498.0594 - val_loss: 321.6898\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 619.2547 - val_loss: 391.4891\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 620.0334 - val_loss: 360.7323\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 496.5302 - val_loss: 563.8495\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 539.7033 - val_loss: 436.8042\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 532.6674 - val_loss: 376.1189\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 473.3437 - val_loss: 521.6832\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 506.1002 - val_loss: 281.2091\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 438.4589 - val_loss: 473.3085\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 452.9837 - val_loss: 311.1844\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 402.9472 - val_loss: 324.0703\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 459.0328 - val_loss: 748.4883\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 449.1124 - val_loss: 891.6699\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 485.2709 - val_loss: 276.3180\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 393.2261 - val_loss: 258.3789\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 443.7441 - val_loss: 238.4906\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 407.8412 - val_loss: 487.6255\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 560.9863 - val_loss: 462.6070\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 475.2940 - val_loss: 269.9794\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 412.0949 - val_loss: 316.9411\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 392.6236 - val_loss: 265.2629\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 388.3045 - val_loss: 404.7493\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 375.3016 - val_loss: 242.6577\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 396.3683 - val_loss: 246.5391\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 396.7365 - val_loss: 270.1342\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 368.7968 - val_loss: 326.3251\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 375.7019 - val_loss: 304.1322\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 371.1672 - val_loss: 253.7319\n",
      "Epoch 64/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 396.2333 - val_loss: 291.8823\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 454.2952 - val_loss: 270.2225\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 353.7432 - val_loss: 467.1428\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 359.6191 - val_loss: 268.4131\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 401.5869 - val_loss: 766.3363\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 392.9442 - val_loss: 233.9079\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 375.3981 - val_loss: 265.3163\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 335.8735 - val_loss: 199.9573\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 356.8466 - val_loss: 306.9685\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 336.2092 - val_loss: 304.6381\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 464.0545 - val_loss: 413.8912\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 395.8168 - val_loss: 226.6318\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 371.6870 - val_loss: 247.7664\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 369.7153 - val_loss: 200.9285\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 395.7494 - val_loss: 247.2228\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 344.6825 - val_loss: 290.1777\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 324.3379 - val_loss: 305.3766\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 357.8815 - val_loss: 526.2118\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 431.7453 - val_loss: 341.6141\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 394.2701 - val_loss: 588.5792\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 316.5969 - val_loss: 209.8236\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 280.0831 - val_loss: 211.3518\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 342.1907 - val_loss: 205.3180\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 358.1414 - val_loss: 905.9388\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 347.1544 - val_loss: 231.4201\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 355.5399 - val_loss: 354.8706\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 367.8411 - val_loss: 212.8088\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 307.0499 - val_loss: 274.1372\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 362.8574 - val_loss: 395.5274\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 309.4966 - val_loss: 195.4094\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 356.5971 - val_loss: 219.8432\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 458.8058 - val_loss: 377.3676\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 313.8539 - val_loss: 331.0896\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 329.3550 - val_loss: 233.8978\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 336.8444 - val_loss: 234.8693\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 299.8524 - val_loss: 266.9199\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 319.3139 - val_loss: 279.9444\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 352.7960 - val_loss: 200.6847\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 324.4778 - val_loss: 286.3306\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 308.3134 - val_loss: 282.3589\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 293.4386 - val_loss: 264.9692\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 336.8151 - val_loss: 192.3621\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 280.4240 - val_loss: 355.3053\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 302.9018 - val_loss: 196.1437\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 508.4847 - val_loss: 316.0458\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 292.3808 - val_loss: 528.8017\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 254.6176 - val_loss: 234.0478\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 280.3406 - val_loss: 302.0198\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 405.4521 - val_loss: 200.8657\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 322.3633 - val_loss: 268.7832\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 320.5609 - val_loss: 222.0158\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 291.9978 - val_loss: 207.6315\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 266.0985 - val_loss: 182.0339\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 1s 126us/step - loss: 275.3477 - val_loss: 295.1072\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 317.8314 - val_loss: 285.1563\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 278.6417 - val_loss: 180.5813\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 379.7491 - val_loss: 221.8130\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 364.9602 - val_loss: 401.0945\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 398.0488 - val_loss: 668.2858\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 278.2124 - val_loss: 199.0867\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 330.8286 - val_loss: 315.7076\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 295.1164 - val_loss: 190.4010\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 253.3222 - val_loss: 394.8366\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 282.3164 - val_loss: 239.8858\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 297.1987 - val_loss: 984.6554\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 288.8980 - val_loss: 234.3570\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 246.2257 - val_loss: 163.0499\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 296.9277 - val_loss: 197.7262\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 292.6816 - val_loss: 217.6214\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 255.8718 - val_loss: 189.1162\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 383.3135 - val_loss: 267.1027\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 253.5236 - val_loss: 203.0642\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 295.1372 - val_loss: 177.2702\n",
      "Epoch 137/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 319.8225 - val_loss: 219.1645\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 263.7018 - val_loss: 583.1090\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 257.1816 - val_loss: 254.3531\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 260.9199 - val_loss: 162.5564\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 277.7993 - val_loss: 246.7048\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 286.8354 - val_loss: 217.0355\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 283.9399 - val_loss: 172.8665\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 247.8921 - val_loss: 196.5411\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 271.5037 - val_loss: 262.1476\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 247.2640 - val_loss: 161.3220\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 294.7310 - val_loss: 251.9230\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 301.2128 - val_loss: 225.2412\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 287.4421 - val_loss: 170.7838\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 295.3122 - val_loss: 214.8216\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 246.8367 - val_loss: 181.8161\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 231.4246 - val_loss: 172.8201\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 223.7866 - val_loss: 598.1366\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 361.8131 - val_loss: 207.8021\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 224.4773 - val_loss: 158.0372\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 379.0470 - val_loss: 214.6583\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 240.7555 - val_loss: 170.9069\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 229.4756 - val_loss: 192.0200\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 311.6812 - val_loss: 168.7572\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 248.3831 - val_loss: 193.9662\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 319.8711 - val_loss: 517.2754\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 294.0526 - val_loss: 162.9675\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 247.3689 - val_loss: 171.5252\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 305.1434 - val_loss: 237.8344\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 366.3236 - val_loss: 227.1423\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 251.0629 - val_loss: 161.7266\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 284.3612 - val_loss: 169.7597\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 251.0681 - val_loss: 283.6968\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 246.5550 - val_loss: 203.6413\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 252.1714 - val_loss: 167.0360\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 227.2456 - val_loss: 159.7322\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 235.1267 - val_loss: 204.2849\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 220.4798 - val_loss: 161.4623\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 446.1870 - val_loss: 223.6758\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 297.4874 - val_loss: 176.5171\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 249.6507 - val_loss: 185.1872\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 271.2630 - val_loss: 187.4127\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 301.6857 - val_loss: 183.3287\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 238.9437 - val_loss: 272.8819\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 297.1554 - val_loss: 198.1353\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 234.9686 - val_loss: 311.3707\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 266.5390 - val_loss: 156.5329\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 284.8014 - val_loss: 230.6588\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 255.1510 - val_loss: 157.7966\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 294.7804 - val_loss: 225.9650\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 241.0317 - val_loss: 211.5760\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 216.3600 - val_loss: 161.9864\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 263.0231 - val_loss: 169.9740\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 510.4982 - val_loss: 277.3412\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 324.9246 - val_loss: 241.9697\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 250.3274 - val_loss: 209.0814\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 267.8984 - val_loss: 213.5652\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 217.5249 - val_loss: 277.9482\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 340.1710 - val_loss: 315.0316\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 231.0789 - val_loss: 162.5979\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 226.6802 - val_loss: 228.8857\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 235.6530 - val_loss: 244.7899\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 342.9231 - val_loss: 193.4914\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 220.2663 - val_loss: 149.7104\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 225.8090 - val_loss: 146.7933\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 260.2088 - val_loss: 240.0297\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 326.3129 - val_loss: 173.4517\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 222.1683 - val_loss: 147.4866\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 223.5176 - val_loss: 163.7299\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 255.7007 - val_loss: 184.2894\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 301.3084 - val_loss: 346.1082\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 235.2789 - val_loss: 154.9779\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 200.7459 - val_loss: 190.2484\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 206.028 - 1s 70us/step - loss: 204.8059 - val_loss: 196.4276\n",
      "Epoch 210/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 66us/step - loss: 231.0892 - val_loss: 181.5278\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 207.2721 - val_loss: 154.1402\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 503.6573 - val_loss: 356.9332\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 424.1414 - val_loss: 202.2342\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 294.4936 - val_loss: 350.8525\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 334.5709 - val_loss: 203.4403\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 279.8284 - val_loss: 198.1042\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 230.1331 - val_loss: 174.2823\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 382.1776 - val_loss: 172.1283\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 258.6839 - val_loss: 228.8812\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 233.5345 - val_loss: 160.3054\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 292.7847 - val_loss: 163.6555\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 225.7331 - val_loss: 152.7175\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 228.2606 - val_loss: 148.2635\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 240.3649 - val_loss: 225.7041\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 266.3059 - val_loss: 145.0574\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 217.0080 - val_loss: 171.4601\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 361.7824 - val_loss: 216.5515\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 299.1074 - val_loss: 185.4830\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 263.2862 - val_loss: 168.2757\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 287.2062 - val_loss: 177.7036\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 377.5383 - val_loss: 212.2519\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 218.7965 - val_loss: 181.8627\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 256.6767 - val_loss: 204.1084\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 205.1792 - val_loss: 145.7543\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 294.5945 - val_loss: 272.5445\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 232.0489 - val_loss: 156.9715\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 204.0453 - val_loss: 174.9902\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 241.8360 - val_loss: 282.9681\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 222.9617 - val_loss: 142.5172\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 271.2771 - val_loss: 233.1997\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 225.5458 - val_loss: 140.4191\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 236.5699 - val_loss: 139.9731\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 222.5653 - val_loss: 205.5253\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 191.5186 - val_loss: 145.8404\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 202.6175 - val_loss: 157.1538\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 296.0325 - val_loss: 312.1152\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 257.3830 - val_loss: 179.4890\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 220.2351 - val_loss: 173.9805\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 223.5850 - val_loss: 227.5401\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 328.0959 - val_loss: 164.0756\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 223.6655 - val_loss: 168.3200\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 223.8596 - val_loss: 148.6467\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 216.3262 - val_loss: 171.6079\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 283.7846 - val_loss: 198.0882\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 188.5074 - val_loss: 147.5363\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 206.7458 - val_loss: 352.2428\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 400.6429 - val_loss: 144.6094\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 192.7624 - val_loss: 321.8427\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 212.5277 - val_loss: 185.9542\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 215.0663 - val_loss: 163.5736\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 202.1349 - val_loss: 174.8180\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 227.0579 - val_loss: 259.0338\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 1282.7022 - val_loss: 382.5456\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 411.5123 - val_loss: 232.5920\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 292.5189 - val_loss: 201.5686\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 284.0152 - val_loss: 171.5289\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 267.5993 - val_loss: 202.3684\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 295.4629 - val_loss: 201.9491\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 277.0213 - val_loss: 172.2018\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 235.8557 - val_loss: 154.9300\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 228.0875 - val_loss: 152.6029\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 328.6145 - val_loss: 185.9772\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 245.0828 - val_loss: 152.7755\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 213.2615 - val_loss: 164.9372\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 245.4320 - val_loss: 332.9605\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 316.7150 - val_loss: 159.5121\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 253.7137 - val_loss: 153.3895\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 217.0305 - val_loss: 233.1546\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 233.6863 - val_loss: 192.4648\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 246.6886 - val_loss: 153.9426\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 278.4853 - val_loss: 199.7747\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 210.0278 - val_loss: 222.2600\n",
      "Epoch 283/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 65us/step - loss: 281.0980 - val_loss: 143.2393\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 212.7724 - val_loss: 467.6600\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 230.8692 - val_loss: 171.3709\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 193.2749 - val_loss: 168.6464\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 230.9309 - val_loss: 170.4964\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 241.383 - 1s 64us/step - loss: 241.1901 - val_loss: 307.9031\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 215.8692 - val_loss: 166.4797\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 214.3324 - val_loss: 139.5337\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 204.7394 - val_loss: 169.9958\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 214.6514 - val_loss: 136.4607\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 208.0544 - val_loss: 216.8973\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 277.8251 - val_loss: 155.2368\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 217.9721 - val_loss: 185.2759\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 228.1646 - val_loss: 145.4213\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 238.1621 - val_loss: 241.3461\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 207.3885 - val_loss: 194.3630\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 204.7045 - val_loss: 161.5214\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 226.5803 - val_loss: 276.6737\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 187.4408 - val_loss: 176.9484\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 203.2678 - val_loss: 315.7941\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 202.9927 - val_loss: 167.3722\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 347.2899 - val_loss: 145.0146\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 238.8748 - val_loss: 155.6639\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 224.3688 - val_loss: 150.3229\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 211.8986 - val_loss: 250.7575\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 216.6432 - val_loss: 142.5794\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 218.2570 - val_loss: 286.1646\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 230.5909 - val_loss: 289.9135\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 201.7520 - val_loss: 148.6941\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 198.6932 - val_loss: 182.3825\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 188.5094 - val_loss: 134.9785\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 177.5904 - val_loss: 297.2349\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 318.7723 - val_loss: 155.4709\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 194.5145 - val_loss: 148.5451\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 218.5344 - val_loss: 166.2080\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 193.8514 - val_loss: 153.4165\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 301.1734 - val_loss: 272.5363\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 224.4485 - val_loss: 306.9146\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 252.7871 - val_loss: 178.9023\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 217.5411 - val_loss: 256.6975\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 193.9440 - val_loss: 154.8260\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 229.2331 - val_loss: 156.1244\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 203.4720 - val_loss: 150.5234\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 188.4772 - val_loss: 147.9826\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 174.7802 - val_loss: 146.5358\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 223.9810 - val_loss: 191.4908\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 204.2438 - val_loss: 321.5048\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 385.0733 - val_loss: 240.8900\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 182.1623 - val_loss: 162.1518\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 196.4657 - val_loss: 138.8259\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 196.7455 - val_loss: 130.6020\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 179.6254 - val_loss: 157.4165\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 198.9559 - val_loss: 310.7623\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 204.0371 - val_loss: 283.2465\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 266.9565 - val_loss: 154.0177\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 179.1548 - val_loss: 152.8113\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 196.9215 - val_loss: 137.3193\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 193.6875 - val_loss: 131.8668\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 212.2786 - val_loss: 487.2386\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 222.5567 - val_loss: 136.9091\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 234.4798 - val_loss: 248.4737\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 194.2211 - val_loss: 145.5683\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 271.2838 - val_loss: 145.4526\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 186.7289 - val_loss: 126.9364\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 189.7248 - val_loss: 139.0156\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 204.0200 - val_loss: 175.6750\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 193.9584 - val_loss: 181.9289\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 171.4584 - val_loss: 265.7830\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 204.6293 - val_loss: 130.9239\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 182.7350 - val_loss: 178.7424\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 226.2824 - val_loss: 267.5691\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 300.9955 - val_loss: 536.3519\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 217.5893 - val_loss: 150.4505\n",
      "Epoch 356/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 76us/step - loss: 184.7327 - val_loss: 131.6216\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 181.6118 - val_loss: 175.0999\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 166.2367 - val_loss: 130.1665\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 171.6732 - val_loss: 122.5099\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 173.0769 - val_loss: 170.5596\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 199.5557 - val_loss: 135.5427\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 194.970 - 1s 77us/step - loss: 194.8905 - val_loss: 299.6939\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 208.7413 - val_loss: 149.2465\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 278.0415 - val_loss: 147.4073\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 217.0274 - val_loss: 142.4365\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 189.8646 - val_loss: 128.4828\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 200.5783 - val_loss: 191.2707\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 205.7272 - val_loss: 168.3887\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 173.7723 - val_loss: 237.7336\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 184.8213 - val_loss: 248.5359\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 255.4551 - val_loss: 205.7101\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 203.5930 - val_loss: 149.0698\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 210.8267 - val_loss: 138.8267\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 186.8698 - val_loss: 144.1834\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 179.0852 - val_loss: 155.4512\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 187.1676 - val_loss: 240.3190\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 254.9580 - val_loss: 147.1657\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 201.3777 - val_loss: 154.6064\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 194.7023 - val_loss: 137.7351\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 190.8307 - val_loss: 238.9993\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 185.4324 - val_loss: 192.7026\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 237.5059 - val_loss: 253.8125\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 519.7949 - val_loss: 290.1073\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 311.7691 - val_loss: 169.2878\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 220.6774 - val_loss: 129.6173\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 189.7868 - val_loss: 245.4109\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 184.9511 - val_loss: 183.9291\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 187.0343 - val_loss: 145.9122\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 230.0388 - val_loss: 157.0370\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 196.5653 - val_loss: 220.8061\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 207.7171 - val_loss: 148.0175\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 202.0470 - val_loss: 136.9936\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 185.4774 - val_loss: 132.6155\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 293.9463 - val_loss: 149.2107\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 173.9364 - val_loss: 127.3585\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 179.8745 - val_loss: 187.7764\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 171.6404 - val_loss: 128.2592\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 183.6632 - val_loss: 170.5011\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 189.3859 - val_loss: 137.9759\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 292.0069 - val_loss: 146.0791\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 182.5387 - val_loss: 135.7380\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 180.0848 - val_loss: 161.7697\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 213.0278 - val_loss: 151.5721\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 303.4880 - val_loss: 205.8806\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 177.7325 - val_loss: 137.0736\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 190.6162 - val_loss: 140.5542\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 216.8633 - val_loss: 158.4833\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 157.3653 - val_loss: 128.3622\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 169.3605 - val_loss: 177.2322\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 180.8550 - val_loss: 197.7446\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 182.2666 - val_loss: 129.0602\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 178.4158 - val_loss: 271.4120\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 546.2212 - val_loss: 142.7563\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 203.8531 - val_loss: 145.2293\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 160.5348 - val_loss: 134.0545\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.2481 - val_loss: 138.1253\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 173.8267 - val_loss: 166.5207\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 180.0721 - val_loss: 151.7575\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 179.5148 - val_loss: 131.5850\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 177.6340 - val_loss: 128.7510\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 489.4068 - val_loss: 150.5037\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 215.2922 - val_loss: 143.2505\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 198.7239 - val_loss: 158.7889\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 176.0496 - val_loss: 144.1443\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 184.3319 - val_loss: 139.8246\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 191.4420 - val_loss: 261.1292\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 252.4529 - val_loss: 187.0733\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 201.7947 - val_loss: 181.4968\n",
      "Epoch 429/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 331.0265 - val_loss: 806.1010\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 257.5510 - val_loss: 150.9079\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 168.1354 - val_loss: 139.1974\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 206.6839 - val_loss: 192.5741\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 174.8114 - val_loss: 165.9099\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 181.5219 - val_loss: 198.7216\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 175.6942 - val_loss: 128.1166\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 187.1940 - val_loss: 154.0604\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 197.0300 - val_loss: 202.3853\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 176.0243 - val_loss: 226.8703\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.8561 - val_loss: 137.3488\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 200.2069 - val_loss: 149.8947\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 187.4225 - val_loss: 139.6473\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 353.2468 - val_loss: 348.1987\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 234.5539 - val_loss: 234.2193\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 174.4042 - val_loss: 140.7000\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 232.5983 - val_loss: 148.7204\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 198.164 - 1s 70us/step - loss: 196.3612 - val_loss: 251.7934\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 242.2726 - val_loss: 181.7519\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 227.6476 - val_loss: 144.4451\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 194.1033 - val_loss: 151.3451\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 196.7515 - val_loss: 128.7615\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 180.4142 - val_loss: 146.1088\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 193.7990 - val_loss: 219.3876\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 333.8021 - val_loss: 177.7592\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 178.1983 - val_loss: 134.2337\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 283.6446 - val_loss: 139.3433\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 180.4817 - val_loss: 143.4799\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 165.9559 - val_loss: 134.2916\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 189.0054 - val_loss: 125.8069\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 183.8274 - val_loss: 139.9738\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 207.9723 - val_loss: 263.7791\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 188.8467 - val_loss: 131.0196\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 179.9492 - val_loss: 144.2355\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 180.4511 - val_loss: 127.4098\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 192.3971 - val_loss: 149.3481\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 478.9593 - val_loss: 198.0719\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 191.8912 - val_loss: 149.9392\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 177.6775 - val_loss: 126.5602\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 189.9379 - val_loss: 122.4929\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 174.6198 - val_loss: 153.0072\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 264.9273 - val_loss: 127.6850\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 161.8171 - val_loss: 127.8988\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 173.9813 - val_loss: 125.1441\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 165.5122 - val_loss: 123.2783\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 202.4278 - val_loss: 632.0710\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 717.9467 - val_loss: 210.5306\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 224.6638 - val_loss: 144.2552\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 181.7145 - val_loss: 164.0349\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 181.5286 - val_loss: 189.3932\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 179.3197 - val_loss: 125.3920\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 162.9911 - val_loss: 139.0810\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 234.3636 - val_loss: 146.0806\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 186.2593 - val_loss: 163.6338\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 164.3164 - val_loss: 151.9396\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 222.9723 - val_loss: 223.2637\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 216.9918 - val_loss: 180.6945\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 209.2071 - val_loss: 123.9450\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 227.0199 - val_loss: 150.7046\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 216.5106 - val_loss: 140.5077\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 217.7122 - val_loss: 200.3473\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 175.1149 - val_loss: 145.8321\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 181.0713 - val_loss: 316.4088\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 239.7510 - val_loss: 176.4354\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 181.9410 - val_loss: 159.7540\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 199.6867 - val_loss: 166.6374\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 221.9213 - val_loss: 151.4422\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 211.412 - 1s 68us/step - loss: 210.8781 - val_loss: 132.2617\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 270.9038 - val_loss: 135.2069\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 173.8859 - val_loss: 235.0940\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 215.4696 - val_loss: 156.4675\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 307.0704 - val_loss: 381.2953\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 235.8534 - val_loss: 129.7130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 228.8036 - val_loss: 320.5924\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 350.6782 - val_loss: 127.2833\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 185.3655 - val_loss: 221.4051\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 193.1646 - val_loss: 216.8605\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 174.0233 - val_loss: 124.3165\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 172.4909 - val_loss: 133.3098\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 179.8367 - val_loss: 132.3590\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 170.9708 - val_loss: 181.5986\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 176.6188 - val_loss: 130.5361\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 196.7603 - val_loss: 227.8517\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 169.5415 - val_loss: 144.0349\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 204.5157 - val_loss: 136.6649\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 269.5303 - val_loss: 646.9385\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 273.0603 - val_loss: 268.3780\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 268.8396 - val_loss: 159.0234\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 203.3779 - val_loss: 262.0079\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 379.0185 - val_loss: 126.3872\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 170.9986 - val_loss: 139.2340\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 163.2247 - val_loss: 129.1798\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 169.1127 - val_loss: 139.5782\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 172.3956 - val_loss: 180.6972\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 188.4509 - val_loss: 133.2637\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 271.3687 - val_loss: 622.6686\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 267.0774 - val_loss: 189.1951\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 187.7601 - val_loss: 222.2576\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 346.9976 - val_loss: 436.5947\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 226.0356 - val_loss: 143.9866\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 184.6855 - val_loss: 127.0901\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 211.8606 - val_loss: 133.5581\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 161.4895 - val_loss: 145.3944\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 172.3543 - val_loss: 152.7426\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 173.2349 - val_loss: 191.5674\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 214.2931 - val_loss: 180.2955\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 207.3966 - val_loss: 134.9531\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 202.2465 - val_loss: 338.4725\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 248.1768 - val_loss: 141.7554\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 198.4935 - val_loss: 417.3978\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 268.0281 - val_loss: 121.3143\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 192.0113 - val_loss: 177.7683\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 178.8912 - val_loss: 236.7415\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 171.5598 - val_loss: 134.3505\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 214.1092 - val_loss: 320.0627\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 180.0770 - val_loss: 137.6897\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 192.8371 - val_loss: 236.8125\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 186.2788 - val_loss: 174.4812\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 250.0873 - val_loss: 503.7790\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 193.1207 - val_loss: 127.4481\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 171.3294 - val_loss: 147.0058\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 169.1958 - val_loss: 134.1315\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 186.5824 - val_loss: 122.6841\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 296.2116 - val_loss: 129.9574\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 191.4439 - val_loss: 238.5259\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 167.3055 - val_loss: 153.7326\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 174.8145 - val_loss: 156.8578\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 218.2656 - val_loss: 171.3094\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 212.6405 - val_loss: 169.4311\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 290.6784 - val_loss: 123.5235\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 171.6680 - val_loss: 132.3576\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 155.7589 - val_loss: 183.1966\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 168.2346 - val_loss: 141.4895\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 175.8125 - val_loss: 124.6774\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 212.1179 - val_loss: 134.1416\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 197.4767 - val_loss: 233.0195\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 197.1215 - val_loss: 145.3668\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 189.7204 - val_loss: 154.3587\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 181.6258 - val_loss: 134.4508\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 280.9641 - val_loss: 135.0098\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 158.4550 - val_loss: 127.0091\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 165.1977 - val_loss: 125.9673\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 161.1123 - val_loss: 130.1310\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 164.5423 - val_loss: 124.5283\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 164.8357 - val_loss: 118.9905\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 163.3188 - val_loss: 291.4922\n",
      "Epoch 575/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 68us/step - loss: 186.4545 - val_loss: 125.2384\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 171.7887 - val_loss: 162.1437\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.4523 - val_loss: 125.6294\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 213.0161 - val_loss: 153.2685\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 237.9755 - val_loss: 132.3438\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 235.9565 - val_loss: 137.6945\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 170.6333 - val_loss: 152.6657\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 166.7265 - val_loss: 129.6958\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 158.8653 - val_loss: 124.1149\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 272.8931 - val_loss: 126.8342\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 212.4520 - val_loss: 135.5228\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 169.3749 - val_loss: 172.0300\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 165.6492 - val_loss: 118.8242\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 222.1927 - val_loss: 236.6707\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 215.8817 - val_loss: 125.8362\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 164.3300 - val_loss: 119.4249\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 175.0921 - val_loss: 126.3323\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 188.8023 - val_loss: 128.3063\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 185.3091 - val_loss: 134.0263\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 228.9663 - val_loss: 323.8621\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 403.7647 - val_loss: 161.6946\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 185.8055 - val_loss: 141.7416\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 160.5379 - val_loss: 127.2360\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 182.0143 - val_loss: 124.8329\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 323.7364 - val_loss: 136.2794\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 243.0439 - val_loss: 152.1798\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.9803 - val_loss: 155.6424\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.1478 - val_loss: 154.1435\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 186.9695 - val_loss: 124.4599\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 195.2972 - val_loss: 158.0795\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 179.0428 - val_loss: 129.0944\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 168.0975 - val_loss: 157.1939\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 167.6067 - val_loss: 151.6961\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 166.1312 - val_loss: 123.3701\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 159.0465 - val_loss: 128.3126\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 209.2750 - val_loss: 139.3985\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 166.5674 - val_loss: 129.1644\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 166.2925 - val_loss: 162.0075\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 166.9387 - val_loss: 119.1999\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 282.0568 - val_loss: 211.2104\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 206.8731 - val_loss: 164.9242\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 178.6643 - val_loss: 243.2583\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 177.6963 - val_loss: 127.0373\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 175.7795 - val_loss: 191.8638\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 195.1443 - val_loss: 173.7758\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 170.1160 - val_loss: 221.3750\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 168.9720 - val_loss: 125.5700\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 182.3346 - val_loss: 169.5732\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 166.7159 - val_loss: 160.9761\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 163.9934 - val_loss: 121.2758\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 163.5322 - val_loss: 152.2701\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 161.9893 - val_loss: 181.2571\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 169.4959 - val_loss: 147.8239\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 176.9458 - val_loss: 161.4411\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 435.2150 - val_loss: 148.3289\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 153.2056 - val_loss: 123.8189\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 151.8964 - val_loss: 121.2055\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 149.6732 - val_loss: 116.7755\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 152.9363 - val_loss: 171.0912\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 164.5582 - val_loss: 127.4335\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 165.4868 - val_loss: 155.2661\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 159.5368 - val_loss: 121.5858\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 159.3161 - val_loss: 189.6334\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 291.9632 - val_loss: 135.7566\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 243.6722 - val_loss: 133.8467\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 192.7993 - val_loss: 138.9686\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 173.3974 - val_loss: 130.4026\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 197.4969 - val_loss: 171.1797\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 301.4100 - val_loss: 159.3288\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 187.1861 - val_loss: 124.0239\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 168.8054 - val_loss: 160.4368\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 208.8166 - val_loss: 121.0931\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 172.9841 - val_loss: 119.0284\n",
      "Epoch 648/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 64us/step - loss: 151.1212 - val_loss: 159.0340\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 158.4908 - val_loss: 167.2891\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 174.3883 - val_loss: 223.1393\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 220.2378 - val_loss: 136.3225\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 198.9146 - val_loss: 145.4504\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 155.6294 - val_loss: 125.4954\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 170.0468 - val_loss: 119.7925\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 164.0146 - val_loss: 164.4851\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 463.0199 - val_loss: 124.2377\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 199.5506 - val_loss: 126.4602\n",
      "Epoch 658/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 173.3387 - val_loss: 126.8962\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 177.0911 - val_loss: 150.4075\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 158.2514 - val_loss: 160.8784\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 152.9050 - val_loss: 127.0692\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 187.7442 - val_loss: 120.6880\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 168.9368 - val_loss: 121.7570\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 158.6630 - val_loss: 191.6083\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 154.8941 - val_loss: 135.3655\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 168.9217 - val_loss: 128.3776\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 296.6874 - val_loss: 130.6947\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 150.2440 - val_loss: 119.4309\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 153.8731 - val_loss: 131.4495\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 146.0929 - val_loss: 160.4476\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 304.2062 - val_loss: 161.1045\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 203.5264 - val_loss: 185.5253\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 167.1577 - val_loss: 139.4892\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 244.2190 - val_loss: 238.2201\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.7191 - val_loss: 140.4397\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 157.2949 - val_loss: 139.4509\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 161.469 - 1s 64us/step - loss: 161.5514 - val_loss: 125.8590\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 257.7702 - val_loss: 140.1677\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 154.1948 - val_loss: 119.0514\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 151.5063 - val_loss: 124.6520\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 172.6811 - val_loss: 183.9217\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 161.1864 - val_loss: 126.9726\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 152.9168 - val_loss: 156.1299\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 170.0241 - val_loss: 125.9427\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 216.7006 - val_loss: 130.4879\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 173.4650 - val_loss: 133.4418\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 153.7937 - val_loss: 159.9277\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 159.8520 - val_loss: 125.0248\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 190.9696 - val_loss: 131.8263\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 310.5920 - val_loss: 120.4498\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 154.4937 - val_loss: 120.4246\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 155.0976 - val_loss: 154.1387\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 161.4020 - val_loss: 134.6876\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 162.3653 - val_loss: 144.2783\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 159.3510 - val_loss: 159.2031\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.8567 - val_loss: 138.2571\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 582.0789 - val_loss: 199.9226\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 276.7124 - val_loss: 156.2807\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 226.2559 - val_loss: 146.4130\n",
      "Epoch 700/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 202.5987 - val_loss: 191.5477\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 210.8280 - val_loss: 132.2325\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 205.0390 - val_loss: 181.4065\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 186.5115 - val_loss: 171.8675\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 189.7715 - val_loss: 129.6840\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 190.5614 - val_loss: 145.5572\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 214.5090 - val_loss: 158.0032\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 180.1295 - val_loss: 125.1250\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 175.5764 - val_loss: 157.7610\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 175.3420 - val_loss: 152.8610\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 176.1329 - val_loss: 178.9329\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 179.6128 - val_loss: 186.9324\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 178.5094 - val_loss: 213.0069\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 207.0782 - val_loss: 205.0173\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 170.1363 - val_loss: 120.5820\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 189.9752 - val_loss: 125.8690\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 164.8898 - val_loss: 119.9166\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 199.9979 - val_loss: 153.3911\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 174.6497 - val_loss: 149.1087\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 170.3711 - val_loss: 203.4524\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 171.7324 - val_loss: 131.7136\n",
      "Epoch 721/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 65us/step - loss: 165.6162 - val_loss: 117.3019\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 160.6248 - val_loss: 235.7372\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 235.5075 - val_loss: 139.5163\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 160.1784 - val_loss: 134.8387\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 190.0359 - val_loss: 229.7760\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 161.8238 - val_loss: 158.2903\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 164.0004 - val_loss: 132.2819\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 169.2183 - val_loss: 176.0002\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 163.7831 - val_loss: 143.5912\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 186.3819 - val_loss: 151.2987\n",
      "Epoch 731/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 167.4932 - val_loss: 131.0897\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 194.8777 - val_loss: 130.0555\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 167.4743 - val_loss: 125.4374\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 184.5992 - val_loss: 130.2482\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 167.6431 - val_loss: 126.1461\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 170.5082 - val_loss: 186.6786\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 166.0825 - val_loss: 118.4186\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 172.4839 - val_loss: 285.6381\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 174.4582 - val_loss: 146.0866\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 173.3715 - val_loss: 120.6903\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 201.9921 - val_loss: 243.7403\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 164.1968 - val_loss: 122.0858\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 200.9927 - val_loss: 128.6418\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 161.8560 - val_loss: 143.8922\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 162.4891 - val_loss: 223.9990\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 181.6768 - val_loss: 124.3544\n",
      "Epoch 747/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 163.4897 - val_loss: 134.8722\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 288.8393 - val_loss: 295.5748\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 577.9737 - val_loss: 190.7406\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 347.7439 - val_loss: 200.2158\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 235.1638 - val_loss: 365.2156\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 224.5085 - val_loss: 172.8179\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 228.1921 - val_loss: 229.1756\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 231.2410 - val_loss: 179.1576\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 244.9125 - val_loss: 167.6357\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 215.7147 - val_loss: 257.7748\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 199.9235 - val_loss: 130.3328\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 183.1693 - val_loss: 145.6453\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 229.5284 - val_loss: 188.2250\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 180.4655 - val_loss: 150.0006\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 181.7225 - val_loss: 131.2037\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 217.3363 - val_loss: 229.2817\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 213.2800 - val_loss: 131.3114\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 188.9914 - val_loss: 197.5737\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 218.5622 - val_loss: 140.0249\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 172.8781 - val_loss: 141.8495\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 169.4791 - val_loss: 193.3504\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 187.2129 - val_loss: 183.5067\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 331.8335 - val_loss: 139.4291\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 199.8616 - val_loss: 137.7439\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 173.9745 - val_loss: 131.2870\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 184.8099 - val_loss: 153.2799\n",
      "Epoch 773/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 177.1182 - val_loss: 136.7427\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 228.6398 - val_loss: 191.5790\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 201.0346 - val_loss: 253.1192\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 222.3690 - val_loss: 134.4628\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 294.1875 - val_loss: 131.7206\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.5164 - val_loss: 277.3754\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 194.4073 - val_loss: 133.3606\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 192.7991 - val_loss: 166.6517\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 266.1247 - val_loss: 172.5428\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 176.7836 - val_loss: 144.7390\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 168.4706 - val_loss: 141.0246\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 167.2119 - val_loss: 124.3717\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 165.3877 - val_loss: 124.0646\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 184.3303 - val_loss: 173.0043\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 196.9587 - val_loss: 528.9876\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 304.1054 - val_loss: 125.2514\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 165.2740 - val_loss: 125.3692\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 179.2047 - val_loss: 121.9502\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 178.1818 - val_loss: 125.0253\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 163.2766 - val_loss: 160.2083\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 172.4608 - val_loss: 123.1474\n",
      "Epoch 794/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 66us/step - loss: 270.6554 - val_loss: 191.7719\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 204.0471 - val_loss: 138.7971\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 185.1851 - val_loss: 126.9524\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 177.4994 - val_loss: 128.2202\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 185.4879 - val_loss: 165.0543\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 212.2657 - val_loss: 124.1711\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 169.5033 - val_loss: 172.9473\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 184.8244 - val_loss: 142.6810\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 166.3032 - val_loss: 129.4669\n",
      "Epoch 803/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 150.5513 - val_loss: 137.9615\n",
      "Epoch 804/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 338.5132 - val_loss: 133.0691\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 170.1825 - val_loss: 138.3961\n",
      "Epoch 806/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 158.2595 - val_loss: 123.0763\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 212.1982 - val_loss: 133.5347\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 171.9452 - val_loss: 122.4654\n",
      "Epoch 809/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 201.7514 - val_loss: 151.3816\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 158.1667 - val_loss: 120.8597\n",
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 171.0483 - val_loss: 130.0219\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 161.3797 - val_loss: 129.1290\n",
      "Epoch 813/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 171.9300 - val_loss: 127.6409\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 164.1110 - val_loss: 133.3990\n",
      "Epoch 815/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 173.6241 - val_loss: 188.1011\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 200.3548 - val_loss: 134.1609\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 207.1139 - val_loss: 364.2152\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 199.3355 - val_loss: 157.6472\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.9823 - val_loss: 193.9688\n",
      "Epoch 820/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 162.5938 - val_loss: 140.7921\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 163.7413 - val_loss: 132.9685\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 159.9849 - val_loss: 137.6350\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.4016 - val_loss: 146.1286\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 176.3212 - val_loss: 129.0663\n",
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 159.2719 - val_loss: 122.7459\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 163.8581 - val_loss: 130.3159\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 220.9393 - val_loss: 151.2891\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 163.7627 - val_loss: 126.1972\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 186.2553 - val_loss: 234.7754\n",
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 166.5297 - val_loss: 127.2349\n",
      "Epoch 831/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 157.8616 - val_loss: 126.9496\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 183.3418 - val_loss: 192.9081\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 230.6896 - val_loss: 189.4653\n",
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 161.3336 - val_loss: 127.7192\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 169.2846 - val_loss: 130.4462\n",
      "Epoch 836/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 145.0176 - val_loss: 124.1376\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 175.3887 - val_loss: 120.9668\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 156.1966 - val_loss: 122.3292\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 207.0216 - val_loss: 382.5137\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 155.3808 - val_loss: 131.6398\n",
      "Epoch 841/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 183.2226 - val_loss: 325.8970\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 184.4734 - val_loss: 120.7806\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 175.3404 - val_loss: 124.0919\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 265.5845 - val_loss: 144.5057\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 181.7179 - val_loss: 176.4274\n",
      "Epoch 846/10000\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 148.1683 - val_loss: 163.4568\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 164.2083 - val_loss: 122.8872\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 155.9998 - val_loss: 152.9703\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 168.6143 - val_loss: 147.4508\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 197.5610 - val_loss: 144.3534\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 152.2387 - val_loss: 122.9819\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 167.3272 - val_loss: 136.9308\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 155.8585 - val_loss: 374.2138\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 173.1607 - val_loss: 146.6629\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 227.0086 - val_loss: 188.5686\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 175.0207 - val_loss: 128.5157\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 145.7559 - val_loss: 128.9729\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 147.8169 - val_loss: 133.4593\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 159.2874 - val_loss: 121.1053\n",
      "Epoch 860/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 143.8558 - val_loss: 142.4382\n",
      "Epoch 861/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 298.0683 - val_loss: 726.7776\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 175.0556 - val_loss: 124.1159\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 152.1731 - val_loss: 117.5715\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 148.0933 - val_loss: 128.8106\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 160.9763 - val_loss: 119.6195\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 154.8696 - val_loss: 129.4544\n",
      "Epoch 867/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 79us/step - loss: 222.0923 - val_loss: 121.2709\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 145.6156 - val_loss: 184.0411\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 331.8274 - val_loss: 143.9539\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 158.1815 - val_loss: 243.3734\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 167.6251 - val_loss: 130.5619\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 225.0902 - val_loss: 2177.4787\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 593.9136 - val_loss: 249.3442\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 304.2339 - val_loss: 195.6114\n",
      "Epoch 875/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 218.6258 - val_loss: 155.5935\n",
      "Epoch 876/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 275.6993 - val_loss: 147.8873\n",
      "Epoch 877/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 196.4652 - val_loss: 147.6754\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 193.5191 - val_loss: 147.5144\n",
      "Epoch 879/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 200.4714 - val_loss: 157.0378\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 179.0611 - val_loss: 299.3230\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 183.0035 - val_loss: 137.3951\n",
      "Epoch 882/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 175.2807 - val_loss: 194.7700\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 185.0279 - val_loss: 129.5666\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 193.0429 - val_loss: 135.8571\n",
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 167.2906 - val_loss: 157.0477\n",
      "Epoch 886/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 199.5766 - val_loss: 178.6189\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 197.9288 - val_loss: 136.7330\n",
      "Epoch 888/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 207.6186 - val_loss: 141.3667\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 182.8308 - val_loss: 137.9910\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 203.3925 - val_loss: 136.2945\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 384.6335 - val_loss: 268.9225\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 191.0031 - val_loss: 133.7756\n",
      "Epoch 893/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 165.7471 - val_loss: 153.8194\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 161.0986 - val_loss: 137.5974\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.0068 - val_loss: 128.1583\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 165.7348 - val_loss: 178.1957\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 164.6098 - val_loss: 140.9742\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 174.9150 - val_loss: 135.6836\n",
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 167.6060 - val_loss: 156.2462\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 162.2573 - val_loss: 139.3250\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 189.1074 - val_loss: 134.7077\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 185.4333 - val_loss: 168.4492\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 162.6971 - val_loss: 141.7343\n",
      "Epoch 904/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 170.3164 - val_loss: 137.4863\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 191.5597 - val_loss: 160.8145\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 182.3880 - val_loss: 155.7549\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 170.9913 - val_loss: 246.4065\n",
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 187.7137 - val_loss: 134.4477\n",
      "Epoch 909/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 180.9062 - val_loss: 130.8590\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 164.3233 - val_loss: 137.6021\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 204.4024 - val_loss: 152.0211\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 172.3507 - val_loss: 124.7451\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 156.8930 - val_loss: 149.4225\n",
      "Epoch 914/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 171.7777 - val_loss: 129.8919\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 182.0966 - val_loss: 152.4914\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 264.3522 - val_loss: 134.7392\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 175.8009 - val_loss: 139.3071\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 217.4485 - val_loss: 269.9801\n",
      "Epoch 919/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 191.4613 - val_loss: 144.9486\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 162.4935 - val_loss: 124.5606\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 169.3930 - val_loss: 118.2097\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 171.0538 - val_loss: 142.2968\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 249.7801 - val_loss: 141.2713\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 171.7760 - val_loss: 122.4081\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 175.5987 - val_loss: 127.6228\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 217.7241 - val_loss: 312.8276\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 232.2372 - val_loss: 188.1494\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 161.5616 - val_loss: 167.1296\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 160.9380 - val_loss: 127.2979\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 189.4503 - val_loss: 168.5506\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 206.1104 - val_loss: 173.3240\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 160.8985 - val_loss: 134.3805\n",
      "Epoch 933/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 161.3719 - val_loss: 194.1827\n",
      "Epoch 934/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 158.6731 - val_loss: 119.5124\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 171.8246 - val_loss: 153.6494\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 158.8864 - val_loss: 147.6848\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 169.9093 - val_loss: 121.2124\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 166.6073 - val_loss: 139.3701\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 153.2421 - val_loss: 141.9671\n",
      "Epoch 940/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 159.0750 - val_loss: 302.6174\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 187.4455 - val_loss: 151.8761\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 171.0688 - val_loss: 143.4094\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 164.6357 - val_loss: 123.1774\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 154.5097 - val_loss: 122.3608\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 184.7558 - val_loss: 161.6797\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 238.4584 - val_loss: 126.8293\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 155.3118 - val_loss: 124.7580\n",
      "Epoch 948/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.2526 - val_loss: 147.2863\n",
      "Epoch 949/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 165.7004 - val_loss: 148.1693\n",
      "Epoch 950/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 187.2779 - val_loss: 153.5319\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 154.8271 - val_loss: 134.4398\n",
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 154.5163 - val_loss: 123.4571\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 181.7878 - val_loss: 125.7739\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 183.9341 - val_loss: 1668.5743\n",
      "Epoch 955/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 181.9179 - val_loss: 133.0541\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 148.3815 - val_loss: 128.9054\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 150.4798 - val_loss: 119.1779\n",
      "Epoch 958/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 158.6724 - val_loss: 163.3482\n",
      "Epoch 959/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 170.3200 - val_loss: 129.0603\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 157.0105 - val_loss: 136.4009\n",
      "Epoch 961/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 179.9192 - val_loss: 130.0016\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 163.7471 - val_loss: 231.3174\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 169.3096 - val_loss: 126.1105\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 164.2459 - val_loss: 135.2452\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 163.3605 - val_loss: 124.7785\n",
      "Epoch 966/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 159.3569 - val_loss: 123.3682\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 175.6441 - val_loss: 129.4952\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 156.5295 - val_loss: 148.9311\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 173.3115 - val_loss: 152.8227\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 150.4390 - val_loss: 142.6095\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 162.2113 - val_loss: 175.2609\n",
      "Epoch 972/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 176.1866 - val_loss: 143.3487\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 225.4256 - val_loss: 149.0984\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 150.3437 - val_loss: 124.4648\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 161.3339 - val_loss: 218.8292\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 176.8559 - val_loss: 130.4872\n",
      "Epoch 977/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 211.8974 - val_loss: 132.8569\n",
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 149.9098 - val_loss: 121.3280\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 217.1337 - val_loss: 138.7964\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 164.5321 - val_loss: 153.1691\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 151.7741 - val_loss: 131.8816\n",
      "Epoch 982/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 156.2199 - val_loss: 123.6675\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 153.0428 - val_loss: 127.8207\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 155.8901 - val_loss: 182.9519\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 153.3910 - val_loss: 123.3384\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 158.8755 - val_loss: 131.3712\n",
      "Epoch 987/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 157.0066 - val_loss: 143.4032\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.0908 - val_loss: 119.5555\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 188.0562 - val_loss: 267.4431\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 318.1596 - val_loss: 157.1204\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 182.2475 - val_loss: 537.7686\n",
      "Epoch 992/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 214.2100 - val_loss: 165.1685\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 193.6038 - val_loss: 264.2417\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 190.9553 - val_loss: 143.4205\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 194.2627 - val_loss: 122.3043\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 157.6976 - val_loss: 136.5988\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 156.5537 - val_loss: 133.5358\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 171.4392 - val_loss: 118.5750\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 211.4999 - val_loss: 140.3062\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.7937 - val_loss: 137.1757\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 155.6244 - val_loss: 123.5423\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 170.9427 - val_loss: 164.6774\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 191.4394 - val_loss: 125.2917\n",
      "Epoch 1004/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 151.6768 - val_loss: 126.6686\n",
      "Epoch 1005/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 307.1223 - val_loss: 139.8057\n",
      "Epoch 1006/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 192.7623 - val_loss: 133.9531\n",
      "Epoch 1007/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 180.0083 - val_loss: 158.7383\n",
      "Epoch 1008/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 175.2230 - val_loss: 129.3605\n",
      "Epoch 1009/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 158.5893 - val_loss: 142.1859\n",
      "Epoch 1010/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 161.6063 - val_loss: 138.5116\n",
      "Epoch 1011/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 150.0433 - val_loss: 120.9325\n",
      "Epoch 1012/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 189.0131 - val_loss: 136.5347\n",
      "Epoch 1013/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 65us/step - loss: 422.1993 - val_loss: 185.0111\n",
      "Epoch 1014/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 189.5399 - val_loss: 133.2742\n",
      "Epoch 1015/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 168.1235 - val_loss: 170.7467\n",
      "Epoch 1016/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 159.9301 - val_loss: 124.3646\n",
      "Epoch 1017/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 151.0542 - val_loss: 189.0779\n",
      "Epoch 1018/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.9212 - val_loss: 247.3402\n",
      "Epoch 1019/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 167.6232 - val_loss: 227.9242\n",
      "Epoch 1020/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 166.7668 - val_loss: 122.4275\n",
      "Epoch 1021/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 167.2164 - val_loss: 122.5044\n",
      "Epoch 1022/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 169.6469 - val_loss: 138.3604\n",
      "Epoch 1023/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 147.2588 - val_loss: 129.6895\n",
      "Epoch 1024/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 160.6037 - val_loss: 200.8155\n",
      "Epoch 1025/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.8994 - val_loss: 132.7159\n",
      "Epoch 1026/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 158.8361 - val_loss: 119.3553\n",
      "Epoch 1027/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.8949 - val_loss: 129.8260\n",
      "Epoch 1028/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 232.6084 - val_loss: 117.0174\n",
      "Epoch 1029/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.6278 - val_loss: 151.4514\n",
      "Epoch 1030/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 147.6292 - val_loss: 128.4730\n",
      "Epoch 1031/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 148.1660 - val_loss: 128.5090\n",
      "Epoch 1032/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 155.5813 - val_loss: 126.5538\n",
      "Epoch 1033/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.4230 - val_loss: 130.6191\n",
      "Epoch 1034/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 171.7227 - val_loss: 119.2366\n",
      "Epoch 1035/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 158.1054 - val_loss: 132.8764\n",
      "Epoch 1036/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 154.5264 - val_loss: 148.5528\n",
      "Epoch 1037/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 306.3473 - val_loss: 156.9259\n",
      "Epoch 1038/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 159.0303 - val_loss: 128.5281\n",
      "Epoch 1039/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.3391 - val_loss: 124.2336\n",
      "Epoch 1040/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 167.0739 - val_loss: 123.8787\n",
      "Epoch 1041/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 178.0442 - val_loss: 133.4658\n",
      "Epoch 1042/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 247.9486 - val_loss: 130.8282\n",
      "Epoch 1043/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 160.5518 - val_loss: 131.1622\n",
      "Epoch 1044/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 149.5343 - val_loss: 145.4441\n",
      "Epoch 1045/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 145.4947 - val_loss: 131.2452\n",
      "Epoch 1046/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 188.8525 - val_loss: 127.7507\n",
      "Epoch 1047/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 166.8318 - val_loss: 155.4088\n",
      "Epoch 1048/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 151.7551 - val_loss: 129.3352\n",
      "Epoch 1049/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 164.8121 - val_loss: 127.7396\n",
      "Epoch 1050/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 158.7037 - val_loss: 120.8417\n",
      "Epoch 1051/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 163.7444 - val_loss: 143.5899\n",
      "Epoch 1052/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 146.8199 - val_loss: 118.7868\n",
      "Epoch 1053/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 153.9503 - val_loss: 116.8591\n",
      "Epoch 1054/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 276.5504 - val_loss: 187.4133\n",
      "Epoch 1055/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 151.3928 - val_loss: 120.9549\n",
      "Epoch 1056/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 172.6161 - val_loss: 143.0974\n",
      "Epoch 1057/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 155.9166 - val_loss: 121.8834\n",
      "Epoch 1058/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.2338 - val_loss: 118.6854\n",
      "Epoch 1059/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 494.8072 - val_loss: 482.1899\n",
      "Epoch 1060/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 314.3162 - val_loss: 186.2046\n",
      "Epoch 1061/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 187.5256 - val_loss: 142.6914\n",
      "Epoch 1062/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 172.7416 - val_loss: 148.6519\n",
      "Epoch 1063/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 182.3365 - val_loss: 199.8712\n",
      "Epoch 1064/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 193.6561 - val_loss: 144.8669\n",
      "Epoch 1065/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 165.9889 - val_loss: 150.1276\n",
      "Epoch 1066/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 167.6764 - val_loss: 176.4603\n",
      "Epoch 1067/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 148.0184 - val_loss: 138.3896\n",
      "Epoch 1068/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.0001 - val_loss: 173.5230\n",
      "Epoch 1069/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 148.4814 - val_loss: 124.0229\n",
      "Epoch 1070/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 188.2707 - val_loss: 131.3169\n",
      "Epoch 1071/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 156.7504 - val_loss: 149.5549\n",
      "Epoch 1072/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 232.3123 - val_loss: 160.7852\n",
      "Epoch 1073/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 154.0614 - val_loss: 132.9703\n",
      "Epoch 1074/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 151.8223 - val_loss: 346.4766\n",
      "Epoch 1075/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 190.4343 - val_loss: 126.3641\n",
      "Epoch 1076/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 163.3870 - val_loss: 159.7280\n",
      "Epoch 1077/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 241.9618 - val_loss: 134.7405\n",
      "Epoch 1078/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 149.1097 - val_loss: 144.2252\n",
      "Epoch 1079/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 143.7940 - val_loss: 122.9426\n",
      "Epoch 1080/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 148.9597 - val_loss: 135.6864\n",
      "Epoch 1081/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 148.1361 - val_loss: 227.5476\n",
      "Epoch 1082/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 216.1221 - val_loss: 131.7637\n",
      "Epoch 1083/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 166.8299 - val_loss: 123.2084\n",
      "Epoch 1084/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.6469 - val_loss: 122.5159\n",
      "Epoch 1085/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 71us/step - loss: 151.0289 - val_loss: 165.5696\n",
      "Epoch 1086/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 153.4634 - val_loss: 129.2120\n",
      "Epoch 1087/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 150.2750 - val_loss: 117.8436\n",
      "Epoch 1088/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 149.8057 - val_loss: 198.0706\n",
      "Epoch 1089/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 564.2083 - val_loss: 146.2108\n",
      "Epoch 1090/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.3996 - val_loss: 138.8708\n",
      "Epoch 1091/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 149.0844 - val_loss: 136.3236\n",
      "Epoch 1092/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 140.3234 - val_loss: 115.2605\n",
      "Epoch 1093/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.5196 - val_loss: 126.7277\n",
      "Epoch 1094/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.0965 - val_loss: 126.8878\n",
      "Epoch 1095/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 150.6298 - val_loss: 133.7167\n",
      "Epoch 1096/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 139.5127 - val_loss: 142.2492\n",
      "Epoch 1097/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 167.9928 - val_loss: 165.5057\n",
      "Epoch 1098/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 195.7052 - val_loss: 120.2636\n",
      "Epoch 1099/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.7028 - val_loss: 120.5851\n",
      "Epoch 1100/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.5668 - val_loss: 120.1623\n",
      "Epoch 1101/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.7469 - val_loss: 127.5062\n",
      "Epoch 1102/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 147.9153 - val_loss: 131.9544\n",
      "Epoch 1103/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 153.2173 - val_loss: 138.6811\n",
      "Epoch 1104/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 156.1590 - val_loss: 135.8963\n",
      "Epoch 1105/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 289.3062 - val_loss: 149.8463\n",
      "Epoch 1106/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 160.6761 - val_loss: 163.4825\n",
      "Epoch 1107/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 157.0363 - val_loss: 136.2883\n",
      "Epoch 1108/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 151.9463 - val_loss: 138.2782\n",
      "Epoch 1109/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 155.8480 - val_loss: 122.0870\n",
      "Epoch 1110/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 147.3686 - val_loss: 117.2101\n",
      "Epoch 1111/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 155.8739 - val_loss: 130.9043\n",
      "Epoch 1112/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 157.3837 - val_loss: 137.0809\n",
      "Epoch 1113/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 153.5280 - val_loss: 229.2031\n",
      "Epoch 1114/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 158.1836 - val_loss: 213.6472\n",
      "Epoch 1115/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 183.0232 - val_loss: 194.2015\n",
      "Epoch 1116/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 151.9855 - val_loss: 137.3069\n",
      "Epoch 1117/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 147.2088 - val_loss: 118.9715\n",
      "Epoch 1118/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 184.8397 - val_loss: 126.4168\n",
      "Epoch 1119/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 165.5276 - val_loss: 134.0453\n",
      "Epoch 1120/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 152.1411 - val_loss: 123.4333\n",
      "Epoch 1121/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 156.5473 - val_loss: 166.3142\n",
      "Epoch 1122/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 142.3973 - val_loss: 123.3639\n",
      "Epoch 1123/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 148.7677 - val_loss: 126.6380\n",
      "Epoch 1124/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 149.8559 - val_loss: 145.6128\n",
      "Epoch 1125/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 195.0597 - val_loss: 121.1416\n",
      "Epoch 1126/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 153.5990 - val_loss: 119.6655\n",
      "Epoch 1127/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 153.2203 - val_loss: 125.1425\n",
      "Epoch 1128/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 150.3923 - val_loss: 158.1632\n",
      "Epoch 1129/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 237.6992 - val_loss: 142.9055\n",
      "Epoch 1130/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 146.8897 - val_loss: 277.4676\n",
      "Epoch 1131/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 215.1927 - val_loss: 122.9704\n",
      "Epoch 1132/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 143.8074 - val_loss: 160.0239\n",
      "Epoch 1133/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 147.5226 - val_loss: 132.0412\n",
      "Epoch 1134/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 153.0490 - val_loss: 151.7056\n",
      "Epoch 1135/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 161.4488 - val_loss: 144.3499\n",
      "Epoch 1136/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 171.4649 - val_loss: 160.3795\n",
      "Epoch 1137/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 149.1289 - val_loss: 199.6779\n",
      "Epoch 1138/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 148.7927 - val_loss: 132.5178\n",
      "Epoch 1139/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 156.3528 - val_loss: 132.8440\n",
      "Epoch 1140/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.4254 - val_loss: 120.4343\n",
      "Epoch 1141/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 206.6018 - val_loss: 137.7281\n",
      "Epoch 1142/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 152.3418 - val_loss: 130.4778\n",
      "Epoch 1143/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 273.3341 - val_loss: 123.4344\n",
      "Epoch 1144/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 147.9148 - val_loss: 172.2019\n",
      "Epoch 1145/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 162.4241 - val_loss: 126.8968\n",
      "Epoch 1146/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 154.8169 - val_loss: 125.3408\n",
      "Epoch 1147/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 144.8120 - val_loss: 119.5017\n",
      "Epoch 1148/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 147.6092 - val_loss: 119.6665\n",
      "Epoch 1149/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 145.5198 - val_loss: 119.2518\n",
      "Epoch 1150/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 145.4028 - val_loss: 250.8426\n",
      "Epoch 1151/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 149.1756 - val_loss: 124.6982\n",
      "Epoch 1152/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 175.4377 - val_loss: 128.9563\n",
      "Epoch 1153/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 149.9073 - val_loss: 124.9007\n",
      "Epoch 1154/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.1324 - val_loss: 123.3236\n",
      "Epoch 1155/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.7409 - val_loss: 159.5950\n",
      "Epoch 1156/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 158.6964 - val_loss: 156.2335\n",
      "Epoch 1157/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 70us/step - loss: 152.7023 - val_loss: 155.0880\n",
      "Epoch 1158/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 172.8304 - val_loss: 135.7785\n",
      "Epoch 1159/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.1459 - val_loss: 138.5408\n",
      "Epoch 1160/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 271.8856 - val_loss: 135.2865\n",
      "Epoch 1161/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 182.2082 - val_loss: 131.2856\n",
      "Epoch 1162/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 158.7894 - val_loss: 128.5345\n",
      "Epoch 1163/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 201.0555 - val_loss: 183.0086\n",
      "Epoch 1164/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 161.4879 - val_loss: 145.1129\n",
      "Epoch 1165/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 144.7080 - val_loss: 126.4385\n",
      "Epoch 1166/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 195.4555 - val_loss: 133.1060\n",
      "Epoch 1167/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 197.4336 - val_loss: 133.5368\n",
      "Epoch 1168/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 162.1744 - val_loss: 152.9787\n",
      "Epoch 1169/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.7488 - val_loss: 141.2042\n",
      "Epoch 1170/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 148.1629 - val_loss: 130.5907\n",
      "Epoch 1171/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 212.8637 - val_loss: 131.0632\n",
      "Epoch 1172/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.7966 - val_loss: 152.3977\n",
      "Epoch 1173/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 143.1244 - val_loss: 154.9574\n",
      "Epoch 1174/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 148.7917 - val_loss: 128.2442\n",
      "Epoch 1175/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 155.8005 - val_loss: 119.3500\n",
      "Epoch 1176/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.1577 - val_loss: 121.1812\n",
      "Epoch 1177/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 155.3690 - val_loss: 132.1191\n",
      "Epoch 1178/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 177.8110 - val_loss: 130.4596\n",
      "Epoch 1179/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.0494 - val_loss: 121.3142\n",
      "Epoch 1180/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 171.6194 - val_loss: 129.9380\n",
      "Epoch 1181/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 235.1149 - val_loss: 133.6893\n",
      "Epoch 1182/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 140.8552 - val_loss: 124.7008\n",
      "Epoch 1183/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 152.9590 - val_loss: 123.7761\n",
      "Epoch 1184/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 156.5135 - val_loss: 132.5352\n",
      "Epoch 1185/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 160.7935 - val_loss: 135.3334\n",
      "Epoch 1186/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 155.7113 - val_loss: 119.1108\n",
      "Epoch 1187/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 153.3679 - val_loss: 121.9658\n",
      "Epoch 1188/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 158.7285 - val_loss: 122.1372\n",
      "Epoch 1189/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 154.6066 - val_loss: 124.9002\n",
      "Epoch 1190/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 154.2684 - val_loss: 140.5861\n",
      "Epoch 1191/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 155.7942 - val_loss: 126.5221\n",
      "Epoch 1192/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 157.5664 - val_loss: 129.3232\n",
      "Epoch 1193/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 159.4087 - val_loss: 137.5059\n",
      "Epoch 1194/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 158.3863 - val_loss: 149.6081\n",
      "Epoch 1195/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 170.3024 - val_loss: 155.7855\n",
      "Epoch 1196/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 146.6471 - val_loss: 116.9702\n",
      "Epoch 1197/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 156.6638 - val_loss: 155.4460\n",
      "Epoch 1198/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 178.5067 - val_loss: 139.9693\n",
      "Epoch 1199/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 170.0851 - val_loss: 136.9924\n",
      "Epoch 1200/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 386.6282 - val_loss: 118.9157\n",
      "Epoch 1201/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.4088 - val_loss: 126.5323\n",
      "Epoch 1202/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 142.4034 - val_loss: 119.4214\n",
      "Epoch 1203/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.5874 - val_loss: 129.4968\n",
      "Epoch 1204/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 146.3826 - val_loss: 145.0473\n",
      "Epoch 1205/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 148.7433 - val_loss: 137.8585\n",
      "Epoch 1206/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.6395 - val_loss: 132.6431\n",
      "Epoch 1207/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 155.9913 - val_loss: 133.5165\n",
      "Epoch 1208/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 163.7952 - val_loss: 181.6816\n",
      "Epoch 1209/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 165.6864 - val_loss: 124.5428\n",
      "Epoch 1210/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 158.1956 - val_loss: 146.3602\n",
      "Epoch 1211/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 160.8393 - val_loss: 140.8091\n",
      "Epoch 1212/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 149.4771 - val_loss: 126.0750\n",
      "Epoch 1213/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 144.7756 - val_loss: 145.3704\n",
      "Epoch 1214/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 150.8528 - val_loss: 142.5457\n",
      "Epoch 1215/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 148.6921 - val_loss: 123.9458\n",
      "Epoch 1216/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 153.8854 - val_loss: 126.5859\n",
      "Epoch 1217/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 173.0236 - val_loss: 141.2783\n",
      "Epoch 1218/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 165.4332 - val_loss: 130.4730\n",
      "Epoch 1219/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 166.4549 - val_loss: 119.6189\n",
      "Epoch 1220/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 528.2825 - val_loss: 170.6154\n",
      "Epoch 1221/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 224.7357 - val_loss: 168.4908\n",
      "Epoch 1222/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 198.6554 - val_loss: 164.6464\n",
      "Epoch 1223/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 183.0275 - val_loss: 151.0126\n",
      "Epoch 1224/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 172.5525 - val_loss: 133.0837\n",
      "Epoch 1225/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 273.4058 - val_loss: 331.6740\n",
      "Epoch 1226/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 160.4112 - val_loss: 207.6794\n",
      "Epoch 1227/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 163.0902 - val_loss: 161.9026\n",
      "Epoch 1228/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 183.3344 - val_loss: 125.8297\n",
      "Epoch 1229/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 156.2653 - val_loss: 128.4850\n",
      "Epoch 1230/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 147.9809 - val_loss: 131.5276\n",
      "Epoch 1231/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 156.6622 - val_loss: 125.2050\n",
      "Epoch 1232/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.3259 - val_loss: 126.8689\n",
      "Epoch 1233/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 155.6461 - val_loss: 146.3373\n",
      "Epoch 1234/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 184.5132 - val_loss: 176.6199\n",
      "Epoch 1235/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 158.5937 - val_loss: 130.3870\n",
      "Epoch 1236/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 154.8408 - val_loss: 133.8900\n",
      "Epoch 1237/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 201.0672 - val_loss: 127.4973\n",
      "Epoch 1238/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 174.2609 - val_loss: 127.1932\n",
      "Epoch 1239/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 157.7843 - val_loss: 142.5271\n",
      "Epoch 1240/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 157.4302 - val_loss: 122.5732\n",
      "Epoch 1241/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 167.7582 - val_loss: 176.8270\n",
      "Epoch 1242/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 160.7475 - val_loss: 126.9996\n",
      "Epoch 1243/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 162.9101 - val_loss: 143.2750\n",
      "Epoch 1244/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 160.3365 - val_loss: 122.7551\n",
      "Epoch 1245/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 174.5938 - val_loss: 145.9785\n",
      "Epoch 1246/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 156.2102 - val_loss: 173.3267\n",
      "Epoch 1247/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 162.6941 - val_loss: 127.0993\n",
      "Epoch 1248/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 147.6895 - val_loss: 127.1007\n",
      "Epoch 1249/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 189.7940 - val_loss: 147.9626\n",
      "Epoch 1250/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 152.7471 - val_loss: 133.1794\n",
      "Epoch 1251/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 157.6190 - val_loss: 123.2271\n",
      "Epoch 1252/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 271.2403 - val_loss: 142.4957\n",
      "Epoch 1253/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 152.1844 - val_loss: 120.3922\n",
      "Epoch 1254/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 144.9996 - val_loss: 129.2177\n",
      "Epoch 1255/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 155.4052 - val_loss: 146.2030\n",
      "Epoch 1256/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 184.4349 - val_loss: 158.9659\n",
      "Epoch 1257/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 168.6588 - val_loss: 189.8329\n",
      "Epoch 1258/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 165.4546 - val_loss: 122.7320\n",
      "Epoch 1259/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 163.3877 - val_loss: 139.6871\n",
      "Epoch 1260/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.0315 - val_loss: 257.7191\n",
      "Epoch 1261/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.5988 - val_loss: 195.7447\n",
      "Epoch 1262/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 153.2604 - val_loss: 133.7764\n",
      "Epoch 1263/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 156.1773 - val_loss: 182.5403\n",
      "Epoch 1264/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 277.0764 - val_loss: 131.9738\n",
      "Epoch 1265/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.4013 - val_loss: 171.3194\n",
      "Epoch 1266/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 149.6141 - val_loss: 123.7808\n",
      "Epoch 1267/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 160.3134 - val_loss: 119.4373\n",
      "Epoch 1268/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 142.0941 - val_loss: 126.0107\n",
      "Epoch 1269/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 149.7808 - val_loss: 129.2299\n",
      "Epoch 1270/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 161.0122 - val_loss: 156.3550\n",
      "Epoch 1271/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 151.4941 - val_loss: 132.7975\n",
      "Epoch 1272/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 152.2535 - val_loss: 127.2215\n",
      "Epoch 1273/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 265.9197 - val_loss: 328.6226\n",
      "Epoch 1274/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 204.8070 - val_loss: 132.5280\n",
      "Epoch 1275/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 166.6885 - val_loss: 128.7846\n",
      "Epoch 1276/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.3517 - val_loss: 122.0771\n",
      "Epoch 1277/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 150.0248 - val_loss: 134.2096\n",
      "Epoch 1278/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 151.8852 - val_loss: 146.3740\n",
      "Epoch 1279/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 156.8208 - val_loss: 124.6575\n",
      "Epoch 1280/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 156.3340 - val_loss: 148.5927\n",
      "Epoch 1281/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 150.4393 - val_loss: 134.6480\n",
      "Epoch 1282/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 151.8331 - val_loss: 126.0287\n",
      "Epoch 1283/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 157.9769 - val_loss: 184.3291\n",
      "Epoch 1284/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 157.9184 - val_loss: 118.9152\n",
      "Epoch 1285/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 153.0906 - val_loss: 125.4180\n",
      "Epoch 1286/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 149.3432 - val_loss: 172.9828\n",
      "Epoch 1287/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 149.4714 - val_loss: 145.0889\n",
      "Epoch 1288/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 154.7898 - val_loss: 120.5318\n",
      "Epoch 1289/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 153.2085 - val_loss: 125.6061\n",
      "Epoch 1290/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 186.7528 - val_loss: 133.2850\n",
      "Epoch 1291/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 175.5329 - val_loss: 128.3206\n",
      "Epoch 1292/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 160.9988 - val_loss: 141.0277\n",
      "Epoch 1293/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 143.8021 - val_loss: 125.5979\n",
      "Epoch 1294/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 154.5802 - val_loss: 153.9621\n",
      "Epoch 1295/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 243.8632 - val_loss: 690.6036\n",
      "Epoch 1296/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 289.7477 - val_loss: 121.7672\n",
      "Epoch 1297/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 144.3233 - val_loss: 128.9728\n",
      "Epoch 1298/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 164.0286 - val_loss: 125.0276\n",
      "Epoch 1299/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 154.5007 - val_loss: 124.9863\n",
      "Epoch 1300/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 213.6451 - val_loss: 145.6217\n",
      "Epoch 1301/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 73us/step - loss: 144.8549 - val_loss: 123.3058\n",
      "Epoch 1302/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 154.4339 - val_loss: 287.3096\n",
      "Epoch 1303/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 235.0774 - val_loss: 155.4995\n",
      "Epoch 1304/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 166.0248 - val_loss: 137.2876\n",
      "Epoch 1305/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 158.1665 - val_loss: 144.1679\n",
      "Epoch 1306/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 156.4805 - val_loss: 119.7589\n",
      "Epoch 1307/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 239.4038 - val_loss: 152.9466\n",
      "Epoch 1308/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 169.447 - 1s 70us/step - loss: 168.2228 - val_loss: 128.6628\n",
      "Epoch 1309/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 144.6149 - val_loss: 148.3018\n",
      "Epoch 1310/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 151.4242 - val_loss: 117.9590\n",
      "Epoch 1311/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 152.0905 - val_loss: 133.1623\n",
      "Epoch 1312/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 153.5564 - val_loss: 128.2496\n",
      "Epoch 1313/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 142.0562 - val_loss: 127.4022\n",
      "Epoch 1314/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 154.3607 - val_loss: 134.6883\n",
      "Epoch 1315/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 173.6991 - val_loss: 130.1009\n",
      "Epoch 1316/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 148.6362 - val_loss: 134.4210\n",
      "Epoch 1317/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 156.2723 - val_loss: 176.1273\n",
      "Epoch 1318/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 154.6294 - val_loss: 187.7872\n",
      "Epoch 1319/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 149.3000 - val_loss: 130.3678\n",
      "Epoch 1320/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 169.2520 - val_loss: 122.4063\n",
      "Epoch 1321/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 209.6930 - val_loss: 127.2253\n",
      "Epoch 1322/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 147.3886 - val_loss: 121.4452\n",
      "Epoch 1323/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 144.6634 - val_loss: 127.5619\n",
      "Epoch 1324/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 139.8633 - val_loss: 135.1339\n",
      "Epoch 1325/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 150.0802 - val_loss: 123.9156\n",
      "Epoch 1326/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 146.0633 - val_loss: 118.8572\n",
      "Epoch 1327/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 237.7070 - val_loss: 367.7175\n",
      "Epoch 1328/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 165.4299 - val_loss: 128.1257\n",
      "Epoch 1329/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 144.9263 - val_loss: 122.8293\n",
      "Epoch 1330/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.6490 - val_loss: 120.7394\n",
      "Epoch 1331/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 182.8396 - val_loss: 137.0355\n",
      "Epoch 1332/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 167.8376 - val_loss: 122.6076\n",
      "Epoch 1333/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 154.4206 - val_loss: 139.5321\n",
      "Epoch 1334/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 148.1431 - val_loss: 126.0143\n",
      "Epoch 1335/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 162.3687 - val_loss: 173.1816\n",
      "Epoch 1336/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 149.0939 - val_loss: 159.3316\n",
      "Epoch 1337/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 148.3189 - val_loss: 136.2508\n",
      "Epoch 1338/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 159.2982 - val_loss: 162.8292\n",
      "Epoch 1339/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 149.2935 - val_loss: 156.3695\n",
      "Epoch 1340/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 155.0054 - val_loss: 133.7982\n",
      "Epoch 1341/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 154.7136 - val_loss: 120.2063\n",
      "Epoch 1342/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 152.4037 - val_loss: 136.2274\n",
      "Epoch 1343/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 150.7467 - val_loss: 126.0230\n",
      "Epoch 1344/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 152.4635 - val_loss: 122.5828\n",
      "Epoch 1345/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 146.0432 - val_loss: 119.2626\n",
      "Epoch 1346/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 147.9066 - val_loss: 255.9280\n",
      "Epoch 1347/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 151.9375 - val_loss: 156.6344\n",
      "Epoch 1348/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 147.5565 - val_loss: 137.4586\n",
      "Epoch 1349/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 153.1417 - val_loss: 123.7093\n",
      "Epoch 1350/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 153.0999 - val_loss: 135.8847\n",
      "Epoch 1351/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 153.0902 - val_loss: 123.4528\n",
      "Epoch 1352/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 149.6230 - val_loss: 148.8097\n",
      "Epoch 1353/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 158.5851 - val_loss: 128.6794\n",
      "Epoch 1354/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 206.5312 - val_loss: 155.6770\n",
      "Epoch 1355/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 145.6647 - val_loss: 137.1217\n",
      "Epoch 1356/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 145.9939 - val_loss: 118.7974\n",
      "Epoch 1357/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 145.2111 - val_loss: 172.9863\n",
      "Epoch 1358/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 196.7980 - val_loss: 115.7001\n",
      "Epoch 1359/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 147.2634 - val_loss: 143.6611\n",
      "Epoch 1360/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 148.4286 - val_loss: 145.4681\n",
      "Epoch 1361/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.7815 - val_loss: 161.7478\n",
      "Epoch 1362/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 160.5303 - val_loss: 203.0271\n",
      "Epoch 1363/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 153.2840 - val_loss: 205.8589\n",
      "Epoch 1364/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 159.9823 - val_loss: 140.0417\n",
      "Epoch 1365/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 157.6088 - val_loss: 171.4924\n",
      "Epoch 1366/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 160.7737 - val_loss: 160.7178\n",
      "Epoch 1367/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 166.9235 - val_loss: 167.1272\n",
      "Epoch 1368/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.5440 - val_loss: 144.6112\n",
      "Epoch 1369/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 156.5740 - val_loss: 142.2412\n",
      "Epoch 1370/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 160.5885 - val_loss: 138.5591\n",
      "Epoch 1371/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 153.3304 - val_loss: 142.6682\n",
      "Epoch 1372/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.7432 - val_loss: 121.7653\n",
      "Epoch 1373/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 76us/step - loss: 150.6735 - val_loss: 141.4829\n",
      "Epoch 1374/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 214.9576 - val_loss: 133.0655\n",
      "Epoch 1375/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 150.1267 - val_loss: 136.3675\n",
      "Epoch 1376/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 153.4952 - val_loss: 122.6302\n",
      "Epoch 1377/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 170.1382 - val_loss: 149.5406\n",
      "Epoch 1378/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 170.6897 - val_loss: 124.5520\n",
      "Epoch 1379/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 157.6108 - val_loss: 117.2983\n",
      "Epoch 1380/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 154.0076 - val_loss: 155.3965\n",
      "Epoch 1381/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 159.4826 - val_loss: 121.3175\n",
      "Epoch 1382/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 148.3706 - val_loss: 121.7004\n",
      "Epoch 1383/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 148.5350 - val_loss: 145.6713\n",
      "Epoch 1384/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 156.4825 - val_loss: 126.9837\n",
      "Epoch 1385/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 149.3461 - val_loss: 128.6856\n",
      "Epoch 1386/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 147.5905 - val_loss: 142.5847\n",
      "Epoch 1387/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 152.1295 - val_loss: 125.5137\n",
      "Epoch 1388/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 152.4689 - val_loss: 127.4296\n",
      "Epoch 1389/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 146.2799 - val_loss: 118.7727\n",
      "Epoch 1390/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 149.5071 - val_loss: 116.8872\n",
      "Epoch 1391/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 164.3010 - val_loss: 171.9817\n",
      "Epoch 1392/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 171.1499 - val_loss: 424.8071\n",
      "Epoch 1393/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 229.7505 - val_loss: 119.3509\n",
      "Epoch 1394/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 151.1683 - val_loss: 128.4604\n",
      "Epoch 1395/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 152.1519 - val_loss: 129.4647\n",
      "Epoch 1396/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 148.2645 - val_loss: 178.2118\n",
      "Epoch 1397/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 150.8354 - val_loss: 153.1578\n",
      "Epoch 1398/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.1307 - val_loss: 121.2834\n",
      "Epoch 1399/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 175.8197 - val_loss: 122.8761\n",
      "Epoch 1400/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 160.6421 - val_loss: 137.4344\n",
      "Epoch 1401/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 142.1491 - val_loss: 121.3410\n",
      "Epoch 1402/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 210.5635 - val_loss: 120.2966\n",
      "Epoch 1403/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 158.3986 - val_loss: 136.5350\n",
      "Epoch 1404/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 142.9898 - val_loss: 146.5652\n",
      "Epoch 1405/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 142.2065 - val_loss: 133.6399\n",
      "Epoch 1406/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 146.7943 - val_loss: 157.1765\n",
      "Epoch 1407/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 150.5130 - val_loss: 164.1983\n",
      "Epoch 1408/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 147.8048 - val_loss: 130.0801\n",
      "Epoch 1409/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 166.0650 - val_loss: 118.0315\n",
      "Epoch 1410/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 145.9483 - val_loss: 117.4190\n",
      "Epoch 1411/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.9828 - val_loss: 127.4134\n",
      "Epoch 1412/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 153.1333 - val_loss: 143.2584\n",
      "Epoch 1413/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 357.1096 - val_loss: 172.5432\n",
      "Epoch 1414/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.6147 - val_loss: 150.2190\n",
      "Epoch 1415/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 139.9514 - val_loss: 137.3978\n",
      "Epoch 1416/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 144.3262 - val_loss: 118.2333\n",
      "Epoch 1417/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 159.5422 - val_loss: 122.0771\n",
      "Epoch 1418/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 137.3264 - val_loss: 118.2839\n",
      "Epoch 1419/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 135.7028 - val_loss: 120.3324\n",
      "Epoch 1420/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 139.7301 - val_loss: 141.3013\n",
      "Epoch 1421/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 181.0272 - val_loss: 161.1316\n",
      "Epoch 1422/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 156.2505 - val_loss: 124.1757\n",
      "Epoch 1423/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 143.4281 - val_loss: 128.6081\n",
      "Epoch 1424/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 140.5744 - val_loss: 122.6617\n",
      "Epoch 1425/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 148.5539 - val_loss: 126.7977\n",
      "Epoch 1426/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 160.3454 - val_loss: 193.5328\n",
      "Epoch 1427/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 145.4603 - val_loss: 123.3880\n",
      "Epoch 1428/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 145.4664 - val_loss: 138.0958\n",
      "Epoch 1429/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 159.5657 - val_loss: 123.3252\n",
      "Epoch 1430/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 141.8359 - val_loss: 127.4431\n",
      "Epoch 1431/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 168.6129 - val_loss: 174.5252\n",
      "Epoch 1432/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 160.6430 - val_loss: 121.5175\n",
      "Epoch 1433/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 250.1745 - val_loss: 170.6969\n",
      "Epoch 1434/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 218.3323 - val_loss: 123.9831\n",
      "Epoch 1435/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 217.1020 - val_loss: 146.5251\n",
      "Epoch 1436/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 183.7134 - val_loss: 119.1168\n",
      "Epoch 1437/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 164.7453 - val_loss: 123.6727\n",
      "Epoch 1438/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 151.1351 - val_loss: 118.5838\n",
      "Epoch 1439/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 151.5080 - val_loss: 161.5166\n",
      "Epoch 1440/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 147.7712 - val_loss: 184.4989\n",
      "Epoch 1441/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 144.7989 - val_loss: 121.4120\n",
      "Epoch 1442/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 144.5066 - val_loss: 146.2763\n",
      "Epoch 1443/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 148.9030 - val_loss: 257.5089\n",
      "Epoch 1444/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 156.3839 - val_loss: 153.9244\n",
      "Epoch 1445/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 71us/step - loss: 142.1860 - val_loss: 176.5318\n",
      "Epoch 1446/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 151.8012 - val_loss: 168.0108\n",
      "Epoch 1447/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 155.9367 - val_loss: 118.2593\n",
      "Epoch 1448/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 149.2641 - val_loss: 122.2302\n",
      "Epoch 1449/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.1596 - val_loss: 155.8392\n",
      "Epoch 1450/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 166.4532 - val_loss: 122.4380\n",
      "Epoch 1451/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 156.4251 - val_loss: 162.6761\n",
      "Epoch 1452/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 158.0267 - val_loss: 182.1456\n",
      "Epoch 1453/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 230.7652 - val_loss: 123.1853\n",
      "Epoch 1454/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.5806 - val_loss: 119.2797\n",
      "Epoch 1455/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.5210 - val_loss: 117.4018\n",
      "Epoch 1456/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 142.2110 - val_loss: 124.3395\n",
      "Epoch 1457/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 142.7222 - val_loss: 130.9180\n",
      "Epoch 1458/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.4608 - val_loss: 209.2116\n",
      "Epoch 1459/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 160.3251 - val_loss: 120.1594\n",
      "Epoch 1460/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 165.8065 - val_loss: 150.7317\n",
      "Epoch 1461/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 205.0015 - val_loss: 124.3675\n",
      "Epoch 1462/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.3101 - val_loss: 123.3621\n",
      "Epoch 1463/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 147.1097 - val_loss: 119.7965\n",
      "Epoch 1464/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 147.5901 - val_loss: 136.2845\n",
      "Epoch 1465/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.9415 - val_loss: 128.2453\n",
      "Epoch 1466/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 143.2353 - val_loss: 148.1968\n",
      "Epoch 1467/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 160.2814 - val_loss: 139.5065\n",
      "Epoch 1468/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 149.8111 - val_loss: 124.1528\n",
      "Epoch 1469/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 156.6065 - val_loss: 143.8079\n",
      "Epoch 1470/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 141.3974 - val_loss: 118.5002\n",
      "Epoch 1471/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.7331 - val_loss: 129.1254\n",
      "Epoch 1472/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 159.2448 - val_loss: 126.9542\n",
      "Epoch 1473/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 154.4195 - val_loss: 133.3015\n",
      "Epoch 1474/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 183.9092 - val_loss: 297.6313\n",
      "Epoch 1475/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 228.1638 - val_loss: 172.5459\n",
      "Epoch 1476/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 169.4075 - val_loss: 144.4497\n",
      "Epoch 1477/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 153.9858 - val_loss: 119.8719\n",
      "Epoch 1478/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 145.6765 - val_loss: 122.3704\n",
      "Epoch 1479/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 145.1409 - val_loss: 162.0016\n",
      "Epoch 1480/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 145.5548 - val_loss: 136.6761\n",
      "Epoch 1481/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 161.9712 - val_loss: 118.4009\n",
      "Epoch 1482/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 146.6902 - val_loss: 132.6907\n",
      "Epoch 1483/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 149.7303 - val_loss: 198.5555\n",
      "Epoch 1484/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 180.7621 - val_loss: 121.1718\n",
      "Epoch 1485/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 143.5076 - val_loss: 124.1053\n",
      "Epoch 1486/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 156.0883 - val_loss: 119.9813\n",
      "Epoch 1487/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.5417 - val_loss: 164.3918\n",
      "Epoch 1488/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 145.9251 - val_loss: 137.4692\n",
      "Epoch 1489/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 179.7871 - val_loss: 167.9941\n",
      "Epoch 1490/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 215.2240 - val_loss: 130.4910\n",
      "Epoch 1491/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 145.2541 - val_loss: 185.4316\n",
      "Epoch 1492/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 142.2382 - val_loss: 125.9344\n",
      "Epoch 1493/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 138.7231 - val_loss: 120.1953\n",
      "Epoch 1494/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 159.4153 - val_loss: 157.3379\n",
      "Epoch 1495/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.7181 - val_loss: 147.3859\n",
      "Epoch 1496/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 160.1184 - val_loss: 117.0959\n",
      "Epoch 1497/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 144.2302 - val_loss: 120.5628\n",
      "Epoch 1498/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 163.5869 - val_loss: 149.2912\n",
      "Epoch 1499/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 258.9744 - val_loss: 141.6919\n",
      "Epoch 1500/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 183.3651 - val_loss: 119.5194\n",
      "Epoch 1501/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 146.0298 - val_loss: 130.7770\n",
      "Epoch 1502/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.6437 - val_loss: 139.1889\n",
      "Epoch 1503/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 137.9125 - val_loss: 121.7854\n",
      "Epoch 1504/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 149.7114 - val_loss: 121.6581\n",
      "Epoch 1505/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 154.3599 - val_loss: 134.0830\n",
      "Epoch 1506/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 161.4844 - val_loss: 129.8381\n",
      "Epoch 1507/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 145.3541 - val_loss: 117.7348\n",
      "Epoch 1508/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 199.4438 - val_loss: 125.5603\n",
      "Epoch 1509/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 166.2688 - val_loss: 125.0471\n",
      "Epoch 1510/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 155.2740 - val_loss: 121.7765\n",
      "Epoch 1511/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 157.4160 - val_loss: 142.5892\n",
      "Epoch 1512/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 180.3812 - val_loss: 121.5412\n",
      "Epoch 1513/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.6035 - val_loss: 116.3471\n",
      "Epoch 1514/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 146.6404 - val_loss: 117.8427\n",
      "Epoch 1515/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 162.2490 - val_loss: 193.5703\n",
      "Epoch 1516/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.3093 - val_loss: 132.2325\n",
      "Epoch 1517/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 68us/step - loss: 144.1283 - val_loss: 115.9232\n",
      "Epoch 1518/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 152.4970 - val_loss: 218.7888\n",
      "Epoch 1519/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 155.1230 - val_loss: 125.2828\n",
      "Epoch 1520/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 161.7080 - val_loss: 120.9982\n",
      "Epoch 1521/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 161.0321 - val_loss: 153.4709\n",
      "Epoch 1522/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.0182 - val_loss: 150.6987\n",
      "Epoch 1523/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 142.6550 - val_loss: 122.7629\n",
      "Epoch 1524/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 147.7979 - val_loss: 145.7445\n",
      "Epoch 1525/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 216.4098 - val_loss: 141.4908\n",
      "Epoch 1526/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.7470 - val_loss: 120.2116\n",
      "Epoch 1527/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 158.6554 - val_loss: 120.5196\n",
      "Epoch 1528/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 136.7724 - val_loss: 164.5766\n",
      "Epoch 1529/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 135.6263 - val_loss: 118.5616\n",
      "Epoch 1530/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.5292 - val_loss: 145.6764\n",
      "Epoch 1531/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 146.4444 - val_loss: 180.2664\n",
      "Epoch 1532/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 151.0033 - val_loss: 122.0259\n",
      "Epoch 1533/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.1056 - val_loss: 121.9340\n",
      "Epoch 1534/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 138.0845 - val_loss: 160.6031\n",
      "Epoch 1535/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 150.1505 - val_loss: 118.5529\n",
      "Epoch 1536/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 149.5121 - val_loss: 160.4576\n",
      "Epoch 1537/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 149.4276 - val_loss: 121.9330\n",
      "Epoch 1538/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 157.9040 - val_loss: 126.6933\n",
      "Epoch 1539/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 143.5624 - val_loss: 130.6880\n",
      "Epoch 1540/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 159.6557 - val_loss: 130.2461\n",
      "Epoch 1541/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 151.4471 - val_loss: 127.7524\n",
      "Epoch 1542/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 160.5595 - val_loss: 119.3450\n",
      "Epoch 1543/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 142.2443 - val_loss: 177.2432\n",
      "Epoch 1544/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 148.2594 - val_loss: 200.9393\n",
      "Epoch 1545/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 153.6573 - val_loss: 125.1781\n",
      "Epoch 1546/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 153.4483 - val_loss: 120.2514\n",
      "Epoch 1547/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 157.7953 - val_loss: 223.4384\n",
      "Epoch 1548/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 153.2887 - val_loss: 121.8997\n",
      "Epoch 1549/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 165.5494 - val_loss: 120.2751\n",
      "Epoch 1550/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 152.4443 - val_loss: 128.0134\n",
      "Epoch 1551/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.7736 - val_loss: 161.6015\n",
      "Epoch 1552/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 196.7496 - val_loss: 122.2687\n",
      "Epoch 1553/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 283.0774 - val_loss: 650.5432\n",
      "Epoch 1554/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 172.3199 - val_loss: 120.0385\n",
      "Epoch 1555/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 141.9410 - val_loss: 135.5201\n",
      "Epoch 1556/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 154.6193 - val_loss: 204.3801\n",
      "Epoch 1557/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 177.8175 - val_loss: 127.7476\n",
      "Epoch 1558/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.5774 - val_loss: 117.4847\n",
      "Epoch 1559/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 148.0467 - val_loss: 123.0772\n",
      "Epoch 1560/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 140.2683 - val_loss: 124.9299\n",
      "Epoch 1561/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 140.8748 - val_loss: 119.5836\n",
      "Epoch 1562/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.3658 - val_loss: 128.8446\n",
      "Epoch 1563/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 145.9406 - val_loss: 247.1075\n",
      "Epoch 1564/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 233.8891 - val_loss: 128.4245\n",
      "Epoch 1565/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 174.0082 - val_loss: 133.0669\n",
      "Epoch 1566/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.3976 - val_loss: 122.0868\n",
      "Epoch 1567/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.3798 - val_loss: 122.4474\n",
      "Epoch 1568/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.4889 - val_loss: 125.8146\n",
      "Epoch 1569/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 150.9529 - val_loss: 126.0928\n",
      "Epoch 1570/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 160.1129 - val_loss: 458.9508\n",
      "Epoch 1571/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 236.6408 - val_loss: 139.9454\n",
      "Epoch 1572/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 141.4887 - val_loss: 116.6433\n",
      "Epoch 1573/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.1289 - val_loss: 124.6646\n",
      "Epoch 1574/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 154.2848 - val_loss: 181.4839\n",
      "Epoch 1575/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 258.2817 - val_loss: 141.7320\n",
      "Epoch 1576/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 139.1631 - val_loss: 129.1578\n",
      "Epoch 1577/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.6178 - val_loss: 122.6848\n",
      "Epoch 1578/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.6707 - val_loss: 121.5917\n",
      "Epoch 1579/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.5897 - val_loss: 160.1888\n",
      "Epoch 1580/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.1551 - val_loss: 141.6723\n",
      "Epoch 1581/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.6901 - val_loss: 138.6988\n",
      "Epoch 1582/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.9303 - val_loss: 123.0259\n",
      "Epoch 1583/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 177.0269 - val_loss: 130.1741\n",
      "Epoch 1584/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.1924 - val_loss: 133.5703\n",
      "Epoch 1585/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.8252 - val_loss: 165.8467\n",
      "Epoch 1586/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.1951 - val_loss: 126.5189\n",
      "Epoch 1587/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.4762 - val_loss: 131.6281\n",
      "Epoch 1588/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 247.0191 - val_loss: 168.9836\n",
      "Epoch 1589/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 86us/step - loss: 150.6246 - val_loss: 143.7990\n",
      "Epoch 1590/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 151.4188 - val_loss: 119.6092\n",
      "Epoch 1591/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 148.2515 - val_loss: 122.6275\n",
      "Epoch 1592/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.2020 - val_loss: 148.7476\n",
      "Epoch 1593/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.0136 - val_loss: 150.0017\n",
      "Epoch 1594/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 149.5678 - val_loss: 140.7689\n",
      "Epoch 1595/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 166.1268 - val_loss: 125.7708\n",
      "Epoch 1596/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.9791 - val_loss: 135.8314\n",
      "Epoch 1597/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 172.9665 - val_loss: 117.6887\n",
      "Epoch 1598/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 159.4892 - val_loss: 119.4928\n",
      "Epoch 1599/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 145.1881 - val_loss: 233.8930\n",
      "Epoch 1600/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.8947 - val_loss: 118.7661\n",
      "Epoch 1601/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.3403 - val_loss: 203.0708\n",
      "Epoch 1602/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 190.0290 - val_loss: 121.5248\n",
      "Epoch 1603/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 144.5944 - val_loss: 127.2237\n",
      "Epoch 1604/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.9190 - val_loss: 125.3332\n",
      "Epoch 1605/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 148.4846 - val_loss: 130.8933\n",
      "Epoch 1606/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.1301 - val_loss: 128.1789\n",
      "Epoch 1607/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.5972 - val_loss: 145.7520\n",
      "Epoch 1608/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 158.8346 - val_loss: 139.0159\n",
      "Epoch 1609/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 145.5366 - val_loss: 160.5404\n",
      "Epoch 1610/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 147.2593 - val_loss: 128.4631\n",
      "Epoch 1611/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 148.6570 - val_loss: 128.5884\n",
      "Epoch 1612/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 152.4104 - val_loss: 124.6922\n",
      "Epoch 1613/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 145.7306 - val_loss: 132.3587\n",
      "Epoch 1614/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 143.7437 - val_loss: 126.3417\n",
      "Epoch 1615/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 161.6288 - val_loss: 164.0908\n",
      "Epoch 1616/10000\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 152.2681 - val_loss: 160.7717\n",
      "Epoch 1617/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 149.4293 - val_loss: 144.8478\n",
      "Epoch 1618/10000\n",
      "8000/8000 [==============================] - 1s 127us/step - loss: 155.7412 - val_loss: 172.6896\n",
      "Epoch 1619/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 339.7487 - val_loss: 1059.3686\n",
      "Epoch 1620/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 313.5141 - val_loss: 173.0684\n",
      "Epoch 1621/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 174.4924 - val_loss: 144.9382\n",
      "Epoch 1622/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 172.1553 - val_loss: 142.8811\n",
      "Epoch 1623/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 159.4474 - val_loss: 130.0866\n",
      "Epoch 1624/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 153.5083 - val_loss: 145.1360\n",
      "Epoch 1625/10000\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 148.9159 - val_loss: 162.3823\n",
      "Epoch 1626/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 145.2865 - val_loss: 141.9682\n",
      "Epoch 1627/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 152.3868 - val_loss: 175.9418\n",
      "Epoch 1628/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 149.8756 - val_loss: 125.9297\n",
      "Epoch 1629/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 150.9274 - val_loss: 136.3356\n",
      "Epoch 1630/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 146.0667 - val_loss: 175.9722\n",
      "Epoch 1631/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 151.6022 - val_loss: 152.3819\n",
      "Epoch 1632/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 158.9767 - val_loss: 150.6405\n",
      "Epoch 1633/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 161.1338 - val_loss: 188.0454\n",
      "Epoch 1634/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 152.3198 - val_loss: 133.8955\n",
      "Epoch 1635/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 153.4436 - val_loss: 124.9243\n",
      "Epoch 1636/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 157.5535 - val_loss: 139.1343\n",
      "Epoch 1637/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 150.7982 - val_loss: 125.7335\n",
      "Epoch 1638/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 149.5930 - val_loss: 126.8337\n",
      "Epoch 1639/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 154.6004 - val_loss: 135.9032\n",
      "Epoch 1640/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 160.0024 - val_loss: 123.8632\n",
      "Epoch 1641/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 172.5704 - val_loss: 130.4120\n",
      "Epoch 1642/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 148.6253 - val_loss: 141.8601\n",
      "Epoch 1643/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 153.3701 - val_loss: 132.0873\n",
      "Epoch 1644/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 166.5403 - val_loss: 121.5652\n",
      "Epoch 1645/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 162.9833 - val_loss: 209.2432\n",
      "Epoch 1646/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 157.2976 - val_loss: 138.3375\n",
      "Epoch 1647/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 166.6731 - val_loss: 174.9639\n",
      "Epoch 1648/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 155.7512 - val_loss: 119.8618\n",
      "Epoch 1649/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 157.3077 - val_loss: 121.5208\n",
      "Epoch 1650/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 142.7607 - val_loss: 141.4415\n",
      "Epoch 1651/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 158.4233 - val_loss: 124.9202\n",
      "Epoch 1652/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 146.8131 - val_loss: 128.9936\n",
      "Epoch 1653/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 137.4765 - val_loss: 145.2385\n",
      "Epoch 1654/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.9718 - val_loss: 169.9417\n",
      "Epoch 1655/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 169.0779 - val_loss: 439.3145\n",
      "Epoch 1656/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 155.1233 - val_loss: 145.5288\n",
      "Epoch 1657/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 150.0162 - val_loss: 124.5414\n",
      "Epoch 1658/10000\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 162.5037 - val_loss: 120.6360\n",
      "Epoch 1659/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 151.9632 - val_loss: 121.2032\n",
      "Epoch 1660/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 160.1991 - val_loss: 126.1947\n",
      "Epoch 1661/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 77us/step - loss: 154.8757 - val_loss: 130.7976\n",
      "Epoch 1662/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 145.992 - 1s 96us/step - loss: 145.7489 - val_loss: 118.1802\n",
      "Epoch 1663/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 154.3120 - val_loss: 168.8335\n",
      "Epoch 1664/10000\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 169.3288 - val_loss: 124.1162\n",
      "Epoch 1665/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 152.6236 - val_loss: 120.5156\n",
      "Epoch 1666/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 159.6362 - val_loss: 120.8783\n",
      "Epoch 1667/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 149.0725 - val_loss: 121.6506\n",
      "Epoch 1668/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 158.7396 - val_loss: 129.0758\n",
      "Epoch 1669/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 165.1250 - val_loss: 119.6042\n",
      "Epoch 1670/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 151.2337 - val_loss: 118.8500\n",
      "Epoch 1671/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 155.0012 - val_loss: 131.4483\n",
      "Epoch 1672/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 145.8816 - val_loss: 154.2921\n",
      "Epoch 1673/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 154.5797 - val_loss: 160.6074\n",
      "Epoch 1674/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 164.3202 - val_loss: 132.3776\n",
      "Epoch 1675/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 214.8103 - val_loss: 175.8479\n",
      "Epoch 1676/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 166.0047 - val_loss: 651.2597\n",
      "Epoch 1677/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 158.0345 - val_loss: 121.2786\n",
      "Epoch 1678/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 145.8189 - val_loss: 118.1395\n",
      "Epoch 1679/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 173.1586 - val_loss: 122.8314\n",
      "Epoch 1680/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 144.0321 - val_loss: 121.6378\n",
      "Epoch 1681/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 153.3627 - val_loss: 169.3099\n",
      "Epoch 1682/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 145.8907 - val_loss: 119.1797\n",
      "Epoch 1683/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 155.8612 - val_loss: 128.1243\n",
      "Epoch 1684/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 141.8440 - val_loss: 128.5021\n",
      "Epoch 1685/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 183.4068 - val_loss: 130.5374\n",
      "Epoch 1686/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 145.1362 - val_loss: 135.0645\n",
      "Epoch 1687/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 142.6035 - val_loss: 142.9946\n",
      "Epoch 1688/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 182.2232 - val_loss: 121.7017\n",
      "Epoch 1689/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 183.5928 - val_loss: 145.2866\n",
      "Epoch 1690/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 152.5545 - val_loss: 122.9475\n",
      "Epoch 1691/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.9536 - val_loss: 143.3449\n",
      "Epoch 1692/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 144.3344 - val_loss: 134.8074\n",
      "Epoch 1693/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.0554 - val_loss: 122.3501\n",
      "Epoch 1694/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 144.6555 - val_loss: 136.3975\n",
      "Epoch 1695/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 147.0888 - val_loss: 125.2535\n",
      "Epoch 1696/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 143.0994 - val_loss: 129.1604\n",
      "Epoch 1697/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 150.3336 - val_loss: 142.9093\n",
      "Epoch 1698/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.8932 - val_loss: 120.4520\n",
      "Epoch 1699/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 147.3704 - val_loss: 127.8554\n",
      "Epoch 1700/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 148.2203 - val_loss: 121.7834\n",
      "Epoch 1701/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 158.0439 - val_loss: 121.4394\n",
      "Epoch 1702/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 166.1341 - val_loss: 140.0070\n",
      "Epoch 1703/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 158.0301 - val_loss: 148.8421\n",
      "Epoch 1704/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 151.8908 - val_loss: 138.2954\n",
      "Epoch 1705/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 146.6294 - val_loss: 154.4065\n",
      "Epoch 1706/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 147.1979 - val_loss: 132.4897\n",
      "Epoch 1707/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 193.6830 - val_loss: 137.9926\n",
      "Epoch 1708/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 141.4544 - val_loss: 193.7686\n",
      "Epoch 1709/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 149.4633 - val_loss: 134.5938\n",
      "Epoch 1710/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 149.7562 - val_loss: 182.4278\n",
      "Epoch 1711/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 149.4646 - val_loss: 166.4049\n",
      "Epoch 1712/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 150.3185 - val_loss: 140.7733\n",
      "Epoch 1713/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 190.5899 - val_loss: 139.1276\n",
      "Epoch 1714/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 143.1030 - val_loss: 131.6980\n",
      "Epoch 1715/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.0097 - val_loss: 122.2176\n",
      "Epoch 1716/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.7132 - val_loss: 177.8253\n",
      "Epoch 1717/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 145.9656 - val_loss: 131.8979\n",
      "Epoch 1718/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 156.3472 - val_loss: 139.1641\n",
      "Epoch 1719/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.1458 - val_loss: 141.4283\n",
      "Epoch 1720/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 144.3395 - val_loss: 136.0615\n",
      "Epoch 1721/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 170.9393 - val_loss: 130.2614\n",
      "Epoch 1722/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 154.2792 - val_loss: 128.2065\n",
      "Epoch 1723/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 154.0421 - val_loss: 176.8991\n",
      "Epoch 1724/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 143.4345 - val_loss: 133.8285\n",
      "Epoch 1725/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 146.9320 - val_loss: 139.5958\n",
      "Epoch 1726/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 195.9242 - val_loss: 130.2157\n",
      "Epoch 1727/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 144.0902 - val_loss: 172.1408\n",
      "Epoch 1728/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 151.5728 - val_loss: 147.6790\n",
      "Epoch 1729/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 139.7357 - val_loss: 128.0261\n",
      "Epoch 1730/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.3531 - val_loss: 135.3801\n",
      "Epoch 1731/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 141.3478 - val_loss: 143.9968\n",
      "Epoch 1732/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 148.6066 - val_loss: 139.2778\n",
      "Epoch 1733/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 157.5138 - val_loss: 593.4585\n",
      "Epoch 1734/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 157.9697 - val_loss: 134.1606\n",
      "Epoch 1735/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 147.6813 - val_loss: 121.2495\n",
      "Epoch 1736/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 142.5409 - val_loss: 149.5993\n",
      "Epoch 1737/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 159.4270 - val_loss: 152.8962\n",
      "Epoch 1738/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.6693 - val_loss: 121.6901\n",
      "Epoch 1739/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 191.8410 - val_loss: 267.4045\n",
      "Epoch 1740/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 166.1492 - val_loss: 127.3849\n",
      "Epoch 1741/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 173.1571 - val_loss: 160.8666\n",
      "Epoch 1742/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 151.9698 - val_loss: 136.5914\n",
      "Epoch 1743/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 139.7557 - val_loss: 116.7052\n",
      "Epoch 1744/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.9717 - val_loss: 173.0722\n",
      "Epoch 1745/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.3617 - val_loss: 133.3612\n",
      "Epoch 1746/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 152.2993 - val_loss: 118.1824\n",
      "Epoch 1747/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.8595 - val_loss: 117.5543\n",
      "Epoch 1748/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.7128 - val_loss: 198.4766\n",
      "Epoch 1749/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 190.5122 - val_loss: 123.3540\n",
      "Epoch 1750/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 148.0013 - val_loss: 146.0420\n",
      "Epoch 1751/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 137.6077 - val_loss: 165.2087\n",
      "Epoch 1752/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 161.7418 - val_loss: 163.0072\n",
      "Epoch 1753/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 146.8699 - val_loss: 129.8486\n",
      "Epoch 1754/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 165.4305 - val_loss: 125.2459\n",
      "Epoch 1755/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 167.2824 - val_loss: 133.3735\n",
      "Epoch 1756/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 183.9659 - val_loss: 140.8803\n",
      "Epoch 1757/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 149.0138 - val_loss: 124.6986\n",
      "Epoch 1758/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 156.8625 - val_loss: 120.7438\n",
      "Epoch 1759/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 156.9184 - val_loss: 202.0370\n",
      "Epoch 1760/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 182.8928 - val_loss: 168.2904\n",
      "Epoch 1761/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 166.0407 - val_loss: 141.1081\n",
      "Epoch 1762/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 161.7216 - val_loss: 122.4483\n",
      "Epoch 1763/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 148.7913 - val_loss: 134.9357\n",
      "Epoch 1764/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.8540 - val_loss: 134.1814\n",
      "Epoch 1765/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 172.3208 - val_loss: 298.0274\n",
      "Epoch 1766/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 154.2749 - val_loss: 128.5085\n",
      "Epoch 1767/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 162.0098 - val_loss: 135.3829\n",
      "Epoch 1768/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 148.5432 - val_loss: 120.6946\n",
      "Epoch 1769/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.2710 - val_loss: 176.8058\n",
      "Epoch 1770/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 147.0457 - val_loss: 124.2086\n",
      "Epoch 1771/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 148.8403 - val_loss: 123.2823\n",
      "Epoch 1772/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 148.4452 - val_loss: 164.0129\n",
      "Epoch 1773/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 144.1293 - val_loss: 126.4534\n",
      "Epoch 1774/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 161.4404 - val_loss: 124.6331\n",
      "Epoch 1775/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 150.7419 - val_loss: 150.8957\n",
      "Epoch 1776/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 264.4673 - val_loss: 122.6812\n",
      "Epoch 1777/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.3679 - val_loss: 136.7457\n",
      "Epoch 1778/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 147.1269 - val_loss: 117.7096\n",
      "Epoch 1779/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 140.7784 - val_loss: 120.6069\n",
      "Epoch 1780/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 152.7110 - val_loss: 133.6984\n",
      "Epoch 1781/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.4801 - val_loss: 117.5294\n",
      "Epoch 1782/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.6175 - val_loss: 145.2693\n",
      "Epoch 1783/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 147.8379 - val_loss: 127.7305\n",
      "Epoch 1784/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 142.4545 - val_loss: 121.8321\n",
      "Epoch 1785/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 137.3680 - val_loss: 147.6152\n",
      "Epoch 1786/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 146.5413 - val_loss: 116.3776\n",
      "Epoch 1787/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 154.3601 - val_loss: 137.4761\n",
      "Epoch 1788/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 145.9420 - val_loss: 212.6816\n",
      "Epoch 1789/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 154.4782 - val_loss: 143.1855\n",
      "Epoch 1790/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 156.9027 - val_loss: 285.3341\n",
      "Epoch 1791/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 167.3495 - val_loss: 126.3371\n",
      "Epoch 1792/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 147.0097 - val_loss: 126.6162\n",
      "Epoch 1793/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 138.8612 - val_loss: 178.6116\n",
      "Epoch 1794/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 153.1086 - val_loss: 175.5799\n",
      "Epoch 1795/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 144.7212 - val_loss: 120.2203\n",
      "Epoch 1796/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 139.7630 - val_loss: 124.0355\n",
      "Epoch 1797/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 154.2084 - val_loss: 155.1177\n",
      "Epoch 1798/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 171.9824 - val_loss: 212.5437\n",
      "Epoch 1799/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.0261 - val_loss: 123.1278\n",
      "Epoch 1800/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.4652 - val_loss: 186.5462\n",
      "Epoch 1801/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 144.7820 - val_loss: 125.1400\n",
      "Epoch 1802/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.9708 - val_loss: 127.8691\n",
      "Epoch 1803/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 172.3379 - val_loss: 135.6364\n",
      "Epoch 1804/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.9662 - val_loss: 147.9983\n",
      "Epoch 1805/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 68us/step - loss: 139.4875 - val_loss: 126.7023\n",
      "Epoch 1806/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 135.3537 - val_loss: 125.2926\n",
      "Epoch 1807/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 146.0595 - val_loss: 118.3510\n",
      "Epoch 1808/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.2235 - val_loss: 122.9298\n",
      "Epoch 1809/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 149.6522 - val_loss: 117.8391\n",
      "Epoch 1810/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 170.7221 - val_loss: 148.6364\n",
      "Epoch 1811/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 165.3789 - val_loss: 206.1897\n",
      "Epoch 1812/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 157.3592 - val_loss: 129.1380\n",
      "Epoch 1813/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.8051 - val_loss: 124.8777\n",
      "Epoch 1814/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 157.4076 - val_loss: 174.5565\n",
      "Epoch 1815/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 215.6231 - val_loss: 178.6766\n",
      "Epoch 1816/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 139.0661 - val_loss: 127.1224\n",
      "Epoch 1817/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 144.4454 - val_loss: 123.4817\n",
      "Epoch 1818/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 142.2839 - val_loss: 124.4532\n",
      "Epoch 1819/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.0381 - val_loss: 117.9775\n",
      "Epoch 1820/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 141.9562 - val_loss: 147.2940\n",
      "Epoch 1821/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 139.5817 - val_loss: 131.3594\n",
      "Epoch 1822/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.0118 - val_loss: 139.9934\n",
      "Epoch 1823/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 143.6493 - val_loss: 125.7610\n",
      "Epoch 1824/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.1089 - val_loss: 161.2839\n",
      "Epoch 1825/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 237.0164 - val_loss: 121.7433\n",
      "Epoch 1826/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.5437 - val_loss: 130.1329\n",
      "Epoch 1827/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 137.8255 - val_loss: 128.2683\n",
      "Epoch 1828/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 136.0233 - val_loss: 122.0058\n",
      "Epoch 1829/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 149.1171 - val_loss: 123.7861\n",
      "Epoch 1830/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.9402 - val_loss: 134.2886\n",
      "Epoch 1831/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 147.1528 - val_loss: 129.0648\n",
      "Epoch 1832/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 385.7445 - val_loss: 182.7945\n",
      "Epoch 1833/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 183.7301 - val_loss: 126.6867\n",
      "Epoch 1834/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 168.4749 - val_loss: 120.0481\n",
      "Epoch 1835/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 185.8768 - val_loss: 142.3663\n",
      "Epoch 1836/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.3145 - val_loss: 121.7337\n",
      "Epoch 1837/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.7230 - val_loss: 137.9907\n",
      "Epoch 1838/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 160.2618 - val_loss: 121.0978\n",
      "Epoch 1839/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.3638 - val_loss: 118.3172\n",
      "Epoch 1840/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.6719 - val_loss: 129.7903\n",
      "Epoch 1841/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 166.4313 - val_loss: 132.3039\n",
      "Epoch 1842/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 153.4179 - val_loss: 123.6308\n",
      "Epoch 1843/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.1170 - val_loss: 129.5905\n",
      "Epoch 1844/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 158.2730 - val_loss: 150.2393\n",
      "Epoch 1845/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.4302 - val_loss: 120.4722\n",
      "Epoch 1846/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 143.9380 - val_loss: 130.8293\n",
      "Epoch 1847/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 142.4229 - val_loss: 258.5427\n",
      "Epoch 1848/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 162.7699 - val_loss: 123.9486\n",
      "Epoch 1849/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 153.6250 - val_loss: 138.8481\n",
      "Epoch 1850/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 137.6659 - val_loss: 120.6052\n",
      "Epoch 1851/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 141.4675 - val_loss: 120.5029\n",
      "Epoch 1852/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 161.8091 - val_loss: 177.9803\n",
      "Epoch 1853/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 146.8927 - val_loss: 124.1468\n",
      "Epoch 1854/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 145.7913 - val_loss: 122.5490\n",
      "Epoch 1855/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 146.9828 - val_loss: 121.4484\n",
      "Epoch 1856/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 149.9765 - val_loss: 134.3915\n",
      "Epoch 1857/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.6567 - val_loss: 144.4331\n",
      "Epoch 1858/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 254.9874 - val_loss: 182.4538\n",
      "Epoch 1859/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 151.3638 - val_loss: 145.8471\n",
      "Epoch 1860/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 137.5815 - val_loss: 120.2629\n",
      "Epoch 1861/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.5398 - val_loss: 123.1240\n",
      "Epoch 1862/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 172.9765 - val_loss: 120.6006\n",
      "Epoch 1863/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.2248 - val_loss: 184.5329\n",
      "Epoch 1864/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 143.2876 - val_loss: 167.6422\n",
      "Epoch 1865/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.9481 - val_loss: 117.4520\n",
      "Epoch 1866/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 138.3058 - val_loss: 120.5685\n",
      "Epoch 1867/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 155.0628 - val_loss: 145.8876\n",
      "Epoch 1868/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 142.6124 - val_loss: 121.3753\n",
      "Epoch 1869/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 160.2397 - val_loss: 116.1827\n",
      "Epoch 1870/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.4304 - val_loss: 139.0320\n",
      "Epoch 1871/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 142.0446 - val_loss: 131.2109\n",
      "Epoch 1872/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 145.0928 - val_loss: 149.6490\n",
      "Epoch 1873/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 156.3214 - val_loss: 123.5090\n",
      "Epoch 1874/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.9543 - val_loss: 146.9881\n",
      "Epoch 1875/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 147.3160 - val_loss: 146.9023\n",
      "Epoch 1876/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.8296 - val_loss: 144.0856\n",
      "Epoch 1877/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 224.9629 - val_loss: 129.2968\n",
      "Epoch 1878/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 138.9238 - val_loss: 128.0395\n",
      "Epoch 1879/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 139.0040 - val_loss: 119.9418\n",
      "Epoch 1880/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.6202 - val_loss: 122.8061\n",
      "Epoch 1881/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.7439 - val_loss: 129.1962\n",
      "Epoch 1882/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 138.2049 - val_loss: 131.4427\n",
      "Epoch 1883/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 144.2022 - val_loss: 128.5961\n",
      "Epoch 1884/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 190.7065 - val_loss: 126.6704\n",
      "Epoch 1885/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 152.1914 - val_loss: 133.0231\n",
      "Epoch 1886/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 147.1132 - val_loss: 164.5692\n",
      "Epoch 1887/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 159.2016 - val_loss: 145.9454\n",
      "Epoch 1888/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.4344 - val_loss: 129.6309\n",
      "Epoch 1889/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 139.1913 - val_loss: 117.8761\n",
      "Epoch 1890/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 230.2892 - val_loss: 127.7958\n",
      "Epoch 1891/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 186.3121 - val_loss: 156.8237\n",
      "Epoch 1892/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 151.6822 - val_loss: 127.1906\n",
      "Epoch 1893/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 164.820 - 1s 70us/step - loss: 164.1733 - val_loss: 131.5189\n",
      "Epoch 1894/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.5218 - val_loss: 117.4881\n",
      "Epoch 1895/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 157.0321 - val_loss: 115.6640\n",
      "Epoch 1896/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 147.8009 - val_loss: 124.0630\n",
      "Epoch 1897/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 139.6044 - val_loss: 119.0104\n",
      "Epoch 1898/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 134.9771 - val_loss: 182.6236\n",
      "Epoch 1899/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.2506 - val_loss: 145.4542\n",
      "Epoch 1900/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 156.6208 - val_loss: 124.8282\n",
      "Epoch 1901/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 142.1338 - val_loss: 124.3286\n",
      "Epoch 1902/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 151.5455 - val_loss: 185.6590\n",
      "Epoch 1903/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 148.7969 - val_loss: 118.8318\n",
      "Epoch 1904/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 145.8599 - val_loss: 176.1268\n",
      "Epoch 1905/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 144.3419 - val_loss: 125.8768\n",
      "Epoch 1906/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 145.6406 - val_loss: 191.6570\n",
      "Epoch 1907/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 143.3055 - val_loss: 116.7785\n",
      "Epoch 1908/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 151.4849 - val_loss: 122.6429\n",
      "Epoch 1909/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.5772 - val_loss: 149.5474\n",
      "Epoch 1910/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 155.6826 - val_loss: 136.9264\n",
      "Epoch 1911/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 146.7209 - val_loss: 126.3740\n",
      "Epoch 1912/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 141.2406 - val_loss: 148.6981\n",
      "Epoch 1913/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 150.5461 - val_loss: 119.4024\n",
      "Epoch 1914/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 321.5766 - val_loss: 136.4226\n",
      "Epoch 1915/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 157.8987 - val_loss: 161.0420\n",
      "Epoch 1916/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.2161 - val_loss: 134.4521\n",
      "Epoch 1917/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.5111 - val_loss: 120.3122\n",
      "Epoch 1918/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 140.6929 - val_loss: 124.1275\n",
      "Epoch 1919/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 144.2351 - val_loss: 125.7650\n",
      "Epoch 1920/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 185.3221 - val_loss: 139.4950\n",
      "Epoch 1921/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.9549 - val_loss: 120.0518\n",
      "Epoch 1922/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 141.5429 - val_loss: 128.5394\n",
      "Epoch 1923/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 147.6144 - val_loss: 130.3580\n",
      "Epoch 1924/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 158.5841 - val_loss: 115.2904\n",
      "Epoch 1925/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 145.1778 - val_loss: 138.7226\n",
      "Epoch 1926/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 134.4271 - val_loss: 119.5206\n",
      "Epoch 1927/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.5305 - val_loss: 123.9464\n",
      "Epoch 1928/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 147.0955 - val_loss: 127.7092\n",
      "Epoch 1929/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.0462 - val_loss: 125.0484\n",
      "Epoch 1930/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 150.7025 - val_loss: 133.6363\n",
      "Epoch 1931/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 138.4558 - val_loss: 481.9698\n",
      "Epoch 1932/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.7808 - val_loss: 156.2308\n",
      "Epoch 1933/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 146.0331 - val_loss: 133.2466\n",
      "Epoch 1934/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 153.8272 - val_loss: 123.5496\n",
      "Epoch 1935/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.1389 - val_loss: 164.9044\n",
      "Epoch 1936/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.7247 - val_loss: 119.3838\n",
      "Epoch 1937/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 283.2572 - val_loss: 167.4667\n",
      "Epoch 1938/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 143.6668 - val_loss: 122.7018\n",
      "Epoch 1939/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 136.8286 - val_loss: 115.2736\n",
      "Epoch 1940/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 161.2754 - val_loss: 135.8624\n",
      "Epoch 1941/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 154.1818 - val_loss: 126.3856\n",
      "Epoch 1942/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 152.3667 - val_loss: 137.9251\n",
      "Epoch 1943/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 144.8131 - val_loss: 122.1572\n",
      "Epoch 1944/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 133.4306 - val_loss: 119.2021\n",
      "Epoch 1945/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 141.7580 - val_loss: 139.4681\n",
      "Epoch 1946/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 139.5712 - val_loss: 125.4506\n",
      "Epoch 1947/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 145.1174 - val_loss: 119.7047\n",
      "Epoch 1948/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 141.9630 - val_loss: 126.3600\n",
      "Epoch 1949/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 64us/step - loss: 161.4219 - val_loss: 180.0814\n",
      "Epoch 1950/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 151.1077 - val_loss: 117.1255\n",
      "Epoch 1951/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 148.4799 - val_loss: 127.6507\n",
      "Epoch 1952/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 153.8823 - val_loss: 159.5400\n",
      "Epoch 1953/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.1994 - val_loss: 122.2927\n",
      "Epoch 1954/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.5570 - val_loss: 124.8423\n",
      "Epoch 1955/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.9721 - val_loss: 131.3156\n",
      "Epoch 1956/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 150.4024 - val_loss: 137.2618\n",
      "Epoch 1957/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 162.4352 - val_loss: 134.7041\n",
      "Epoch 1958/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 165.2503 - val_loss: 122.7664\n",
      "Epoch 1959/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 159.8688 - val_loss: 645.3186\n",
      "Epoch 1960/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 177.3064 - val_loss: 144.0751\n",
      "Epoch 1961/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 166.5336 - val_loss: 122.9975\n",
      "Epoch 1962/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 139.0733 - val_loss: 133.2883\n",
      "Epoch 1963/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 138.4841 - val_loss: 156.4585\n",
      "Epoch 1964/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 151.6977 - val_loss: 148.7150\n",
      "Epoch 1965/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 139.2259 - val_loss: 146.6202\n",
      "Epoch 1966/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 145.2742 - val_loss: 124.5838\n",
      "Epoch 1967/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.5963 - val_loss: 173.9027\n",
      "Epoch 1968/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.7227 - val_loss: 128.6047\n",
      "Epoch 1969/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 156.6260 - val_loss: 135.8520\n",
      "Epoch 1970/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 149.6738 - val_loss: 120.0664\n",
      "Epoch 1971/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 147.0199 - val_loss: 122.3133\n",
      "Epoch 1972/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 136.0462 - val_loss: 156.8524\n",
      "Epoch 1973/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.3665 - val_loss: 143.5230\n",
      "Epoch 1974/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 148.6581 - val_loss: 135.5576\n",
      "Epoch 1975/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 168.5602 - val_loss: 119.4150\n",
      "Epoch 1976/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 167.8556 - val_loss: 120.8854\n",
      "Epoch 1977/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 151.5046 - val_loss: 141.8714\n",
      "Epoch 1978/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.3617 - val_loss: 119.2458\n",
      "Epoch 1979/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.4877 - val_loss: 121.9635\n",
      "Epoch 1980/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.3481 - val_loss: 120.7899\n",
      "Epoch 1981/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 144.9414 - val_loss: 139.4666\n",
      "Epoch 1982/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 208.4821 - val_loss: 120.3412\n",
      "Epoch 1983/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.7770 - val_loss: 150.0472\n",
      "Epoch 1984/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.1939 - val_loss: 138.0840\n",
      "Epoch 1985/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 137.3756 - val_loss: 120.9978\n",
      "Epoch 1986/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 141.9238 - val_loss: 129.0279\n",
      "Epoch 1987/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 140.9708 - val_loss: 126.6427\n",
      "Epoch 1988/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 145.6819 - val_loss: 118.4130\n",
      "Epoch 1989/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 139.3786 - val_loss: 139.0645\n",
      "Epoch 1990/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.3630 - val_loss: 146.9081\n",
      "Epoch 1991/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 143.8493 - val_loss: 125.3803\n",
      "Epoch 1992/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 141.3313 - val_loss: 241.8441\n",
      "Epoch 1993/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 163.2776 - val_loss: 182.5686\n",
      "Epoch 1994/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 202.1953 - val_loss: 121.5456\n",
      "Epoch 1995/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 145.5041 - val_loss: 154.8278\n",
      "Epoch 1996/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 163.5323 - val_loss: 145.7535\n",
      "Epoch 1997/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 141.9813 - val_loss: 122.0403\n",
      "Epoch 1998/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 146.7024 - val_loss: 152.0471\n",
      "Epoch 1999/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.6834 - val_loss: 218.1877\n",
      "Epoch 2000/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 141.9167 - val_loss: 136.6347\n",
      "Epoch 2001/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.7504 - val_loss: 120.6003\n",
      "Epoch 2002/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.8982 - val_loss: 216.6835\n",
      "Epoch 2003/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 209.4804 - val_loss: 132.5296\n",
      "Epoch 2004/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 143.9754 - val_loss: 120.5774\n",
      "Epoch 2005/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 141.8234 - val_loss: 130.2040\n",
      "Epoch 2006/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 138.8252 - val_loss: 120.8273\n",
      "Epoch 2007/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 138.0289 - val_loss: 119.4078\n",
      "Epoch 2008/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 137.9094 - val_loss: 121.6783\n",
      "Epoch 2009/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.4184 - val_loss: 125.5831\n",
      "Epoch 2010/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 142.1097 - val_loss: 120.1376\n",
      "Epoch 2011/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 161.7864 - val_loss: 203.8186\n",
      "Epoch 2012/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 176.5344 - val_loss: 136.1968\n",
      "Epoch 2013/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 146.9445 - val_loss: 159.6587\n",
      "Epoch 2014/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 137.0977 - val_loss: 119.2664\n",
      "Epoch 2015/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 150.4950 - val_loss: 121.5030\n",
      "Epoch 2016/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 150.4412 - val_loss: 122.7507\n",
      "Epoch 2017/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.6561 - val_loss: 117.0431\n",
      "Epoch 2018/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 139.8240 - val_loss: 128.2183\n",
      "Epoch 2019/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.5776 - val_loss: 122.2928\n",
      "Epoch 2020/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.6701 - val_loss: 119.3989\n",
      "Epoch 2021/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 70us/step - loss: 150.9844 - val_loss: 145.8400\n",
      "Epoch 2022/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 170.6691 - val_loss: 127.8722\n",
      "Epoch 2023/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 148.5406 - val_loss: 164.6165\n",
      "Epoch 2024/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 203.1191 - val_loss: 127.8843\n",
      "Epoch 2025/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.0121 - val_loss: 123.8546\n",
      "Epoch 2026/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 146.8403 - val_loss: 126.1781\n",
      "Epoch 2027/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 139.0446 - val_loss: 117.3448\n",
      "Epoch 2028/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 160.5100 - val_loss: 133.1210\n",
      "Epoch 2029/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.7676 - val_loss: 121.9626\n",
      "Epoch 2030/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.3164 - val_loss: 143.3621\n",
      "Epoch 2031/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 146.5965 - val_loss: 118.2769\n",
      "Epoch 2032/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.0652 - val_loss: 116.5909\n",
      "Epoch 2033/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.5468 - val_loss: 126.2694\n",
      "Epoch 2034/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 159.7576 - val_loss: 133.4980\n",
      "Epoch 2035/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 145.6050 - val_loss: 120.8834\n",
      "Epoch 2036/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.5584 - val_loss: 120.4131\n",
      "Epoch 2037/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 149.3248 - val_loss: 126.6916\n",
      "Epoch 2038/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 142.4917 - val_loss: 138.8134\n",
      "Epoch 2039/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 174.0038 - val_loss: 119.6370\n",
      "Epoch 2040/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 414.2732 - val_loss: 750.3254\n",
      "Epoch 2041/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 322.7289 - val_loss: 200.4547\n",
      "Epoch 2042/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 242.3358 - val_loss: 169.2772\n",
      "Epoch 2043/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 200.5729 - val_loss: 240.4739\n",
      "Epoch 2044/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 203.8353 - val_loss: 150.9407\n",
      "Epoch 2045/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 199.8035 - val_loss: 156.8013\n",
      "Epoch 2046/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 178.7470 - val_loss: 143.8080\n",
      "Epoch 2047/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 188.4025 - val_loss: 137.9614\n",
      "Epoch 2048/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 183.3472 - val_loss: 141.2090\n",
      "Epoch 2049/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 203.7660 - val_loss: 329.7888\n",
      "Epoch 2050/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 190.3291 - val_loss: 197.3301\n",
      "Epoch 2051/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 182.8606 - val_loss: 146.2818\n",
      "Epoch 2052/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 172.9452 - val_loss: 136.8510\n",
      "Epoch 2053/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 180.7217 - val_loss: 148.5308\n",
      "Epoch 2054/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 175.0036 - val_loss: 145.7529\n",
      "Epoch 2055/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 170.1096 - val_loss: 135.9290\n",
      "Epoch 2056/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 173.3249 - val_loss: 135.9679\n",
      "Epoch 2057/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 180.5574 - val_loss: 142.9403\n",
      "Epoch 2058/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 176.4757 - val_loss: 128.4959\n",
      "Epoch 2059/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 167.5768 - val_loss: 133.5424\n",
      "Epoch 2060/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 173.5044 - val_loss: 135.4598\n",
      "Epoch 2061/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 175.7262 - val_loss: 134.3200\n",
      "Epoch 2062/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 203.9728 - val_loss: 132.3517\n",
      "Epoch 2063/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 165.5360 - val_loss: 279.4478\n",
      "Epoch 2064/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 180.5397 - val_loss: 142.2439\n",
      "Epoch 2065/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 165.9675 - val_loss: 127.8459\n",
      "Epoch 2066/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 167.9770 - val_loss: 136.3068\n",
      "Epoch 2067/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 175.9187 - val_loss: 122.9881\n",
      "Epoch 2068/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 182.9947 - val_loss: 126.7878\n",
      "Epoch 2069/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 180.2337 - val_loss: 131.0032\n",
      "Epoch 2070/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 177.4505 - val_loss: 184.4092\n",
      "Epoch 2071/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 176.7166 - val_loss: 140.6920\n",
      "Epoch 2072/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 161.5849 - val_loss: 130.7114\n",
      "Epoch 2073/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 156.2223 - val_loss: 133.1347\n",
      "Epoch 2074/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 194.6483 - val_loss: 208.5780\n",
      "Epoch 2075/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 164.2620 - val_loss: 150.8950\n",
      "Epoch 2076/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 185.6932 - val_loss: 165.5451\n",
      "Epoch 2077/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 167.5479 - val_loss: 136.9127\n",
      "Epoch 2078/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 176.0278 - val_loss: 150.6597\n",
      "Epoch 2079/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 166.3269 - val_loss: 135.0865\n",
      "Epoch 2080/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 166.6550 - val_loss: 127.7009\n",
      "Epoch 2081/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 188.2958 - val_loss: 138.8997\n",
      "Epoch 2082/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 167.7592 - val_loss: 139.0599\n",
      "Epoch 2083/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 174.3288 - val_loss: 140.4795\n",
      "Epoch 2084/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 160.6433 - val_loss: 144.9270\n",
      "Epoch 2085/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 165.5898 - val_loss: 224.7990\n",
      "Epoch 2086/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 180.6587 - val_loss: 125.8087\n",
      "Epoch 2087/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 158.7111 - val_loss: 134.9924\n",
      "Epoch 2088/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 166.4457 - val_loss: 134.0892\n",
      "Epoch 2089/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 169.6334 - val_loss: 167.5910\n",
      "Epoch 2090/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 191.7306 - val_loss: 131.0498\n",
      "Epoch 2091/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 167.8422 - val_loss: 127.0079\n",
      "Epoch 2092/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 168.6288 - val_loss: 136.4404\n",
      "Epoch 02092: early stopping\n",
      "Fold score (RMSE): 11.390941619873047\n",
      "Fold #5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 5825.3270 - val_loss: 4503.8951\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 4994.8037 - val_loss: 4006.2263\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 4600.6737 - val_loss: 4100.0313\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 4499.8000 - val_loss: 3693.9974\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 4382.4051 - val_loss: 3609.3209\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 4366.3723 - val_loss: 3827.0174\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 4159.9656 - val_loss: 3525.4270\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 4147.4821 - val_loss: 3481.7016\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 4114.0280 - val_loss: 3383.4945\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 3822.6748 - val_loss: 3143.2777\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 3626.9836 - val_loss: 2864.3771\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 3669.9641 - val_loss: 2980.7478\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 3422.7495 - val_loss: 2879.2828\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 2962.1032 - val_loss: 2289.6798\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 2708.3171 - val_loss: 2436.7923\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 2627.6616 - val_loss: 2056.0877\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 2284.4433 - val_loss: 2495.8211\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 1780.0473 - val_loss: 1497.8574\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 1407.8040 - val_loss: 822.1219\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 1366.5730 - val_loss: 648.4413\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 1056.2918 - val_loss: 1316.4076\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 928.8410 - val_loss: 505.4383\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 985.9655 - val_loss: 464.3843\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 908.5320 - val_loss: 582.7168\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 806.9080 - val_loss: 544.8884\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 648.7441 - val_loss: 427.2078\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 805.6334 - val_loss: 532.3639\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 643.1886 - val_loss: 419.2820\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 590.3130 - val_loss: 456.2906\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 833.1546 - val_loss: 838.3412\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 749.9522 - val_loss: 940.2451\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 638.8985 - val_loss: 389.6959\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 564.1478 - val_loss: 824.5512\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 642.0693 - val_loss: 509.0085\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 648.5104 - val_loss: 424.1773\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 557.5652 - val_loss: 490.5296\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 545.9540 - val_loss: 336.2132\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 486.6527 - val_loss: 352.7596\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 586.6351 - val_loss: 358.6006\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 528.4059 - val_loss: 327.7190\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 705.2610 - val_loss: 330.6615\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 506.4025 - val_loss: 390.5667\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 530.8245 - val_loss: 710.1344\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 499.0171 - val_loss: 928.8286\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 500.5953 - val_loss: 320.5183\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 480.2362 - val_loss: 516.0752\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 459.5320 - val_loss: 527.6927\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 655.0492 - val_loss: 344.3913\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 518.4941 - val_loss: 737.7650\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 469.3930 - val_loss: 358.2156\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 426.7464 - val_loss: 395.9471\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 430.0661 - val_loss: 281.9868\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 425.0297 - val_loss: 266.0007\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 524.0996 - val_loss: 379.3090\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 476.6101 - val_loss: 301.2609\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 530.7132 - val_loss: 302.2353\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 411.6995 - val_loss: 296.0278\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 384.8656 - val_loss: 875.2162\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 373.3569 - val_loss: 316.3654\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 409.3187 - val_loss: 278.5185\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 402.7388 - val_loss: 646.9795\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 371.1370 - val_loss: 402.5821\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 564.1030 - val_loss: 231.0496\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 392.3472 - val_loss: 345.0062\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 375.8132 - val_loss: 392.3823\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 359.6839 - val_loss: 236.3614\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 351.3471 - val_loss: 273.6722\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 358.2699 - val_loss: 234.0028\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 342.4676 - val_loss: 317.3408\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 487.2630 - val_loss: 407.4918\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 371.6903 - val_loss: 267.4004\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 438.0860 - val_loss: 422.8298\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 354.5040 - val_loss: 327.2772\n",
      "Epoch 74/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 69us/step - loss: 417.1742 - val_loss: 249.2831\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 332.4133 - val_loss: 264.8434\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 360.6347 - val_loss: 246.0242\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 352.7907 - val_loss: 242.0614\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 351.1309 - val_loss: 323.1241\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 325.3798 - val_loss: 215.2402\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 299.7165 - val_loss: 277.0439\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 336.0661 - val_loss: 213.1732\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 292.1153 - val_loss: 231.9111\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 413.1156 - val_loss: 224.2933\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 506.8642 - val_loss: 319.6352\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 370.5098 - val_loss: 233.2769\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 320.3302 - val_loss: 330.9765\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 288.3001 - val_loss: 246.1426\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 282.4113 - val_loss: 197.9143\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 312.0327 - val_loss: 408.1723\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 336.3783 - val_loss: 208.7013\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 357.9556 - val_loss: 215.6225\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 351.7372 - val_loss: 536.9965\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 341.2430 - val_loss: 379.4903\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 357.8822 - val_loss: 613.4296\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 337.1255 - val_loss: 804.7953\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 324.5269 - val_loss: 200.1689\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 319.3677 - val_loss: 192.5019\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 335.3602 - val_loss: 240.1365\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 344.6597 - val_loss: 192.3763\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 356.8311 - val_loss: 283.2671\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 301.0652 - val_loss: 207.5038\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 392.0656 - val_loss: 232.4451\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 290.1694 - val_loss: 240.4078\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 507.9146 - val_loss: 195.0118\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 363.9283 - val_loss: 225.5466\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 293.8921 - val_loss: 196.8100\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 280.3973 - val_loss: 293.5500\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 318.6851 - val_loss: 190.7426\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 301.1945 - val_loss: 261.5858\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 319.8399 - val_loss: 184.7080\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 286.4768 - val_loss: 187.0845\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 261.8571 - val_loss: 316.7201\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 308.4930 - val_loss: 325.9315\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 344.7911 - val_loss: 217.6200\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 288.5684 - val_loss: 233.0058\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 417.5920 - val_loss: 228.1650\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 354.6692 - val_loss: 411.5821\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 294.2730 - val_loss: 278.3609\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 245.3258 - val_loss: 196.2049\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 478.8237 - val_loss: 187.1767\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 269.6334 - val_loss: 272.6682\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 300.5507 - val_loss: 289.8245\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 263.3314 - val_loss: 187.2505\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 267.3744 - val_loss: 186.5806\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 315.5868 - val_loss: 198.3865\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 352.1151 - val_loss: 180.3041\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 277.0161 - val_loss: 217.7351\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 295.5515 - val_loss: 187.5262\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 259.3716 - val_loss: 202.1974\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 251.6664 - val_loss: 186.1815\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 285.0850 - val_loss: 182.7524\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 302.5132 - val_loss: 219.3601\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 356.3241 - val_loss: 191.0223\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 314.0062 - val_loss: 208.3732\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 240.1389 - val_loss: 200.9746\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 232.7358 - val_loss: 166.4665\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 261.1625 - val_loss: 170.1167\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 442.5451 - val_loss: 631.2488\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 260.7189 - val_loss: 208.0903\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 329.3523 - val_loss: 256.7502\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 231.1317 - val_loss: 273.3344\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 255.8617 - val_loss: 238.2576\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 341.4056 - val_loss: 269.1584\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 238.3138 - val_loss: 169.3880\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 275.3140 - val_loss: 229.2255\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 316.3568 - val_loss: 261.2012\n",
      "Epoch 147/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 276.8046 - val_loss: 309.3304\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 260.0960 - val_loss: 201.5014\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 293.7181 - val_loss: 166.8503\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 295.4909 - val_loss: 248.4286\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 247.3033 - val_loss: 236.1036\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 323.3373 - val_loss: 167.3463\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 226.0469 - val_loss: 340.8118\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 248.2067 - val_loss: 426.3412\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 253.0844 - val_loss: 222.5199\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 281.5159 - val_loss: 177.3945\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 252.2112 - val_loss: 253.8027\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 247.8223 - val_loss: 165.8374\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 380.9257 - val_loss: 288.0947\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 316.5183 - val_loss: 179.8528\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 226.9213 - val_loss: 168.1701\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 237.2576 - val_loss: 197.2614\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 453.7700 - val_loss: 178.8686\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 217.3294 - val_loss: 278.7559\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 237.7966 - val_loss: 163.0366\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 215.8663 - val_loss: 276.2392\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 286.7304 - val_loss: 215.9358\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 289.6798 - val_loss: 216.7138\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 237.2417 - val_loss: 187.4860\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 246.5105 - val_loss: 195.6973\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 207.0010 - val_loss: 172.9847\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 276.5096 - val_loss: 211.7818\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 255.9798 - val_loss: 288.6655\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 306.3993 - val_loss: 583.6061\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 322.6523 - val_loss: 160.4335\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 277.3516 - val_loss: 150.3976\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 374.9316 - val_loss: 505.1844\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 486.8951 - val_loss: 229.5154- ETA: 0s - loss: \n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 275.5276 - val_loss: 154.4943\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 198.8103 - val_loss: 172.0601\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 232.7917 - val_loss: 154.2374\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 300.4154 - val_loss: 312.6592\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 292.5494 - val_loss: 240.7033\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 241.3622 - val_loss: 180.6866\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 204.9055 - val_loss: 172.2165\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 322.633 - 1s 83us/step - loss: 321.0845 - val_loss: 163.7756\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 238.7962 - val_loss: 181.2623\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 222.2385 - val_loss: 445.3124\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 465.8951 - val_loss: 289.7742\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 313.1292 - val_loss: 217.4575\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 305.3108 - val_loss: 164.2181\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 265.5998 - val_loss: 326.6986\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 242.4639 - val_loss: 195.7560\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 279.1534 - val_loss: 431.0809\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 340.1605 - val_loss: 175.9996\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 216.4436 - val_loss: 160.9621\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 313.9095 - val_loss: 178.5895\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 213.8920 - val_loss: 193.4911\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 314.1281 - val_loss: 375.5161\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 285.7990 - val_loss: 331.8293\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 314.3887 - val_loss: 150.4545\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 206.7866 - val_loss: 229.1695\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 216.6759 - val_loss: 170.9537\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 235.5197 - val_loss: 178.7046\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 213.9941 - val_loss: 170.8057\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 289.0213 - val_loss: 163.7357\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 274.6156 - val_loss: 172.0085\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 283.8566 - val_loss: 173.1691\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 208.3751 - val_loss: 240.5196\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 430.9107 - val_loss: 201.6409\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 208.1128 - val_loss: 142.7607\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 216.6331 - val_loss: 146.0588\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 273.3196 - val_loss: 146.6986\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 264.7461 - val_loss: 162.7026\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 228.8540 - val_loss: 399.5950\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 214.7536 - val_loss: 161.5049\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 225.0430 - val_loss: 208.9269\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 268.3139 - val_loss: 220.2618\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 294.8862 - val_loss: 263.2775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 202.4477 - val_loss: 170.8731\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 301.9548 - val_loss: 323.0766\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 264.0773 - val_loss: 155.0206\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 233.4970 - val_loss: 148.7065\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 214.0561 - val_loss: 140.9527\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 218.9774 - val_loss: 158.2177\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 418.4367 - val_loss: 368.2122\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 229.4690 - val_loss: 155.4185\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 204.8230 - val_loss: 144.7845\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 310.5397 - val_loss: 157.5441\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 262.7194 - val_loss: 150.6299\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 221.0326 - val_loss: 172.7145\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 377.4273 - val_loss: 213.9942\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 257.6992 - val_loss: 150.1023\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 214.5554 - val_loss: 173.5598\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 188.1409 - val_loss: 139.4544\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 201.9976 - val_loss: 150.4663\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 276.2260 - val_loss: 315.8198\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 263.2981 - val_loss: 329.6998\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 243.8158 - val_loss: 141.2963\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 214.1355 - val_loss: 302.8050\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 218.7138 - val_loss: 174.6673\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 273.8578 - val_loss: 168.5770\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 244.1472 - val_loss: 148.2397\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 206.6590 - val_loss: 211.3936\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 231.6721 - val_loss: 173.5964\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 231.9266 - val_loss: 146.0443\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 200.5756 - val_loss: 255.0889\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 256.9380 - val_loss: 169.1498\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 209.9305 - val_loss: 165.8994\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 242.4872 - val_loss: 147.3590\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 550.2533 - val_loss: 217.5577\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 278.9347 - val_loss: 182.0502\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 235.3278 - val_loss: 168.0995\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 236.1863 - val_loss: 173.1145\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 246.6544 - val_loss: 149.9789\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 226.1209 - val_loss: 248.5066\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 234.9190 - val_loss: 223.7677\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 284.9754 - val_loss: 162.8671\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 304.2933 - val_loss: 246.3832\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 208.0572 - val_loss: 150.9597\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 207.5856- ETA: 0s - loss:  - 1s 70us/step - loss: 218.6189 - val_loss: 356.9552\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 276.3931 - val_loss: 213.8124\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 216.7227 - val_loss: 141.8378\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 283.102 - 1s 70us/step - loss: 285.7945 - val_loss: 169.0032\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 210.8289 - val_loss: 153.9308\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 244.2162 - val_loss: 158.5756\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 228.1556 - val_loss: 182.9515\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 213.9098 - val_loss: 271.4881\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 288.9578 - val_loss: 382.2259\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 336.1276 - val_loss: 160.2192\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 211.4505 - val_loss: 155.8576\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 211.3378 - val_loss: 256.0984\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 277.4020 - val_loss: 147.8937\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 208.6286 - val_loss: 142.4481\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 261.9668 - val_loss: 188.8606\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 414.8831 - val_loss: 302.0206\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 242.2031 - val_loss: 144.4887\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 220.2535 - val_loss: 150.6986\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 186.4142 - val_loss: 197.6862\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 199.8151 - val_loss: 538.7320\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 223.8793 - val_loss: 136.5107\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 203.3340 - val_loss: 142.5090\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 251.2213 - val_loss: 369.9326\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 203.2089 - val_loss: 137.6497\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 277.3493 - val_loss: 157.5120\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 218.0407 - val_loss: 310.3404\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 429.8951 - val_loss: 374.7691\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 276.0023 - val_loss: 197.4928\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 212.9268 - val_loss: 145.6965\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 209.4199 - val_loss: 150.8581\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 239.3895 - val_loss: 299.8380\n",
      "Epoch 292/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 234.1512 - val_loss: 143.6172\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 223.4294 - val_loss: 142.8542\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 200.0815 - val_loss: 143.0481\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 204.8526 - val_loss: 185.4846\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 256.4727 - val_loss: 136.5090\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 225.2419 - val_loss: 228.2531\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 409.6850 - val_loss: 202.7789\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 200.6266 - val_loss: 152.2595\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 194.4289 - val_loss: 306.3786\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 292.5829 - val_loss: 160.1359\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 211.3948 - val_loss: 345.9186\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 309.4564 - val_loss: 143.0636\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 210.4635 - val_loss: 138.6607\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 194.6436 - val_loss: 142.2032\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 190.7661 - val_loss: 442.2438\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 423.8668 - val_loss: 201.3713\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 283.4185 - val_loss: 192.2847\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 292.3160 - val_loss: 244.8090\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 252.6476 - val_loss: 188.3272\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 253.9951 - val_loss: 212.4992\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 247.2793 - val_loss: 222.8874\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 268.1292 - val_loss: 156.2944\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 219.5825 - val_loss: 161.8500\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 244.6726 - val_loss: 154.3065\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 248.8088 - val_loss: 169.8746\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 247.4451 - val_loss: 239.1725\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 235.6922 - val_loss: 173.3407\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 237.6229 - val_loss: 197.1772\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 245.3902 - val_loss: 168.6764\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 220.2637 - val_loss: 156.7676\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 247.2771 - val_loss: 190.2443\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 230.2901 - val_loss: 148.1519\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 248.6962 - val_loss: 180.5436\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 206.5119 - val_loss: 163.1369\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 225.6210 - val_loss: 173.9103\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 254.1604 - val_loss: 171.2904\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 262.5232 - val_loss: 157.2444\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 234.1140 - val_loss: 198.5890\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 224.7751 - val_loss: 217.2661\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 231.0799 - val_loss: 153.2240\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 205.9933 - val_loss: 166.7287\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 237.8711 - val_loss: 201.5563\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 212.1385 - val_loss: 689.2645\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 317.0047 - val_loss: 236.6445\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 190.1853 - val_loss: 146.6943\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 199.4392 - val_loss: 159.1916\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 198.5752 - val_loss: 192.6319\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 234.7003 - val_loss: 262.9971\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 207.6216 - val_loss: 140.6314\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 282.5191 - val_loss: 559.5176\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 287.6523 - val_loss: 192.9642\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 214.3084 - val_loss: 128.8360\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 199.5362 - val_loss: 151.8592\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 197.8281 - val_loss: 146.1176\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 189.4237 - val_loss: 145.9897\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 218.8395 - val_loss: 143.4227\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 214.8240 - val_loss: 150.9593\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 230.4292 - val_loss: 144.5764\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 242.2250 - val_loss: 278.7178\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 284.8137 - val_loss: 171.5787\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 178.1521 - val_loss: 135.5650\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 198.2618 - val_loss: 152.6454\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 231.2380 - val_loss: 147.1659\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 183.2110 - val_loss: 153.6727\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 193.5610 - val_loss: 194.0174\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 196.1855 - val_loss: 189.6594\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 216.2381 - val_loss: 530.1998\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 325.2313 - val_loss: 154.8436\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 211.6718 - val_loss: 147.1102\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 285.2598 - val_loss: 338.2170\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 229.9534 - val_loss: 155.2624\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 221.0728 - val_loss: 134.2085\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 259.9103 - val_loss: 338.8396\n",
      "Epoch 365/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 72us/step - loss: 234.4633 - val_loss: 169.3256\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 230.6171 - val_loss: 149.1518\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 216.9159 - val_loss: 158.4830\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 231.7129 - val_loss: 188.9396\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 239.3201 - val_loss: 171.0502\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 237.9850 - val_loss: 164.9356\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 227.7453 - val_loss: 183.3865\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 223.9621 - val_loss: 136.1936\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 247.8700 - val_loss: 222.9176\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 219.0175 - val_loss: 361.0595\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 321.0611 - val_loss: 179.6183\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 195.0156 - val_loss: 135.3337\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 221.2798 - val_loss: 170.7540\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 210.7903 - val_loss: 161.1184\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 206.7751 - val_loss: 205.5744\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 213.2132 - val_loss: 135.6366\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 197.0113 - val_loss: 196.1798\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 283.0771 - val_loss: 138.7523\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 203.881 - 1s 70us/step - loss: 202.8452 - val_loss: 136.4839\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 286.3618 - val_loss: 202.8735\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 209.6764 - val_loss: 129.1973\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 180.7920 - val_loss: 134.5306\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 180.5496 - val_loss: 178.2764\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 348.9741 - val_loss: 232.5365\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 250.8131 - val_loss: 126.7067\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 206.7442 - val_loss: 132.1323\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 339.5975 - val_loss: 186.6790\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 243.7540 - val_loss: 426.0565\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 262.9413 - val_loss: 171.2649\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 244.2518 - val_loss: 151.3209\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 206.4046 - val_loss: 152.4189\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 258.3537 - val_loss: 144.8296\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 248.7810 - val_loss: 181.0030\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 221.0443 - val_loss: 219.4551\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 202.0476 - val_loss: 229.8857\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 218.2298 - val_loss: 672.6015\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 228.1407 - val_loss: 338.8204\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 250.8825 - val_loss: 227.2290\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 230.8059 - val_loss: 365.1977\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 247.9236 - val_loss: 160.0822\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 250.3449 - val_loss: 143.3280\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 194.1980 - val_loss: 167.7385\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 205.4250 - val_loss: 281.7200\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 215.5179 - val_loss: 162.8356\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 200.7144 - val_loss: 141.6872\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 206.4456 - val_loss: 202.4490\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 211.6688 - val_loss: 167.2322\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 216.3646 - val_loss: 217.8873\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 225.9835 - val_loss: 173.7394\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 198.1935 - val_loss: 238.7262\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 351.7470 - val_loss: 202.1600\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 249.6464 - val_loss: 153.9249\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 234.2570 - val_loss: 138.4205\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 230.3025 - val_loss: 156.2825\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 209.1515 - val_loss: 134.9405\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 215.9675 - val_loss: 151.8215\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 213.4531 - val_loss: 163.2850\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 194.7643 - val_loss: 138.9451\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 300.5050 - val_loss: 197.0214\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 199.6562 - val_loss: 180.6418\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 217.0189 - val_loss: 202.6423\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 201.1014 - val_loss: 173.7058\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 181.1956 - val_loss: 178.8795\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 192.0518 - val_loss: 174.6249\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 223.7845 - val_loss: 136.6402\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 248.8045 - val_loss: 138.1472\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 219.9507 - val_loss: 148.4744\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 217.2800 - val_loss: 140.9097\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 266.2832 - val_loss: 265.5988\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 265.0147 - val_loss: 165.4879\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 253.9534 - val_loss: 154.2844\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 251.0111 - val_loss: 320.9736\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 229.9737 - val_loss: 128.4880\n",
      "Epoch 438/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 223.5977 - val_loss: 132.2729\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 196.5607 - val_loss: 129.5478\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 183.6682 - val_loss: 136.6026\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 183.6617 - val_loss: 153.0652\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 379.4468 - val_loss: 169.6514\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 202.2861 - val_loss: 192.9228\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 179.9697 - val_loss: 139.2094\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 213.1516 - val_loss: 162.9133\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 240.3507 - val_loss: 846.2849\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 292.2525 - val_loss: 174.2966\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 174.0322 - val_loss: 182.8270\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 193.4012 - val_loss: 125.5536\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 220.7818 - val_loss: 180.5918\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 235.1539 - val_loss: 144.6302\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 232.6085 - val_loss: 132.8443\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 190.4622 - val_loss: 128.8349\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 182.7966 - val_loss: 128.2854\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 224.8606 - val_loss: 260.9370\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 208.9805 - val_loss: 131.7242\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 219.3804 - val_loss: 152.0373\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 201.2291 - val_loss: 130.2614\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 189.9247 - val_loss: 161.6358\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 173.4685 - val_loss: 131.9555\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 287.9717 - val_loss: 201.5188\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 188.5285 - val_loss: 169.9664\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 211.1513 - val_loss: 137.8190\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 213.5466 - val_loss: 177.0595\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 213.1697 - val_loss: 217.0777\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 206.3257 - val_loss: 144.6548\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 278.6311 - val_loss: 140.7974\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 324.6715 - val_loss: 477.9920\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 182.4236 - val_loss: 134.9997\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 195.490 - 1s 71us/step - loss: 194.6734 - val_loss: 123.2317\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 199.1796 - val_loss: 181.4149\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 186.7952 - val_loss: 152.0841\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 206.5736 - val_loss: 196.8215\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 179.3321 - val_loss: 147.5481\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 188.0154 - val_loss: 234.4728\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 205.6682 - val_loss: 127.4956\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 198.7761 - val_loss: 146.4822\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 198.5765 - val_loss: 124.4483\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 222.9750 - val_loss: 204.7338\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 186.0721 - val_loss: 137.2912\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 188.1423 - val_loss: 131.8652\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 218.5108 - val_loss: 135.4172\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 179.3846 - val_loss: 130.1879\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 223.1889 - val_loss: 129.1694\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 184.9922 - val_loss: 129.9792\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 170.8367 - val_loss: 254.9302\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 188.3196 - val_loss: 217.6892\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 203.1787 - val_loss: 142.3704\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 183.3399 - val_loss: 156.2521\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 202.0224 - val_loss: 132.3771\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 183.6864 - val_loss: 137.5407\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 232.7616 - val_loss: 137.2347\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 222.3069 - val_loss: 142.7931\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 197.0571 - val_loss: 131.7914\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 191.6785 - val_loss: 125.8109\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 165.3674 - val_loss: 156.9193\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 212.8843 - val_loss: 137.0201\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 191.2871 - val_loss: 169.0935\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 232.8445 - val_loss: 137.3069\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 172.1661 - val_loss: 154.0329\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 184.8225 - val_loss: 133.2374\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 192.3425 - val_loss: 207.5378\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 186.0626 - val_loss: 211.9695\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 191.9241 - val_loss: 155.1601\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 186.9066 - val_loss: 145.0605\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 199.2174 - val_loss: 131.2189\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 234.3371 - val_loss: 330.0863\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 175.3674 - val_loss: 119.8518\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 170.9996 - val_loss: 190.7829\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 173.9113 - val_loss: 142.6646\n",
      "Epoch 511/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 68us/step - loss: 212.9695 - val_loss: 747.3119\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 251.0058 - val_loss: 153.2084\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 191.0588 - val_loss: 133.2467\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 210.7000 - val_loss: 149.6127\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 184.5495 - val_loss: 128.3081\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 177.0210 - val_loss: 128.3973\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 172.7524 - val_loss: 125.9706\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 174.6325 - val_loss: 168.6378\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 188.7102 - val_loss: 217.7504\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 201.3714 - val_loss: 213.5042\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 209.3366 - val_loss: 131.4912\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 187.5482 - val_loss: 160.3765\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 180.1987 - val_loss: 154.9569\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 173.8464 - val_loss: 134.1897\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 179.4789 - val_loss: 158.1708\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 183.2073 - val_loss: 128.6510\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 192.5312 - val_loss: 167.8272\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 192.0241 - val_loss: 161.7674\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 207.6089 - val_loss: 269.5233\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 211.2394 - val_loss: 129.0902\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 172.0623 - val_loss: 129.8404\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 164.5220 - val_loss: 139.0911\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 156.6257 - val_loss: 134.2456\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 186.2720 - val_loss: 139.9225\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 196.4393 - val_loss: 126.3893\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 203.5129 - val_loss: 133.5346\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 168.7166 - val_loss: 129.2570\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 194.3202 - val_loss: 170.4441\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 166.8631 - val_loss: 125.2321\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 184.5751 - val_loss: 151.5623\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 192.6531 - val_loss: 147.4769\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 231.0812 - val_loss: 133.2274\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 189.6635 - val_loss: 120.8622\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 185.8908 - val_loss: 148.3174\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 176.6626 - val_loss: 130.9352\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 195.1749 - val_loss: 298.4139\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 164.9914 - val_loss: 209.0883\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 168.4141 - val_loss: 135.7886\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 259.9841 - val_loss: 149.3974\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 196.2755 - val_loss: 180.8431\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 192.1826 - val_loss: 164.8901\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 171.6038 - val_loss: 143.7602\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 170.8270 - val_loss: 175.0497\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 201.5431 - val_loss: 170.8706\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 183.9780 - val_loss: 134.9475\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 171.3549 - val_loss: 181.5061\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 199.8865 - val_loss: 167.5274\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 195.3163 - val_loss: 134.1543\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 206.5621 - val_loss: 160.5495\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 172.5050 - val_loss: 157.8773\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 174.7322 - val_loss: 127.3152\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 297.3218 - val_loss: 369.6382\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 203.1417 - val_loss: 149.8791\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 165.7213 - val_loss: 126.6360\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 193.5190 - val_loss: 168.3856\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 162.1355 - val_loss: 124.8554\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 173.6848 - val_loss: 124.8909\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 187.7594 - val_loss: 206.2384\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 253.6523 - val_loss: 136.4917\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 176.3996 - val_loss: 136.6273\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 164.3623 - val_loss: 143.9323\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 192.7588 - val_loss: 183.0985\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 179.1935 - val_loss: 244.3400\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 216.8561 - val_loss: 124.2003\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 177.2106 - val_loss: 121.4139\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 175.2033 - val_loss: 155.8175\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 177.1594 - val_loss: 127.9881\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 190.4514 - val_loss: 158.8065\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 241.2333 - val_loss: 163.6987\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 197.2960 - val_loss: 132.3367\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 162.7069 - val_loss: 126.1313\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 170.5375 - val_loss: 126.8208\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 167.7471 - val_loss: 130.6818\n",
      "Epoch 584/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 70us/step - loss: 209.2550 - val_loss: 123.0467\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 182.4158 - val_loss: 168.9018\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 181.6624 - val_loss: 123.7322\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 189.9254 - val_loss: 141.8933\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 186.2984 - val_loss: 170.8825\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 180.7607 - val_loss: 148.4372\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 175.0540 - val_loss: 193.6762\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 165.3688 - val_loss: 124.4638\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 172.5485 - val_loss: 177.5024\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 195.1496 - val_loss: 171.8553\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 174.2864 - val_loss: 139.4804\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 178.5559 - val_loss: 128.5266\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 195.4005 - val_loss: 160.6681\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 164.3290 - val_loss: 162.5070\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 194.6876 - val_loss: 210.8588\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 198.9656 - val_loss: 138.0162\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 173.8626 - val_loss: 119.5707\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 170.1467 - val_loss: 149.5469\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 186.9106 - val_loss: 129.7468\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 161.8250 - val_loss: 203.4600\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 180.2269 - val_loss: 126.6060\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 179.9134 - val_loss: 127.2525\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 175.8384 - val_loss: 155.3374\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 182.1906 - val_loss: 164.8093\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 215.5362 - val_loss: 123.3179\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 168.4804 - val_loss: 123.7949\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 180.5036 - val_loss: 135.4206\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 175.8858 - val_loss: 135.6336\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 176.2273 - val_loss: 210.2148\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 207.9453 - val_loss: 120.3279\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 169.9937 - val_loss: 135.5202\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 162.5332 - val_loss: 143.0951\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 169.7124 - val_loss: 120.9676\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 181.9047 - val_loss: 143.3225\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 221.7262 - val_loss: 160.9596\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 200.2510 - val_loss: 224.6142\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 184.5245 - val_loss: 132.7485\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 189.5640 - val_loss: 163.2587\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 183.8171 - val_loss: 301.1953\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 194.4161 - val_loss: 177.1166\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 166.6227 - val_loss: 121.2980\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 159.9101 - val_loss: 193.5878\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 165.1583 - val_loss: 140.2325\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 160.3177 - val_loss: 186.9523\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 173.2820 - val_loss: 138.4653\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 172.7915 - val_loss: 130.1370\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 193.4475 - val_loss: 131.9299\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 184.6528 - val_loss: 127.2528\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 174.4067 - val_loss: 166.3755\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 172.3729 - val_loss: 206.5206\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 211.1111 - val_loss: 125.5314\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 160.1062 - val_loss: 208.4489\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 237.8435 - val_loss: 254.3180\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 198.1546 - val_loss: 129.0308\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 153.9244 - val_loss: 377.7028\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 176.7001 - val_loss: 134.2027\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 180.1455 - val_loss: 121.5098\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 171.2753 - val_loss: 218.2504\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 366.8072 - val_loss: 139.9016\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 214.3928 - val_loss: 133.7177\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 200.3534 - val_loss: 157.7519\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 206.5166 - val_loss: 123.3362\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 244.7879 - val_loss: 138.2175\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 194.2833 - val_loss: 136.1130\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 216.8343 - val_loss: 290.0610\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 219.9382 - val_loss: 158.7474\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 175.0550 - val_loss: 126.3768\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 181.8620 - val_loss: 447.4751\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 222.0098 - val_loss: 292.2954\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 221.7128 - val_loss: 195.3018\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 191.4059 - val_loss: 137.3895\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 190.9348 - val_loss: 178.0983\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 186.6544 - val_loss: 222.1715\n",
      "Epoch 657/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 73us/step - loss: 357.4690 - val_loss: 168.3565\n",
      "Epoch 658/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 202.3803 - val_loss: 122.4907\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 190.4659 - val_loss: 159.7562\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 206.3683 - val_loss: 162.8757\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 379.5215 - val_loss: 238.8305\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 192.3023 - val_loss: 128.8132\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 179.2159 - val_loss: 138.2276\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 189.4101 - val_loss: 143.6040\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 211.5491 - val_loss: 175.6348\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 243.0183 - val_loss: 146.1272\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 173.2841 - val_loss: 125.2919\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 242.3465 - val_loss: 144.5417\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 172.9782 - val_loss: 168.0789\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 177.0985 - val_loss: 122.6740\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 245.9215 - val_loss: 128.8183\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 175.9881 - val_loss: 182.1827\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 189.8328 - val_loss: 160.5936\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 190.1369 - val_loss: 141.8235\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 215.8067 - val_loss: 189.7321\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 174.6218 - val_loss: 129.9320\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 195.0001 - val_loss: 155.9372\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 184.7014 - val_loss: 126.9835\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 192.2835 - val_loss: 133.5713\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 201.7853 - val_loss: 185.6980\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 185.6440 - val_loss: 143.5604\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 241.3937 - val_loss: 136.4796\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 219.6364 - val_loss: 137.2143\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 173.2959 - val_loss: 126.4690\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 188.4653 - val_loss: 128.9922\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 173.0455 - val_loss: 141.0952\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 190.4329 - val_loss: 140.8207\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 204.9819 - val_loss: 168.2393\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 195.0942 - val_loss: 122.9135\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 254.1470 - val_loss: 579.1580\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 211.1091 - val_loss: 134.2703\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 165.0229 - val_loss: 127.4795\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 186.5801 - val_loss: 153.5065\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 181.0039 - val_loss: 191.5502\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 208.7256 - val_loss: 158.3632\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 183.4481 - val_loss: 144.1366\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 189.9796 - val_loss: 118.4647\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 196.9784 - val_loss: 134.6098\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 181.2644 - val_loss: 127.5674\n",
      "Epoch 700/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 187.4449 - val_loss: 136.0237\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 215.9959 - val_loss: 140.0272\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 172.7206 - val_loss: 132.8281\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 185.4811 - val_loss: 121.9353\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 179.7066 - val_loss: 193.9134\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 205.6427 - val_loss: 147.9889\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 184.8720 - val_loss: 129.9624\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 183.9577 - val_loss: 140.0974- ETA: 0s - loss: \n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 164.9170 - val_loss: 128.1335\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 172.1434 - val_loss: 166.0106\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 225.5339 - val_loss: 141.0216\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 243.9748 - val_loss: 195.8715\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 179.4237 - val_loss: 180.4741\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 214.9477 - val_loss: 135.2539\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 175.7735 - val_loss: 147.2084\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 174.4019 - val_loss: 120.6379\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 173.8373 - val_loss: 138.6847\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 174.4727 - val_loss: 130.1069\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 200.8669 - val_loss: 182.8172\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 206.0089 - val_loss: 223.8550\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 178.5784 - val_loss: 177.6087\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 189.2156 - val_loss: 125.2441\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 180.7952 - val_loss: 229.6014\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 174.8019 - val_loss: 125.1847\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 202.6229 - val_loss: 134.7564\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 269.1974 - val_loss: 133.2079\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 170.8036 - val_loss: 128.1838\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 169.0908 - val_loss: 175.0119\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 176.4033 - val_loss: 133.6417\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 245.7940 - val_loss: 148.7128\n",
      "Epoch 730/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 68us/step - loss: 167.6339 - val_loss: 208.3246\n",
      "Epoch 731/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 179.8660 - val_loss: 118.2310\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 176.8633 - val_loss: 119.3575\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 208.2199 - val_loss: 119.1276\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 158.7647 - val_loss: 125.0999\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 169.3468 - val_loss: 137.5205\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 200.6185 - val_loss: 168.3506\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 204.3989 - val_loss: 120.8532\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 187.9938 - val_loss: 120.3104\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 163.9925 - val_loss: 127.6962\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 201.7554 - val_loss: 168.1600\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 373.3154 - val_loss: 158.6627\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 193.7443 - val_loss: 133.0676\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 174.5108 - val_loss: 124.4168\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 188.8613 - val_loss: 194.5691\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 202.7975 - val_loss: 150.9742\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 201.2349 - val_loss: 262.3798\n",
      "Epoch 747/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 210.9165 - val_loss: 140.4025\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 187.0622 - val_loss: 125.5163\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 188.3098 - val_loss: 144.0121\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 209.5706 - val_loss: 122.5983\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 186.6858 - val_loss: 252.7724\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 208.5199 - val_loss: 165.5129\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 179.9902 - val_loss: 209.0988\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 205.4512 - val_loss: 158.4320\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 186.3540 - val_loss: 122.4620\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 165.9994 - val_loss: 139.7456\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 202.7089 - val_loss: 126.0647\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 167.5627 - val_loss: 145.6859\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 199.7134 - val_loss: 215.7520\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 152.3665 - val_loss: 122.6095\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 175.9421 - val_loss: 125.8624\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 217.1028 - val_loss: 134.3318\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 172.3540 - val_loss: 149.0299\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 197.8075 - val_loss: 128.3010\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 163.8800 - val_loss: 124.1369\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 166.6754 - val_loss: 124.8312\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 161.2955 - val_loss: 177.9613\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 169.5715 - val_loss: 219.9905\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 180.9452 - val_loss: 128.1702\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 161.4073 - val_loss: 190.6434\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 235.1588 - val_loss: 123.3731\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 194.9325 - val_loss: 152.8431\n",
      "Epoch 773/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 186.4104 - val_loss: 130.5603\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 172.6307 - val_loss: 141.1929\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 183.1330 - val_loss: 118.3256\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 259.6365 - val_loss: 119.1713\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 169.3695 - val_loss: 121.4978\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 173.1655 - val_loss: 124.7153\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 175.8800 - val_loss: 153.6408\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 182.0276 - val_loss: 166.7922\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 213.2278 - val_loss: 131.0796\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 159.2651 - val_loss: 128.9378\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 263.9677 - val_loss: 130.1984\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 172.6900 - val_loss: 148.7348\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 183.0452 - val_loss: 125.4149\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 205.1872 - val_loss: 130.5166\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 179.2069 - val_loss: 144.3234\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 302.1600 - val_loss: 134.7254\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 165.3130 - val_loss: 177.5351\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 187.1134 - val_loss: 131.8870\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 166.8052 - val_loss: 181.4026\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 198.2404 - val_loss: 125.1251\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 186.5395 - val_loss: 117.5195\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 164.1192 - val_loss: 208.9364\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 177.7198 - val_loss: 134.2699\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 179.4180 - val_loss: 125.5992\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 163.5188 - val_loss: 153.2791\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 191.7793 - val_loss: 133.3779\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 204.4675 - val_loss: 123.7883\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 177.6486 - val_loss: 201.8585\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 147.6464 - val_loss: 134.3817\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 331.6142 - val_loss: 142.8176\n",
      "Epoch 803/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 68us/step - loss: 228.2888 - val_loss: 272.8620\n",
      "Epoch 804/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 184.5297 - val_loss: 125.1001\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 163.3092 - val_loss: 120.1067\n",
      "Epoch 806/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 165.7648 - val_loss: 153.9844\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 166.3168 - val_loss: 151.6815\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 162.4676 - val_loss: 135.4525\n",
      "Epoch 809/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 163.7899 - val_loss: 136.0268\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 168.8737 - val_loss: 138.3099\n",
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 167.6348 - val_loss: 129.6617\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 176.4134 - val_loss: 140.5796\n",
      "Epoch 813/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 165.8963 - val_loss: 134.4821\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 165.7561 - val_loss: 126.8534\n",
      "Epoch 815/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 350.1346 - val_loss: 219.7405\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 218.3561 - val_loss: 192.7869\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 166.6115 - val_loss: 127.7513\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 164.2358 - val_loss: 132.7673\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 162.2651 - val_loss: 169.7479\n",
      "Epoch 820/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 162.3745 - val_loss: 204.2136\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 178.5947 - val_loss: 146.6706\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 168.6120 - val_loss: 120.3414\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 161.4188 - val_loss: 126.5306\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 166.7608 - val_loss: 126.5679\n",
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 176.6928 - val_loss: 142.0115\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 201.3733 - val_loss: 128.7562\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 199.5793 - val_loss: 133.1898\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 208.0880 - val_loss: 125.4298\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 168.2622 - val_loss: 150.7405\n",
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 158.6989 - val_loss: 132.3890\n",
      "Epoch 831/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 155.8574 - val_loss: 151.2682\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 172.4086 - val_loss: 131.9100\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 226.5308 - val_loss: 131.1557\n",
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 189.3722 - val_loss: 128.2742\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 173.8633 - val_loss: 136.7914\n",
      "Epoch 836/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 202.3412 - val_loss: 212.8772\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 180.9411 - val_loss: 141.4960\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 162.0558 - val_loss: 124.9214\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 172.1915 - val_loss: 180.5173\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 269.4192 - val_loss: 143.2694\n",
      "Epoch 841/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 159.0408 - val_loss: 144.3686\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 169.7830 - val_loss: 127.3303\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 175.5810 - val_loss: 119.1619\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 193.5947 - val_loss: 139.6439\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 167.2791 - val_loss: 130.6280\n",
      "Epoch 846/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 169.2633 - val_loss: 125.0445\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 323.7588 - val_loss: 126.3436\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 160.0623 - val_loss: 127.9721\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 157.8135 - val_loss: 170.2020\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 160.4161 - val_loss: 124.6468\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 177.4111 - val_loss: 184.2265\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 167.8407 - val_loss: 264.9780\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 185.6381 - val_loss: 140.4793\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 197.6330 - val_loss: 144.9717\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 163.5427 - val_loss: 174.7403\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 176.7790 - val_loss: 137.7085\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 172.0410 - val_loss: 135.9687\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 183.6741 - val_loss: 123.7647\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 170.8693 - val_loss: 146.5217\n",
      "Epoch 860/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 160.7184 - val_loss: 232.1056\n",
      "Epoch 861/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 180.5288 - val_loss: 157.7528\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 194.9913 - val_loss: 130.7934\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 162.2944 - val_loss: 124.4003\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 154.9777 - val_loss: 133.1551\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 160.8512 - val_loss: 117.3733\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 196.7522 - val_loss: 143.0106\n",
      "Epoch 867/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 166.0380 - val_loss: 127.8778\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 174.3590 - val_loss: 116.1971\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 180.3578 - val_loss: 132.1187\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 161.0493 - val_loss: 126.2894\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 162.0508 - val_loss: 131.2911\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 178.6020 - val_loss: 133.5020\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 175.2412 - val_loss: 119.6160\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 160.2724 - val_loss: 138.9349\n",
      "Epoch 875/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 151.4469 - val_loss: 138.9292\n",
      "Epoch 876/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 72us/step - loss: 304.8439 - val_loss: 224.4666\n",
      "Epoch 877/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 177.0032 - val_loss: 122.5176\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 180.7875 - val_loss: 127.6937\n",
      "Epoch 879/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 164.2157 - val_loss: 123.6730\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 163.1286 - val_loss: 162.8754\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 207.8971 - val_loss: 130.6540\n",
      "Epoch 882/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 157.7340 - val_loss: 128.3430\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 165.9514 - val_loss: 117.2955\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 162.5014 - val_loss: 122.0343\n",
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 190.5346 - val_loss: 131.6208\n",
      "Epoch 886/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 162.5210 - val_loss: 124.6626\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 175.0103 - val_loss: 198.5218\n",
      "Epoch 888/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 169.5405 - val_loss: 122.3958\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 149.846 - 1s 70us/step - loss: 149.1145 - val_loss: 132.6753\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 155.9979 - val_loss: 173.6756\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 233.8900 - val_loss: 201.8726\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 167.4575 - val_loss: 127.9177\n",
      "Epoch 893/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 216.5256 - val_loss: 129.5414\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 245.3309 - val_loss: 124.9177\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 155.8683 - val_loss: 124.4508\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 183.3063 - val_loss: 137.6252\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 177.4836 - val_loss: 120.5425\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 166.5473 - val_loss: 148.1518\n",
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 155.7450 - val_loss: 119.5868\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 158.7316 - val_loss: 127.6390\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 206.0064 - val_loss: 240.5169\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 172.4418 - val_loss: 120.2447\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 174.5956 - val_loss: 147.0705\n",
      "Epoch 904/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 164.7038 - val_loss: 217.9111\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 163.4767 - val_loss: 126.2147\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 218.6631 - val_loss: 125.6986\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 165.6511 - val_loss: 126.7024\n",
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 154.2357 - val_loss: 160.7941\n",
      "Epoch 909/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 173.7160 - val_loss: 123.0000\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 298.3451 - val_loss: 128.7251\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 177.9050 - val_loss: 134.7344\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 172.3504 - val_loss: 118.3487\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 160.9136 - val_loss: 186.4880\n",
      "Epoch 914/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 169.0237 - val_loss: 129.7104\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 168.5521 - val_loss: 134.1105\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 171.7435 - val_loss: 114.5295\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 195.1592 - val_loss: 149.2027\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 157.5739 - val_loss: 175.2204\n",
      "Epoch 919/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 166.4059 - val_loss: 131.9996\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 165.9209 - val_loss: 128.2282\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 156.3400 - val_loss: 221.9313\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 172.8613 - val_loss: 174.3576\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 168.6309 - val_loss: 119.9047\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 253.5473 - val_loss: 126.2546\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 197.2138 - val_loss: 145.7776\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 211.4688 - val_loss: 124.1042\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 156.4440 - val_loss: 118.6882\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 191.5608 - val_loss: 128.0680\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 164.1751 - val_loss: 120.1497\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 166.6408 - val_loss: 143.9360\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 184.9976 - val_loss: 143.8895\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 159.3672 - val_loss: 121.8096\n",
      "Epoch 933/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 167.7266 - val_loss: 158.6309\n",
      "Epoch 934/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 184.3961 - val_loss: 127.8107\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 161.5253 - val_loss: 139.4510\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 174.5394 - val_loss: 116.3872\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 162.0932 - val_loss: 156.1948\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 170.0664 - val_loss: 164.0273\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 198.9100 - val_loss: 205.6708\n",
      "Epoch 940/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 176.8684 - val_loss: 147.4323\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 190.8090 - val_loss: 137.1585\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 165.4479 - val_loss: 131.2325\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 173.1547 - val_loss: 142.7538\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 178.4844 - val_loss: 141.8027\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 182.2943 - val_loss: 185.7168\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 182.9028 - val_loss: 126.0815\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 295.0399 - val_loss: 162.0336\n",
      "Epoch 948/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 164.6037 - val_loss: 120.0752\n",
      "Epoch 949/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 70us/step - loss: 147.0203 - val_loss: 134.3194\n",
      "Epoch 950/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 161.5185 - val_loss: 124.4182\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 182.7276 - val_loss: 151.4041\n",
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 188.0823 - val_loss: 301.6328\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 212.6893 - val_loss: 146.6047\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 168.9661 - val_loss: 119.4191\n",
      "Epoch 955/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 164.5915 - val_loss: 370.8588\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 170.4938 - val_loss: 160.7715\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 168.7946 - val_loss: 117.0844\n",
      "Epoch 958/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 162.8170 - val_loss: 139.4915\n",
      "Epoch 959/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 188.2139 - val_loss: 126.1253\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 156.9472 - val_loss: 137.8708\n",
      "Epoch 961/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 212.1625 - val_loss: 232.7122\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 165.0631 - val_loss: 124.0697\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 153.6369 - val_loss: 146.8344\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 187.3423 - val_loss: 128.7523\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 174.0502 - val_loss: 466.4787\n",
      "Epoch 966/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 171.2035 - val_loss: 149.7818\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 183.1522 - val_loss: 126.2412\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 174.2418 - val_loss: 135.3549\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 173.0472 - val_loss: 120.2280\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 154.7624 - val_loss: 130.9137\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 169.4344 - val_loss: 155.4189\n",
      "Epoch 972/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 241.2903 - val_loss: 130.3102\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 185.4578 - val_loss: 137.2595\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 170.2783 - val_loss: 120.8277\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 147.5309 - val_loss: 124.9794\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 161.9014 - val_loss: 155.4841\n",
      "Epoch 977/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 299.5629 - val_loss: 132.7407\n",
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 182.4604 - val_loss: 159.6445\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 156.5734 - val_loss: 123.4284\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 148.7984 - val_loss: 144.7059\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 165.9834 - val_loss: 160.3445\n",
      "Epoch 982/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 196.5531 - val_loss: 194.9725\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 165.4703 - val_loss: 130.6342\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 156.7635 - val_loss: 116.6744\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 175.3942 - val_loss: 592.0505\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 159.7292 - val_loss: 141.7638\n",
      "Epoch 987/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 153.6647 - val_loss: 125.8622\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 150.7617 - val_loss: 122.7163\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 182.7816 - val_loss: 149.7293\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 162.6599 - val_loss: 139.7580\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 171.4848 - val_loss: 133.4502\n",
      "Epoch 992/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 143.6613 - val_loss: 119.7859\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 171.1155 - val_loss: 137.9388\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 183.3695 - val_loss: 225.1564\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 168.1693 - val_loss: 130.8519\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 151.0158 - val_loss: 211.9639\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 202.6458 - val_loss: 209.4787\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 199.3087 - val_loss: 238.9994\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 150.0963 - val_loss: 144.2480\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 158.9713 - val_loss: 133.8148\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 160.7402 - val_loss: 123.1634\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 174.6288 - val_loss: 126.1831\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 182.7621 - val_loss: 136.6510\n",
      "Epoch 1004/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 163.8495 - val_loss: 130.1780\n",
      "Epoch 1005/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 156.1882 - val_loss: 162.9270\n",
      "Epoch 1006/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 156.7132 - val_loss: 127.4948\n",
      "Epoch 1007/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 186.1329 - val_loss: 331.3509\n",
      "Epoch 1008/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 178.9015 - val_loss: 115.4371\n",
      "Epoch 1009/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 177.1816 - val_loss: 316.8268\n",
      "Epoch 1010/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 189.3625 - val_loss: 128.5755\n",
      "Epoch 1011/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 158.9159 - val_loss: 243.8219\n",
      "Epoch 1012/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 161.1555 - val_loss: 115.8400\n",
      "Epoch 1013/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 218.9594 - val_loss: 160.7517\n",
      "Epoch 1014/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 252.6829 - val_loss: 162.0465\n",
      "Epoch 1015/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 185.9729 - val_loss: 157.2796\n",
      "Epoch 1016/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 166.7062 - val_loss: 123.7311\n",
      "Epoch 1017/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 176.1146 - val_loss: 126.1042\n",
      "Epoch 1018/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 155.8318 - val_loss: 144.4332\n",
      "Epoch 1019/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 175.7901 - val_loss: 118.8308\n",
      "Epoch 1020/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 157.9148 - val_loss: 122.4398\n",
      "Epoch 1021/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 162.2766 - val_loss: 161.7837\n",
      "Epoch 1022/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 178.3773 - val_loss: 121.1904\n",
      "Epoch 1023/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 173.0659 - val_loss: 141.8804\n",
      "Epoch 1024/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 158.1371 - val_loss: 142.2613\n",
      "Epoch 1025/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 166.0457 - val_loss: 132.0593\n",
      "Epoch 1026/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 154.8642 - val_loss: 133.9834\n",
      "Epoch 1027/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 158.4373 - val_loss: 125.4558\n",
      "Epoch 1028/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 181.3696 - val_loss: 167.5641\n",
      "Epoch 1029/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 216.7469 - val_loss: 123.4858\n",
      "Epoch 1030/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 173.4874 - val_loss: 149.2800\n",
      "Epoch 1031/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 167.0145 - val_loss: 122.2962\n",
      "Epoch 1032/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 154.8267 - val_loss: 129.7526\n",
      "Epoch 1033/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 157.5476 - val_loss: 189.2899\n",
      "Epoch 1034/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 156.7696 - val_loss: 128.6678\n",
      "Epoch 1035/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 149.5542 - val_loss: 136.8288\n",
      "Epoch 1036/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 168.8345 - val_loss: 115.6923\n",
      "Epoch 1037/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 159.9831 - val_loss: 158.9420\n",
      "Epoch 1038/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 170.7514 - val_loss: 129.4997\n",
      "Epoch 1039/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 163.9260 - val_loss: 137.4335\n",
      "Epoch 1040/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 159.9985 - val_loss: 132.0401\n",
      "Epoch 1041/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 144.4266 - val_loss: 118.2613\n",
      "Epoch 1042/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 201.0258 - val_loss: 121.2591\n",
      "Epoch 1043/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 170.7426 - val_loss: 135.3787\n",
      "Epoch 1044/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.9592 - val_loss: 139.4628\n",
      "Epoch 1045/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 161.4039 - val_loss: 120.1542\n",
      "Epoch 1046/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 179.4130 - val_loss: 125.7502\n",
      "Epoch 1047/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 195.7606 - val_loss: 139.4318\n",
      "Epoch 1048/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 170.5869 - val_loss: 118.6509\n",
      "Epoch 1049/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 154.0898 - val_loss: 127.7585\n",
      "Epoch 1050/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 197.0860 - val_loss: 128.5917\n",
      "Epoch 1051/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 198.7173 - val_loss: 147.3389\n",
      "Epoch 1052/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 178.6629 - val_loss: 139.1553\n",
      "Epoch 1053/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 152.919 - 1s 71us/step - loss: 151.3979 - val_loss: 142.0523\n",
      "Epoch 1054/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 151.5746 - val_loss: 135.7764\n",
      "Epoch 1055/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 159.9837 - val_loss: 128.3325\n",
      "Epoch 1056/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 152.0063 - val_loss: 158.4370\n",
      "Epoch 1057/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 161.8600 - val_loss: 116.6365\n",
      "Epoch 1058/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 156.7003 - val_loss: 116.5518\n",
      "Epoch 1059/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 150.7524 - val_loss: 122.5595\n",
      "Epoch 1060/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 167.0161 - val_loss: 169.5923\n",
      "Epoch 1061/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 189.0357 - val_loss: 121.2307\n",
      "Epoch 1062/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 209.1996 - val_loss: 506.2078\n",
      "Epoch 1063/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 210.0296 - val_loss: 159.3612\n",
      "Epoch 1064/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 141.2066 - val_loss: 137.6844\n",
      "Epoch 1065/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 166.7254 - val_loss: 569.7684\n",
      "Epoch 1066/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 209.6293 - val_loss: 134.0569\n",
      "Epoch 1067/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.0277 - val_loss: 119.3824\n",
      "Epoch 1068/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 167.2030 - val_loss: 150.1683\n",
      "Epoch 1069/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 157.8161 - val_loss: 142.5483\n",
      "Epoch 1070/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.1928 - val_loss: 122.6955\n",
      "Epoch 1071/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 173.5528 - val_loss: 127.7516\n",
      "Epoch 1072/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 151.0882 - val_loss: 150.6815\n",
      "Epoch 1073/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 160.9054 - val_loss: 141.1823\n",
      "Epoch 1074/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 210.8319 - val_loss: 144.1079\n",
      "Epoch 1075/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 156.3463 - val_loss: 120.5315\n",
      "Epoch 1076/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 160.6022 - val_loss: 134.3940\n",
      "Epoch 1077/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 149.5766 - val_loss: 133.9236\n",
      "Epoch 1078/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 182.8676 - val_loss: 163.8723\n",
      "Epoch 1079/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 151.6304 - val_loss: 118.9130\n",
      "Epoch 1080/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 159.5836 - val_loss: 134.0922\n",
      "Epoch 1081/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 170.5419 - val_loss: 120.5983\n",
      "Epoch 1082/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 147.9689 - val_loss: 117.8523\n",
      "Epoch 1083/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 151.0341 - val_loss: 129.1525\n",
      "Epoch 1084/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 264.4364 - val_loss: 250.5343\n",
      "Epoch 1085/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 157.6742 - val_loss: 122.9676\n",
      "Epoch 1086/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 151.2750 - val_loss: 120.4000\n",
      "Epoch 1087/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.5942 - val_loss: 121.4092\n",
      "Epoch 1088/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 210.0742 - val_loss: 121.7630\n",
      "Epoch 1089/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 162.9667 - val_loss: 292.8778\n",
      "Epoch 1090/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 151.4539 - val_loss: 116.2499\n",
      "Epoch 1091/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 160.5799 - val_loss: 125.2402\n",
      "Epoch 1092/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 148.5610 - val_loss: 163.1538\n",
      "Epoch 1093/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 168.7666 - val_loss: 129.7941\n",
      "Epoch 1094/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 250.4754 - val_loss: 122.5859\n",
      "Epoch 1095/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 148.4625 - val_loss: 122.3355\n",
      "Epoch 1096/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 147.5096 - val_loss: 139.8681\n",
      "Epoch 1097/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 159.5656 - val_loss: 116.6351\n",
      "Epoch 1098/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 155.8095 - val_loss: 133.5660\n",
      "Epoch 1099/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 148.9844 - val_loss: 118.5361\n",
      "Epoch 1100/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 162.6620 - val_loss: 126.0513\n",
      "Epoch 1101/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 158.3636 - val_loss: 124.5935\n",
      "Epoch 1102/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 159.2259 - val_loss: 135.3740\n",
      "Epoch 1103/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 164.0273 - val_loss: 145.3358\n",
      "Epoch 1104/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 171.7247 - val_loss: 119.8232\n",
      "Epoch 1105/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 151.7411 - val_loss: 129.5745\n",
      "Epoch 1106/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 172.1962 - val_loss: 136.9124\n",
      "Epoch 1107/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 163.3773 - val_loss: 135.2559\n",
      "Epoch 1108/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 188.5324 - val_loss: 135.1774\n",
      "Epoch 1109/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 160.9926 - val_loss: 120.3145\n",
      "Epoch 1110/10000\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 152.0649 - val_loss: 129.8026\n",
      "Epoch 1111/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 157.3328 - val_loss: 120.0593\n",
      "Epoch 1112/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 160.6463 - val_loss: 118.4790\n",
      "Epoch 1113/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 151.0423 - val_loss: 121.8686\n",
      "Epoch 1114/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 159.9286 - val_loss: 255.7319\n",
      "Epoch 1115/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 149.7418 - val_loss: 135.0514\n",
      "Epoch 1116/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 251.9753 - val_loss: 124.7532\n",
      "Epoch 1117/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 148.6927 - val_loss: 118.0094\n",
      "Epoch 1118/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 371.3930 - val_loss: 225.7310\n",
      "Epoch 1119/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 177.4561 - val_loss: 142.7304\n",
      "Epoch 1120/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 140.7176 - val_loss: 122.0065\n",
      "Epoch 1121/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 142.5305 - val_loss: 119.5372\n",
      "Epoch 1122/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 140.4846 - val_loss: 123.7095\n",
      "Epoch 1123/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 148.4207 - val_loss: 126.5637\n",
      "Epoch 1124/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 148.6226 - val_loss: 118.4838\n",
      "Epoch 1125/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 142.6283 - val_loss: 124.3461\n",
      "Epoch 1126/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 156.9925 - val_loss: 116.4009\n",
      "Epoch 1127/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 148.7483 - val_loss: 131.3328\n",
      "Epoch 1128/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.6655 - val_loss: 143.3568\n",
      "Epoch 1129/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 161.1488 - val_loss: 119.9783\n",
      "Epoch 1130/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 222.7849 - val_loss: 844.7929\n",
      "Epoch 1131/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 200.2939 - val_loss: 119.2380\n",
      "Epoch 1132/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 150.1617 - val_loss: 144.8661\n",
      "Epoch 1133/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 149.3380 - val_loss: 140.1460\n",
      "Epoch 1134/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 184.8475 - val_loss: 123.2077\n",
      "Epoch 1135/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 152.0325 - val_loss: 121.4481\n",
      "Epoch 1136/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 158.2731 - val_loss: 122.4871\n",
      "Epoch 1137/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 165.2766 - val_loss: 352.0958\n",
      "Epoch 1138/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 376.7358 - val_loss: 485.9031\n",
      "Epoch 1139/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 167.4888 - val_loss: 128.7477\n",
      "Epoch 1140/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 162.8473 - val_loss: 139.1109\n",
      "Epoch 1141/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 141.8728 - val_loss: 118.2298\n",
      "Epoch 1142/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 151.4477 - val_loss: 129.5735\n",
      "Epoch 1143/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 175.5938 - val_loss: 159.6789\n",
      "Epoch 1144/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 194.0068 - val_loss: 120.4759\n",
      "Epoch 1145/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 160.9220 - val_loss: 134.3987\n",
      "Epoch 1146/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 176.9485 - val_loss: 154.5235\n",
      "Epoch 1147/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 157.3625 - val_loss: 123.9270\n",
      "Epoch 1148/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 147.0921 - val_loss: 118.9441\n",
      "Epoch 1149/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 152.9857 - val_loss: 121.8497\n",
      "Epoch 1150/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 165.8312 - val_loss: 120.4275\n",
      "Epoch 1151/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 176.4096 - val_loss: 203.9667\n",
      "Epoch 1152/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 158.0867 - val_loss: 119.7511\n",
      "Epoch 1153/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 163.2390 - val_loss: 126.0007\n",
      "Epoch 1154/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 237.1199 - val_loss: 154.4107\n",
      "Epoch 1155/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 155.7575 - val_loss: 120.2357\n",
      "Epoch 1156/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 142.8452 - val_loss: 116.2627\n",
      "Epoch 1157/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 142.6137 - val_loss: 153.0729\n",
      "Epoch 1158/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 183.9435 - val_loss: 153.1455\n",
      "Epoch 1159/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 165.9270 - val_loss: 127.0007\n",
      "Epoch 1160/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 180.5310 - val_loss: 133.5519\n",
      "Epoch 1161/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 156.9681 - val_loss: 171.0236\n",
      "Epoch 1162/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 168.2675 - val_loss: 116.0602\n",
      "Epoch 1163/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 140.0702 - val_loss: 115.8229\n",
      "Epoch 1164/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 161.8174 - val_loss: 175.2864\n",
      "Epoch 1165/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 179.4838 - val_loss: 126.7980\n",
      "Epoch 1166/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 69us/step - loss: 152.0624 - val_loss: 122.3077\n",
      "Epoch 1167/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 180.8393 - val_loss: 167.9480\n",
      "Epoch 1168/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 159.2694 - val_loss: 118.3236\n",
      "Epoch 1169/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 148.1883 - val_loss: 164.5918\n",
      "Epoch 1170/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 180.7537 - val_loss: 119.3517\n",
      "Epoch 1171/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 153.6390 - val_loss: 145.2222\n",
      "Epoch 1172/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 143.8842 - val_loss: 136.4990\n",
      "Epoch 1173/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 186.4178 - val_loss: 220.7053\n",
      "Epoch 1174/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 200.9878 - val_loss: 189.4288\n",
      "Epoch 1175/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 196.5452 - val_loss: 120.4642\n",
      "Epoch 1176/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 149.9932 - val_loss: 157.3098\n",
      "Epoch 1177/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 151.6637 - val_loss: 115.1684\n",
      "Epoch 1178/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 144.4454 - val_loss: 148.6244\n",
      "Epoch 1179/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 344.9205 - val_loss: 130.1049\n",
      "Epoch 1180/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 163.6690 - val_loss: 138.2675\n",
      "Epoch 1181/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 166.9695 - val_loss: 123.4630\n",
      "Epoch 1182/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 147.8618 - val_loss: 116.6099\n",
      "Epoch 1183/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 195.2095 - val_loss: 119.1645\n",
      "Epoch 1184/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 157.3149 - val_loss: 125.3496\n",
      "Epoch 1185/10000\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 150.7108 - val_loss: 116.2879\n",
      "Epoch 1186/10000\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 155.3750 - val_loss: 184.9092\n",
      "Epoch 1187/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 159.7371 - val_loss: 117.4424\n",
      "Epoch 1188/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 154.6459 - val_loss: 118.4525\n",
      "Epoch 1189/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 146.8906 - val_loss: 127.3166\n",
      "Epoch 1190/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 139.6917 - val_loss: 126.3438\n",
      "Epoch 1191/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 156.0523 - val_loss: 164.0382\n",
      "Epoch 1192/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 158.0500 - val_loss: 129.0708\n",
      "Epoch 1193/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 156.1323 - val_loss: 117.3727\n",
      "Epoch 1194/10000\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 162.4657 - val_loss: 161.5317\n",
      "Epoch 1195/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 155.1384 - val_loss: 191.8532\n",
      "Epoch 1196/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 150.2213 - val_loss: 151.2586\n",
      "Epoch 1197/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 173.4334 - val_loss: 133.1662\n",
      "Epoch 1198/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 150.6936 - val_loss: 138.9098\n",
      "Epoch 1199/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 157.2836 - val_loss: 274.8739\n",
      "Epoch 1200/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 158.9190 - val_loss: 116.6187\n",
      "Epoch 1201/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 159.0094 - val_loss: 127.5005\n",
      "Epoch 1202/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 156.1579 - val_loss: 115.2036\n",
      "Epoch 1203/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 145.9364 - val_loss: 129.7111\n",
      "Epoch 1204/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 146.5338 - val_loss: 122.5401\n",
      "Epoch 1205/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 172.2224 - val_loss: 143.5823\n",
      "Epoch 1206/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 161.6190 - val_loss: 117.1863\n",
      "Epoch 1207/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 248.9777 - val_loss: 120.5072\n",
      "Epoch 1208/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 143.9847 - val_loss: 122.5246\n",
      "Epoch 1209/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 204.0262 - val_loss: 216.3036\n",
      "Epoch 1210/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 191.1487 - val_loss: 147.0870\n",
      "Epoch 1211/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 156.2337 - val_loss: 113.0707\n",
      "Epoch 1212/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 164.8270 - val_loss: 136.7285\n",
      "Epoch 1213/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 161.7695 - val_loss: 129.7676\n",
      "Epoch 1214/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 148.2575 - val_loss: 120.0502\n",
      "Epoch 1215/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.7131 - val_loss: 121.2132\n",
      "Epoch 1216/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 153.5027 - val_loss: 119.1261\n",
      "Epoch 1217/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 166.3564 - val_loss: 137.6648\n",
      "Epoch 1218/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 153.613 - 1s 70us/step - loss: 154.8171 - val_loss: 183.3197\n",
      "Epoch 1219/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 179.6043 - val_loss: 131.8536\n",
      "Epoch 1220/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 164.8508 - val_loss: 120.3415\n",
      "Epoch 1221/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 162.968 - 1s 70us/step - loss: 158.5586 - val_loss: 160.0474\n",
      "Epoch 1222/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 160.3310 - val_loss: 187.3452\n",
      "Epoch 1223/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 188.2355 - val_loss: 131.1479\n",
      "Epoch 1224/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 202.6639 - val_loss: 125.1478\n",
      "Epoch 1225/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 227.4562 - val_loss: 242.9047\n",
      "Epoch 1226/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 182.6408 - val_loss: 116.6575\n",
      "Epoch 1227/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 157.8562 - val_loss: 115.7567\n",
      "Epoch 1228/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 147.8341 - val_loss: 126.9786\n",
      "Epoch 1229/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 177.5161 - val_loss: 131.7461\n",
      "Epoch 1230/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 179.6150 - val_loss: 432.5222\n",
      "Epoch 1231/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 160.2469 - val_loss: 115.8830\n",
      "Epoch 1232/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 153.0737 - val_loss: 122.8481\n",
      "Epoch 1233/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 152.3436 - val_loss: 175.7389\n",
      "Epoch 1234/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 180.3334 - val_loss: 143.3466\n",
      "Epoch 1235/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 156.6296 - val_loss: 125.8141\n",
      "Epoch 1236/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 185.2426 - val_loss: 140.8611\n",
      "Epoch 1237/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 147.9771 - val_loss: 142.2951\n",
      "Epoch 1238/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 76us/step - loss: 148.9977 - val_loss: 118.4566\n",
      "Epoch 1239/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 146.7372 - val_loss: 126.6623\n",
      "Epoch 1240/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 168.5390 - val_loss: 132.7874\n",
      "Epoch 1241/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 146.2877 - val_loss: 149.7034\n",
      "Epoch 1242/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 255.7941 - val_loss: 133.1131\n",
      "Epoch 1243/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 196.6791 - val_loss: 167.3895\n",
      "Epoch 1244/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.1354 - val_loss: 115.6689\n",
      "Epoch 1245/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 155.0550 - val_loss: 143.1166\n",
      "Epoch 1246/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 156.6667 - val_loss: 152.5837\n",
      "Epoch 1247/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 169.1268 - val_loss: 134.3512\n",
      "Epoch 1248/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 174.2502 - val_loss: 126.4592\n",
      "Epoch 1249/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 157.8705 - val_loss: 121.5114\n",
      "Epoch 1250/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 152.0587 - val_loss: 154.6420\n",
      "Epoch 1251/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 257.2562 - val_loss: 243.9050\n",
      "Epoch 1252/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 198.9577 - val_loss: 120.0588\n",
      "Epoch 1253/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 148.6653 - val_loss: 143.8182\n",
      "Epoch 1254/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 143.8426 - val_loss: 128.3827\n",
      "Epoch 1255/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 151.1821 - val_loss: 143.6387\n",
      "Epoch 1256/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 168.1601 - val_loss: 156.1798\n",
      "Epoch 1257/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 151.2038 - val_loss: 114.7582\n",
      "Epoch 1258/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 188.1160 - val_loss: 121.5215\n",
      "Epoch 1259/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 149.8066 - val_loss: 126.7525\n",
      "Epoch 1260/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 161.7571 - val_loss: 118.4249\n",
      "Epoch 1261/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 180.2174 - val_loss: 118.5033\n",
      "Epoch 1262/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 156.6322 - val_loss: 154.2760\n",
      "Epoch 1263/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 204.5799 - val_loss: 217.8225\n",
      "Epoch 1264/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 162.0083 - val_loss: 137.3046\n",
      "Epoch 1265/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 150.2360 - val_loss: 142.7244\n",
      "Epoch 1266/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 148.3343 - val_loss: 130.6753\n",
      "Epoch 1267/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 146.0330 - val_loss: 161.2771\n",
      "Epoch 1268/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 175.1142 - val_loss: 142.5350\n",
      "Epoch 1269/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 153.4071 - val_loss: 133.8868\n",
      "Epoch 1270/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 155.3912 - val_loss: 124.9615\n",
      "Epoch 1271/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 175.7693 - val_loss: 141.3930\n",
      "Epoch 1272/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 154.6921 - val_loss: 161.6528\n",
      "Epoch 1273/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 142.9251 - val_loss: 118.6427\n",
      "Epoch 1274/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 151.1822 - val_loss: 128.4003\n",
      "Epoch 1275/10000\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 359.0970 - val_loss: 188.9753\n",
      "Epoch 1276/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 170.4572 - val_loss: 148.1100\n",
      "Epoch 1277/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 173.0273 - val_loss: 140.2685\n",
      "Epoch 1278/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 154.0263 - val_loss: 128.9615\n",
      "Epoch 1279/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 155.5366 - val_loss: 133.9234\n",
      "Epoch 1280/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 152.6229 - val_loss: 131.9711\n",
      "Epoch 1281/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 139.8479 - val_loss: 144.4465\n",
      "Epoch 1282/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 174.0613 - val_loss: 130.2300\n",
      "Epoch 1283/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 208.5049 - val_loss: 179.3060\n",
      "Epoch 1284/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 157.4496 - val_loss: 121.9717\n",
      "Epoch 1285/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 148.4069 - val_loss: 127.0527\n",
      "Epoch 1286/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 144.9702 - val_loss: 146.1658\n",
      "Epoch 1287/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 148.1740 - val_loss: 115.3340\n",
      "Epoch 1288/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 143.4736 - val_loss: 124.2865\n",
      "Epoch 1289/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 174.5376 - val_loss: 122.9252\n",
      "Epoch 1290/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 159.0203 - val_loss: 125.1357\n",
      "Epoch 1291/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 177.9404 - val_loss: 126.4662\n",
      "Epoch 1292/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 160.1399 - val_loss: 146.9419\n",
      "Epoch 1293/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 151.1374 - val_loss: 119.3315\n",
      "Epoch 1294/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 151.1113 - val_loss: 127.0904\n",
      "Epoch 1295/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 147.0596 - val_loss: 135.9871\n",
      "Epoch 1296/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 202.2682 - val_loss: 120.5138\n",
      "Epoch 1297/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 149.7713 - val_loss: 116.0903\n",
      "Epoch 1298/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 148.6296 - val_loss: 122.7328\n",
      "Epoch 1299/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 144.0967 - val_loss: 130.6461\n",
      "Epoch 1300/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 148.2111 - val_loss: 119.7352\n",
      "Epoch 1301/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 150.9521 - val_loss: 112.4028\n",
      "Epoch 1302/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 155.6954 - val_loss: 616.1144\n",
      "Epoch 1303/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 165.4867 - val_loss: 113.9622\n",
      "Epoch 1304/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 180.3457 - val_loss: 124.9859\n",
      "Epoch 1305/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 152.6854 - val_loss: 118.1252\n",
      "Epoch 1306/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 168.6195 - val_loss: 134.4696\n",
      "Epoch 1307/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 142.9340 - val_loss: 155.1374\n",
      "Epoch 1308/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 158.4950 - val_loss: 124.8529\n",
      "Epoch 1309/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 224.2587 - val_loss: 134.0959\n",
      "Epoch 1310/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 71us/step - loss: 150.5035 - val_loss: 119.0820\n",
      "Epoch 1311/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 179.3624 - val_loss: 126.8061\n",
      "Epoch 1312/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 141.6148 - val_loss: 175.0390\n",
      "Epoch 1313/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 159.6230 - val_loss: 126.1839\n",
      "Epoch 1314/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 147.5979 - val_loss: 115.5162\n",
      "Epoch 1315/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 148.6105 - val_loss: 118.9956\n",
      "Epoch 1316/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 138.6163 - val_loss: 129.4235\n",
      "Epoch 1317/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 160.7700 - val_loss: 145.9267\n",
      "Epoch 1318/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 194.3401 - val_loss: 160.3108\n",
      "Epoch 1319/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 177.0088 - val_loss: 207.4144\n",
      "Epoch 1320/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 175.6133 - val_loss: 119.8684\n",
      "Epoch 1321/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 157.0219 - val_loss: 123.3884\n",
      "Epoch 1322/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 155.1529 - val_loss: 162.3525\n",
      "Epoch 1323/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 141.6418 - val_loss: 122.1163\n",
      "Epoch 1324/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 155.0106 - val_loss: 122.4132\n",
      "Epoch 1325/10000\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 171.8388 - val_loss: 117.1414\n",
      "Epoch 1326/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 195.5553 - val_loss: 119.6461\n",
      "Epoch 1327/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 203.9155 - val_loss: 200.6356\n",
      "Epoch 1328/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 173.0413 - val_loss: 128.3624\n",
      "Epoch 1329/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 154.8355 - val_loss: 145.4956\n",
      "Epoch 1330/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 142.3240 - val_loss: 153.8402\n",
      "Epoch 1331/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 185.3897 - val_loss: 127.6166\n",
      "Epoch 1332/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 144.1231 - val_loss: 130.5801\n",
      "Epoch 1333/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 143.0072 - val_loss: 253.3905\n",
      "Epoch 1334/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 157.1022 - val_loss: 127.0398\n",
      "Epoch 1335/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 145.2021 - val_loss: 119.9706\n",
      "Epoch 1336/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 165.8150 - val_loss: 122.1557\n",
      "Epoch 1337/10000\n",
      "8000/8000 [==============================] - 1s 186us/step - loss: 172.0274 - val_loss: 138.4929\n",
      "Epoch 1338/10000\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 160.6171 - val_loss: 119.1906\n",
      "Epoch 1339/10000\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 149.6148 - val_loss: 151.8217\n",
      "Epoch 1340/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 220.2994 - val_loss: 129.2740\n",
      "Epoch 1341/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 144.5485 - val_loss: 122.2312\n",
      "Epoch 1342/10000\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 138.7125 - val_loss: 124.2771\n",
      "Epoch 1343/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 146.0383 - val_loss: 146.2872\n",
      "Epoch 1344/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 154.5534 - val_loss: 115.7047\n",
      "Epoch 1345/10000\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 147.1801 - val_loss: 139.7421\n",
      "Epoch 1346/10000\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 164.1734 - val_loss: 122.3459\n",
      "Epoch 1347/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 168.6530 - val_loss: 115.9226\n",
      "Epoch 1348/10000\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 153.3476 - val_loss: 123.2064\n",
      "Epoch 1349/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 151.431 - 1s 136us/step - loss: 153.7454 - val_loss: 143.4769\n",
      "Epoch 1350/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 167.0418 - val_loss: 125.6612\n",
      "Epoch 1351/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 160.4005 - val_loss: 146.2100\n",
      "Epoch 1352/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 152.0079 - val_loss: 121.5111\n",
      "Epoch 1353/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 147.3278 - val_loss: 145.2962\n",
      "Epoch 1354/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 146.3179 - val_loss: 167.2579\n",
      "Epoch 1355/10000\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 205.7757 - val_loss: 119.5234\n",
      "Epoch 1356/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 155.1625 - val_loss: 121.6027\n",
      "Epoch 1357/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 150.7603 - val_loss: 117.0471\n",
      "Epoch 1358/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 138.3641 - val_loss: 134.6173\n",
      "Epoch 1359/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 188.4658 - val_loss: 143.4284\n",
      "Epoch 1360/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 150.4974 - val_loss: 144.5748\n",
      "Epoch 1361/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 171.3514 - val_loss: 408.4270\n",
      "Epoch 1362/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 161.3213 - val_loss: 121.2080\n",
      "Epoch 1363/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 156.2529 - val_loss: 115.4542\n",
      "Epoch 1364/10000\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 140.8136 - val_loss: 133.7473\n",
      "Epoch 1365/10000\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 147.9596 - val_loss: 114.5090\n",
      "Epoch 1366/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 176.4694 - val_loss: 122.2446\n",
      "Epoch 1367/10000\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 147.4621 - val_loss: 114.1729\n",
      "Epoch 1368/10000\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 148.1855 - val_loss: 112.3119\n",
      "Epoch 1369/10000\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 157.0723 - val_loss: 119.1953\n",
      "Epoch 1370/10000\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 155.4088 - val_loss: 183.4150\n",
      "Epoch 1371/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 146.1807 - val_loss: 112.6173\n",
      "Epoch 1372/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 185.5031 - val_loss: 123.8810\n",
      "Epoch 1373/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 145.8285 - val_loss: 132.9327\n",
      "Epoch 1374/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 159.1846 - val_loss: 118.9966\n",
      "Epoch 1375/10000\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 158.2697 - val_loss: 121.3012\n",
      "Epoch 1376/10000\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 145.2396 - val_loss: 122.2577\n",
      "Epoch 1377/10000\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 144.5950 - val_loss: 138.3986\n",
      "Epoch 1378/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 166.5277 - val_loss: 119.7660\n",
      "Epoch 1379/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 145.6269 - val_loss: 118.7392\n",
      "Epoch 1380/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 173.6416 - val_loss: 188.1276\n",
      "Epoch 1381/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 203.8187 - val_loss: 119.3746\n",
      "Epoch 1382/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 93us/step - loss: 163.5807 - val_loss: 132.5688\n",
      "Epoch 1383/10000\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 144.5339 - val_loss: 117.8481\n",
      "Epoch 1384/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 144.7918 - val_loss: 143.5425\n",
      "Epoch 1385/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 144.9663 - val_loss: 136.1990\n",
      "Epoch 1386/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 142.6453 - val_loss: 137.7447\n",
      "Epoch 1387/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 151.7979 - val_loss: 132.0839\n",
      "Epoch 1388/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 224.7971 - val_loss: 223.5861\n",
      "Epoch 1389/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 179.4959 - val_loss: 125.2081\n",
      "Epoch 1390/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 167.9348 - val_loss: 124.1778\n",
      "Epoch 1391/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 191.5456 - val_loss: 228.6061\n",
      "Epoch 1392/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 297.0431 - val_loss: 146.0814\n",
      "Epoch 1393/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 186.9788 - val_loss: 127.4124\n",
      "Epoch 1394/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 185.5357 - val_loss: 195.9556\n",
      "Epoch 1395/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 200.7850 - val_loss: 136.0250\n",
      "Epoch 1396/10000\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 176.1786 - val_loss: 165.3207\n",
      "Epoch 1397/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 186.3334 - val_loss: 137.4583\n",
      "Epoch 1398/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 159.4753 - val_loss: 128.8826\n",
      "Epoch 1399/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 162.4125 - val_loss: 125.8231\n",
      "Epoch 1400/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 149.0035 - val_loss: 115.8021\n",
      "Epoch 1401/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 202.0970 - val_loss: 121.5527\n",
      "Epoch 1402/10000\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 167.2312 - val_loss: 120.7868\n",
      "Epoch 1403/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 154.7557 - val_loss: 143.1299\n",
      "Epoch 1404/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 143.9096 - val_loss: 123.8740\n",
      "Epoch 1405/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 146.3755 - val_loss: 118.9524\n",
      "Epoch 1406/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 193.3852 - val_loss: 118.2578\n",
      "Epoch 1407/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 138.5491 - val_loss: 135.3884\n",
      "Epoch 1408/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 145.5231 - val_loss: 127.6999\n",
      "Epoch 1409/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 169.0969 - val_loss: 143.6622\n",
      "Epoch 1410/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 153.9813 - val_loss: 132.4207\n",
      "Epoch 1411/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 162.1130 - val_loss: 111.9277\n",
      "Epoch 1412/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 151.8103 - val_loss: 114.4826\n",
      "Epoch 1413/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 149.5540 - val_loss: 143.6548\n",
      "Epoch 1414/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 160.5133 - val_loss: 132.7675\n",
      "Epoch 1415/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 145.4835 - val_loss: 117.6976\n",
      "Epoch 1416/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 149.2962 - val_loss: 134.0311\n",
      "Epoch 1417/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 160.9410 - val_loss: 261.0555\n",
      "Epoch 1418/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 159.6363 - val_loss: 195.5850\n",
      "Epoch 1419/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 178.4310 - val_loss: 121.0234\n",
      "Epoch 1420/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 157.3966 - val_loss: 126.4789\n",
      "Epoch 1421/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 161.2010 - val_loss: 120.3400\n",
      "Epoch 1422/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 154.3612 - val_loss: 162.4228\n",
      "Epoch 1423/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 151.2662 - val_loss: 176.1385\n",
      "Epoch 1424/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 155.0584 - val_loss: 144.4489\n",
      "Epoch 1425/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 152.4641 - val_loss: 192.9936\n",
      "Epoch 1426/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 155.5125 - val_loss: 449.9243\n",
      "Epoch 1427/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 146.1649 - val_loss: 123.9874\n",
      "Epoch 1428/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 144.9967 - val_loss: 123.1705\n",
      "Epoch 1429/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 158.0980 - val_loss: 123.2039\n",
      "Epoch 1430/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 161.2422 - val_loss: 121.1588\n",
      "Epoch 1431/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 157.9945 - val_loss: 141.6947\n",
      "Epoch 1432/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 149.6067 - val_loss: 127.0079\n",
      "Epoch 1433/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 145.4206 - val_loss: 114.4471\n",
      "Epoch 1434/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 148.6533 - val_loss: 135.5680\n",
      "Epoch 1435/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 208.8055 - val_loss: 199.2390\n",
      "Epoch 1436/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 170.0288 - val_loss: 117.4351\n",
      "Epoch 1437/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 189.1522 - val_loss: 118.6164\n",
      "Epoch 1438/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 141.5217 - val_loss: 120.7369\n",
      "Epoch 1439/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 147.1835 - val_loss: 120.1852\n",
      "Epoch 1440/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 145.2614 - val_loss: 139.6011\n",
      "Epoch 1441/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 153.3783 - val_loss: 121.6970\n",
      "Epoch 1442/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 139.1898 - val_loss: 114.9517\n",
      "Epoch 1443/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 163.2845 - val_loss: 151.0365\n",
      "Epoch 1444/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 149.1724 - val_loss: 153.5420\n",
      "Epoch 1445/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 148.2888 - val_loss: 174.3362\n",
      "Epoch 1446/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 139.7181 - val_loss: 127.0388\n",
      "Epoch 1447/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 145.3770 - val_loss: 117.6555\n",
      "Epoch 1448/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 211.1314 - val_loss: 123.5647\n",
      "Epoch 1449/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 151.2540 - val_loss: 113.3441\n",
      "Epoch 1450/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 141.4959 - val_loss: 116.0869\n",
      "Epoch 1451/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 143.0413 - val_loss: 121.4439\n",
      "Epoch 1452/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 152.9203 - val_loss: 120.6133\n",
      "Epoch 1453/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 157.0640 - val_loss: 132.6280\n",
      "Epoch 1454/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 114us/step - loss: 140.7422 - val_loss: 129.4611\n",
      "Epoch 1455/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 150.8438 - val_loss: 116.6315\n",
      "Epoch 1456/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 152.1555 - val_loss: 118.2684\n",
      "Epoch 1457/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 146.7092 - val_loss: 130.5858\n",
      "Epoch 1458/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 141.1538 - val_loss: 138.7651\n",
      "Epoch 1459/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 140.5018 - val_loss: 113.7072\n",
      "Epoch 1460/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 155.3811 - val_loss: 145.0624\n",
      "Epoch 1461/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 233.4180 - val_loss: 134.0365\n",
      "Epoch 1462/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 162.4727 - val_loss: 113.9510\n",
      "Epoch 1463/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 146.1924 - val_loss: 114.7378\n",
      "Epoch 1464/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 146.3302 - val_loss: 137.3453\n",
      "Epoch 1465/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 140.8712 - val_loss: 116.0744\n",
      "Epoch 1466/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 174.2148 - val_loss: 331.1416\n",
      "Epoch 1467/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 179.9067 - val_loss: 117.3433\n",
      "Epoch 1468/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 148.9730 - val_loss: 116.0135\n",
      "Epoch 1469/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 143.4778 - val_loss: 137.6787\n",
      "Epoch 1470/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 169.8290 - val_loss: 139.2491\n",
      "Epoch 1471/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 164.4862 - val_loss: 185.4665\n",
      "Epoch 1472/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 147.1583 - val_loss: 126.8234\n",
      "Epoch 1473/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 208.5316 - val_loss: 144.4922\n",
      "Epoch 1474/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 144.4780 - val_loss: 115.5359\n",
      "Epoch 1475/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 145.5581 - val_loss: 130.8088\n",
      "Epoch 1476/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 152.1260 - val_loss: 170.9645\n",
      "Epoch 1477/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 145.2046 - val_loss: 131.0502\n",
      "Epoch 1478/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 144.1424 - val_loss: 117.9695\n",
      "Epoch 1479/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 159.8850 - val_loss: 131.4886\n",
      "Epoch 1480/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 151.9840 - val_loss: 160.0685\n",
      "Epoch 1481/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 216.7353 - val_loss: 115.1051\n",
      "Epoch 1482/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 155.2093 - val_loss: 136.8557\n",
      "Epoch 1483/10000\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 146.6733 - val_loss: 115.0527\n",
      "Epoch 1484/10000\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 154.1870 - val_loss: 117.6588\n",
      "Epoch 1485/10000\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 148.3381 - val_loss: 169.6449\n",
      "Epoch 1486/10000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 148.8134 - val_loss: 143.7588\n",
      "Epoch 1487/10000\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 173.1021 - val_loss: 114.9274\n",
      "Epoch 1488/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 142.6295 - val_loss: 125.1158\n",
      "Epoch 1489/10000\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 139.7633 - val_loss: 117.5620\n",
      "Epoch 1490/10000\n",
      "8000/8000 [==============================] - 1s 134us/step - loss: 136.5801 - val_loss: 115.3887\n",
      "Epoch 1491/10000\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 143.5810 - val_loss: 138.2817\n",
      "Epoch 1492/10000\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 143.2774 - val_loss: 162.2835\n",
      "Epoch 1493/10000\n",
      "8000/8000 [==============================] - 1s 129us/step - loss: 169.2699 - val_loss: 127.5367\n",
      "Epoch 1494/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 157.5654 - val_loss: 135.9835\n",
      "Epoch 1495/10000\n",
      "8000/8000 [==============================] - 1s 126us/step - loss: 144.3774 - val_loss: 154.5258\n",
      "Epoch 1496/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 157.9544 - val_loss: 128.8543\n",
      "Epoch 1497/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 158.4324 - val_loss: 126.0610\n",
      "Epoch 1498/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 249.7400 - val_loss: 133.6561\n",
      "Epoch 1499/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 146.1895 - val_loss: 115.3314\n",
      "Epoch 1500/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 167.9006 - val_loss: 121.9040\n",
      "Epoch 1501/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 154.3139 - val_loss: 167.6112\n",
      "Epoch 1502/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 141.1066 - val_loss: 112.8048\n",
      "Epoch 1503/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 140.5741 - val_loss: 142.8763\n",
      "Epoch 1504/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 143.4674 - val_loss: 116.8229\n",
      "Epoch 1505/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 139.5157 - val_loss: 126.6106\n",
      "Epoch 1506/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 162.2371 - val_loss: 143.3315\n",
      "Epoch 1507/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 138.0250 - val_loss: 165.5166\n",
      "Epoch 1508/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 143.9450 - val_loss: 139.9065\n",
      "Epoch 1509/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 152.8813 - val_loss: 151.1027\n",
      "Epoch 1510/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 168.7234 - val_loss: 115.2954\n",
      "Epoch 1511/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 142.8519 - val_loss: 117.9870\n",
      "Epoch 1512/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 202.4631 - val_loss: 123.2978\n",
      "Epoch 1513/10000\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 176.4668 - val_loss: 114.2219\n",
      "Epoch 1514/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 151.5755 - val_loss: 125.8847\n",
      "Epoch 1515/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 156.4778 - val_loss: 121.7199\n",
      "Epoch 1516/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 139.4467 - val_loss: 118.9574\n",
      "Epoch 1517/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 154.0026 - val_loss: 117.2964\n",
      "Epoch 1518/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 168.0309 - val_loss: 154.9290\n",
      "Epoch 1519/10000\n",
      "8000/8000 [==============================] - 1s 172us/step - loss: 172.3586 - val_loss: 118.0646\n",
      "Epoch 1520/10000\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 134.8184 - val_loss: 113.7909\n",
      "Epoch 1521/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 230.5524 - val_loss: 128.7056\n",
      "Epoch 1522/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 148.9765 - val_loss: 126.3041\n",
      "Epoch 1523/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 166.2195 - val_loss: 135.9402\n",
      "Epoch 1524/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 144.1691 - val_loss: 138.9095\n",
      "Epoch 1525/10000\n",
      "8000/8000 [==============================] - 1s 124us/step - loss: 148.5670 - val_loss: 160.1081\n",
      "Epoch 1526/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 101us/step - loss: 163.4400 - val_loss: 123.6205\n",
      "Epoch 1527/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 149.6970 - val_loss: 156.6333\n",
      "Epoch 1528/10000\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 157.6813 - val_loss: 149.9193\n",
      "Epoch 1529/10000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 163.1135 - val_loss: 114.8997\n",
      "Epoch 1530/10000\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 172.1066 - val_loss: 121.7162\n",
      "Epoch 1531/10000\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 144.9491 - val_loss: 119.9127\n",
      "Epoch 1532/10000\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 144.6800 - val_loss: 123.9319\n",
      "Epoch 1533/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 189.0275 - val_loss: 203.1282\n",
      "Epoch 1534/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 164.9915 - val_loss: 118.3373\n",
      "Epoch 1535/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 144.3626 - val_loss: 117.1762\n",
      "Epoch 1536/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 145.6744 - val_loss: 116.0443\n",
      "Epoch 1537/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 145.5315 - val_loss: 139.3119\n",
      "Epoch 1538/10000\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 134.6505 - val_loss: 118.6808\n",
      "Epoch 1539/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 140.7777 - val_loss: 214.7425\n",
      "Epoch 1540/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 192.6061 - val_loss: 131.3136\n",
      "Epoch 1541/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 152.2569 - val_loss: 121.6029\n",
      "Epoch 1542/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 156.6633 - val_loss: 124.2343\n",
      "Epoch 1543/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 141.9114 - val_loss: 114.8357\n",
      "Epoch 1544/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 197.0085 - val_loss: 149.7104\n",
      "Epoch 1545/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 143.5706 - val_loss: 121.2679\n",
      "Epoch 1546/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 146.5759 - val_loss: 207.5434\n",
      "Epoch 1547/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 174.3549 - val_loss: 203.5501\n",
      "Epoch 1548/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 206.8746 - val_loss: 130.7888\n",
      "Epoch 1549/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 188.2881 - val_loss: 123.9065\n",
      "Epoch 1550/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 148.5338 - val_loss: 152.2216\n",
      "Epoch 1551/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 155.0985 - val_loss: 127.3612\n",
      "Epoch 1552/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 153.5237 - val_loss: 116.4378\n",
      "Epoch 1553/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 145.9709 - val_loss: 120.8621\n",
      "Epoch 1554/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 152.5994 - val_loss: 112.9285\n",
      "Epoch 1555/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 147.2583 - val_loss: 126.1670\n",
      "Epoch 1556/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 143.7306 - val_loss: 125.7602\n",
      "Epoch 1557/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 169.6823 - val_loss: 120.5171\n",
      "Epoch 1558/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 178.8019 - val_loss: 151.9262\n",
      "Epoch 1559/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 161.6340 - val_loss: 241.0583\n",
      "Epoch 1560/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 144.4124 - val_loss: 120.5112\n",
      "Epoch 1561/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 140.1364 - val_loss: 117.8124\n",
      "Epoch 1562/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 187.1903 - val_loss: 131.5612\n",
      "Epoch 1563/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 160.0124 - val_loss: 172.3624\n",
      "Epoch 1564/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 161.2191 - val_loss: 117.1507\n",
      "Epoch 1565/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 136.8822 - val_loss: 122.8713\n",
      "Epoch 1566/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 159.6211 - val_loss: 122.2324\n",
      "Epoch 1567/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 179.3320 - val_loss: 124.3440\n",
      "Epoch 1568/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 144.4689 - val_loss: 114.3310\n",
      "Epoch 1569/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 161.6632 - val_loss: 115.7172\n",
      "Epoch 1570/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 153.2461 - val_loss: 125.8065\n",
      "Epoch 1571/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 146.4425 - val_loss: 163.3617\n",
      "Epoch 1572/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 159.8043 - val_loss: 127.1119\n",
      "Epoch 1573/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 190.8908 - val_loss: 133.3397\n",
      "Epoch 1574/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 141.5920 - val_loss: 135.4163\n",
      "Epoch 1575/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 168.3769 - val_loss: 142.6288\n",
      "Epoch 1576/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 144.5299 - val_loss: 118.5062\n",
      "Epoch 1577/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 170.4297 - val_loss: 116.0494\n",
      "Epoch 1578/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 166.2695 - val_loss: 122.6280\n",
      "Epoch 1579/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 158.4978 - val_loss: 125.9641\n",
      "Epoch 1580/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 149.9218 - val_loss: 119.3090\n",
      "Epoch 1581/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 141.5997 - val_loss: 113.8561\n",
      "Epoch 1582/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 149.1864 - val_loss: 125.2925\n",
      "Epoch 1583/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 154.0304 - val_loss: 115.4207\n",
      "Epoch 1584/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 190.8804 - val_loss: 124.1947\n",
      "Epoch 1585/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 148.0580 - val_loss: 117.7547\n",
      "Epoch 1586/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 140.6409 - val_loss: 117.3129\n",
      "Epoch 1587/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 156.3479 - val_loss: 126.8035\n",
      "Epoch 1588/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 145.1322 - val_loss: 172.9065\n",
      "Epoch 1589/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 161.7023 - val_loss: 128.7635\n",
      "Epoch 1590/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 170.8802 - val_loss: 311.0395\n",
      "Epoch 1591/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 187.5695 - val_loss: 124.5560\n",
      "Epoch 1592/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 150.4528 - val_loss: 118.5175\n",
      "Epoch 1593/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 146.1172 - val_loss: 128.9401\n",
      "Epoch 1594/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 162.9916 - val_loss: 118.6122\n",
      "Epoch 1595/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 144.5592 - val_loss: 133.6973\n",
      "Epoch 1596/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 195.8840 - val_loss: 135.8029\n",
      "Epoch 1597/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 141.5257 - val_loss: 115.0303\n",
      "Epoch 1598/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 79us/step - loss: 157.5945 - val_loss: 227.8563\n",
      "Epoch 1599/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 145.9446 - val_loss: 128.4525\n",
      "Epoch 1600/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 140.2983 - val_loss: 132.0603\n",
      "Epoch 1601/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 144.8124 - val_loss: 119.1347\n",
      "Epoch 1602/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 143.6684 - val_loss: 117.8040\n",
      "Epoch 1603/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 146.0305 - val_loss: 118.7174\n",
      "Epoch 1604/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 155.0558 - val_loss: 130.5639\n",
      "Epoch 1605/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 140.2528 - val_loss: 138.4230\n",
      "Epoch 1606/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 155.0613 - val_loss: 122.6817\n",
      "Epoch 1607/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 144.1963 - val_loss: 115.7395\n",
      "Epoch 1608/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 141.8633 - val_loss: 127.0311\n",
      "Epoch 1609/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 190.2045 - val_loss: 117.3703\n",
      "Epoch 1610/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 139.0099 - val_loss: 113.7120\n",
      "Epoch 1611/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 137.5720 - val_loss: 115.5771\n",
      "Epoch 1612/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 134.7566 - val_loss: 113.9014\n",
      "Epoch 1613/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 140.2499 - val_loss: 114.3446\n",
      "Epoch 1614/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 152.6007 - val_loss: 116.3038\n",
      "Epoch 1615/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 148.3587 - val_loss: 112.6964\n",
      "Epoch 1616/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 191.6522 - val_loss: 148.8168\n",
      "Epoch 1617/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 139.7930 - val_loss: 124.4444\n",
      "Epoch 1618/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 139.6756 - val_loss: 119.1023\n",
      "Epoch 1619/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 207.4408 - val_loss: 255.2915\n",
      "Epoch 1620/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 156.6823 - val_loss: 119.8814\n",
      "Epoch 1621/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 140.9743 - val_loss: 119.0786\n",
      "Epoch 1622/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 141.2695 - val_loss: 118.9333\n",
      "Epoch 1623/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 151.7141 - val_loss: 145.8651\n",
      "Epoch 1624/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 151.6015 - val_loss: 117.8205\n",
      "Epoch 1625/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 145.0987 - val_loss: 139.3201\n",
      "Epoch 1626/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 156.3671 - val_loss: 150.9325\n",
      "Epoch 1627/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 140.5323 - val_loss: 112.7530\n",
      "Epoch 1628/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 145.9543 - val_loss: 114.7797\n",
      "Epoch 1629/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 146.5949 - val_loss: 128.3314\n",
      "Epoch 1630/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 162.1577 - val_loss: 124.8601\n",
      "Epoch 1631/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 155.8636 - val_loss: 168.2102\n",
      "Epoch 1632/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 219.1754 - val_loss: 131.1982\n",
      "Epoch 1633/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 142.8834 - val_loss: 117.1882\n",
      "Epoch 1634/10000\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 141.6457 - val_loss: 155.5008\n",
      "Epoch 1635/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 147.2689 - val_loss: 115.9379\n",
      "Epoch 1636/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 142.2035 - val_loss: 129.7181\n",
      "Epoch 1637/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 191.2660 - val_loss: 147.1673\n",
      "Epoch 1638/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 151.1718 - val_loss: 150.0202\n",
      "Epoch 1639/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 172.7682 - val_loss: 142.7550\n",
      "Epoch 1640/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 143.6188 - val_loss: 118.1884\n",
      "Epoch 1641/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 140.5740 - val_loss: 190.3760\n",
      "Epoch 1642/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 142.5601 - val_loss: 122.9459\n",
      "Epoch 1643/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 142.6719 - val_loss: 116.1123\n",
      "Epoch 1644/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 145.3252 - val_loss: 130.4010\n",
      "Epoch 1645/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 184.2606 - val_loss: 121.6242\n",
      "Epoch 1646/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 146.3155 - val_loss: 114.9468\n",
      "Epoch 1647/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 144.3289 - val_loss: 173.5801\n",
      "Epoch 1648/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 185.6500 - val_loss: 126.5146\n",
      "Epoch 1649/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 139.0592 - val_loss: 155.7641\n",
      "Epoch 1650/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 144.4067 - val_loss: 115.7560\n",
      "Epoch 1651/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 146.6740 - val_loss: 114.0099\n",
      "Epoch 1652/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 144.7576 - val_loss: 130.2852\n",
      "Epoch 1653/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 146.8867 - val_loss: 137.5476\n",
      "Epoch 1654/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 157.0149 - val_loss: 203.7587\n",
      "Epoch 1655/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 146.4455 - val_loss: 127.2605\n",
      "Epoch 1656/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 140.9989 - val_loss: 112.3455\n",
      "Epoch 1657/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 139.2521 - val_loss: 111.8700\n",
      "Epoch 1658/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 156.3000 - val_loss: 115.4365\n",
      "Epoch 1659/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 140.6054 - val_loss: 113.4085\n",
      "Epoch 1660/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 181.5937 - val_loss: 143.2748\n",
      "Epoch 1661/10000\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 147.8191 - val_loss: 135.6429\n",
      "Epoch 1662/10000\n",
      "8000/8000 [==============================] - 1s 154us/step - loss: 147.6784 - val_loss: 112.1445\n",
      "Epoch 1663/10000\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 144.4304 - val_loss: 125.0951\n",
      "Epoch 1664/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 143.3030 - val_loss: 113.4938\n",
      "Epoch 1665/10000\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 181.9200 - val_loss: 152.3744\n",
      "Epoch 1666/10000\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 146.2326 - val_loss: 130.2424\n",
      "Epoch 1667/10000\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 159.6392 - val_loss: 158.3464\n",
      "Epoch 1668/10000\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 138.6245 - val_loss: 124.2171\n",
      "Epoch 1669/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 149.5235 - val_loss: 115.3233\n",
      "Epoch 1670/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 69us/step - loss: 138.7143 - val_loss: 112.4373\n",
      "Epoch 1671/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 194.5270 - val_loss: 115.0040\n",
      "Epoch 1672/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.1564 - val_loss: 128.1143\n",
      "Epoch 1673/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 142.4257 - val_loss: 115.3605\n",
      "Epoch 1674/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 132.672 - 1s 70us/step - loss: 132.3356 - val_loss: 119.0459\n",
      "Epoch 1675/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 134.8611 - val_loss: 125.2765\n",
      "Epoch 1676/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 157.6110 - val_loss: 116.1390\n",
      "Epoch 1677/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 143.5572 - val_loss: 117.4684\n",
      "Epoch 1678/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 146.8972 - val_loss: 132.6534\n",
      "Epoch 1679/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 150.3979 - val_loss: 117.3940\n",
      "Epoch 1680/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 158.8512 - val_loss: 117.8177\n",
      "Epoch 1681/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 161.8259 - val_loss: 113.7196\n",
      "Epoch 1682/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 147.5587 - val_loss: 130.5892\n",
      "Epoch 1683/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 150.4620 - val_loss: 132.8581\n",
      "Epoch 1684/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 146.4307 - val_loss: 158.9790\n",
      "Epoch 1685/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 140.7574 - val_loss: 126.2134\n",
      "Epoch 1686/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 144.1357 - val_loss: 139.1035\n",
      "Epoch 1687/10000\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 148.6408 - val_loss: 112.3352\n",
      "Epoch 1688/10000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 141.6254 - val_loss: 115.4014\n",
      "Epoch 1689/10000\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 163.3244 - val_loss: 131.7742\n",
      "Epoch 1690/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 133.8769 - val_loss: 121.6772\n",
      "Epoch 1691/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 143.1710 - val_loss: 113.5616\n",
      "Epoch 1692/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 156.6606 - val_loss: 116.5043\n",
      "Epoch 1693/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 146.1548 - val_loss: 124.6247\n",
      "Epoch 1694/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 141.8009 - val_loss: 117.7757\n",
      "Epoch 1695/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 150.8264 - val_loss: 121.4457\n",
      "Epoch 1696/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 174.0361 - val_loss: 114.8208\n",
      "Epoch 1697/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 170.8544 - val_loss: 141.3835\n",
      "Epoch 1698/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 146.5337 - val_loss: 141.9871\n",
      "Epoch 1699/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 136.9007 - val_loss: 137.6543\n",
      "Epoch 1700/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 147.1783 - val_loss: 120.9846\n",
      "Epoch 1701/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 163.0896 - val_loss: 118.5241\n",
      "Epoch 1702/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 142.0699 - val_loss: 131.0124\n",
      "Epoch 1703/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 140.1503 - val_loss: 151.2672\n",
      "Epoch 1704/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 149.7986 - val_loss: 113.3295\n",
      "Epoch 1705/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 155.1120 - val_loss: 134.3458\n",
      "Epoch 1706/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 161.6700 - val_loss: 150.6333\n",
      "Epoch 1707/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 135.2340 - val_loss: 118.5255\n",
      "Epoch 1708/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 147.9745 - val_loss: 143.6605\n",
      "Epoch 1709/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 138.4907 - val_loss: 118.3926\n",
      "Epoch 1710/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 153.2892 - val_loss: 126.9206\n",
      "Epoch 1711/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 138.4188 - val_loss: 148.0896\n",
      "Epoch 1712/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 166.5396 - val_loss: 114.7229\n",
      "Epoch 1713/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 142.0313 - val_loss: 133.2140\n",
      "Epoch 1714/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 222.8144 - val_loss: 287.0006\n",
      "Epoch 1715/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 147.8419 - val_loss: 120.9651\n",
      "Epoch 1716/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 136.5051 - val_loss: 128.0378\n",
      "Epoch 1717/10000\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 143.6932 - val_loss: 120.6831\n",
      "Epoch 1718/10000\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 142.1907 - val_loss: 117.0777\n",
      "Epoch 1719/10000\n",
      "8000/8000 [==============================] - 1s 187us/step - loss: 151.7000 - val_loss: 132.9022\n",
      "Epoch 1720/10000\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 136.6184 - val_loss: 121.6479\n",
      "Epoch 1721/10000\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 142.4433 - val_loss: 120.9942\n",
      "Epoch 1722/10000\n",
      "8000/8000 [==============================] - 1s 154us/step - loss: 146.0130 - val_loss: 120.8353\n",
      "Epoch 1723/10000\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 145.8497 - val_loss: 183.3580\n",
      "Epoch 1724/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 193.3113 - val_loss: 121.6882\n",
      "Epoch 1725/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 214.8158 - val_loss: 144.7668\n",
      "Epoch 1726/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.8646 - val_loss: 112.5443\n",
      "Epoch 1727/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 150.8891 - val_loss: 118.0593\n",
      "Epoch 1728/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 138.9515 - val_loss: 111.1071\n",
      "Epoch 1729/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 134.1944 - val_loss: 113.6069\n",
      "Epoch 1730/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 141.1162 - val_loss: 114.5142\n",
      "Epoch 1731/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 142.9836 - val_loss: 120.6844\n",
      "Epoch 1732/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 161.9228 - val_loss: 123.6888\n",
      "Epoch 1733/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 153.9128 - val_loss: 114.8439\n",
      "Epoch 1734/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 151.6416 - val_loss: 132.2814\n",
      "Epoch 1735/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.8102 - val_loss: 117.2527\n",
      "Epoch 1736/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 144.0035 - val_loss: 134.9639\n",
      "Epoch 1737/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 160.5763 - val_loss: 136.5880\n",
      "Epoch 1738/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 187.7628 - val_loss: 172.1413\n",
      "Epoch 1739/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 148.5315 - val_loss: 214.7749\n",
      "Epoch 1740/10000\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 138.4042 - val_loss: 119.9434\n",
      "Epoch 1741/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 148.6721 - val_loss: 128.3367\n",
      "Epoch 1742/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 80us/step - loss: 138.2034 - val_loss: 115.1002\n",
      "Epoch 1743/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 143.9696 - val_loss: 113.3497\n",
      "Epoch 1744/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 146.8750 - val_loss: 117.9559\n",
      "Epoch 1745/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 214.5210 - val_loss: 153.4263\n",
      "Epoch 1746/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 145.2224 - val_loss: 117.8547\n",
      "Epoch 1747/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 138.0625 - val_loss: 113.9196\n",
      "Epoch 1748/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 142.6181 - val_loss: 126.9561\n",
      "Epoch 1749/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 149.7297 - val_loss: 147.6856\n",
      "Epoch 1750/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 142.8532 - val_loss: 115.3099\n",
      "Epoch 1751/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 137.2501 - val_loss: 113.2504\n",
      "Epoch 1752/10000\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 169.4564 - val_loss: 114.1995TA: 0s - loss: 163\n",
      "Epoch 1753/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 198.9386 - val_loss: 181.1810\n",
      "Epoch 1754/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 145.8772 - val_loss: 130.7566\n",
      "Epoch 1755/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 135.8993 - val_loss: 118.2008\n",
      "Epoch 1756/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 153.5217 - val_loss: 117.7125\n",
      "Epoch 1757/10000\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 177.3668 - val_loss: 185.0877\n",
      "Epoch 1758/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 151.8591 - val_loss: 238.1183\n",
      "Epoch 1759/10000\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 146.3694 - val_loss: 120.5902\n",
      "Epoch 1760/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 139.6450 - val_loss: 111.1172\n",
      "Epoch 1761/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 188.1213 - val_loss: 115.2363\n",
      "Epoch 1762/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 144.1518 - val_loss: 115.9701\n",
      "Epoch 1763/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 153.9883 - val_loss: 165.4159\n",
      "Epoch 1764/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 170.7230 - val_loss: 134.3764\n",
      "Epoch 1765/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 138.7299 - val_loss: 122.0538\n",
      "Epoch 1766/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 204.8489 - val_loss: 558.3102\n",
      "Epoch 1767/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 174.4677 - val_loss: 129.5243\n",
      "Epoch 1768/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 142.1750 - val_loss: 114.8426\n",
      "Epoch 1769/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 134.8808 - val_loss: 150.8829\n",
      "Epoch 1770/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 146.2599 - val_loss: 137.5721\n",
      "Epoch 1771/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 159.0055 - val_loss: 115.1431\n",
      "Epoch 1772/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 135.5115 - val_loss: 114.3106\n",
      "Epoch 1773/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 138.8769 - val_loss: 117.0106\n",
      "Epoch 1774/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 140.7534 - val_loss: 126.6633\n",
      "Epoch 1775/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 182.9211 - val_loss: 187.6751\n",
      "Epoch 1776/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.6667 - val_loss: 192.3685\n",
      "Epoch 1777/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 141.4242 - val_loss: 118.3311\n",
      "Epoch 1778/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.8471 - val_loss: 187.1773\n",
      "Epoch 1779/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 147.5386 - val_loss: 120.4447\n",
      "Epoch 1780/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 169.2273 - val_loss: 138.9560\n",
      "Epoch 1781/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 145.7874 - val_loss: 117.5109\n",
      "Epoch 1782/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 137.0398 - val_loss: 127.0084\n",
      "Epoch 1783/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 151.2388 - val_loss: 175.5809\n",
      "Epoch 1784/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 143.3872 - val_loss: 126.7283\n",
      "Epoch 1785/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 179.0564 - val_loss: 137.2133\n",
      "Epoch 1786/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 148.1626 - val_loss: 127.0068\n",
      "Epoch 1787/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 140.0946 - val_loss: 121.9741\n",
      "Epoch 1788/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 141.9994 - val_loss: 133.8546\n",
      "Epoch 1789/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 159.7937 - val_loss: 154.8543\n",
      "Epoch 1790/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 155.8335 - val_loss: 120.5063\n",
      "Epoch 1791/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 153.5123 - val_loss: 143.2965\n",
      "Epoch 1792/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 152.3974 - val_loss: 135.1024\n",
      "Epoch 1793/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 143.8215 - val_loss: 134.5840\n",
      "Epoch 1794/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 140.7894 - val_loss: 114.0198\n",
      "Epoch 1795/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 171.3274 - val_loss: 123.7864\n",
      "Epoch 1796/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 155.5455 - val_loss: 128.0988\n",
      "Epoch 1797/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 145.8320 - val_loss: 134.0218\n",
      "Epoch 1798/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 147.9027 - val_loss: 127.8393\n",
      "Epoch 1799/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 140.2683 - val_loss: 114.8398\n",
      "Epoch 1800/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 137.4396 - val_loss: 114.7639\n",
      "Epoch 1801/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 142.9125 - val_loss: 130.6430\n",
      "Epoch 1802/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 144.4932 - val_loss: 117.2863\n",
      "Epoch 1803/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 151.8245 - val_loss: 127.1083\n",
      "Epoch 1804/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 181.6646 - val_loss: 124.8429\n",
      "Epoch 1805/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.5485 - val_loss: 181.8619\n",
      "Epoch 1806/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 149.5066 - val_loss: 123.6270\n",
      "Epoch 1807/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 182.2844 - val_loss: 136.8445\n",
      "Epoch 1808/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 137.4959 - val_loss: 122.1412\n",
      "Epoch 1809/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 162.0363 - val_loss: 312.5938\n",
      "Epoch 1810/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 203.6696 - val_loss: 116.1221\n",
      "Epoch 1811/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 144.2208 - val_loss: 112.9225\n",
      "Epoch 1812/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 135.4141 - val_loss: 122.5766\n",
      "Epoch 1813/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 133.9380 - val_loss: 119.9084\n",
      "Epoch 1814/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 139.4500 - val_loss: 128.5083\n",
      "Epoch 1815/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 167.6847 - val_loss: 118.6848\n",
      "Epoch 1816/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 151.6348 - val_loss: 121.0535\n",
      "Epoch 1817/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 186.1419 - val_loss: 166.2791\n",
      "Epoch 1818/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 143.3833 - val_loss: 125.4668\n",
      "Epoch 1819/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 145.7646 - val_loss: 133.5719\n",
      "Epoch 1820/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 187.9318 - val_loss: 121.2352\n",
      "Epoch 1821/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 182.6895 - val_loss: 133.7947\n",
      "Epoch 1822/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 140.3944 - val_loss: 118.7114\n",
      "Epoch 1823/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 139.5500 - val_loss: 116.3585\n",
      "Epoch 1824/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 141.2962 - val_loss: 111.4640\n",
      "Epoch 1825/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 201.9946 - val_loss: 136.4832\n",
      "Epoch 1826/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.5564 - val_loss: 127.2818\n",
      "Epoch 1827/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 166.7800 - val_loss: 140.9038\n",
      "Epoch 1828/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 149.9944 - val_loss: 114.5308\n",
      "Epoch 1829/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 136.1724 - val_loss: 122.6735\n",
      "Epoch 1830/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 133.9785 - val_loss: 121.2817\n",
      "Epoch 1831/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.8601 - val_loss: 117.9183\n",
      "Epoch 1832/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 133.6239 - val_loss: 114.6104\n",
      "Epoch 1833/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 138.7506 - val_loss: 115.6536\n",
      "Epoch 1834/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 139.7692 - val_loss: 153.1581\n",
      "Epoch 1835/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 173.4216 - val_loss: 156.5341\n",
      "Epoch 1836/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 149.6952 - val_loss: 115.3829\n",
      "Epoch 1837/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 146.4708 - val_loss: 191.0004\n",
      "Epoch 1838/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 186.0025 - val_loss: 285.1373\n",
      "Epoch 1839/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 146.2944 - val_loss: 123.3389\n",
      "Epoch 1840/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 153.7985 - val_loss: 169.5120\n",
      "Epoch 1841/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.0884 - val_loss: 184.3908\n",
      "Epoch 1842/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 168.0398 - val_loss: 121.5286\n",
      "Epoch 1843/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 154.6322 - val_loss: 116.8769\n",
      "Epoch 1844/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 150.8555 - val_loss: 117.5883\n",
      "Epoch 1845/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 141.3077 - val_loss: 115.0126\n",
      "Epoch 1846/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 139.2167 - val_loss: 124.0415\n",
      "Epoch 1847/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 148.4035 - val_loss: 153.1200\n",
      "Epoch 1848/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 164.0011 - val_loss: 134.2768\n",
      "Epoch 1849/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 141.4596 - val_loss: 123.2696\n",
      "Epoch 1850/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 170.9508 - val_loss: 161.9466\n",
      "Epoch 1851/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 180.8070 - val_loss: 216.7092\n",
      "Epoch 1852/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 155.4921 - val_loss: 123.8140\n",
      "Epoch 1853/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 147.1044 - val_loss: 117.0059\n",
      "Epoch 1854/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 143.1467 - val_loss: 109.0609\n",
      "Epoch 1855/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 142.1267 - val_loss: 124.7432\n",
      "Epoch 1856/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.0717 - val_loss: 122.2960\n",
      "Epoch 1857/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 149.9402 - val_loss: 116.2897\n",
      "Epoch 1858/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.4280 - val_loss: 116.2479\n",
      "Epoch 1859/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 205.3170 - val_loss: 203.9896\n",
      "Epoch 1860/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 201.3791 - val_loss: 188.8499\n",
      "Epoch 1861/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 170.3254 - val_loss: 138.4056\n",
      "Epoch 1862/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 191.4435 - val_loss: 126.8610\n",
      "Epoch 1863/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 152.9529 - val_loss: 144.3043\n",
      "Epoch 1864/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.3478 - val_loss: 114.7886\n",
      "Epoch 1865/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 146.7645 - val_loss: 130.3034\n",
      "Epoch 1866/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 138.6254 - val_loss: 147.8235\n",
      "Epoch 1867/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 164.0792 - val_loss: 161.1212\n",
      "Epoch 1868/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 152.3614 - val_loss: 118.3780\n",
      "Epoch 1869/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 146.0525 - val_loss: 114.1339\n",
      "Epoch 1870/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 145.9549 - val_loss: 113.8775\n",
      "Epoch 1871/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 186.2561 - val_loss: 377.5502\n",
      "Epoch 1872/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 216.7901 - val_loss: 125.6000\n",
      "Epoch 1873/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 173.4275 - val_loss: 117.1348\n",
      "Epoch 1874/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 138.0499 - val_loss: 128.7800\n",
      "Epoch 1875/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 140.5145 - val_loss: 195.7857\n",
      "Epoch 1876/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 144.2130 - val_loss: 112.5932\n",
      "Epoch 1877/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 191.8116 - val_loss: 150.9596\n",
      "Epoch 1878/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 141.4049 - val_loss: 123.8136\n",
      "Epoch 1879/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 173.3962 - val_loss: 482.1245\n",
      "Epoch 1880/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 164.0732 - val_loss: 120.0426\n",
      "Epoch 1881/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 174.8808 - val_loss: 117.6691\n",
      "Epoch 1882/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.6026 - val_loss: 174.8842\n",
      "Epoch 1883/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.8486 - val_loss: 135.4610\n",
      "Epoch 1884/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 145.7381 - val_loss: 127.7578\n",
      "Epoch 1885/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 157.7031 - val_loss: 129.8229\n",
      "Epoch 1886/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 73us/step - loss: 141.1241 - val_loss: 115.5317\n",
      "Epoch 1887/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 148.5242 - val_loss: 121.4210\n",
      "Epoch 1888/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 170.4747 - val_loss: 118.0210\n",
      "Epoch 1889/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 139.4935 - val_loss: 126.7627\n",
      "Epoch 1890/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 147.0634 - val_loss: 136.9124\n",
      "Epoch 1891/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 135.9618 - val_loss: 149.9283\n",
      "Epoch 1892/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.2387 - val_loss: 127.6054\n",
      "Epoch 1893/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 174.5521 - val_loss: 131.9779\n",
      "Epoch 1894/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 145.8552 - val_loss: 123.3308\n",
      "Epoch 1895/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 142.1528 - val_loss: 109.8316\n",
      "Epoch 1896/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 137.5724 - val_loss: 150.4615\n",
      "Epoch 1897/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.3037 - val_loss: 120.1155\n",
      "Epoch 1898/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 142.9647 - val_loss: 112.2418\n",
      "Epoch 1899/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 150.9757 - val_loss: 119.5254\n",
      "Epoch 1900/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 137.2302 - val_loss: 124.1498\n",
      "Epoch 1901/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.1711 - val_loss: 149.0453\n",
      "Epoch 1902/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 148.6331 - val_loss: 115.1822\n",
      "Epoch 1903/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.2251 - val_loss: 167.7471\n",
      "Epoch 1904/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.1620 - val_loss: 135.8464\n",
      "Epoch 1905/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 145.144 - 1s 64us/step - loss: 144.4699 - val_loss: 117.7353\n",
      "Epoch 1906/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 165.4739 - val_loss: 178.0631\n",
      "Epoch 1907/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 151.9509 - val_loss: 114.4062\n",
      "Epoch 1908/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 137.4477 - val_loss: 117.6727\n",
      "Epoch 1909/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 142.9178 - val_loss: 154.5324\n",
      "Epoch 1910/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 160.7754 - val_loss: 117.2614\n",
      "Epoch 1911/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 137.9340 - val_loss: 139.5142\n",
      "Epoch 1912/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.7633 - val_loss: 118.2404\n",
      "Epoch 1913/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 140.4562 - val_loss: 139.7392\n",
      "Epoch 1914/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 148.2752 - val_loss: 114.4127\n",
      "Epoch 1915/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 142.3482 - val_loss: 181.4302\n",
      "Epoch 1916/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.9322 - val_loss: 147.8443\n",
      "Epoch 1917/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 141.9614 - val_loss: 114.9365\n",
      "Epoch 1918/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.9270 - val_loss: 111.8738\n",
      "Epoch 1919/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 144.9389 - val_loss: 189.6153\n",
      "Epoch 1920/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 160.8296 - val_loss: 1457.8041\n",
      "Epoch 1921/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 386.6534 - val_loss: 151.3555\n",
      "Epoch 1922/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.2102 - val_loss: 131.8083\n",
      "Epoch 1923/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 241.3206 - val_loss: 175.5745\n",
      "Epoch 1924/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 204.4785 - val_loss: 133.7243\n",
      "Epoch 1925/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 176.2802 - val_loss: 125.8694\n",
      "Epoch 1926/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 185.5976 - val_loss: 140.2223\n",
      "Epoch 1927/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 190.5501 - val_loss: 176.8888\n",
      "Epoch 1928/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 183.4671 - val_loss: 168.1437\n",
      "Epoch 1929/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 169.8780 - val_loss: 134.0799\n",
      "Epoch 1930/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 174.6846 - val_loss: 143.4852\n",
      "Epoch 1931/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 155.9859 - val_loss: 189.7230\n",
      "Epoch 1932/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 158.7252 - val_loss: 125.5234\n",
      "Epoch 1933/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 149.9087 - val_loss: 120.2520\n",
      "Epoch 1934/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 149.8360 - val_loss: 120.9507\n",
      "Epoch 1935/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 162.0012 - val_loss: 127.0448\n",
      "Epoch 1936/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 164.2572 - val_loss: 127.7153\n",
      "Epoch 1937/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 256.0519 - val_loss: 186.8440\n",
      "Epoch 1938/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 214.1781 - val_loss: 139.0940\n",
      "Epoch 1939/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 176.8188 - val_loss: 128.2333\n",
      "Epoch 1940/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 277.6150 - val_loss: 143.9630\n",
      "Epoch 1941/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 171.8553 - val_loss: 141.1751\n",
      "Epoch 1942/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 180.9159 - val_loss: 165.7323\n",
      "Epoch 1943/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 171.9015 - val_loss: 144.9065\n",
      "Epoch 1944/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 168.6554 - val_loss: 136.8874\n",
      "Epoch 1945/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 178.1944 - val_loss: 141.6450\n",
      "Epoch 1946/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 164.7038 - val_loss: 147.1202\n",
      "Epoch 1947/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 167.4524 - val_loss: 119.8313\n",
      "Epoch 1948/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 194.1560 - val_loss: 193.7867\n",
      "Epoch 1949/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 160.5219 - val_loss: 167.5973\n",
      "Epoch 1950/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 167.3326 - val_loss: 121.3921\n",
      "Epoch 1951/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 157.5759 - val_loss: 146.8393\n",
      "Epoch 1952/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 166.4630 - val_loss: 123.8549\n",
      "Epoch 1953/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 154.6359 - val_loss: 125.4218\n",
      "Epoch 1954/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 160.2171 - val_loss: 126.6417\n",
      "Epoch 1955/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 148.0658 - val_loss: 153.1883\n",
      "Epoch 1956/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 155.0276 - val_loss: 123.6774\n",
      "Epoch 1957/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 184.4668 - val_loss: 127.4983\n",
      "Epoch 1958/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.9894 - val_loss: 154.3838\n",
      "Epoch 1959/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 155.6499 - val_loss: 128.3905\n",
      "Epoch 1960/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 168.6978 - val_loss: 146.9669\n",
      "Epoch 1961/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 167.6520 - val_loss: 120.3494\n",
      "Epoch 1962/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 171.2765 - val_loss: 124.2848\n",
      "Epoch 1963/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 160.2061 - val_loss: 121.6072\n",
      "Epoch 1964/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 161.0387 - val_loss: 122.6734\n",
      "Epoch 1965/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 152.2600 - val_loss: 121.0810\n",
      "Epoch 1966/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 157.0791 - val_loss: 123.5112\n",
      "Epoch 1967/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 162.5617 - val_loss: 116.5386\n",
      "Epoch 1968/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 146.6043 - val_loss: 122.9215\n",
      "Epoch 1969/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 164.9058 - val_loss: 147.5597\n",
      "Epoch 1970/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 160.7592 - val_loss: 120.6124\n",
      "Epoch 1971/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 454.0593 - val_loss: 171.9272\n",
      "Epoch 1972/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 202.3783 - val_loss: 126.0196\n",
      "Epoch 1973/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 194.1816 - val_loss: 135.8135\n",
      "Epoch 1974/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 175.2030 - val_loss: 134.0305\n",
      "Epoch 1975/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 179.2527 - val_loss: 141.0673\n",
      "Epoch 1976/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 159.5257 - val_loss: 125.4295\n",
      "Epoch 1977/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 184.6648 - val_loss: 132.3952\n",
      "Epoch 1978/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 173.3274 - val_loss: 135.6413\n",
      "Epoch 1979/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 182.1288 - val_loss: 129.3677\n",
      "Epoch 1980/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 164.5406 - val_loss: 125.2996\n",
      "Epoch 1981/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 175.9494 - val_loss: 123.5650\n",
      "Epoch 1982/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 203.3736 - val_loss: 167.6378\n",
      "Epoch 1983/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 186.3651 - val_loss: 131.2041\n",
      "Epoch 1984/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 169.2636 - val_loss: 140.8880\n",
      "Epoch 1985/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 173.5462 - val_loss: 132.6471\n",
      "Epoch 1986/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 187.4237 - val_loss: 136.4426\n",
      "Epoch 1987/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.6859 - val_loss: 128.8654\n",
      "Epoch 1988/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 165.3925 - val_loss: 133.8092\n",
      "Epoch 1989/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 175.8509 - val_loss: 123.8831\n",
      "Epoch 1990/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.5400 - val_loss: 153.7433\n",
      "Epoch 1991/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 160.2221 - val_loss: 118.2956\n",
      "Epoch 1992/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 168.1950 - val_loss: 123.7795\n",
      "Epoch 1993/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 157.5107 - val_loss: 122.4282\n",
      "Epoch 1994/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 160.2362 - val_loss: 163.5312\n",
      "Epoch 1995/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 154.5676 - val_loss: 123.1030\n",
      "Epoch 1996/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 200.4549 - val_loss: 125.7721\n",
      "Epoch 1997/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 152.3086 - val_loss: 122.1623\n",
      "Epoch 1998/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 185.5854 - val_loss: 125.1443\n",
      "Epoch 1999/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 144.7075 - val_loss: 141.5592\n",
      "Epoch 2000/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 145.6409 - val_loss: 133.0343\n",
      "Epoch 2001/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 150.5616 - val_loss: 123.7984\n",
      "Epoch 2002/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 143.7784 - val_loss: 131.1766\n",
      "Epoch 2003/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 176.3496 - val_loss: 201.0148\n",
      "Epoch 2004/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 147.5875 - val_loss: 128.4609\n",
      "Epoch 2005/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 148.8529 - val_loss: 125.7704\n",
      "Epoch 2006/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 154.8283 - val_loss: 164.7936\n",
      "Epoch 2007/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 162.2031 - val_loss: 128.5423\n",
      "Epoch 2008/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 158.3396 - val_loss: 128.9387\n",
      "Epoch 2009/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 150.3589 - val_loss: 121.5039\n",
      "Epoch 2010/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 151.9720 - val_loss: 128.8646\n",
      "Epoch 2011/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 163.4102 - val_loss: 127.0788\n",
      "Epoch 2012/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 147.7397 - val_loss: 114.9223\n",
      "Epoch 2013/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.4787 - val_loss: 114.6283\n",
      "Epoch 2014/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 168.6485 - val_loss: 131.2153\n",
      "Epoch 2015/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 142.5414 - val_loss: 119.2993\n",
      "Epoch 2016/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 184.2170 - val_loss: 145.7209\n",
      "Epoch 2017/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 168.2712 - val_loss: 132.4939\n",
      "Epoch 2018/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.0948 - val_loss: 127.1614\n",
      "Epoch 2019/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 139.1703 - val_loss: 159.2214\n",
      "Epoch 2020/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 152.0652 - val_loss: 117.2436\n",
      "Epoch 2021/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 148.6615 - val_loss: 135.2743\n",
      "Epoch 2022/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 142.5543 - val_loss: 119.9394\n",
      "Epoch 2023/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 136.8225 - val_loss: 143.7042\n",
      "Epoch 2024/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 144.3760 - val_loss: 120.4204\n",
      "Epoch 2025/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 157.1495 - val_loss: 128.2555\n",
      "Epoch 2026/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 143.8494 - val_loss: 128.7679\n",
      "Epoch 2027/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.1945 - val_loss: 198.9877\n",
      "Epoch 2028/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 155.0732 - val_loss: 136.4685\n",
      "Epoch 2029/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 147.1540 - val_loss: 132.5673\n",
      "Epoch 2030/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 71us/step - loss: 162.0225 - val_loss: 332.9103\n",
      "Epoch 2031/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 155.3698 - val_loss: 115.0057\n",
      "Epoch 2032/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 145.0273 - val_loss: 127.6118\n",
      "Epoch 2033/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.3666 - val_loss: 113.5652\n",
      "Epoch 2034/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.1620 - val_loss: 119.9284\n",
      "Epoch 2035/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 141.2533 - val_loss: 145.4674\n",
      "Epoch 2036/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 172.0471 - val_loss: 121.1701\n",
      "Epoch 2037/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 152.9150 - val_loss: 232.9149\n",
      "Epoch 2038/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.3314 - val_loss: 120.2807\n",
      "Epoch 2039/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 169.7032 - val_loss: 115.5261\n",
      "Epoch 2040/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 135.4188 - val_loss: 139.7031\n",
      "Epoch 2041/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 143.8199 - val_loss: 123.8390\n",
      "Epoch 2042/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.4955 - val_loss: 127.4946\n",
      "Epoch 2043/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 157.8982 - val_loss: 126.3324\n",
      "Epoch 2044/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 151.8745 - val_loss: 122.1653\n",
      "Epoch 2045/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 140.3938 - val_loss: 115.1745\n",
      "Epoch 2046/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 145.9171 - val_loss: 175.9824\n",
      "Epoch 2047/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 226.3994 - val_loss: 192.4226\n",
      "Epoch 2048/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 155.4385 - val_loss: 130.1109\n",
      "Epoch 2049/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 145.1097 - val_loss: 118.9867\n",
      "Epoch 2050/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 148.6376 - val_loss: 140.1480\n",
      "Epoch 2051/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 219.9559 - val_loss: 118.2109\n",
      "Epoch 2052/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 141.7545 - val_loss: 133.7640\n",
      "Epoch 2053/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 144.1376 - val_loss: 116.7859\n",
      "Epoch 2054/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 134.6145 - val_loss: 116.4886\n",
      "Epoch 2055/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 135.9964 - val_loss: 113.7682\n",
      "Epoch 2056/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 182.0907 - val_loss: 120.6041\n",
      "Epoch 2057/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 163.6092 - val_loss: 129.3288\n",
      "Epoch 2058/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.9632 - val_loss: 119.3708\n",
      "Epoch 2059/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 149.4422 - val_loss: 122.4572\n",
      "Epoch 2060/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 136.0651 - val_loss: 115.3592\n",
      "Epoch 2061/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.5224 - val_loss: 181.1283\n",
      "Epoch 2062/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 143.7023 - val_loss: 118.6016\n",
      "Epoch 2063/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 165.1981 - val_loss: 137.5779\n",
      "Epoch 2064/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 214.3548 - val_loss: 116.5242\n",
      "Epoch 2065/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.3263 - val_loss: 114.0138\n",
      "Epoch 2066/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 139.4948 - val_loss: 118.6751\n",
      "Epoch 2067/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 154.1352 - val_loss: 137.0381\n",
      "Epoch 2068/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 139.3663 - val_loss: 129.0422\n",
      "Epoch 2069/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 147.2496 - val_loss: 124.5935\n",
      "Epoch 2070/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 144.6549 - val_loss: 117.0650\n",
      "Epoch 2071/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 148.9171 - val_loss: 115.7126\n",
      "Epoch 2072/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.8814 - val_loss: 113.6076\n",
      "Epoch 2073/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 154.0405 - val_loss: 129.4905\n",
      "Epoch 2074/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 141.6533 - val_loss: 116.0436\n",
      "Epoch 2075/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 137.6080 - val_loss: 133.8678\n",
      "Epoch 2076/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 148.0835 - val_loss: 148.8986\n",
      "Epoch 2077/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 173.1571 - val_loss: 122.5743\n",
      "Epoch 2078/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 147.2656 - val_loss: 116.6334\n",
      "Epoch 2079/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 148.9596 - val_loss: 115.9838\n",
      "Epoch 2080/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 143.1569 - val_loss: 141.2197\n",
      "Epoch 2081/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 168.4725 - val_loss: 181.4336\n",
      "Epoch 2082/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 155.9801 - val_loss: 136.7575\n",
      "Epoch 2083/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.6896 - val_loss: 121.7976\n",
      "Epoch 2084/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 169.5214 - val_loss: 177.5553\n",
      "Epoch 2085/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 195.4621 - val_loss: 149.0229\n",
      "Epoch 2086/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.2225 - val_loss: 114.2400\n",
      "Epoch 2087/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 144.3336 - val_loss: 171.4268\n",
      "Epoch 2088/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 145.5295 - val_loss: 112.7180\n",
      "Epoch 2089/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 145.5868 - val_loss: 169.1037\n",
      "Epoch 2090/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.4551 - val_loss: 128.3437\n",
      "Epoch 2091/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 148.2742 - val_loss: 116.1688\n",
      "Epoch 2092/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 149.8501 - val_loss: 119.4111\n",
      "Epoch 2093/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 147.3371 - val_loss: 119.8036\n",
      "Epoch 2094/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 145.8957 - val_loss: 134.5350\n",
      "Epoch 2095/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 145.6249 - val_loss: 187.3001\n",
      "Epoch 2096/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 142.0197 - val_loss: 135.2582\n",
      "Epoch 2097/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 150.2771 - val_loss: 130.1694\n",
      "Epoch 2098/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 145.3665 - val_loss: 167.7024\n",
      "Epoch 2099/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 154.6067 - val_loss: 116.3710\n",
      "Epoch 2100/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 148.1285 - val_loss: 113.6297\n",
      "Epoch 2101/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 145.5223 - val_loss: 127.7586\n",
      "Epoch 2102/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 81us/step - loss: 146.8433 - val_loss: 145.4811\n",
      "Epoch 2103/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 144.3286 - val_loss: 141.4100\n",
      "Epoch 2104/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 138.9911 - val_loss: 117.5034\n",
      "Epoch 2105/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 159.2813 - val_loss: 123.9504\n",
      "Epoch 2106/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 149.1611 - val_loss: 115.1190\n",
      "Epoch 2107/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 151.9247 - val_loss: 111.9545\n",
      "Epoch 2108/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 144.7773 - val_loss: 132.4886\n",
      "Epoch 2109/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 155.1949 - val_loss: 129.6872\n",
      "Epoch 2110/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 162.3120 - val_loss: 124.9825\n",
      "Epoch 2111/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 148.0378 - val_loss: 125.3312\n",
      "Epoch 2112/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 170.3894 - val_loss: 119.5210\n",
      "Epoch 2113/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.7308 - val_loss: 142.4028\n",
      "Epoch 2114/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 139.5847 - val_loss: 125.4459\n",
      "Epoch 2115/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 142.7338 - val_loss: 233.1453\n",
      "Epoch 2116/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 199.0970 - val_loss: 111.4633\n",
      "Epoch 2117/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.4857 - val_loss: 117.5325- ETA: 0s - loss: \n",
      "Epoch 2118/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 147.7750 - val_loss: 118.4504\n",
      "Epoch 2119/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 134.8486 - val_loss: 113.0381\n",
      "Epoch 2120/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.6560 - val_loss: 129.4891\n",
      "Epoch 2121/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 165.7064 - val_loss: 142.6967\n",
      "Epoch 2122/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 143.2248 - val_loss: 127.2471\n",
      "Epoch 2123/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 139.9895 - val_loss: 129.9278\n",
      "Epoch 2124/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 172.9165 - val_loss: 114.5204\n",
      "Epoch 2125/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 142.1828 - val_loss: 113.3488\n",
      "Epoch 2126/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 139.3433 - val_loss: 113.6238\n",
      "Epoch 2127/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 156.4562 - val_loss: 113.2567\n",
      "Epoch 2128/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.4676 - val_loss: 181.2684\n",
      "Epoch 2129/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 146.3948 - val_loss: 112.8194\n",
      "Epoch 2130/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.3007 - val_loss: 154.1812\n",
      "Epoch 2131/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 140.2391 - val_loss: 113.9020\n",
      "Epoch 2132/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 168.8941 - val_loss: 509.1634\n",
      "Epoch 2133/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 153.1838 - val_loss: 127.8972\n",
      "Epoch 2134/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 135.8503 - val_loss: 113.6102\n",
      "Epoch 2135/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 143.5359 - val_loss: 130.2765\n",
      "Epoch 2136/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 153.9194 - val_loss: 126.8254\n",
      "Epoch 2137/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 141.8272 - val_loss: 134.7143\n",
      "Epoch 2138/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 171.3659 - val_loss: 116.3589\n",
      "Epoch 2139/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 140.7766 - val_loss: 116.8424\n",
      "Epoch 2140/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 184.9155 - val_loss: 159.7554\n",
      "Epoch 2141/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 137.9599 - val_loss: 132.4367\n",
      "Epoch 2142/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 132.8504 - val_loss: 131.3155\n",
      "Epoch 2143/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.7353 - val_loss: 122.2864\n",
      "Epoch 2144/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 136.3737 - val_loss: 165.4562\n",
      "Epoch 2145/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 145.3965 - val_loss: 120.3344\n",
      "Epoch 2146/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 144.8801 - val_loss: 112.6794\n",
      "Epoch 2147/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.9921 - val_loss: 120.2808\n",
      "Epoch 2148/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 149.8910 - val_loss: 120.4711\n",
      "Epoch 2149/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 157.7016 - val_loss: 160.7351\n",
      "Epoch 2150/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 148.3196 - val_loss: 126.4645\n",
      "Epoch 2151/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 195.6732 - val_loss: 278.5191\n",
      "Epoch 2152/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 185.6713 - val_loss: 160.3406\n",
      "Epoch 2153/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 166.7700 - val_loss: 141.5580\n",
      "Epoch 2154/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 144.5294 - val_loss: 225.2174\n",
      "Epoch 2155/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 162.6020 - val_loss: 129.2512\n",
      "Epoch 2156/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 175.0194 - val_loss: 191.7985\n",
      "Epoch 2157/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 153.6365 - val_loss: 117.4483\n",
      "Epoch 2158/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 165.8164 - val_loss: 122.9485\n",
      "Epoch 2159/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 168.1714 - val_loss: 138.3022\n",
      "Epoch 2160/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 150.8785 - val_loss: 113.9979\n",
      "Epoch 2161/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.7376 - val_loss: 116.0362\n",
      "Epoch 2162/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 153.1703 - val_loss: 135.3608\n",
      "Epoch 2163/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 148.0508 - val_loss: 127.5036\n",
      "Epoch 2164/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.9013 - val_loss: 131.9172\n",
      "Epoch 2165/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 147.3823 - val_loss: 119.4848\n",
      "Epoch 2166/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 154.5266 - val_loss: 166.6213\n",
      "Epoch 2167/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 144.3345 - val_loss: 135.9784\n",
      "Epoch 2168/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.9970 - val_loss: 118.3401\n",
      "Epoch 2169/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 149.4164 - val_loss: 130.8498\n",
      "Epoch 2170/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 172.8827 - val_loss: 113.6253\n",
      "Epoch 2171/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.5479 - val_loss: 119.8595\n",
      "Epoch 2172/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 139.8924 - val_loss: 121.8721\n",
      "Epoch 2173/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 141.0003 - val_loss: 124.1514\n",
      "Epoch 2174/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.7477 - val_loss: 124.4032\n",
      "Epoch 2175/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 136.0143 - val_loss: 119.0623\n",
      "Epoch 2176/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 141.0576 - val_loss: 113.1141\n",
      "Epoch 2177/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 140.6236 - val_loss: 120.8880\n",
      "Epoch 2178/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 180.5687 - val_loss: 141.5228\n",
      "Epoch 2179/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 154.6041 - val_loss: 145.1100\n",
      "Epoch 2180/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 150.0913 - val_loss: 117.0774\n",
      "Epoch 2181/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 135.4079 - val_loss: 118.3094\n",
      "Epoch 2182/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.7721 - val_loss: 113.2801\n",
      "Epoch 2183/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 159.8259 - val_loss: 121.0496\n",
      "Epoch 2184/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 151.4291 - val_loss: 159.0095\n",
      "Epoch 2185/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 164.5320 - val_loss: 126.0366\n",
      "Epoch 2186/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 143.2442 - val_loss: 131.5570\n",
      "Epoch 2187/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 173.8421 - val_loss: 127.3984\n",
      "Epoch 2188/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 188.3693 - val_loss: 116.4604\n",
      "Epoch 2189/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 157.7591 - val_loss: 130.1180\n",
      "Epoch 2190/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 132.4250 - val_loss: 117.8218\n",
      "Epoch 2191/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 196.4517 - val_loss: 118.6926\n",
      "Epoch 2192/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 163.6138 - val_loss: 120.5026\n",
      "Epoch 2193/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 159.4448 - val_loss: 149.9836\n",
      "Epoch 2194/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.9756 - val_loss: 123.1905\n",
      "Epoch 2195/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 141.6470 - val_loss: 114.7663\n",
      "Epoch 2196/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 180.5814 - val_loss: 113.9169\n",
      "Epoch 2197/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 137.3760 - val_loss: 111.9378\n",
      "Epoch 2198/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 142.5285 - val_loss: 135.0388\n",
      "Epoch 2199/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 143.5851 - val_loss: 154.3767\n",
      "Epoch 2200/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 137.5267 - val_loss: 134.8231\n",
      "Epoch 2201/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 137.9359 - val_loss: 114.0027\n",
      "Epoch 2202/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 135.5733 - val_loss: 134.8705\n",
      "Epoch 2203/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 139.9652 - val_loss: 126.1985\n",
      "Epoch 2204/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.8376 - val_loss: 135.1367\n",
      "Epoch 2205/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 143.0278 - val_loss: 122.6194\n",
      "Epoch 2206/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 158.5209 - val_loss: 127.9619\n",
      "Epoch 2207/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 171.3476 - val_loss: 126.5975\n",
      "Epoch 2208/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 162.0816 - val_loss: 157.3275\n",
      "Epoch 2209/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 190.0987 - val_loss: 123.5565\n",
      "Epoch 2210/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 148.2031 - val_loss: 128.3565\n",
      "Epoch 2211/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 149.2466 - val_loss: 115.5145\n",
      "Epoch 2212/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 139.6508 - val_loss: 119.8558\n",
      "Epoch 2213/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 143.3888 - val_loss: 123.8957\n",
      "Epoch 2214/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 180.5381 - val_loss: 158.9710\n",
      "Epoch 2215/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 151.2361 - val_loss: 129.1959\n",
      "Epoch 2216/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 140.3086 - val_loss: 118.4085\n",
      "Epoch 2217/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 179.6087 - val_loss: 129.7253\n",
      "Epoch 2218/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 139.4351 - val_loss: 117.5862\n",
      "Epoch 2219/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 152.1829 - val_loss: 112.5120\n",
      "Epoch 2220/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 137.0157 - val_loss: 128.4222\n",
      "Epoch 2221/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 137.2837 - val_loss: 119.0483\n",
      "Epoch 2222/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 144.0005 - val_loss: 268.3576\n",
      "Epoch 2223/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 141.4783 - val_loss: 135.1426\n",
      "Epoch 2224/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 143.8992 - val_loss: 130.9586\n",
      "Epoch 2225/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 141.5691 - val_loss: 126.8573\n",
      "Epoch 2226/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 148.1703 - val_loss: 116.0592\n",
      "Epoch 2227/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 143.2528 - val_loss: 133.5919\n",
      "Epoch 2228/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 140.0850 - val_loss: 132.2053\n",
      "Epoch 2229/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 140.4206 - val_loss: 132.7078\n",
      "Epoch 2230/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 156.4228 - val_loss: 120.0352\n",
      "Epoch 2231/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 140.3006 - val_loss: 127.8002\n",
      "Epoch 2232/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 157.9504 - val_loss: 119.3139\n",
      "Epoch 2233/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.0288 - val_loss: 136.6831\n",
      "Epoch 2234/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.1565 - val_loss: 142.3972\n",
      "Epoch 2235/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 151.4469 - val_loss: 118.3981\n",
      "Epoch 2236/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 155.5152 - val_loss: 164.0794\n",
      "Epoch 2237/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 151.6801 - val_loss: 122.8595\n",
      "Epoch 2238/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 144.0860 - val_loss: 122.2697\n",
      "Epoch 2239/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 143.5796 - val_loss: 116.6085\n",
      "Epoch 2240/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 144.9161 - val_loss: 128.3253\n",
      "Epoch 2241/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 142.5593 - val_loss: 110.5343\n",
      "Epoch 2242/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 161.0261 - val_loss: 113.9603\n",
      "Epoch 2243/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 144.1798 - val_loss: 113.3443\n",
      "Epoch 2244/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 138.4912 - val_loss: 122.0951\n",
      "Epoch 2245/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 145.9055 - val_loss: 122.7081\n",
      "Epoch 2246/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 139.0537 - val_loss: 115.1104\n",
      "Epoch 2247/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 143.3379 - val_loss: 118.9333\n",
      "Epoch 2248/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 191.5946 - val_loss: 126.6943\n",
      "Epoch 2249/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 135.5090 - val_loss: 113.8429\n",
      "Epoch 2250/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 153.8273 - val_loss: 133.2010\n",
      "Epoch 2251/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 137.8662 - val_loss: 129.4570\n",
      "Epoch 2252/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 138.2996 - val_loss: 120.0426\n",
      "Epoch 2253/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 137.7076 - val_loss: 136.5674\n",
      "Epoch 2254/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 142.3713 - val_loss: 124.3504\n",
      "Epoch 2255/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 149.3096 - val_loss: 133.6659\n",
      "Epoch 2256/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 185.0389 - val_loss: 116.8443\n",
      "Epoch 2257/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 146.5574 - val_loss: 143.4656\n",
      "Epoch 2258/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 182.7606 - val_loss: 124.9645\n",
      "Epoch 2259/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 155.3466 - val_loss: 129.6408\n",
      "Epoch 2260/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 167.2849 - val_loss: 118.6532\n",
      "Epoch 2261/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 151.1519 - val_loss: 114.7935\n",
      "Epoch 2262/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 139.4170 - val_loss: 117.2982\n",
      "Epoch 2263/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 139.1273 - val_loss: 118.7886\n",
      "Epoch 2264/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 151.7498 - val_loss: 118.8195\n",
      "Epoch 2265/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 133.3976 - val_loss: 123.5075\n",
      "Epoch 2266/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 145.4213 - val_loss: 137.4733\n",
      "Epoch 2267/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 147.1959 - val_loss: 112.4739\n",
      "Epoch 2268/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.5493 - val_loss: 120.8866\n",
      "Epoch 2269/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 140.6432 - val_loss: 133.5657\n",
      "Epoch 2270/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 134.7290 - val_loss: 117.3663\n",
      "Epoch 2271/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 149.0225 - val_loss: 150.2334\n",
      "Epoch 2272/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 136.8698 - val_loss: 117.9189\n",
      "Epoch 2273/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 145.0099 - val_loss: 187.0492\n",
      "Epoch 2274/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 148.4412 - val_loss: 114.6855\n",
      "Epoch 2275/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 138.7045 - val_loss: 140.4265\n",
      "Epoch 2276/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 154.7844 - val_loss: 120.4905\n",
      "Epoch 2277/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 142.8810 - val_loss: 144.4513\n",
      "Epoch 2278/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 147.8978 - val_loss: 148.5769\n",
      "Epoch 2279/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 148.6909 - val_loss: 141.2270\n",
      "Epoch 2280/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 304.7517 - val_loss: 170.8666\n",
      "Epoch 2281/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 149.7874 - val_loss: 170.9517\n",
      "Epoch 2282/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 153.6901 - val_loss: 116.5328\n",
      "Epoch 2283/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 137.5752 - val_loss: 117.7969\n",
      "Epoch 2284/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 144.0374 - val_loss: 117.7647\n",
      "Epoch 2285/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 134.4478 - val_loss: 123.6870\n",
      "Epoch 2286/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 145.4269 - val_loss: 140.5903\n",
      "Epoch 2287/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 177.8939 - val_loss: 117.1505\n",
      "Epoch 2288/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 138.9969 - val_loss: 137.2153\n",
      "Epoch 2289/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 140.4253 - val_loss: 128.6244\n",
      "Epoch 2290/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 161.5555 - val_loss: 116.4231\n",
      "Epoch 2291/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 137.4488 - val_loss: 121.3045\n",
      "Epoch 2292/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 133.2285 - val_loss: 124.5946\n",
      "Epoch 2293/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 139.7776 - val_loss: 116.6340\n",
      "Epoch 2294/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 136.2528 - val_loss: 111.7852\n",
      "Epoch 2295/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 148.5019 - val_loss: 119.2671\n",
      "Epoch 2296/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 212.7023 - val_loss: 134.1274\n",
      "Epoch 2297/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 187.8823 - val_loss: 141.2417\n",
      "Epoch 2298/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 176.4225 - val_loss: 116.8176\n",
      "Epoch 2299/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 234.0798 - val_loss: 134.3108\n",
      "Epoch 2300/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 152.0798 - val_loss: 115.7455\n",
      "Epoch 2301/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 144.3616 - val_loss: 173.9773\n",
      "Epoch 2302/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 134.4816 - val_loss: 135.5741\n",
      "Epoch 2303/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 156.8423 - val_loss: 194.0035\n",
      "Epoch 2304/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 141.3525 - val_loss: 122.9527\n",
      "Epoch 2305/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 131.6079 - val_loss: 115.2489\n",
      "Epoch 2306/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.4136 - val_loss: 132.9923\n",
      "Epoch 2307/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 139.251 - 1s 70us/step - loss: 140.4285 - val_loss: 113.3901\n",
      "Epoch 2308/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 169.2403 - val_loss: 118.0110\n",
      "Epoch 2309/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 135.0022 - val_loss: 114.6600\n",
      "Epoch 2310/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 140.6611 - val_loss: 114.8586\n",
      "Epoch 2311/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 148.1318 - val_loss: 122.8844\n",
      "Epoch 2312/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 136.2372 - val_loss: 121.4189\n",
      "Epoch 2313/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.9069 - val_loss: 115.3635\n",
      "Epoch 2314/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 163.0377 - val_loss: 114.2033\n",
      "Epoch 2315/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 137.1197 - val_loss: 133.5410\n",
      "Epoch 2316/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 148.7733 - val_loss: 119.4115\n",
      "Epoch 2317/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 133.4612 - val_loss: 120.2080\n",
      "Epoch 2318/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 69us/step - loss: 138.3230 - val_loss: 121.5433\n",
      "Epoch 2319/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 139.6452 - val_loss: 117.8730\n",
      "Epoch 2320/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 139.5783 - val_loss: 178.6177\n",
      "Epoch 2321/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 140.3561 - val_loss: 115.1491\n",
      "Epoch 2322/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 139.8316 - val_loss: 137.1785\n",
      "Epoch 2323/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 176.5996 - val_loss: 121.6314\n",
      "Epoch 2324/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.8853 - val_loss: 113.2824\n",
      "Epoch 2325/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 139.4895 - val_loss: 125.7772\n",
      "Epoch 2326/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 204.3156 - val_loss: 133.5094\n",
      "Epoch 2327/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 143.3861 - val_loss: 114.1260\n",
      "Epoch 2328/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.6959 - val_loss: 183.2562\n",
      "Epoch 2329/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 141.4562 - val_loss: 115.4923\n",
      "Epoch 2330/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 151.7724 - val_loss: 125.3927\n",
      "Epoch 2331/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 142.2145 - val_loss: 123.1508\n",
      "Epoch 2332/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 133.6451 - val_loss: 122.4822\n",
      "Epoch 2333/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.2228 - val_loss: 117.5111\n",
      "Epoch 2334/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 147.2007 - val_loss: 143.7275\n",
      "Epoch 2335/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 145.4440 - val_loss: 127.3732\n",
      "Epoch 2336/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 139.2388 - val_loss: 115.2692\n",
      "Epoch 2337/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 141.4330 - val_loss: 117.3987\n",
      "Epoch 2338/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 144.5952 - val_loss: 116.4527\n",
      "Epoch 2339/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 142.3497 - val_loss: 113.6990\n",
      "Epoch 2340/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 160.7403 - val_loss: 128.8383\n",
      "Epoch 2341/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 133.5483 - val_loss: 116.8463\n",
      "Epoch 2342/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 137.9824 - val_loss: 136.6254\n",
      "Epoch 2343/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 137.5730 - val_loss: 141.8220\n",
      "Epoch 2344/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 136.1582 - val_loss: 108.2723\n",
      "Epoch 2345/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 161.5004 - val_loss: 413.8180\n",
      "Epoch 2346/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 151.9176 - val_loss: 122.6210\n",
      "Epoch 2347/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 134.5602 - val_loss: 130.6132\n",
      "Epoch 2348/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 143.3723 - val_loss: 155.9584\n",
      "Epoch 2349/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 153.9577 - val_loss: 121.3575\n",
      "Epoch 2350/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 148.3163 - val_loss: 141.7243\n",
      "Epoch 2351/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 160.8506 - val_loss: 125.8596\n",
      "Epoch 2352/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 163.0015 - val_loss: 151.7407\n",
      "Epoch 2353/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 137.7253 - val_loss: 112.8079\n",
      "Epoch 2354/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 137.3302 - val_loss: 109.8679\n",
      "Epoch 2355/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 141.4465 - val_loss: 113.7221\n",
      "Epoch 2356/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 137.5139 - val_loss: 164.1999\n",
      "Epoch 2357/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 147.9990 - val_loss: 109.2329\n",
      "Epoch 2358/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 175.2784 - val_loss: 117.4532\n",
      "Epoch 2359/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 160.3173 - val_loss: 158.4466\n",
      "Epoch 2360/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.4391 - val_loss: 110.2845\n",
      "Epoch 2361/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 161.5164 - val_loss: 150.5934\n",
      "Epoch 2362/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 162.1074 - val_loss: 117.7089\n",
      "Epoch 2363/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 135.9780 - val_loss: 136.1890\n",
      "Epoch 2364/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 134.7717 - val_loss: 140.4100\n",
      "Epoch 2365/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 136.4127 - val_loss: 119.2338\n",
      "Epoch 2366/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 154.0764 - val_loss: 159.2256\n",
      "Epoch 2367/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 181.0423 - val_loss: 109.9142\n",
      "Epoch 2368/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 136.4591 - val_loss: 118.6849\n",
      "Epoch 2369/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 136.1559 - val_loss: 135.6642\n",
      "Epoch 2370/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 141.5818 - val_loss: 162.1916\n",
      "Epoch 2371/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 147.7916 - val_loss: 216.3513\n",
      "Epoch 2372/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 148.6191 - val_loss: 155.9938\n",
      "Epoch 2373/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 141.5570 - val_loss: 116.2879\n",
      "Epoch 2374/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 138.4520 - val_loss: 129.0639\n",
      "Epoch 2375/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 134.7073 - val_loss: 133.6513\n",
      "Epoch 2376/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 139.6576 - val_loss: 116.0601\n",
      "Epoch 2377/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 142.3273 - val_loss: 188.0272\n",
      "Epoch 2378/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 172.8483 - val_loss: 144.5982\n",
      "Epoch 2379/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 142.6163 - val_loss: 113.8141\n",
      "Epoch 2380/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 147.9643 - val_loss: 111.3578\n",
      "Epoch 2381/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 142.8556 - val_loss: 115.7501\n",
      "Epoch 2382/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.6698 - val_loss: 123.8031\n",
      "Epoch 2383/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 224.5737 - val_loss: 123.0972\n",
      "Epoch 2384/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 139.1470 - val_loss: 111.7521\n",
      "Epoch 2385/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 143.7868 - val_loss: 117.8398\n",
      "Epoch 2386/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 145.2997 - val_loss: 121.6308\n",
      "Epoch 2387/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 132.2405 - val_loss: 126.5120\n",
      "Epoch 2388/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 161.8838 - val_loss: 123.0849\n",
      "Epoch 2389/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 142.0538 - val_loss: 162.1108\n",
      "Epoch 2390/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 69us/step - loss: 149.4329 - val_loss: 116.8950\n",
      "Epoch 2391/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 134.0516 - val_loss: 112.7382\n",
      "Epoch 2392/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 137.9502 - val_loss: 114.1794\n",
      "Epoch 2393/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 141.9972 - val_loss: 121.2660\n",
      "Epoch 2394/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 137.5681 - val_loss: 110.8195\n",
      "Epoch 2395/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 143.7566 - val_loss: 136.0222\n",
      "Epoch 2396/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 136.2714 - val_loss: 114.1611\n",
      "Epoch 2397/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 148.5459 - val_loss: 122.8290\n",
      "Epoch 2398/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 143.3008 - val_loss: 127.9817\n",
      "Epoch 2399/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 138.2009 - val_loss: 116.9616\n",
      "Epoch 2400/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 175.2417 - val_loss: 131.9828\n",
      "Epoch 2401/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 141.0332 - val_loss: 122.4352\n",
      "Epoch 2402/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 223.5451 - val_loss: 115.2600\n",
      "Epoch 2403/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 135.3273 - val_loss: 116.7497\n",
      "Epoch 2404/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 134.0199 - val_loss: 122.0570\n",
      "Epoch 2405/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 143.1257 - val_loss: 116.1221\n",
      "Epoch 2406/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 132.9823 - val_loss: 113.1778\n",
      "Epoch 2407/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 133.6800 - val_loss: 114.9116\n",
      "Epoch 2408/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 133.0590 - val_loss: 126.1879\n",
      "Epoch 2409/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 132.9739 - val_loss: 112.5820\n",
      "Epoch 2410/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 134.6518 - val_loss: 120.0000\n",
      "Epoch 2411/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 132.6562 - val_loss: 117.8459\n",
      "Epoch 2412/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 138.6138 - val_loss: 115.1480\n",
      "Epoch 2413/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 141.1279 - val_loss: 119.7016\n",
      "Epoch 2414/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 143.4388 - val_loss: 129.2685\n",
      "Epoch 2415/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 141.5096 - val_loss: 127.1027\n",
      "Epoch 2416/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 137.9688 - val_loss: 120.6139\n",
      "Epoch 2417/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 157.3399 - val_loss: 129.7711\n",
      "Epoch 2418/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 141.1540 - val_loss: 112.4281\n",
      "Epoch 2419/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 144.6107 - val_loss: 126.0710\n",
      "Epoch 2420/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 140.4740 - val_loss: 111.9799\n",
      "Epoch 2421/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 143.5851 - val_loss: 172.3637\n",
      "Epoch 2422/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 159.4008 - val_loss: 118.4417\n",
      "Epoch 2423/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 136.1885 - val_loss: 125.1242\n",
      "Epoch 2424/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 176.0881 - val_loss: 155.4797\n",
      "Epoch 2425/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 154.0263 - val_loss: 113.1382\n",
      "Epoch 2426/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 149.2014 - val_loss: 124.2968\n",
      "Epoch 2427/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 146.1250 - val_loss: 112.8640\n",
      "Epoch 2428/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 137.0224 - val_loss: 118.3063\n",
      "Epoch 2429/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 137.6995 - val_loss: 110.7509\n",
      "Epoch 2430/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 240.2106 - val_loss: 154.9961\n",
      "Epoch 2431/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 197.7358 - val_loss: 153.4020\n",
      "Epoch 2432/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 167.6531 - val_loss: 151.1640\n",
      "Epoch 2433/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 141.2270 - val_loss: 156.9879\n",
      "Epoch 2434/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 143.7600 - val_loss: 115.5728\n",
      "Epoch 2435/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 154.2279 - val_loss: 169.7672\n",
      "Epoch 2436/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 139.1225 - val_loss: 113.8827\n",
      "Epoch 2437/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 134.9519 - val_loss: 117.0035\n",
      "Epoch 2438/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 136.4985 - val_loss: 121.3765\n",
      "Epoch 2439/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 142.6225 - val_loss: 133.9184\n",
      "Epoch 2440/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 137.0761 - val_loss: 114.4392\n",
      "Epoch 2441/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 133.6101 - val_loss: 158.9881\n",
      "Epoch 2442/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 144.2834 - val_loss: 140.2349\n",
      "Epoch 2443/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 138.6778 - val_loss: 123.9539\n",
      "Epoch 2444/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 144.1771 - val_loss: 141.1969\n",
      "Epoch 2445/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 162.3850 - val_loss: 122.6371\n",
      "Epoch 2446/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 150.3396 - val_loss: 117.6771\n",
      "Epoch 2447/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 137.9980 - val_loss: 126.5828\n",
      "Epoch 2448/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 156.8258 - val_loss: 119.1194\n",
      "Epoch 2449/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 134.0293 - val_loss: 115.9098\n",
      "Epoch 2450/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 161.5879 - val_loss: 115.6944\n",
      "Epoch 2451/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 139.3299 - val_loss: 114.5281\n",
      "Epoch 2452/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 143.7528 - val_loss: 120.2798\n",
      "Epoch 2453/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 135.0004 - val_loss: 117.6256\n",
      "Epoch 2454/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 131.7180 - val_loss: 114.2329\n",
      "Epoch 2455/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 139.5113 - val_loss: 144.2492\n",
      "Epoch 2456/10000\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 149.9466 - val_loss: 119.6742\n",
      "Epoch 2457/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 140.5035 - val_loss: 119.4243\n",
      "Epoch 2458/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 136.2784 - val_loss: 136.5848\n",
      "Epoch 2459/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 134.8525 - val_loss: 119.8895\n",
      "Epoch 2460/10000\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 156.6114 - val_loss: 188.9624\n",
      "Epoch 2461/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 136.8598 - val_loss: 114.7261\n",
      "Epoch 2462/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 82us/step - loss: 141.3276 - val_loss: 111.6351\n",
      "Epoch 2463/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 137.3579 - val_loss: 129.3537\n",
      "Epoch 2464/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 134.6073 - val_loss: 118.6433\n",
      "Epoch 2465/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 135.8844 - val_loss: 113.9777\n",
      "Epoch 2466/10000\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 150.3975 - val_loss: 138.1426\n",
      "Epoch 2467/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 145.9448 - val_loss: 115.9355\n",
      "Epoch 2468/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 140.6316 - val_loss: 119.1483\n",
      "Epoch 2469/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 143.8433 - val_loss: 135.1109\n",
      "Epoch 2470/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 144.7227 - val_loss: 115.1880\n",
      "Epoch 2471/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 149.0744 - val_loss: 192.0556\n",
      "Epoch 2472/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 186.5423 - val_loss: 119.7212\n",
      "Epoch 2473/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 143.1034 - val_loss: 138.5813\n",
      "Epoch 2474/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 144.2501 - val_loss: 117.5617\n",
      "Epoch 2475/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 139.9585 - val_loss: 135.6285\n",
      "Epoch 2476/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 148.2195 - val_loss: 113.1138\n",
      "Epoch 2477/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 146.9713 - val_loss: 181.7610\n",
      "Epoch 2478/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 140.8223 - val_loss: 124.9235\n",
      "Epoch 2479/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 132.4715 - val_loss: 112.8276\n",
      "Epoch 2480/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 145.8784 - val_loss: 136.8489\n",
      "Epoch 2481/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 142.8694 - val_loss: 147.4171\n",
      "Epoch 2482/10000\n",
      "8000/8000 [==============================] - 1s 126us/step - loss: 136.3031 - val_loss: 162.9231\n",
      "Epoch 2483/10000\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 137.0169 - val_loss: 119.3838\n",
      "Epoch 2484/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 139.1591 - val_loss: 133.4382\n",
      "Epoch 2485/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 141.3159 - val_loss: 111.4550\n",
      "Epoch 2486/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 140.6612 - val_loss: 110.9891\n",
      "Epoch 2487/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 144.2967 - val_loss: 146.4251\n",
      "Epoch 2488/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 146.3186 - val_loss: 113.4933\n",
      "Epoch 2489/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 143.5187 - val_loss: 126.4657\n",
      "Epoch 2490/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 205.3764 - val_loss: 155.8015\n",
      "Epoch 2491/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 148.2993 - val_loss: 133.4301\n",
      "Epoch 2492/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 154.8356 - val_loss: 122.8433\n",
      "Epoch 2493/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 136.7304 - val_loss: 131.2440\n",
      "Epoch 2494/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 141.4973 - val_loss: 141.8139\n",
      "Epoch 2495/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 135.0510 - val_loss: 116.2271\n",
      "Epoch 2496/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 135.8148 - val_loss: 139.4952\n",
      "Epoch 2497/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 133.7818 - val_loss: 115.5896\n",
      "Epoch 2498/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 147.8362 - val_loss: 133.9654\n",
      "Epoch 2499/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 136.3024 - val_loss: 118.8937\n",
      "Epoch 2500/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 139.6637 - val_loss: 120.9480\n",
      "Epoch 2501/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 175.2647 - val_loss: 135.0711\n",
      "Epoch 2502/10000\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 169.9665 - val_loss: 115.1483\n",
      "Epoch 2503/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 136.3595 - val_loss: 113.9106\n",
      "Epoch 2504/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 135.3125 - val_loss: 141.5025\n",
      "Epoch 2505/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 136.0897 - val_loss: 114.1422\n",
      "Epoch 2506/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 135.9699 - val_loss: 114.5346\n",
      "Epoch 2507/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 140.7852 - val_loss: 246.2324\n",
      "Epoch 2508/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 169.5486 - val_loss: 175.1758\n",
      "Epoch 2509/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 231.5680 - val_loss: 144.0636\n",
      "Epoch 2510/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 134.1810 - val_loss: 125.1454\n",
      "Epoch 2511/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 134.8611 - val_loss: 123.5001\n",
      "Epoch 2512/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 139.4007 - val_loss: 122.3957\n",
      "Epoch 2513/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 134.3846 - val_loss: 119.7428\n",
      "Epoch 2514/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 136.0878 - val_loss: 136.9849\n",
      "Epoch 2515/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 137.4113 - val_loss: 116.1649\n",
      "Epoch 2516/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 139.5317 - val_loss: 121.8738\n",
      "Epoch 2517/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 148.9117 - val_loss: 126.1194\n",
      "Epoch 2518/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 140.9387 - val_loss: 131.7253\n",
      "Epoch 2519/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 142.7702 - val_loss: 169.0421\n",
      "Epoch 2520/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 167.5403 - val_loss: 115.0170\n",
      "Epoch 2521/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 147.4050 - val_loss: 135.8882\n",
      "Epoch 2522/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 137.9054 - val_loss: 134.0836\n",
      "Epoch 2523/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 143.5504 - val_loss: 111.5856\n",
      "Epoch 2524/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 140.4948 - val_loss: 149.6790\n",
      "Epoch 2525/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 139.0174 - val_loss: 112.2466\n",
      "Epoch 2526/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 166.3529 - val_loss: 130.1823\n",
      "Epoch 2527/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 137.1039 - val_loss: 119.6485\n",
      "Epoch 2528/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 136.5574 - val_loss: 130.1617\n",
      "Epoch 2529/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 135.7058 - val_loss: 144.0021\n",
      "Epoch 2530/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 165.8997 - val_loss: 127.6405\n",
      "Epoch 2531/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 149.3992 - val_loss: 182.1433\n",
      "Epoch 2532/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 153.2614 - val_loss: 119.4942\n",
      "Epoch 2533/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 148.8181 - val_loss: 123.4100\n",
      "Epoch 2534/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 74us/step - loss: 143.9109 - val_loss: 113.5608\n",
      "Epoch 2535/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 142.5140 - val_loss: 149.7261\n",
      "Epoch 2536/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 135.2397 - val_loss: 111.6526\n",
      "Epoch 2537/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 134.6299 - val_loss: 129.5893\n",
      "Epoch 2538/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 150.8544 - val_loss: 123.1605\n",
      "Epoch 2539/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 154.3626 - val_loss: 112.5472\n",
      "Epoch 2540/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 142.6750 - val_loss: 119.1241\n",
      "Epoch 2541/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 133.7924 - val_loss: 122.5506\n",
      "Epoch 2542/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 139.2491 - val_loss: 114.2751\n",
      "Epoch 2543/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 145.6935 - val_loss: 155.9044\n",
      "Epoch 2544/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 141.2838 - val_loss: 145.4621\n",
      "Epoch 2545/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 152.5691 - val_loss: 130.5433\n",
      "Epoch 2546/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 181.5756 - val_loss: 136.4868\n",
      "Epoch 2547/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 146.8016 - val_loss: 111.9788\n",
      "Epoch 2548/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 140.7235 - val_loss: 146.3937\n",
      "Epoch 2549/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 134.5936 - val_loss: 110.0367\n",
      "Epoch 2550/10000\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 137.7267 - val_loss: 181.3309\n",
      "Epoch 2551/10000\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 134.4258 - val_loss: 118.2245\n",
      "Epoch 2552/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 140.9141 - val_loss: 108.5706\n",
      "Epoch 2553/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 165.5331 - val_loss: 192.5806\n",
      "Epoch 2554/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 144.0157 - val_loss: 116.7510\n",
      "Epoch 2555/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 141.0979 - val_loss: 113.4655\n",
      "Epoch 2556/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 136.7722 - val_loss: 168.8132\n",
      "Epoch 2557/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 143.9233 - val_loss: 120.4844\n",
      "Epoch 2558/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 133.8141 - val_loss: 114.4882\n",
      "Epoch 2559/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 136.0062 - val_loss: 115.0386\n",
      "Epoch 2560/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 141.5690 - val_loss: 137.4342\n",
      "Epoch 2561/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 142.3777 - val_loss: 115.5420\n",
      "Epoch 2562/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 136.8319 - val_loss: 113.4282\n",
      "Epoch 2563/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 142.8146 - val_loss: 116.6397\n",
      "Epoch 2564/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 142.3407 - val_loss: 122.9310\n",
      "Epoch 2565/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 143.8117 - val_loss: 201.9753\n",
      "Epoch 2566/10000\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 144.1795 - val_loss: 111.2303\n",
      "Epoch 2567/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 139.4258 - val_loss: 125.4753\n",
      "Epoch 2568/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 142.0590 - val_loss: 111.1066\n",
      "Epoch 2569/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 142.0319 - val_loss: 114.0503\n",
      "Epoch 2570/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 143.3534 - val_loss: 116.7346\n",
      "Epoch 2571/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 147.2628 - val_loss: 131.8275\n",
      "Epoch 2572/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 144.5167 - val_loss: 114.3903\n",
      "Epoch 2573/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 159.2545 - val_loss: 127.4234\n",
      "Epoch 2574/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 139.1892 - val_loss: 113.1233\n",
      "Epoch 2575/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 140.4163 - val_loss: 115.1762\n",
      "Epoch 2576/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 139.7379 - val_loss: 146.4849\n",
      "Epoch 2577/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 150.9146 - val_loss: 151.7520\n",
      "Epoch 2578/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 137.2990 - val_loss: 116.2956\n",
      "Epoch 2579/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 138.8820 - val_loss: 117.4391\n",
      "Epoch 2580/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 134.0741 - val_loss: 119.5236\n",
      "Epoch 2581/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 141.0373 - val_loss: 111.0749\n",
      "Epoch 2582/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 140.8859 - val_loss: 116.0559\n",
      "Epoch 2583/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 149.9289 - val_loss: 131.8506\n",
      "Epoch 2584/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 142.8408 - val_loss: 116.6903\n",
      "Epoch 2585/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 137.5668 - val_loss: 116.7795\n",
      "Epoch 2586/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 147.2501 - val_loss: 116.5261\n",
      "Epoch 2587/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 139.3145 - val_loss: 114.4703\n",
      "Epoch 2588/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 132.5327 - val_loss: 116.0687\n",
      "Epoch 2589/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 136.2319 - val_loss: 116.2770\n",
      "Epoch 2590/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 153.9180 - val_loss: 112.7407\n",
      "Epoch 2591/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 142.3275 - val_loss: 112.9387\n",
      "Epoch 2592/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 135.6378 - val_loss: 146.2294\n",
      "Epoch 2593/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 142.5156 - val_loss: 121.0083\n",
      "Epoch 2594/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 140.2093 - val_loss: 127.3024\n",
      "Epoch 2595/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 136.0558 - val_loss: 111.0988\n",
      "Epoch 2596/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 142.2825 - val_loss: 128.5448\n",
      "Epoch 2597/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 139.9959 - val_loss: 124.9038\n",
      "Epoch 2598/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 138.7907 - val_loss: 126.6145\n",
      "Epoch 2599/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 144.0575 - val_loss: 135.0052\n",
      "Epoch 2600/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 211.9098 - val_loss: 120.0529\n",
      "Epoch 2601/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 137.0549 - val_loss: 113.0239\n",
      "Epoch 2602/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 135.2909 - val_loss: 109.3173\n",
      "Epoch 2603/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 167.7140 - val_loss: 133.4110\n",
      "Epoch 2604/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 134.8330 - val_loss: 149.5200\n",
      "Epoch 2605/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 132.6479 - val_loss: 111.6537\n",
      "Epoch 2606/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 73us/step - loss: 153.5614 - val_loss: 116.3520\n",
      "Epoch 2607/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 144.1809 - val_loss: 142.2978\n",
      "Epoch 2608/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 133.5702 - val_loss: 128.7863\n",
      "Epoch 2609/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 135.1917 - val_loss: 120.8242\n",
      "Epoch 2610/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 134.8718 - val_loss: 130.8971\n",
      "Epoch 2611/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 131.8528 - val_loss: 138.4343\n",
      "Epoch 2612/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 142.8767 - val_loss: 126.3200\n",
      "Epoch 2613/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 145.1074 - val_loss: 131.1905\n",
      "Epoch 2614/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 143.1040 - val_loss: 122.8008\n",
      "Epoch 2615/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 164.9462 - val_loss: 123.3666\n",
      "Epoch 2616/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 145.6735 - val_loss: 121.5731\n",
      "Epoch 2617/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 139.6355 - val_loss: 118.7780\n",
      "Epoch 2618/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 145.9398 - val_loss: 116.0493\n",
      "Epoch 2619/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 149.3498 - val_loss: 161.0291\n",
      "Epoch 2620/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 195.5853 - val_loss: 169.5911\n",
      "Epoch 2621/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 146.3865 - val_loss: 115.5905\n",
      "Epoch 2622/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 136.4012 - val_loss: 112.8349\n",
      "Epoch 2623/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 162.8236 - val_loss: 147.8760\n",
      "Epoch 2624/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 137.8737 - val_loss: 116.5238\n",
      "Epoch 2625/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 140.9773 - val_loss: 117.7770\n",
      "Epoch 2626/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 156.9434 - val_loss: 153.7125\n",
      "Epoch 2627/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 138.7032 - val_loss: 111.5810\n",
      "Epoch 2628/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 146.1759 - val_loss: 115.3306\n",
      "Epoch 2629/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 140.6113 - val_loss: 123.0568\n",
      "Epoch 2630/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 134.6856 - val_loss: 110.6449\n",
      "Epoch 2631/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 143.2949 - val_loss: 118.5122\n",
      "Epoch 2632/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 133.4441 - val_loss: 127.8317\n",
      "Epoch 2633/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 133.9191 - val_loss: 113.3634\n",
      "Epoch 2634/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 134.5086 - val_loss: 114.6268\n",
      "Epoch 2635/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 154.4017 - val_loss: 118.2011\n",
      "Epoch 2636/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 134.1331 - val_loss: 117.6243\n",
      "Epoch 2637/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 138.4815 - val_loss: 299.2652\n",
      "Epoch 2638/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 146.5894 - val_loss: 125.5939\n",
      "Epoch 2639/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 205.9167 - val_loss: 131.7991\n",
      "Epoch 2640/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 148.7997 - val_loss: 116.0774\n",
      "Epoch 2641/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 193.5810 - val_loss: 147.2146\n",
      "Epoch 2642/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 139.9106 - val_loss: 112.9336\n",
      "Epoch 2643/10000\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 140.0216 - val_loss: 125.5812\n",
      "Epoch 2644/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 132.4430 - val_loss: 113.1917\n",
      "Epoch 2645/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 135.2502 - val_loss: 113.4822\n",
      "Epoch 2646/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 136.8309 - val_loss: 126.7607\n",
      "Epoch 2647/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 168.3627 - val_loss: 117.5805\n",
      "Epoch 2648/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 134.8576 - val_loss: 118.1844\n",
      "Epoch 2649/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 163.2863 - val_loss: 110.5513\n",
      "Epoch 2650/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 134.2888 - val_loss: 116.1349\n",
      "Epoch 2651/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 138.8440 - val_loss: 115.9032\n",
      "Epoch 2652/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 145.7541 - val_loss: 117.4858\n",
      "Epoch 2653/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 141.2110 - val_loss: 133.7037\n",
      "Epoch 2654/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 142.8661 - val_loss: 119.2857\n",
      "Epoch 2655/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 138.2551 - val_loss: 143.7444\n",
      "Epoch 2656/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 140.8930 - val_loss: 122.4620\n",
      "Epoch 2657/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.2996 - val_loss: 125.1766\n",
      "Epoch 2658/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 135.8976 - val_loss: 123.7575\n",
      "Epoch 2659/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 155.7648 - val_loss: 327.8768\n",
      "Epoch 2660/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 147.1049 - val_loss: 129.0643\n",
      "Epoch 2661/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 137.7981 - val_loss: 113.8933\n",
      "Epoch 2662/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 143.8906 - val_loss: 121.0064\n",
      "Epoch 2663/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 149.8542 - val_loss: 166.9336\n",
      "Epoch 2664/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 136.4223 - val_loss: 112.1446\n",
      "Epoch 2665/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 147.1072 - val_loss: 114.0419\n",
      "Epoch 2666/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 141.0233 - val_loss: 114.9913\n",
      "Epoch 2667/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 130.9003 - val_loss: 156.5752\n",
      "Epoch 2668/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 162.0256 - val_loss: 112.8674\n",
      "Epoch 2669/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 134.9885 - val_loss: 110.2845\n",
      "Epoch 2670/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 157.3050 - val_loss: 132.8001\n",
      "Epoch 2671/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 201.1976 - val_loss: 117.2689\n",
      "Epoch 2672/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 135.9636 - val_loss: 116.3382\n",
      "Epoch 2673/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 138.3224 - val_loss: 122.4764\n",
      "Epoch 2674/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 135.2696 - val_loss: 135.4697\n",
      "Epoch 2675/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 132.7686 - val_loss: 115.8416\n",
      "Epoch 2676/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 135.5409 - val_loss: 117.1423\n",
      "Epoch 2677/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 139.8432 - val_loss: 116.7919\n",
      "Epoch 2678/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 93us/step - loss: 144.5541 - val_loss: 130.1396\n",
      "Epoch 2679/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 141.4518 - val_loss: 139.4881\n",
      "Epoch 2680/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 154.6416 - val_loss: 112.4903\n",
      "Epoch 2681/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 137.2628 - val_loss: 143.5806\n",
      "Epoch 2682/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 158.1020 - val_loss: 115.3009\n",
      "Epoch 2683/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 156.8037 - val_loss: 114.9636\n",
      "Epoch 2684/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 147.9279 - val_loss: 141.9374\n",
      "Epoch 2685/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 149.3578 - val_loss: 134.7809\n",
      "Epoch 2686/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 145.1838 - val_loss: 155.1784\n",
      "Epoch 2687/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 143.9525 - val_loss: 115.2473\n",
      "Epoch 2688/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 140.1377 - val_loss: 114.9397\n",
      "Epoch 2689/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 140.6279 - val_loss: 119.3181\n",
      "Epoch 2690/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 142.2200 - val_loss: 147.2609\n",
      "Epoch 2691/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 138.6196 - val_loss: 112.2121\n",
      "Epoch 2692/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 139.5566 - val_loss: 166.4867\n",
      "Epoch 2693/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 135.1543 - val_loss: 110.8771\n",
      "Epoch 2694/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 135.6137 - val_loss: 116.5168\n",
      "Epoch 2695/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 143.2721 - val_loss: 146.9518\n",
      "Epoch 2696/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 144.1884 - val_loss: 120.1097\n",
      "Epoch 2697/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 147.0248 - val_loss: 126.6338\n",
      "Epoch 2698/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 145.8204 - val_loss: 118.5345\n",
      "Epoch 2699/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 134.8090 - val_loss: 150.2233\n",
      "Epoch 2700/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 132.3636 - val_loss: 114.9970\n",
      "Epoch 2701/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 143.2024 - val_loss: 117.0830\n",
      "Epoch 2702/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 168.2676 - val_loss: 111.4325\n",
      "Epoch 2703/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 183.1793 - val_loss: 112.2239\n",
      "Epoch 2704/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 136.3734 - val_loss: 111.1581\n",
      "Epoch 2705/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 141.6905 - val_loss: 117.9648\n",
      "Epoch 2706/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 141.3145 - val_loss: 139.0934\n",
      "Epoch 2707/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 139.8280 - val_loss: 136.8677\n",
      "Epoch 2708/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 134.3032 - val_loss: 134.9733\n",
      "Epoch 2709/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 132.4610 - val_loss: 157.8980\n",
      "Epoch 2710/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 137.1612 - val_loss: 129.3649\n",
      "Epoch 2711/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 144.4352 - val_loss: 122.2564\n",
      "Epoch 2712/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 146.9216 - val_loss: 116.6483\n",
      "Epoch 2713/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 146.6116 - val_loss: 114.4653\n",
      "Epoch 2714/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 135.4154 - val_loss: 117.3305\n",
      "Epoch 2715/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 135.3560 - val_loss: 121.3662\n",
      "Epoch 2716/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 150.1093 - val_loss: 135.0591\n",
      "Epoch 2717/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 130.4665 - val_loss: 122.5469\n",
      "Epoch 2718/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 133.6796 - val_loss: 120.7206\n",
      "Epoch 2719/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 143.2683 - val_loss: 113.1180\n",
      "Epoch 2720/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 136.1600 - val_loss: 134.2950\n",
      "Epoch 2721/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 177.7995 - val_loss: 206.3204\n",
      "Epoch 2722/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 148.8098 - val_loss: 119.9477\n",
      "Epoch 2723/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 133.5693 - val_loss: 116.2038\n",
      "Epoch 2724/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 138.9940 - val_loss: 122.7117\n",
      "Epoch 2725/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 141.6823 - val_loss: 135.5016\n",
      "Epoch 2726/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 159.5048 - val_loss: 126.8897\n",
      "Epoch 2727/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 143.4666 - val_loss: 111.3747\n",
      "Epoch 2728/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 131.3372 - val_loss: 112.8143\n",
      "Epoch 2729/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 138.8513 - val_loss: 120.8086\n",
      "Epoch 2730/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 133.0249 - val_loss: 135.0661\n",
      "Epoch 2731/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 146.2426 - val_loss: 115.2103\n",
      "Epoch 2732/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 147.8265 - val_loss: 147.0250\n",
      "Epoch 2733/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 146.1293 - val_loss: 117.5318\n",
      "Epoch 2734/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 158.3647 - val_loss: 131.8010\n",
      "Epoch 2735/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 136.7940 - val_loss: 117.4237\n",
      "Epoch 2736/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 132.3067 - val_loss: 130.7732\n",
      "Epoch 2737/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 139.5705 - val_loss: 117.0776\n",
      "Epoch 2738/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 146.9913 - val_loss: 129.0085\n",
      "Epoch 2739/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 138.3481 - val_loss: 130.4963\n",
      "Epoch 2740/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 139.2249 - val_loss: 128.5550\n",
      "Epoch 2741/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 140.3565 - val_loss: 121.0159\n",
      "Epoch 2742/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 143.8513 - val_loss: 118.5559\n",
      "Epoch 2743/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 142.3755 - val_loss: 150.4724\n",
      "Epoch 2744/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 140.5963 - val_loss: 113.4339\n",
      "Epoch 2745/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 132.5789 - val_loss: 125.1660\n",
      "Epoch 2746/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 133.3087 - val_loss: 135.1220\n",
      "Epoch 2747/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 199.3293 - val_loss: 122.3307\n",
      "Epoch 2748/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 133.6989 - val_loss: 150.3048\n",
      "Epoch 2749/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 132.2758 - val_loss: 150.5524\n",
      "Epoch 2750/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 75us/step - loss: 142.6603 - val_loss: 121.3263\n",
      "Epoch 2751/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 165.9207 - val_loss: 126.9976\n",
      "Epoch 2752/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 131.6494 - val_loss: 117.0252\n",
      "Epoch 2753/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 149.9805 - val_loss: 119.1221\n",
      "Epoch 2754/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 143.2549 - val_loss: 126.8166\n",
      "Epoch 2755/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 132.0882 - val_loss: 115.9197\n",
      "Epoch 2756/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 133.6453 - val_loss: 120.9539\n",
      "Epoch 2757/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 144.5771 - val_loss: 124.2376\n",
      "Epoch 2758/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 138.9988 - val_loss: 111.1301\n",
      "Epoch 2759/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 154.5616 - val_loss: 146.9146\n",
      "Epoch 2760/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 167.4256 - val_loss: 112.4098\n",
      "Epoch 2761/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 143.0513 - val_loss: 125.1847\n",
      "Epoch 2762/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 135.5729 - val_loss: 120.3574\n",
      "Epoch 2763/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 132.9088 - val_loss: 131.0177\n",
      "Epoch 2764/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 146.1892 - val_loss: 115.4698\n",
      "Epoch 2765/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 133.5700 - val_loss: 122.9488\n",
      "Epoch 2766/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 139.0109 - val_loss: 133.0030\n",
      "Epoch 2767/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 153.0580 - val_loss: 123.2101\n",
      "Epoch 2768/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 132.9858 - val_loss: 123.3033\n",
      "Epoch 2769/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 144.8120 - val_loss: 121.8802\n",
      "Epoch 2770/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 149.4374- ETA: 0s - loss:  - 1s 70us/step - loss: 147.8636 - val_loss: 135.6248\n",
      "Epoch 2771/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 134.2508 - val_loss: 112.1343\n",
      "Epoch 2772/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 139.7155 - val_loss: 123.9416\n",
      "Epoch 2773/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 135.5108 - val_loss: 136.3967\n",
      "Epoch 2774/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 144.7983 - val_loss: 114.3386\n",
      "Epoch 2775/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 129.4522 - val_loss: 122.0365\n",
      "Epoch 2776/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 131.1852 - val_loss: 117.8039\n",
      "Epoch 2777/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 132.6141 - val_loss: 120.5039\n",
      "Epoch 2778/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 136.8176 - val_loss: 141.1665\n",
      "Epoch 2779/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.6040 - val_loss: 126.4542\n",
      "Epoch 2780/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 157.4974 - val_loss: 397.6034\n",
      "Epoch 2781/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 160.4028 - val_loss: 112.7707\n",
      "Epoch 2782/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 140.4791 - val_loss: 114.2295\n",
      "Epoch 2783/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 131.2023 - val_loss: 112.4900\n",
      "Epoch 2784/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 136.6729 - val_loss: 113.6567\n",
      "Epoch 2785/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 131.8253 - val_loss: 136.3891\n",
      "Epoch 2786/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 134.1497 - val_loss: 114.4417\n",
      "Epoch 2787/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 137.1201 - val_loss: 115.3821\n",
      "Epoch 2788/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 140.9188 - val_loss: 112.5673\n",
      "Epoch 2789/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 131.8085 - val_loss: 117.9954\n",
      "Epoch 2790/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 140.4737 - val_loss: 141.4599\n",
      "Epoch 2791/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 195.2927 - val_loss: 143.8391\n",
      "Epoch 2792/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 134.6221 - val_loss: 134.2796\n",
      "Epoch 2793/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 134.7204 - val_loss: 111.2558\n",
      "Epoch 2794/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 177.3055 - val_loss: 118.9802\n",
      "Epoch 2795/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 143.8806 - val_loss: 138.0824\n",
      "Epoch 2796/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 151.1020 - val_loss: 162.1552\n",
      "Epoch 2797/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 131.6790 - val_loss: 120.1013\n",
      "Epoch 2798/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 139.8846 - val_loss: 119.1941\n",
      "Epoch 2799/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 140.3273 - val_loss: 197.2942\n",
      "Epoch 2800/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 149.4439 - val_loss: 113.2232\n",
      "Epoch 2801/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 136.6642 - val_loss: 126.6794\n",
      "Epoch 2802/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 144.9139 - val_loss: 120.0797\n",
      "Epoch 2803/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 140.8177 - val_loss: 116.4700\n",
      "Epoch 2804/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 131.7773 - val_loss: 118.6533\n",
      "Epoch 2805/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 157.3451 - val_loss: 117.4930\n",
      "Epoch 2806/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 191.9062 - val_loss: 118.0951\n",
      "Epoch 2807/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 133.9780 - val_loss: 117.8317\n",
      "Epoch 2808/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 141.5292 - val_loss: 120.2283\n",
      "Epoch 2809/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 153.9945 - val_loss: 121.4572\n",
      "Epoch 2810/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 131.5554 - val_loss: 109.9025\n",
      "Epoch 2811/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 131.1199 - val_loss: 120.6779\n",
      "Epoch 2812/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 155.8245 - val_loss: 160.4981\n",
      "Epoch 2813/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 141.7407 - val_loss: 114.6665\n",
      "Epoch 2814/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 134.3486 - val_loss: 128.3536\n",
      "Epoch 2815/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 133.6556 - val_loss: 110.1480\n",
      "Epoch 2816/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 133.1405 - val_loss: 239.9570\n",
      "Epoch 2817/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 144.2672 - val_loss: 178.6998\n",
      "Epoch 2818/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 135.7897 - val_loss: 113.0808\n",
      "Epoch 2819/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 141.1036 - val_loss: 114.8756\n",
      "Epoch 2820/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 128.9056 - val_loss: 113.6518\n",
      "Epoch 2821/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 137.4451 - val_loss: 115.1981\n",
      "Epoch 2822/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 71us/step - loss: 136.5138 - val_loss: 117.8521\n",
      "Epoch 2823/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 156.1467 - val_loss: 116.6204\n",
      "Epoch 2824/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 136.3529 - val_loss: 119.3673\n",
      "Epoch 2825/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 134.3195 - val_loss: 126.0030\n",
      "Epoch 2826/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 139.3809 - val_loss: 164.0917\n",
      "Epoch 2827/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 241.6964 - val_loss: 168.9443\n",
      "Epoch 2828/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 168.8485 - val_loss: 119.9555\n",
      "Epoch 2829/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 238.9339 - val_loss: 128.6665\n",
      "Epoch 2830/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 136.8961 - val_loss: 143.7238\n",
      "Epoch 2831/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 136.4293 - val_loss: 114.9803\n",
      "Epoch 2832/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 137.3745 - val_loss: 131.1655\n",
      "Epoch 2833/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 149.6139 - val_loss: 125.2488\n",
      "Epoch 2834/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 142.6632 - val_loss: 176.1897\n",
      "Epoch 2835/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 135.9269 - val_loss: 113.7361\n",
      "Epoch 2836/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 146.1221 - val_loss: 111.1625\n",
      "Epoch 2837/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 188.7075 - val_loss: 117.3357\n",
      "Epoch 2838/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 136.7910 - val_loss: 110.7114\n",
      "Epoch 2839/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 132.8491 - val_loss: 110.8746\n",
      "Epoch 2840/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 132.1439 - val_loss: 118.8321\n",
      "Epoch 2841/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 137.7401 - val_loss: 116.3496\n",
      "Epoch 2842/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 135.6337 - val_loss: 113.8697\n",
      "Epoch 2843/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 143.2477 - val_loss: 151.9791\n",
      "Epoch 2844/10000\n",
      "8000/8000 [==============================] - 1s 150us/step - loss: 134.0122 - val_loss: 117.3644\n",
      "Epoch 2845/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 136.9759 - val_loss: 114.2031\n",
      "Epoch 2846/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 138.4589 - val_loss: 116.2601\n",
      "Epoch 2847/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 139.1062 - val_loss: 118.9935\n",
      "Epoch 2848/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 138.8627 - val_loss: 115.4919\n",
      "Epoch 2849/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 137.8251 - val_loss: 142.4352\n",
      "Epoch 2850/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 170.4465 - val_loss: 125.4901\n",
      "Epoch 2851/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 140.9425 - val_loss: 117.1088\n",
      "Epoch 2852/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 138.5720 - val_loss: 163.6207\n",
      "Epoch 2853/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 166.2718 - val_loss: 112.8862\n",
      "Epoch 2854/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 131.1724 - val_loss: 113.6527\n",
      "Epoch 2855/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 132.1238 - val_loss: 111.2298\n",
      "Epoch 2856/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 140.4141 - val_loss: 116.3382\n",
      "Epoch 2857/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 132.6668 - val_loss: 121.9771\n",
      "Epoch 2858/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 178.3633 - val_loss: 145.4622\n",
      "Epoch 2859/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 143.2824 - val_loss: 116.7577\n",
      "Epoch 2860/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 139.1919 - val_loss: 155.0605\n",
      "Epoch 2861/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 266.8402 - val_loss: 115.1254\n",
      "Epoch 2862/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 131.3563 - val_loss: 140.2660\n",
      "Epoch 2863/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 129.2348 - val_loss: 112.8337\n",
      "Epoch 2864/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 126.1165 - val_loss: 114.1692\n",
      "Epoch 2865/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 126.2179 - val_loss: 115.2264\n",
      "Epoch 2866/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 141.5296 - val_loss: 114.7824\n",
      "Epoch 2867/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 133.3831 - val_loss: 181.8210\n",
      "Epoch 2868/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 129.4390 - val_loss: 116.7845\n",
      "Epoch 2869/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 130.0856 - val_loss: 132.2202\n",
      "Epoch 2870/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 142.4091 - val_loss: 119.2969\n",
      "Epoch 2871/10000\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 151.7936 - val_loss: 202.2848\n",
      "Epoch 2872/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 140.2792 - val_loss: 112.1017\n",
      "Epoch 2873/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 185.5258 - val_loss: 131.2966\n",
      "Epoch 2874/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 177.0887 - val_loss: 171.0927\n",
      "Epoch 2875/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 157.1705 - val_loss: 146.8756\n",
      "Epoch 2876/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 163.1593 - val_loss: 149.0097\n",
      "Epoch 2877/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 148.3784 - val_loss: 160.1212\n",
      "Epoch 2878/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 154.8955 - val_loss: 119.7544\n",
      "Epoch 2879/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 161.8154 - val_loss: 125.8745\n",
      "Epoch 2880/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 146.1679 - val_loss: 123.1554\n",
      "Epoch 2881/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 161.1679 - val_loss: 119.6080\n",
      "Epoch 2882/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 171.7376 - val_loss: 116.4051\n",
      "Epoch 2883/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 143.3718 - val_loss: 124.8630\n",
      "Epoch 2884/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 141.9390 - val_loss: 135.8028\n",
      "Epoch 2885/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 156.8580 - val_loss: 145.2674\n",
      "Epoch 2886/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 142.5228 - val_loss: 115.1629\n",
      "Epoch 2887/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 135.6388 - val_loss: 120.1273\n",
      "Epoch 2888/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 143.0577 - val_loss: 113.9799\n",
      "Epoch 2889/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 156.3349 - val_loss: 147.2765\n",
      "Epoch 2890/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 140.6388 - val_loss: 114.8779\n",
      "Epoch 2891/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 133.2146 - val_loss: 122.0490\n",
      "Epoch 2892/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 135.5699 - val_loss: 115.7940\n",
      "Epoch 2893/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 139.4843 - val_loss: 127.0503\n",
      "Epoch 2894/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 96us/step - loss: 156.2235 - val_loss: 130.4260\n",
      "Epoch 2895/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 133.7384 - val_loss: 112.2179\n",
      "Epoch 2896/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 146.5951 - val_loss: 115.7985\n",
      "Epoch 2897/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 192.9082 - val_loss: 129.6400\n",
      "Epoch 2898/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 168.6970 - val_loss: 141.2330\n",
      "Epoch 2899/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 150.1620 - val_loss: 127.9554\n",
      "Epoch 2900/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 133.9020 - val_loss: 122.4442\n",
      "Epoch 2901/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 135.2400 - val_loss: 122.4879\n",
      "Epoch 2902/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 133.8802 - val_loss: 113.8524\n",
      "Epoch 2903/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 133.9873 - val_loss: 115.3430\n",
      "Epoch 2904/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 132.2850 - val_loss: 134.4172\n",
      "Epoch 2905/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 233.5896 - val_loss: 167.9322\n",
      "Epoch 2906/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 161.6383 - val_loss: 127.5913\n",
      "Epoch 2907/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 150.4409 - val_loss: 139.3459\n",
      "Epoch 2908/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 184.2927 - val_loss: 166.0991\n",
      "Epoch 2909/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 163.6392 - val_loss: 129.1876\n",
      "Epoch 2910/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 149.4120 - val_loss: 125.5030\n",
      "Epoch 2911/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 272.5895 - val_loss: 131.0211\n",
      "Epoch 2912/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 169.8660 - val_loss: 154.9154\n",
      "Epoch 2913/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 167.3859 - val_loss: 155.2730\n",
      "Epoch 2914/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 164.7659 - val_loss: 135.6139\n",
      "Epoch 2915/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 177.7567 - val_loss: 123.7714\n",
      "Epoch 2916/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 140.6165 - val_loss: 124.0415\n",
      "Epoch 2917/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 131.6247 - val_loss: 139.4783\n",
      "Epoch 2918/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 152.3531 - val_loss: 158.6949\n",
      "Epoch 2919/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 136.4710 - val_loss: 132.4464\n",
      "Epoch 2920/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 133.7112 - val_loss: 180.0785\n",
      "Epoch 2921/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 192.8701 - val_loss: 135.4399\n",
      "Epoch 2922/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 153.0343 - val_loss: 126.7217\n",
      "Epoch 2923/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 131.3476 - val_loss: 110.8449\n",
      "Epoch 2924/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 140.6227 - val_loss: 119.2809\n",
      "Epoch 2925/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 133.3266 - val_loss: 109.5852\n",
      "Epoch 2926/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 132.0082 - val_loss: 131.7098\n",
      "Epoch 2927/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 134.0869 - val_loss: 120.2664\n",
      "Epoch 2928/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 134.8755 - val_loss: 113.0876\n",
      "Epoch 2929/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 177.5088 - val_loss: 122.2982\n",
      "Epoch 2930/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 136.0158 - val_loss: 115.2002\n",
      "Epoch 2931/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 136.3996 - val_loss: 119.5811\n",
      "Epoch 2932/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 134.9412 - val_loss: 123.1827\n",
      "Epoch 2933/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 134.0336 - val_loss: 116.6085\n",
      "Epoch 2934/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 137.7726 - val_loss: 122.8192\n",
      "Epoch 2935/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 144.4201 - val_loss: 122.1002\n",
      "Epoch 2936/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 135.8207 - val_loss: 115.9447\n",
      "Epoch 2937/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 133.3905 - val_loss: 112.0020\n",
      "Epoch 2938/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 208.8570 - val_loss: 146.8540\n",
      "Epoch 2939/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 143.5098 - val_loss: 128.7225\n",
      "Epoch 2940/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 135.6689 - val_loss: 160.1425\n",
      "Epoch 2941/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 130.6176 - val_loss: 129.2414\n",
      "Epoch 2942/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 134.8737 - val_loss: 114.0265\n",
      "Epoch 2943/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 145.6703 - val_loss: 119.4560\n",
      "Epoch 2944/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 136.1905 - val_loss: 114.7865\n",
      "Epoch 2945/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 136.0660 - val_loss: 118.2756\n",
      "Epoch 2946/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 132.4287 - val_loss: 147.3772\n",
      "Epoch 2947/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 141.7709 - val_loss: 114.4534\n",
      "Epoch 2948/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 134.0597 - val_loss: 118.2643\n",
      "Epoch 2949/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 132.8898 - val_loss: 112.1892\n",
      "Epoch 2950/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 138.7039 - val_loss: 143.6211\n",
      "Epoch 2951/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 144.2463 - val_loss: 141.2396\n",
      "Epoch 2952/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.1497 - val_loss: 132.3484\n",
      "Epoch 2953/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 176.4553 - val_loss: 121.9467\n",
      "Epoch 2954/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 170.2131 - val_loss: 182.8491\n",
      "Epoch 2955/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 141.6121 - val_loss: 124.4000\n",
      "Epoch 2956/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 133.4285 - val_loss: 128.6603\n",
      "Epoch 2957/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 133.8051 - val_loss: 137.4743\n",
      "Epoch 2958/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 136.0471 - val_loss: 125.5202\n",
      "Epoch 2959/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 131.9998 - val_loss: 112.2942\n",
      "Epoch 2960/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 133.7419 - val_loss: 113.9410\n",
      "Epoch 2961/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 146.1273 - val_loss: 144.5439\n",
      "Epoch 2962/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 138.8119 - val_loss: 125.3770\n",
      "Epoch 2963/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 140.4396 - val_loss: 119.5668\n",
      "Epoch 2964/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 133.0394 - val_loss: 133.0391\n",
      "Epoch 2965/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 144.4669 - val_loss: 121.9743\n",
      "Epoch 2966/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 74us/step - loss: 164.3331 - val_loss: 136.7321\n",
      "Epoch 2967/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 145.2019 - val_loss: 129.5036\n",
      "Epoch 2968/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 142.4844 - val_loss: 117.7385\n",
      "Epoch 2969/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 131.0506 - val_loss: 114.8586\n",
      "Epoch 2970/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 134.9017 - val_loss: 112.5594\n",
      "Epoch 2971/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 140.1541 - val_loss: 126.5004\n",
      "Epoch 2972/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 144.2350 - val_loss: 118.7554\n",
      "Epoch 2973/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 137.9937 - val_loss: 129.9048\n",
      "Epoch 2974/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 142.7960 - val_loss: 110.8052\n",
      "Epoch 2975/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 144.2563 - val_loss: 111.1958\n",
      "Epoch 2976/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 139.1046 - val_loss: 160.6752\n",
      "Epoch 2977/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 132.7266 - val_loss: 113.7890\n",
      "Epoch 2978/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 133.2820 - val_loss: 114.0204\n",
      "Epoch 2979/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 141.8068 - val_loss: 169.5852\n",
      "Epoch 2980/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 142.8591 - val_loss: 116.0351\n",
      "Epoch 2981/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 136.4291 - val_loss: 109.8205\n",
      "Epoch 2982/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 132.1287 - val_loss: 118.9027\n",
      "Epoch 2983/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 133.9620 - val_loss: 116.7988\n",
      "Epoch 2984/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 142.9744 - val_loss: 173.2246\n",
      "Epoch 2985/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 167.3216 - val_loss: 118.1415\n",
      "Epoch 2986/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 135.8696 - val_loss: 119.6951\n",
      "Epoch 2987/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 148.2710 - val_loss: 115.2879\n",
      "Epoch 2988/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 130.2042 - val_loss: 112.6437\n",
      "Epoch 2989/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 137.9699 - val_loss: 185.9412\n",
      "Epoch 2990/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 147.7404 - val_loss: 119.6767\n",
      "Epoch 2991/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 162.9698 - val_loss: 180.4223\n",
      "Epoch 2992/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 141.1981 - val_loss: 114.9803\n",
      "Epoch 2993/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 133.9971 - val_loss: 112.0191\n",
      "Epoch 2994/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 135.7172 - val_loss: 115.2941\n",
      "Epoch 2995/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 158.8413 - val_loss: 153.0257\n",
      "Epoch 2996/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 168.8787 - val_loss: 119.4085\n",
      "Epoch 2997/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 161.7322 - val_loss: 120.8223\n",
      "Epoch 2998/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 140.9390 - val_loss: 148.9101\n",
      "Epoch 2999/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 136.1271 - val_loss: 115.4199\n",
      "Epoch 3000/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 135.0655 - val_loss: 114.4471\n",
      "Epoch 3001/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 139.2114 - val_loss: 125.3398\n",
      "Epoch 3002/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 132.8715 - val_loss: 129.9173\n",
      "Epoch 3003/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 146.4067 - val_loss: 117.2657\n",
      "Epoch 3004/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 129.8443 - val_loss: 145.0017\n",
      "Epoch 3005/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 136.8302 - val_loss: 117.9829\n",
      "Epoch 3006/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 134.727 - 1s 90us/step - loss: 131.9557 - val_loss: 110.6205\n",
      "Epoch 3007/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 141.0603 - val_loss: 128.5792\n",
      "Epoch 3008/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 142.0722 - val_loss: 111.5303\n",
      "Epoch 3009/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 132.5006 - val_loss: 109.9758\n",
      "Epoch 3010/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 137.8657 - val_loss: 117.7188\n",
      "Epoch 3011/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 136.4270 - val_loss: 140.9840\n",
      "Epoch 3012/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 134.8845 - val_loss: 115.7721\n",
      "Epoch 3013/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 165.5621 - val_loss: 127.0868\n",
      "Epoch 3014/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 144.3879 - val_loss: 121.4270\n",
      "Epoch 3015/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 143.5767 - val_loss: 131.4493\n",
      "Epoch 3016/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 134.2547 - val_loss: 133.7405\n",
      "Epoch 3017/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 144.6979 - val_loss: 129.1379\n",
      "Epoch 3018/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 128.4541 - val_loss: 129.0312\n",
      "Epoch 3019/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 139.5079 - val_loss: 115.5279\n",
      "Epoch 3020/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 142.1325 - val_loss: 110.2819\n",
      "Epoch 3021/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 141.0432 - val_loss: 119.9244\n",
      "Epoch 3022/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 150.6174 - val_loss: 120.4613\n",
      "Epoch 3023/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 140.2747 - val_loss: 117.2257\n",
      "Epoch 3024/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 130.2726 - val_loss: 117.6042\n",
      "Epoch 3025/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 130.1408 - val_loss: 128.2726\n",
      "Epoch 3026/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 137.1602 - val_loss: 113.5976\n",
      "Epoch 3027/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 144.0066 - val_loss: 111.9476\n",
      "Epoch 3028/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 134.8974 - val_loss: 115.1701\n",
      "Epoch 3029/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 138.9915 - val_loss: 118.6631\n",
      "Epoch 3030/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 136.5594 - val_loss: 136.7387\n",
      "Epoch 3031/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 140.3987 - val_loss: 133.5807\n",
      "Epoch 3032/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 154.3660 - val_loss: 115.7102\n",
      "Epoch 3033/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 137.1830 - val_loss: 141.6410\n",
      "Epoch 3034/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 129.7800 - val_loss: 113.4010\n",
      "Epoch 3035/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 149.3216 - val_loss: 113.4941\n",
      "Epoch 3036/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 135.0627 - val_loss: 112.6977\n",
      "Epoch 3037/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 138.8255 - val_loss: 117.4433\n",
      "Epoch 3038/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 90us/step - loss: 131.5959 - val_loss: 132.4207\n",
      "Epoch 3039/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 137.4619 - val_loss: 118.6093\n",
      "Epoch 3040/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 131.3902 - val_loss: 115.6310\n",
      "Epoch 3041/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 131.4194 - val_loss: 152.9615\n",
      "Epoch 3042/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 137.0787 - val_loss: 114.6132\n",
      "Epoch 3043/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 138.5884 - val_loss: 124.4091\n",
      "Epoch 3044/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 134.5427 - val_loss: 116.2905\n",
      "Epoch 3045/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 138.0806 - val_loss: 110.5936\n",
      "Epoch 3046/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 155.7958 - val_loss: 137.8417\n",
      "Epoch 3047/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 136.2229 - val_loss: 119.7792\n",
      "Epoch 3048/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 157.3401 - val_loss: 128.0614\n",
      "Epoch 3049/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 134.5237 - val_loss: 124.9314\n",
      "Epoch 3050/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 142.0712 - val_loss: 138.6584\n",
      "Epoch 3051/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 137.7583 - val_loss: 116.3923\n",
      "Epoch 3052/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 127.6206 - val_loss: 112.2032\n",
      "Epoch 3053/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 132.0152 - val_loss: 118.9963\n",
      "Epoch 3054/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 137.4516 - val_loss: 113.1426\n",
      "Epoch 3055/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 143.5363 - val_loss: 212.3708\n",
      "Epoch 3056/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 151.2261 - val_loss: 120.0617\n",
      "Epoch 3057/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 137.2712 - val_loss: 119.7643\n",
      "Epoch 3058/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 134.2281 - val_loss: 123.5291\n",
      "Epoch 3059/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 205.6713 - val_loss: 112.8062\n",
      "Epoch 3060/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 151.9135 - val_loss: 117.3589\n",
      "Epoch 3061/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 131.9064 - val_loss: 115.7492\n",
      "Epoch 3062/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 129.6070 - val_loss: 113.3163\n",
      "Epoch 3063/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 138.3511 - val_loss: 138.3480\n",
      "Epoch 3064/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 141.2785 - val_loss: 115.5060\n",
      "Epoch 3065/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 132.3761 - val_loss: 119.3602\n",
      "Epoch 3066/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 136.0704 - val_loss: 182.8653\n",
      "Epoch 3067/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 140.0756 - val_loss: 116.6162\n",
      "Epoch 3068/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 134.2036 - val_loss: 115.8539\n",
      "Epoch 3069/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 139.9971 - val_loss: 123.7145\n",
      "Epoch 3070/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 148.4810 - val_loss: 112.7239\n",
      "Epoch 3071/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 130.0254 - val_loss: 112.9777\n",
      "Epoch 3072/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 133.7027 - val_loss: 124.8701\n",
      "Epoch 3073/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 135.1448 - val_loss: 115.9809\n",
      "Epoch 3074/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 148.5644 - val_loss: 122.1515\n",
      "Epoch 3075/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 132.8483 - val_loss: 116.2844\n",
      "Epoch 3076/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 135.3874 - val_loss: 117.9931\n",
      "Epoch 3077/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 126.9192 - val_loss: 128.6638\n",
      "Epoch 3078/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 131.4028 - val_loss: 116.5394\n",
      "Epoch 3079/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 130.7270 - val_loss: 111.5708\n",
      "Epoch 3080/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 136.7141 - val_loss: 120.6855\n",
      "Epoch 3081/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 154.1213 - val_loss: 118.0969\n",
      "Epoch 3082/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 138.8334 - val_loss: 121.6885\n",
      "Epoch 3083/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 130.7599 - val_loss: 115.2697\n",
      "Epoch 3084/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 134.9425 - val_loss: 122.8632\n",
      "Epoch 3085/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 136.7747 - val_loss: 141.2820\n",
      "Epoch 3086/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 149.1016 - val_loss: 123.1925\n",
      "Epoch 3087/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 134.0812 - val_loss: 144.1793\n",
      "Epoch 3088/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 133.6959 - val_loss: 119.5438\n",
      "Epoch 3089/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 196.4100 - val_loss: 142.9435\n",
      "Epoch 3090/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 159.2379 - val_loss: 130.9098\n",
      "Epoch 3091/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 133.3156 - val_loss: 123.5848\n",
      "Epoch 3092/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 149.6123 - val_loss: 124.8205\n",
      "Epoch 3093/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 133.9713 - val_loss: 126.1140\n",
      "Epoch 3094/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 140.2354 - val_loss: 145.2434\n",
      "Epoch 3095/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 132.3696 - val_loss: 112.6376\n",
      "Epoch 3096/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 132.7750 - val_loss: 119.9930\n",
      "Epoch 3097/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 133.1657 - val_loss: 127.2086\n",
      "Epoch 3098/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 146.5306 - val_loss: 118.4782\n",
      "Epoch 3099/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 138.6194 - val_loss: 135.9720\n",
      "Epoch 3100/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 141.5188 - val_loss: 116.0825\n",
      "Epoch 3101/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 133.0064 - val_loss: 196.9235\n",
      "Epoch 3102/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 154.4349 - val_loss: 123.5037\n",
      "Epoch 3103/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 157.6067 - val_loss: 130.0010\n",
      "Epoch 3104/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 184.5285 - val_loss: 116.2151\n",
      "Epoch 3105/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 148.2099 - val_loss: 131.3211\n",
      "Epoch 3106/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 147.5898 - val_loss: 120.1924\n",
      "Epoch 3107/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 134.1312 - val_loss: 129.2238\n",
      "Epoch 3108/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 144.4636 - val_loss: 121.8678\n",
      "Epoch 3109/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 168.0421 - val_loss: 121.2944\n",
      "Epoch 3110/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 79us/step - loss: 136.4016 - val_loss: 126.0262\n",
      "Epoch 3111/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 125.7952 - val_loss: 118.6876\n",
      "Epoch 3112/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 130.9465 - val_loss: 118.4581\n",
      "Epoch 3113/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 131.1525 - val_loss: 112.6102\n",
      "Epoch 3114/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 134.4887 - val_loss: 138.6524\n",
      "Epoch 3115/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 132.3704 - val_loss: 120.0096\n",
      "Epoch 3116/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 131.7033 - val_loss: 113.4368\n",
      "Epoch 3117/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 150.1331 - val_loss: 124.9789\n",
      "Epoch 3118/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 132.7923 - val_loss: 125.4017\n",
      "Epoch 3119/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 151.9387 - val_loss: 115.5231\n",
      "Epoch 3120/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 143.3966 - val_loss: 121.6140\n",
      "Epoch 3121/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 141.2793 - val_loss: 131.1909\n",
      "Epoch 3122/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 136.6517 - val_loss: 125.7601\n",
      "Epoch 3123/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 131.2499 - val_loss: 119.2029\n",
      "Epoch 3124/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 149.1675 - val_loss: 116.5372\n",
      "Epoch 3125/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 129.0675 - val_loss: 128.7956\n",
      "Epoch 3126/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 135.4269 - val_loss: 117.2237\n",
      "Epoch 3127/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 124.8978 - val_loss: 120.2021\n",
      "Epoch 3128/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 144.9130 - val_loss: 170.6188\n",
      "Epoch 3129/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 148.315 - 1s 70us/step - loss: 154.6243 - val_loss: 136.8022\n",
      "Epoch 3130/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 143.1353 - val_loss: 157.7699\n",
      "Epoch 3131/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 137.8352 - val_loss: 121.8944\n",
      "Epoch 3132/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 133.6984 - val_loss: 140.1186\n",
      "Epoch 3133/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 163.0581 - val_loss: 132.5640\n",
      "Epoch 3134/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 143.9434 - val_loss: 124.6932\n",
      "Epoch 3135/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 139.6495 - val_loss: 115.3444\n",
      "Epoch 3136/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 137.7507 - val_loss: 115.3549\n",
      "Epoch 3137/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 145.3767 - val_loss: 110.9041\n",
      "Epoch 3138/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 136.7164 - val_loss: 163.3179\n",
      "Epoch 3139/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 141.0002 - val_loss: 123.4216\n",
      "Epoch 3140/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 134.1583 - val_loss: 110.7383\n",
      "Epoch 3141/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 134.2751 - val_loss: 142.0854\n",
      "Epoch 3142/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 132.6341 - val_loss: 123.9498\n",
      "Epoch 3143/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 139.2508 - val_loss: 120.6397\n",
      "Epoch 3144/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 148.8281 - val_loss: 115.3278\n",
      "Epoch 3145/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 147.3747 - val_loss: 121.3128\n",
      "Epoch 3146/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 135.0993 - val_loss: 113.2722\n",
      "Epoch 3147/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 131.7726 - val_loss: 138.1399\n",
      "Epoch 3148/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 132.5044 - val_loss: 122.1661\n",
      "Epoch 3149/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 131.7892 - val_loss: 115.1033\n",
      "Epoch 3150/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 135.5458 - val_loss: 126.1280\n",
      "Epoch 3151/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 133.6363 - val_loss: 122.4410\n",
      "Epoch 3152/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 139.9655 - val_loss: 158.5375\n",
      "Epoch 3153/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 158.9168 - val_loss: 124.2817\n",
      "Epoch 3154/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 134.7170 - val_loss: 117.6686\n",
      "Epoch 3155/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 138.2388 - val_loss: 119.7344\n",
      "Epoch 3156/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 136.7811 - val_loss: 129.8477\n",
      "Epoch 3157/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 142.6231 - val_loss: 136.9236\n",
      "Epoch 3158/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 140.8869 - val_loss: 138.5363\n",
      "Epoch 3159/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 149.3449 - val_loss: 129.7520\n",
      "Epoch 3160/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 162.8073 - val_loss: 117.4888\n",
      "Epoch 3161/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 138.0949 - val_loss: 111.4201\n",
      "Epoch 3162/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 134.1707 - val_loss: 144.2033\n",
      "Epoch 3163/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 233.9885 - val_loss: 157.0415\n",
      "Epoch 3164/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 153.7191 - val_loss: 114.5862\n",
      "Epoch 3165/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 136.9775 - val_loss: 119.9508\n",
      "Epoch 3166/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 132.3220 - val_loss: 117.7648\n",
      "Epoch 3167/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 139.1448 - val_loss: 126.6176\n",
      "Epoch 3168/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 138.7454 - val_loss: 140.0985\n",
      "Epoch 3169/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 134.4281 - val_loss: 113.2629\n",
      "Epoch 3170/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 129.1450 - val_loss: 116.2541\n",
      "Epoch 3171/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 129.8844 - val_loss: 120.1646\n",
      "Epoch 3172/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 129.3205 - val_loss: 114.8646\n",
      "Epoch 3173/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 134.3392 - val_loss: 122.1182\n",
      "Epoch 3174/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 139.5795 - val_loss: 112.0315\n",
      "Epoch 3175/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 134.8549 - val_loss: 121.8531\n",
      "Epoch 3176/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 142.5114 - val_loss: 139.3586\n",
      "Epoch 3177/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 202.7321 - val_loss: 170.1762\n",
      "Epoch 3178/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 133.6699 - val_loss: 120.2468\n",
      "Epoch 3179/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 127.7982 - val_loss: 119.7676\n",
      "Epoch 3180/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 136.6205 - val_loss: 136.2245\n",
      "Epoch 3181/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 132.2490 - val_loss: 116.2899\n",
      "Epoch 3182/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 86us/step - loss: 128.7968 - val_loss: 116.9981\n",
      "Epoch 3183/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 150.0542 - val_loss: 115.6848\n",
      "Epoch 3184/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 126.4263 - val_loss: 122.3336\n",
      "Epoch 3185/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 137.3891 - val_loss: 144.5310\n",
      "Epoch 3186/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 145.8920 - val_loss: 116.2724\n",
      "Epoch 3187/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 137.5168 - val_loss: 111.8026\n",
      "Epoch 3188/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 135.1860 - val_loss: 121.2165\n",
      "Epoch 3189/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 134.4720 - val_loss: 114.4744\n",
      "Epoch 3190/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 131.7412 - val_loss: 113.1186\n",
      "Epoch 3191/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 130.0944 - val_loss: 124.2994\n",
      "Epoch 3192/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 142.3762 - val_loss: 143.0534\n",
      "Epoch 3193/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 141.4303 - val_loss: 115.3093\n",
      "Epoch 3194/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 134.6346 - val_loss: 140.0817\n",
      "Epoch 3195/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 137.272 - 1s 70us/step - loss: 138.5165 - val_loss: 114.1905\n",
      "Epoch 3196/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 148.0997 - val_loss: 126.9047\n",
      "Epoch 3197/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 139.7623 - val_loss: 117.7441\n",
      "Epoch 3198/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 132.6551 - val_loss: 110.7391\n",
      "Epoch 3199/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 129.8633 - val_loss: 112.7858\n",
      "Epoch 3200/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 130.9507 - val_loss: 115.7899\n",
      "Epoch 3201/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 129.5600 - val_loss: 115.8429\n",
      "Epoch 3202/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 128.9461 - val_loss: 114.9551\n",
      "Epoch 3203/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 138.8865 - val_loss: 121.7854\n",
      "Epoch 3204/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 134.8103 - val_loss: 117.1394\n",
      "Epoch 3205/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 127.0852 - val_loss: 125.3119\n",
      "Epoch 3206/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 138.9322 - val_loss: 122.2020\n",
      "Epoch 3207/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 129.0526 - val_loss: 116.4937\n",
      "Epoch 3208/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 140.3167 - val_loss: 114.6915\n",
      "Epoch 3209/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 148.2007 - val_loss: 120.9000\n",
      "Epoch 3210/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 160.5064 - val_loss: 138.3381\n",
      "Epoch 3211/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 135.5402 - val_loss: 114.5563\n",
      "Epoch 3212/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 137.6310 - val_loss: 123.6805\n",
      "Epoch 3213/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 132.8156 - val_loss: 114.5311\n",
      "Epoch 3214/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 135.5882 - val_loss: 110.2659- ETA: 0s - loss:\n",
      "Epoch 3215/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 128.0203 - val_loss: 126.1766\n",
      "Epoch 3216/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 134.5966 - val_loss: 128.9136\n",
      "Epoch 3217/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 140.7640 - val_loss: 135.6910\n",
      "Epoch 3218/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 133.2035 - val_loss: 140.3092\n",
      "Epoch 3219/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 132.4082 - val_loss: 119.7957\n",
      "Epoch 3220/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 142.0299 - val_loss: 117.2929\n",
      "Epoch 3221/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 132.8348 - val_loss: 120.2984\n",
      "Epoch 3222/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 136.8320 - val_loss: 113.5115\n",
      "Epoch 3223/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 134.8262 - val_loss: 133.7609\n",
      "Epoch 3224/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 139.3206 - val_loss: 124.7322\n",
      "Epoch 3225/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 149.7809 - val_loss: 119.2262\n",
      "Epoch 3226/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 136.5083 - val_loss: 153.1018\n",
      "Epoch 3227/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 136.7749 - val_loss: 154.2939\n",
      "Epoch 3228/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 125.4997 - val_loss: 119.5803\n",
      "Epoch 3229/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 135.3127 - val_loss: 126.9882\n",
      "Epoch 3230/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 132.3087 - val_loss: 150.7508\n",
      "Epoch 3231/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 152.3889 - val_loss: 135.2496\n",
      "Epoch 3232/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 131.7518 - val_loss: 173.2937\n",
      "Epoch 3233/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 138.3622 - val_loss: 129.2351\n",
      "Epoch 3234/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 132.8398 - val_loss: 112.8914\n",
      "Epoch 3235/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 138.1041 - val_loss: 128.7322\n",
      "Epoch 3236/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 155.8349 - val_loss: 134.2712\n",
      "Epoch 3237/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 144.8670 - val_loss: 115.5490\n",
      "Epoch 3238/10000\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 140.1568 - val_loss: 119.3657\n",
      "Epoch 3239/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 136.4161 - val_loss: 211.4742\n",
      "Epoch 3240/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 173.7263 - val_loss: 128.1378\n",
      "Epoch 3241/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 134.3571 - val_loss: 146.1670\n",
      "Epoch 3242/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 129.8527 - val_loss: 127.7009\n",
      "Epoch 3243/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 134.7633 - val_loss: 117.1592\n",
      "Epoch 3244/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 134.3803 - val_loss: 116.7344\n",
      "Epoch 3245/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 132.2881 - val_loss: 115.9173\n",
      "Epoch 3246/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 134.0593 - val_loss: 132.7704\n",
      "Epoch 3247/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 131.4568 - val_loss: 154.0480\n",
      "Epoch 3248/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 132.0328 - val_loss: 123.4595\n",
      "Epoch 3249/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 147.1811 - val_loss: 119.3972\n",
      "Epoch 3250/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 137.3786 - val_loss: 118.3957\n",
      "Epoch 3251/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 129.0812 - val_loss: 123.7373\n",
      "Epoch 3252/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 140.0455 - val_loss: 114.5611\n",
      "Epoch 3253/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 135.1834 - val_loss: 154.6575\n",
      "Epoch 3254/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 74us/step - loss: 135.1067 - val_loss: 151.1441\n",
      "Epoch 3255/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 139.8383 - val_loss: 124.6805\n",
      "Epoch 3256/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 141.0325 - val_loss: 115.1265\n",
      "Epoch 3257/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 130.3187 - val_loss: 110.2602\n",
      "Epoch 3258/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 130.5286 - val_loss: 116.4596\n",
      "Epoch 3259/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 132.3970 - val_loss: 128.1266\n",
      "Epoch 3260/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 127.6408 - val_loss: 111.2391\n",
      "Epoch 3261/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 137.9585 - val_loss: 180.8982\n",
      "Epoch 3262/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 131.9607 - val_loss: 153.8604\n",
      "Epoch 3263/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 138.1706 - val_loss: 110.4052\n",
      "Epoch 3264/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 144.6323 - val_loss: 112.7372\n",
      "Epoch 3265/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 141.4817 - val_loss: 110.5343\n",
      "Epoch 3266/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 134.7169 - val_loss: 127.4789\n",
      "Epoch 3267/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 134.5600 - val_loss: 113.6245\n",
      "Epoch 3268/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 137.7592 - val_loss: 114.2884\n",
      "Epoch 3269/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 145.7755 - val_loss: 140.8499\n",
      "Epoch 3270/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 142.3381 - val_loss: 124.6079\n",
      "Epoch 3271/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 134.5584 - val_loss: 113.9371\n",
      "Epoch 3272/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 130.0880 - val_loss: 125.6774\n",
      "Epoch 3273/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 142.7009 - val_loss: 149.0306\n",
      "Epoch 3274/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 151.9209 - val_loss: 110.7899\n",
      "Epoch 3275/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 135.0727 - val_loss: 157.0668\n",
      "Epoch 3276/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 144.2308 - val_loss: 118.7505\n",
      "Epoch 3277/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 135.1576 - val_loss: 121.6470\n",
      "Epoch 3278/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 135.4509 - val_loss: 129.0461\n",
      "Epoch 3279/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 153.1801 - val_loss: 130.2717\n",
      "Epoch 3280/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 131.4550 - val_loss: 114.2239\n",
      "Epoch 3281/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 150.2418 - val_loss: 117.7367\n",
      "Epoch 3282/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 135.6895 - val_loss: 110.6932\n",
      "Epoch 3283/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 127.3629 - val_loss: 114.1939\n",
      "Epoch 3284/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 132.8066 - val_loss: 131.0371\n",
      "Epoch 3285/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 141.3842 - val_loss: 129.9036\n",
      "Epoch 3286/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 145.9577 - val_loss: 114.3119\n",
      "Epoch 3287/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 129.8101 - val_loss: 135.9226\n",
      "Epoch 3288/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 136.6362 - val_loss: 117.7406\n",
      "Epoch 3289/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 133.7796 - val_loss: 116.7442\n",
      "Epoch 3290/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 131.9056 - val_loss: 131.6430\n",
      "Epoch 3291/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 134.0171 - val_loss: 112.7429\n",
      "Epoch 3292/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 127.4676 - val_loss: 137.0954\n",
      "Epoch 3293/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 130.5538 - val_loss: 114.7759\n",
      "Epoch 3294/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 146.5022 - val_loss: 125.7797\n",
      "Epoch 3295/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 126.3339 - val_loss: 117.1010\n",
      "Epoch 3296/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 130.6276 - val_loss: 137.4768\n",
      "Epoch 3297/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 133.5320 - val_loss: 113.2443\n",
      "Epoch 3298/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 136.9996 - val_loss: 123.9059\n",
      "Epoch 3299/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 141.7007 - val_loss: 111.2581\n",
      "Epoch 3300/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 138.3250 - val_loss: 124.4759\n",
      "Epoch 3301/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 132.5971 - val_loss: 133.2622\n",
      "Epoch 3302/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 148.9666 - val_loss: 113.2273\n",
      "Epoch 3303/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 130.3530 - val_loss: 116.7710\n",
      "Epoch 3304/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 138.5910 - val_loss: 137.9040\n",
      "Epoch 3305/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 130.8013 - val_loss: 131.3034\n",
      "Epoch 3306/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 127.9996 - val_loss: 120.5365\n",
      "Epoch 3307/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 132.0820 - val_loss: 126.5228\n",
      "Epoch 3308/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 133.5842 - val_loss: 110.8460\n",
      "Epoch 3309/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 138.9394 - val_loss: 120.2108\n",
      "Epoch 3310/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 156.2482 - val_loss: 114.1878\n",
      "Epoch 3311/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 131.7725 - val_loss: 116.7828\n",
      "Epoch 3312/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 133.1745 - val_loss: 117.4328\n",
      "Epoch 3313/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 134.6433 - val_loss: 122.0047\n",
      "Epoch 3314/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 130.8005 - val_loss: 111.0354\n",
      "Epoch 3315/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 151.3246 - val_loss: 142.8754\n",
      "Epoch 3316/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 136.1505 - val_loss: 115.9251\n",
      "Epoch 3317/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 133.3216 - val_loss: 254.9778\n",
      "Epoch 3318/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 142.381 - 1s 77us/step - loss: 139.0121 - val_loss: 115.8214\n",
      "Epoch 3319/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 130.2947 - val_loss: 127.8064\n",
      "Epoch 3320/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 141.9457 - val_loss: 123.0259\n",
      "Epoch 3321/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 131.3241 - val_loss: 119.6109\n",
      "Epoch 3322/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 128.1484 - val_loss: 120.2904\n",
      "Epoch 3323/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 135.4469 - val_loss: 137.5720\n",
      "Epoch 3324/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 130.0799 - val_loss: 110.5733\n",
      "Epoch 3325/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 186.5022 - val_loss: 158.4509\n",
      "Epoch 3326/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 76us/step - loss: 137.9558 - val_loss: 112.8770\n",
      "Epoch 3327/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 127.5446 - val_loss: 130.8831\n",
      "Epoch 3328/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 136.6694 - val_loss: 143.0911\n",
      "Epoch 3329/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 132.7464 - val_loss: 122.7329\n",
      "Epoch 3330/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 142.0091 - val_loss: 116.7106\n",
      "Epoch 3331/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 131.5329 - val_loss: 116.8791\n",
      "Epoch 3332/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 133.9870 - val_loss: 113.8964\n",
      "Epoch 3333/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 147.5899 - val_loss: 121.8704\n",
      "Epoch 3334/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 129.8077 - val_loss: 174.7299\n",
      "Epoch 3335/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 129.6295 - val_loss: 128.9862\n",
      "Epoch 3336/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 134.8028 - val_loss: 118.8966\n",
      "Epoch 3337/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 177.0044 - val_loss: 163.4268\n",
      "Epoch 3338/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 142.9402 - val_loss: 122.8619\n",
      "Epoch 3339/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 133.8794 - val_loss: 119.5050\n",
      "Epoch 3340/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 135.3072 - val_loss: 119.0472\n",
      "Epoch 3341/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 139.9293 - val_loss: 117.0773\n",
      "Epoch 3342/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 132.1627 - val_loss: 123.7582\n",
      "Epoch 3343/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 150.4207 - val_loss: 116.4006\n",
      "Epoch 3344/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 132.7248 - val_loss: 128.6902\n",
      "Epoch 03344: early stopping\n",
      "Fold score (RMSE): 11.087796211242676\n"
     ]
    }
   ],
   "source": [
    "# Cross-Validate\n",
    "kf = KFold(5)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=filename_checkpoint, verbose=0, save_best_only=True)\n",
    "\n",
    "# Turn off KFold\n",
    "#if (0):\n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "    \n",
    "for train, test in kf.split(x):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "    model.add(Dropout(0.01)) # Dropout Layer 1\n",
    "    model.add(Dense(50, activation='relu')) # Hidden 2\n",
    "    #model.add(Dropout(0.01)) # Dropout Layer 2\n",
    "    model.add(Dense(25, \n",
    "                    kernel_regularizer=regularizers.l2(0.01), #L2 regularization\n",
    "                    activity_regularizer=regularizers.l1(0.01), #L1 Lasso regularization\n",
    "                    activation='relu')) # Hidden 3 \n",
    "    model.add(Dense(10, activation='relu')) # Hidden 4\n",
    "    model.add(Dense(1)) # Output\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=1000, verbose=1, mode='auto')\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpoint],verbose=1,epochs=10000)\n",
    "    \n",
    "    pred = model.predict(x_test)\n",
    "    \n",
    "    oos_y.append(y_test)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure this fold's RMSE\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "    print(\"Fold score (RMSE): {}\".format(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-sample RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final, out of sample score (RMSE): 11.8844575881958\n"
     ]
    }
   ],
   "source": [
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))\n",
    "print(\"Final, out of sample score (RMSE): {}\".format(score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weight</td>\n",
       "      <td>50712.453125</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pack</td>\n",
       "      <td>9817.169922</td>\n",
       "      <td>0.193585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>item-Tablets</td>\n",
       "      <td>3155.725342</td>\n",
       "      <td>0.062228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>color-Pink</td>\n",
       "      <td>697.980896</td>\n",
       "      <td>0.013764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>surface_area</td>\n",
       "      <td>607.177063</td>\n",
       "      <td>0.011973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>color-Red</td>\n",
       "      <td>594.498230</td>\n",
       "      <td>0.011723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>item-Thumbtacks</td>\n",
       "      <td>406.182556</td>\n",
       "      <td>0.008010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>item-Pencils</td>\n",
       "      <td>372.353241</td>\n",
       "      <td>0.007342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>color-Green</td>\n",
       "      <td>342.997223</td>\n",
       "      <td>0.006764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>color-White</td>\n",
       "      <td>330.731812</td>\n",
       "      <td>0.006522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>item-Paperclips</td>\n",
       "      <td>245.366287</td>\n",
       "      <td>0.004838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>item-Paperweights</td>\n",
       "      <td>170.704636</td>\n",
       "      <td>0.003366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>color-Black</td>\n",
       "      <td>168.771011</td>\n",
       "      <td>0.003328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>color-Blue</td>\n",
       "      <td>156.741974</td>\n",
       "      <td>0.003091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>item-Post It Notes</td>\n",
       "      <td>146.712494</td>\n",
       "      <td>0.002893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>item-Ink Pens</td>\n",
       "      <td>144.479309</td>\n",
       "      <td>0.002849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>quality-High Quality</td>\n",
       "      <td>136.424377</td>\n",
       "      <td>0.002690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>quality-Generic</td>\n",
       "      <td>135.296524</td>\n",
       "      <td>0.002668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>item-Stapler</td>\n",
       "      <td>131.221100</td>\n",
       "      <td>0.002588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>manufacturer-Deep Office Supplies</td>\n",
       "      <td>108.749001</td>\n",
       "      <td>0.002144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>manufacturer-WizBang</td>\n",
       "      <td>107.270233</td>\n",
       "      <td>0.002115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>manufacturer-Offices-R-Us</td>\n",
       "      <td>107.122818</td>\n",
       "      <td>0.002112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>manufacturer-6% Solution</td>\n",
       "      <td>107.046844</td>\n",
       "      <td>0.002111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>manufacturer-Duck Lake</td>\n",
       "      <td>106.757736</td>\n",
       "      <td>0.002105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>color-Brown</td>\n",
       "      <td>105.415298</td>\n",
       "      <td>0.002079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>size-Tiny</td>\n",
       "      <td>105.295059</td>\n",
       "      <td>0.002076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>size-Large</td>\n",
       "      <td>104.143387</td>\n",
       "      <td>0.002054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>size-Small</td>\n",
       "      <td>103.888748</td>\n",
       "      <td>0.002049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>size-Medium</td>\n",
       "      <td>102.706856</td>\n",
       "      <td>0.002025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 name         error  importance\n",
       "0                              weight  50712.453125    1.000000\n",
       "1                                pack   9817.169922    0.193585\n",
       "2                        item-Tablets   3155.725342    0.062228\n",
       "3                          color-Pink    697.980896    0.013764\n",
       "4                        surface_area    607.177063    0.011973\n",
       "5                           color-Red    594.498230    0.011723\n",
       "6                     item-Thumbtacks    406.182556    0.008010\n",
       "7                        item-Pencils    372.353241    0.007342\n",
       "8                         color-Green    342.997223    0.006764\n",
       "9                         color-White    330.731812    0.006522\n",
       "10                    item-Paperclips    245.366287    0.004838\n",
       "11                  item-Paperweights    170.704636    0.003366\n",
       "12                        color-Black    168.771011    0.003328\n",
       "13                         color-Blue    156.741974    0.003091\n",
       "14                 item-Post It Notes    146.712494    0.002893\n",
       "15                      item-Ink Pens    144.479309    0.002849\n",
       "16               quality-High Quality    136.424377    0.002690\n",
       "17                    quality-Generic    135.296524    0.002668\n",
       "18                       item-Stapler    131.221100    0.002588\n",
       "19  manufacturer-Deep Office Supplies    108.749001    0.002144\n",
       "20               manufacturer-WizBang    107.270233    0.002115\n",
       "21          manufacturer-Offices-R-Us    107.122818    0.002112\n",
       "22           manufacturer-6% Solution    107.046844    0.002111\n",
       "23             manufacturer-Duck Lake    106.757736    0.002105\n",
       "24                        color-Brown    105.415298    0.002079\n",
       "25                          size-Tiny    105.295059    0.002076\n",
       "26                         size-Large    104.143387    0.002054\n",
       "27                         size-Small    103.888748    0.002049\n",
       "28                        size-Medium    102.706856    0.002025"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "model.load_weights(filename_checkpoint)\n",
    "\n",
    "names = list(df_train.columns) # x+y column names\n",
    "names.remove(\"cost\") # remove the target(y)\n",
    "rank = perturbation_rank(model, x_test, y_test, names, True)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#pred = model.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "#score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "#print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "#chart_regression(pred.flatten(),y_test)\n",
    "#chart_regression(pred.flatten(),y_test,sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Best Weights and Predict on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(filename_checkpoint)\n",
    "\n",
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "extract_and_encode_features(df_test)\n",
    "\n",
    "ids_test = df_test['id']\n",
    "df_test.drop('id',1,inplace=True)\n",
    "\n",
    "names_test = df_test['name']\n",
    "df_test.drop('name',1,inplace=True)\n",
    "\n",
    "x_submit = df_test.as_matrix().astype(np.float32)\n",
    "pred_submit = model.predict(x_submit)\n",
    "\n",
    "# Handles negative cost values. Use inverse\n",
    "cost = [n if n > 0 else n * -1 for n in pred_submit[:,0]]\n",
    "\n",
    "df_submit = pd.DataFrame({'id': ids_test,'cost': cost})\n",
    "df_submit = df_submit[['id', 'cost']]\n",
    "df_submit.to_csv(filename_submit, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
