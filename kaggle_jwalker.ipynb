{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n",
    "\n",
    "**Kaggle Assignment: **\n",
    "\n",
    "**Student Name: Jason Walker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## Description\n",
    "This is one of the projects from the course T81-855: Applications of Deep Learning at Washington University in St. Louis. All students must create a Kaggle account and submit a solution. Once you have submitted your solution entry log into Blackboard (at WUSTL) and submit a single file telling me your Kaggle name on the leaderboard (you do not need to register to Kaggle with your real name). This competition will be visible to the public, so there may be non-student submissions as well as student.\n",
    "\n",
    "The data set for this competition consists of a number of input columns that should be used to predict a stores sales. This is a regression problem. The inputs are a mixture of discrete and category values. The data set is from a simulation.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The evaluation pages describes how submissions will be scored and how students should format their submissions. The scores are in RMSE.\n",
    "Submission Format\n",
    "\n",
    "For every store in the dataset, submission files should contain a sales volume.\n",
    "\n",
    "The file should contain a header and have the following format:\n",
    "\n",
    "```\n",
    "100000,1.23\n",
    "100001,1.123\n",
    "100002,3.332\n",
    "100003,1.53\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The data contains data and costs for various office supplies. The data came from a simulation and do not directly correspond to any real-world items. See how well you can predict the cost of an item using the provided data. Feature engineering will likely help you. The *name* column may seem useless at first glance; however, it contains information that you can parse to help your predictions.\n",
    "File descriptions\n",
    "```\n",
    "    id - The identifier/primary key.\n",
    "    name - The name of this item.\n",
    "    manufacturer - The manufacturer.\n",
    "    pack - The number of items in this pack.\n",
    "    weight - The weight of a pack of these items.\n",
    "    height - The height of a pack of these items.\n",
    "    width - The width of a pack of these items.\n",
    "    length - The length of a pack of these items.\n",
    "    cost - The cost for this item pack. This is what you are to predict (the target). \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions\n",
    "\n",
    "You will see these at the top of every module and assignment.  These are simply a set of reusable functions that we will make use of.  Each of them will be explained as the semester progresses.  They are explained in greater detail as the course progresses.  Class 4 contains a complete overview of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n",
    "        \n",
    "# This function submits an assignment.  You can submit an assignment as much as you like, only the final\n",
    "# submission counts.  The paramaters are as follows:\n",
    "# data - Pandas dataframe output.\n",
    "# key - Your student key that was emailed to you.\n",
    "# no - The assignment class number, should be 1 through 1.\n",
    "# source_file - The full path to your Python or IPYNB file.  This must have \"_class1\" as part of its name.  \n",
    "# .             The number must match your assignment number.  For example \"_class2\" for class assignment #2.\n",
    "def submit(data,key,no,source_file=None):\n",
    "    if source_file is None and '__file__' not in globals(): raise Exception('Must specify a filename when a Jupyter notebook.')\n",
    "    if source_file is None: source_file = __file__\n",
    "    suffix = '_class{}'.format(no)\n",
    "    if suffix not in source_file: raise Exception('{} must be part of the filename.'.format(suffix))\n",
    "    with open(source_file, \"rb\") as image_file:\n",
    "        encoded_python = base64.b64encode(image_file.read()).decode('ascii')\n",
    "    ext = os.path.splitext(source_file)[-1].lower()\n",
    "    if ext not in ['.ipynb','.py']: raise Exception(\"Source file is {} must be .py or .ipynb\".format(ext))\n",
    "    r = requests.post(\"https://api.heatonresearch.com/assignment-submit\",\n",
    "        headers={'x-api-key':key}, json={'csv':base64.b64encode(data.to_csv(index=False).encode('ascii')).decode(\"ascii\"),\n",
    "        'assignment': no, 'ext':ext, 'py':encoded_python})\n",
    "    if r.status_code == 200:\n",
    "        print(\"Success: {}\".format(r.text))\n",
    "    else: print(\"Failure: {}\".format(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kaggle Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers\n",
    "\n",
    "path = './data'\n",
    "\n",
    "filename_test = os.path.join(path,\"test.csv\")\n",
    "filename_train = os.path.join(path,\"train.csv\")\n",
    "filename_sample = os.path.join(path,\"sample.csv\")\n",
    "filename_submit = os.path.join(path,\"submit.csv\")\n",
    "filename_checkpoint = os.path.join(path,\"checkpoint.hdf5\")\n",
    "\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "\n",
    "np.random.seed(42) # Uncomment this line to get the same shuffle each time\n",
    "df_train = df_train.reindex(np.random.permutation(df_train.index))\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Encode Features\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def extract_and_encode_features(df):\n",
    "    color_regex='(?P<color>red|blue|green|yellow|orange|pink|black|brown|white)'\n",
    "    df['color'] = df.name.str.extract(color_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    quality_regex='(?P<quality>generic|medium\\shigh\\squality|high\\squality)'\n",
    "    df['quality'] = df.name.str.extract(quality_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    size_regex='(?P<size>tiny|small|medium|large)'\n",
    "    df['size'] = df.name.str.extract(size_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    item_regex='(?P<item>paperclips|paperweights|ink\\spens|pencils|stapler|tablets|thumbtacks|post\\sit\\snotes)'\n",
    "    df['item'] = df.name.str.extract(item_regex, flags=re.IGNORECASE, expand=False)\n",
    "    \n",
    "    for column in ['pack','weight','height','width','length']:\n",
    "        missing_median(df_train,column)\n",
    "    \n",
    "    df.insert(1,'surface_area',(df['height']*df['width']*df['length']).astype(int))\n",
    "    \n",
    "    ## encode numeric features\n",
    "    for column in ['pack','weight','height','width','length','surface_area']:\n",
    "        encode_numeric_zscore(df,column)\n",
    "     \n",
    "    # encode text/categorical features\n",
    "    for column in ['manufacturer','color','quality','size','item']:\n",
    "        encode_text_dummy(df,column)\n",
    "  \n",
    "\n",
    "extract_and_encode_features(df_train)\n",
    "ids_train = df_train['id']\n",
    "df_train.drop('id',1,inplace=True)\n",
    "\n",
    "names_train = df_train['name']\n",
    "df_train.drop('name',1,inplace=True)\n",
    "\n",
    "x,y = to_xy(df_train,'cost')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=45)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 32.211875915527344\n",
      "['surface_area', 'pack', 'weight', 'height', 'width', 'length', 'manufacturer-6% Solution', 'manufacturer-Deep Office Supplies', 'manufacturer-Duck Lake', 'manufacturer-Offices-R-Us', 'manufacturer-WizBang', 'color-Black', 'color-Blue', 'color-Brown', 'color-Green', 'color-Pink', 'color-Red', 'color-White', 'quality-Generic', 'quality-High Quality', 'quality-Medium High Quality', 'size-Large', 'size-Medium', 'size-Small', 'size-Tiny', 'item-Ink Pens', 'item-Paperclips', 'item-Paperweights', 'item-Pencils', 'item-Post It Notes', 'item-Stapler', 'item-Tablets', 'item-Thumbtacks']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item-Post It Notes</th>\n",
       "      <td>-76.430450</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Thumbtacks</th>\n",
       "      <td>-57.680199</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperclips</th>\n",
       "      <td>-49.292679</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Pencils</th>\n",
       "      <td>-46.917885</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Red</th>\n",
       "      <td>-38.541794</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Green</th>\n",
       "      <td>-26.209276</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>-16.682915</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>width</th>\n",
       "      <td>-16.682903</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>-16.682266</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Blue</th>\n",
       "      <td>-12.348345</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Tiny</th>\n",
       "      <td>-4.614498</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Generic</th>\n",
       "      <td>-4.514582</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Offices-R-Us</th>\n",
       "      <td>-0.851445</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Deep Office Supplies</th>\n",
       "      <td>-0.733551</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Brown</th>\n",
       "      <td>-0.552734</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack</th>\n",
       "      <td>-0.446889</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Small</th>\n",
       "      <td>-0.333237</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-6% Solution</th>\n",
       "      <td>-0.189339</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Duck Lake</th>\n",
       "      <td>0.473938</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Ink Pens</th>\n",
       "      <td>0.496303</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-WizBang</th>\n",
       "      <td>1.299829</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Medium</th>\n",
       "      <td>4.834967</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Medium High Quality</th>\n",
       "      <td>5.669786</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Large</th>\n",
       "      <td>6.766665</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-High Quality</th>\n",
       "      <td>7.788834</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Black</th>\n",
       "      <td>13.706039</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperweights</th>\n",
       "      <td>14.917278</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surface_area</th>\n",
       "      <td>18.134123</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-White</th>\n",
       "      <td>26.833145</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Pink</th>\n",
       "      <td>38.844139</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Stapler</th>\n",
       "      <td>51.922276</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>105.468979</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Tablets</th>\n",
       "      <td>162.987289</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         coef  positive\n",
       "item-Post It Notes                 -76.430450     False\n",
       "item-Thumbtacks                    -57.680199     False\n",
       "item-Paperclips                    -49.292679     False\n",
       "item-Pencils                       -46.917885     False\n",
       "color-Red                          -38.541794     False\n",
       "color-Green                        -26.209276     False\n",
       "length                             -16.682915     False\n",
       "width                              -16.682903     False\n",
       "height                             -16.682266     False\n",
       "color-Blue                         -12.348345     False\n",
       "size-Tiny                           -4.614498     False\n",
       "quality-Generic                     -4.514582     False\n",
       "manufacturer-Offices-R-Us           -0.851445     False\n",
       "manufacturer-Deep Office Supplies   -0.733551     False\n",
       "color-Brown                         -0.552734     False\n",
       "pack                                -0.446889     False\n",
       "size-Small                          -0.333237     False\n",
       "manufacturer-6% Solution            -0.189339     False\n",
       "manufacturer-Duck Lake               0.473938      True\n",
       "item-Ink Pens                        0.496303      True\n",
       "manufacturer-WizBang                 1.299829      True\n",
       "size-Medium                          4.834967      True\n",
       "quality-Medium High Quality          5.669786      True\n",
       "size-Large                           6.766665      True\n",
       "quality-High Quality                 7.788834      True\n",
       "color-Black                         13.706039      True\n",
       "item-Paperweights                   14.917278      True\n",
       "surface_area                        18.134123      True\n",
       "color-White                         26.833145      True\n",
       "color-Pink                          38.844139      True\n",
       "item-Stapler                        51.922276      True\n",
       "weight                             105.468979      True\n",
       "item-Tablets                       162.987289      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [ 126.38175964]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAD8CAYAAADExYYgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a19681860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple function to evaluate the coefficients of a regression\n",
    "%matplotlib inline    \n",
    "from IPython.display import display, HTML    \n",
    "\n",
    "def report_coef(names,coef,intercept):\n",
    "    r = pd.DataFrame( { 'coef': coef, 'positive': coef>=0  }, index = names )\n",
    "    r = r.sort_values(by=['coef'])\n",
    "    display(r)\n",
    "    print(\"Intercept: {}\".format(intercept))\n",
    "    r['coef'].plot(kind='barh', color=r['positive'].map({True: 'b', False: 'r'}))\n",
    "    \n",
    "# Create linear regression\n",
    "regressor = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# Fit/train linear regression\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "print(names)\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_[0,:],\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 2s 226us/step - loss: 19172.8529 - val_loss: 3322.2586\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1831.1471 - val_loss: 1016.4752\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 860.4785 - val_loss: 664.5477\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 636.1964 - val_loss: 514.8683\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 504.0328 - val_loss: 419.7964\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 421.5858 - val_loss: 367.1151\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 379.3891 - val_loss: 332.9452\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 356.4103 - val_loss: 316.9599\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 345.7957 - val_loss: 306.7524\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 332.2439 - val_loss: 303.3684\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 323.4977 - val_loss: 302.9052\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 322.9648 - val_loss: 297.0020\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 1s 105us/step - loss: 313.0005 - val_loss: 297.3000\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 316.8315 - val_loss: 288.4133\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 307.4844 - val_loss: 287.9480\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 313.5736 - val_loss: 282.1357\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 303.5238 - val_loss: 285.4739\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 306.4441 - val_loss: 278.5054\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 300.3400 - val_loss: 279.9016\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 298.1159 - val_loss: 276.1364\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 300.7996 - val_loss: 277.8127\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 298.6039 - val_loss: 282.0509\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 1s 105us/step - loss: 286.3092 - val_loss: 275.4308\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 288.5170 - val_loss: 281.0774\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 285.0473 - val_loss: 276.6195\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 283.5007 - val_loss: 268.3194\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 283.1526 - val_loss: 275.2874\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 283.7753 - val_loss: 275.7650\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 278.8643 - val_loss: 278.6927\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 285.8134 - val_loss: 265.6530\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 275.8578 - val_loss: 264.4106\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 277.2825 - val_loss: 262.9799\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 273.4995 - val_loss: 277.4028\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 272.7710 - val_loss: 266.4707\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 272.3328 - val_loss: 264.6005\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 273.4118 - val_loss: 286.6266\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 269.1795 - val_loss: 264.3514\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 271.0268 - val_loss: 265.4329\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 265.0862 - val_loss: 256.7573\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 268.4331 - val_loss: 267.4657\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 265.3333 - val_loss: 256.6787\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 261.5333 - val_loss: 253.0290\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 260.0939 - val_loss: 251.3249\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 265.2789 - val_loss: 249.6658\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 258.1559 - val_loss: 252.7249\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 261.7959 - val_loss: 259.1759\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 259.1600 - val_loss: 251.1819\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 258.1112 - val_loss: 249.9292\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 255.6165 - val_loss: 256.5721\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 255.5337 - val_loss: 262.7044\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 255.7561 - val_loss: 265.4271\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 254.1049 - val_loss: 255.5909\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 251.7826 - val_loss: 245.8451\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 255.0607 - val_loss: 247.7602\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 250.5458 - val_loss: 250.6337\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 248.3494 - val_loss: 254.6049\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 252.3569 - val_loss: 249.8727\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 250.1768 - val_loss: 250.2244\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 1s 113us/step - loss: 253.0926 - val_loss: 246.6146\n",
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 243.8208 - val_loss: 254.7837\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 250.4028 - val_loss: 246.0561\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 243.8236 - val_loss: 250.3967\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 1s 113us/step - loss: 246.4734 - val_loss: 241.3366\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 1s 108us/step - loss: 242.7902 - val_loss: 246.4343\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 1s 110us/step - loss: 245.0330 - val_loss: 241.5446\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 245.6119 - val_loss: 257.4121\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 242.8667 - val_loss: 241.9185\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 244.3654 - val_loss: 242.6655\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 245.9518 - val_loss: 240.5766\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 242.9368 - val_loss: 241.9869\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 242.8614 - val_loss: 239.5833\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 240.0099 - val_loss: 240.8708\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 239.4762 - val_loss: 244.7890\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 238.9136 - val_loss: 243.7245\n",
      "Epoch 75/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 80us/step - loss: 241.5613 - val_loss: 251.2621\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 236.6204 - val_loss: 243.4684\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 237.6961 - val_loss: 245.6191\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 242.2740 - val_loss: 251.3156\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 238.8785 - val_loss: 250.5743\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 238.7640 - val_loss: 254.2645\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 236.2765 - val_loss: 251.4815\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 235.0358 - val_loss: 239.3630\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 236.3484 - val_loss: 240.6171\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 232.6939 - val_loss: 250.7997\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 242.7243 - val_loss: 240.5254\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 232.6128 - val_loss: 242.6562\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 234.0781 - val_loss: 255.1683\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 234.4309 - val_loss: 242.1729\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 230.6615 - val_loss: 247.5329\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 233.7187 - val_loss: 238.4797\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 236.8609 - val_loss: 233.7135\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 234.1065 - val_loss: 246.2808\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 231.6634 - val_loss: 236.4003\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 232.7195 - val_loss: 240.6765\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 232.3015 - val_loss: 255.4845\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 230.3664 - val_loss: 232.9249\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 229.2337 - val_loss: 233.5259\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 230.3578 - val_loss: 233.1961\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 230.4517 - val_loss: 230.7787\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 229.5380 - val_loss: 243.0658\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 228.8448 - val_loss: 238.7735\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 230.0119 - val_loss: 235.7538\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 228.6914 - val_loss: 239.9104\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 228.6664 - val_loss: 235.6507\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 227.5438 - val_loss: 231.6141\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 227.8832 - val_loss: 235.9314\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 223.1466 - val_loss: 233.7220\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 230.2757 - val_loss: 249.8331\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 231.1676 - val_loss: 250.1492\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 229.3230 - val_loss: 233.4315\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 228.8295 - val_loss: 231.3261\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 226.3377 - val_loss: 249.9202\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 228.3693 - val_loss: 231.6703\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 225.5561 - val_loss: 235.5649\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 223.1449 - val_loss: 232.7513\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 227.2245 - val_loss: 231.5513\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 224.6587 - val_loss: 234.5072\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 226.9712 - val_loss: 234.3672\n",
      "Epoch 119/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 225.1806 - val_loss: 232.4846\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 224.7199 - val_loss: 231.1303\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 219.8558 - val_loss: 234.3416\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 221.7084 - val_loss: 233.9496\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 221.3871 - val_loss: 230.8533\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 220.8282 - val_loss: 230.3227\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 222.5362 - val_loss: 238.6151\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 221.7579 - val_loss: 238.6440\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 221.0779 - val_loss: 232.1351\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 1s 128us/step - loss: 218.5030 - val_loss: 235.8559\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 214.7704 - val_loss: 236.3316\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 215.7698 - val_loss: 232.0737\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 218.9795 - val_loss: 237.2974\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 220.6335 - val_loss: 239.1000\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 218.1320 - val_loss: 248.8215\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 1s 186us/step - loss: 216.4137 - val_loss: 226.6139\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 215.4479 - val_loss: 229.4765\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 217.3032 - val_loss: 232.0144\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 220.1793 - val_loss: 228.5873\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 219.2497 - val_loss: 226.8949\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 215.7226 - val_loss: 230.8869\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 214.7279 - val_loss: 237.4278\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 1s 104us/step - loss: 221.0970 - val_loss: 237.9145\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 214.8475 - val_loss: 227.4479\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 217.3104 - val_loss: 228.2893\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 213.2845 - val_loss: 227.0839\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 212.3690 - val_loss: 241.5699\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 217.8435 - val_loss: 235.1448\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 211.4114 - val_loss: 231.6538\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 214.0744 - val_loss: 240.5382\n",
      "Epoch 149/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 88us/step - loss: 211.3859 - val_loss: 228.7997\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 214.6669 - val_loss: 226.5163\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 214.4683 - val_loss: 228.3768\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 213.7602 - val_loss: 224.7094\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 210.2072 - val_loss: 230.7533\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 207.4184 - val_loss: 227.0079\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 209.1337 - val_loss: 223.5722\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 211.6689 - val_loss: 239.7728\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 207.2303 - val_loss: 223.1350\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 211.8621 - val_loss: 227.2483\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 206.5844 - val_loss: 224.5056\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 209.7943 - val_loss: 233.6581\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 210.8795 - val_loss: 232.3165\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 206.9825 - val_loss: 228.9911\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 209.4808 - val_loss: 222.1765\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 205.5316 - val_loss: 223.2898\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 208.2800 - val_loss: 220.4021\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 204.5567 - val_loss: 220.7572\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 1s 123us/step - loss: 203.8460 - val_loss: 225.9866\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 1s 113us/step - loss: 205.6083 - val_loss: 228.7391\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 207.7981 - val_loss: 223.5537\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 203.9875 - val_loss: 228.9388\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 207.0978 - val_loss: 241.9812\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 204.0400 - val_loss: 236.2612\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 201.9626 - val_loss: 222.1470\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 198.9118 - val_loss: 222.6934\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 200.5707 - val_loss: 225.7149\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 203.6925 - val_loss: 224.6673\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 200.1410 - val_loss: 222.8006\n",
      "Epoch 178/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 199.9906 - val_loss: 225.4756\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 1s 115us/step - loss: 197.6363 - val_loss: 233.8260\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 1s 159us/step - loss: 197.6963 - val_loss: 216.7327\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 1s 109us/step - loss: 198.8141 - val_loss: 235.6905\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 199.6516 - val_loss: 223.8258\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 197.4749 - val_loss: 217.2295\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 192.2965 - val_loss: 214.6162\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 194.4279 - val_loss: 216.2262\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 198.4107 - val_loss: 227.8449\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 192.8236 - val_loss: 215.5306\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 192.9033 - val_loss: 213.1178\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 193.6170 - val_loss: 223.0506\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 191.1720 - val_loss: 215.1708\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 190.7019 - val_loss: 214.0896\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 196.0022 - val_loss: 211.1865\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 196.6340 - val_loss: 207.6979\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 191.1837 - val_loss: 209.8212\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 191.1205 - val_loss: 204.4520\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 186.5996 - val_loss: 231.3455\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 188.0159 - val_loss: 207.9795\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 184.5130 - val_loss: 204.9451\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 183.8012 - val_loss: 214.9036\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 186.9544 - val_loss: 213.7968\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 188.6241 - val_loss: 237.8621\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 190.0484 - val_loss: 214.0700\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 186.3324 - val_loss: 199.9196\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 179.6151 - val_loss: 212.2410\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 185.3550 - val_loss: 204.4977\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 182.6269 - val_loss: 211.5728\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 183.2327 - val_loss: 200.1555\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 178.9600 - val_loss: 202.9565\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 178.1189 - val_loss: 204.6273\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 181.1522 - val_loss: 205.4334\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 178.027 - 1s 82us/step - loss: 178.1408 - val_loss: 198.2769\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 1s 131us/step - loss: 176.4818 - val_loss: 199.9663\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 176.6348 - val_loss: 195.9430\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 177.5481 - val_loss: 205.9763\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 177.6511 - val_loss: 192.3346\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 174.9354 - val_loss: 211.6570\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 170.3861 - val_loss: 191.0552\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 172.6864 - val_loss: 198.6798\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 177.8618 - val_loss: 212.4443\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 1s 137us/step - loss: 171.8846 - val_loss: 219.3154\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 170.2925 - val_loss: 218.6662\n",
      "Epoch 222/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 127us/step - loss: 168.4369 - val_loss: 188.7838\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 1s 128us/step - loss: 167.2545 - val_loss: 193.3776\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 1s 100us/step - loss: 166.5238 - val_loss: 193.3623\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 166.5499 - val_loss: 189.8864\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 167.0556 - val_loss: 200.8934\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 164.9273 - val_loss: 185.3957\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 158.6168 - val_loss: 184.0677\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 166.1575 - val_loss: 192.2449\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 161.3457 - val_loss: 182.9770\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 161.1219 - val_loss: 185.8357\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 163.1033 - val_loss: 180.9848\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 157.0801 - val_loss: 177.9542\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 157.3326 - val_loss: 175.2810\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 156.0923 - val_loss: 185.7100\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 157.7002 - val_loss: 174.9888\n",
      "Epoch 237/1000\n",
      "7500/7500 [==============================] - 1s 97us/step - loss: 157.0227 - val_loss: 186.7640\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 152.7311 - val_loss: 175.7477\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 153.1561 - val_loss: 215.6107\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 162.2604 - val_loss: 189.2561\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 154.1845 - val_loss: 175.1432\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 1s 97us/step - loss: 148.6201 - val_loss: 170.4796\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 156.2608 - val_loss: 171.6137\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 149.4923 - val_loss: 190.4266\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 153.4820 - val_loss: 184.8461\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 151.3510 - val_loss: 167.4934\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 152.4225 - val_loss: 177.4949\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 148.9853 - val_loss: 167.7002\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 147.4971 - val_loss: 177.1406\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 152.3781 - val_loss: 169.2114\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 145.6191 - val_loss: 165.3538\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 148.2054 - val_loss: 164.5643\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 149.8360 - val_loss: 163.1824\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 149.8616 - val_loss: 166.1479\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 146.7515 - val_loss: 165.2334\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 148.3655 - val_loss: 175.6886\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 151.3636 - val_loss: 204.4524\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 152.0893 - val_loss: 163.6727\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 147.6992 - val_loss: 173.4839\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 147.1778 - val_loss: 172.5294\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 142.2457 - val_loss: 165.0837\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 145.4537 - val_loss: 171.2345\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 144.1578 - val_loss: 163.1191\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 147.2435 - val_loss: 159.7737\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 141.8531 - val_loss: 169.7810\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 140.3908 - val_loss: 173.8954\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 142.1470 - val_loss: 177.2116\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 145.9692 - val_loss: 169.5876\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 145.8797 - val_loss: 171.7888\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 151.8730 - val_loss: 177.1791\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 140.1969 - val_loss: 172.2222\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 141.9173 - val_loss: 158.8904\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 153.2170 - val_loss: 172.9695\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 143.0945 - val_loss: 173.5981\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 141.7679 - val_loss: 165.6750\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 145.9500 - val_loss: 178.6009\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 141.1288 - val_loss: 154.9142\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 138.0708 - val_loss: 160.8715\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 140.9348 - val_loss: 162.1767\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 142.4353 - val_loss: 157.7223\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 140.7939 - val_loss: 155.5483\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 140.8602 - val_loss: 174.6574\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 139.9665 - val_loss: 161.9867\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 141.7355 - val_loss: 156.7883\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 144.1644 - val_loss: 154.5123\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 140.1781 - val_loss: 157.8800\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 140.8422 - val_loss: 158.3143\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 144.6120 - val_loss: 164.6410\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 136.5202 - val_loss: 164.9954\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 136.8713 - val_loss: 158.8728\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 135.2030 - val_loss: 155.1644\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 144.7677 - val_loss: 157.0230\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 135.8326 - val_loss: 162.9742\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 145.0889 - val_loss: 175.5388\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 137.5452 - val_loss: 157.8159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 138.7764 - val_loss: 154.6807\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 135.9500 - val_loss: 165.1092\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 138.8346 - val_loss: 153.0113\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 135.6963 - val_loss: 161.0843\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 137.9089 - val_loss: 156.7388\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 138.2134 - val_loss: 167.6779\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 139.4671 - val_loss: 156.9252\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 135.6985 - val_loss: 156.4714\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 136.5195 - val_loss: 163.6018\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 134.8541 - val_loss: 152.5041\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 136.8658 - val_loss: 155.1600\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 139.4475 - val_loss: 160.4910\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 135.8926 - val_loss: 172.7530\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 134.8398 - val_loss: 166.5504\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 136.6874 - val_loss: 173.2129\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 132.4656 - val_loss: 150.1016\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 134.9277 - val_loss: 196.6813\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 135.1645 - val_loss: 160.8214\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 135.1880 - val_loss: 174.7007\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 134.0771 - val_loss: 155.7413\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 132.5419 - val_loss: 153.9519\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 136.9531 - val_loss: 153.2105\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 130.5749 - val_loss: 152.5183\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 131.9422 - val_loss: 154.1648\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 128.4460 - val_loss: 150.1721\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 132.0100 - val_loss: 153.9965\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 132.6895 - val_loss: 160.5375\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 136.0658 - val_loss: 149.7682\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 136.6748 - val_loss: 156.4293\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 133.0690 - val_loss: 149.0604\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 132.3590 - val_loss: 152.4957\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 131.9901 - val_loss: 151.0332\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 132.6566 - val_loss: 152.3421\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 132.7884 - val_loss: 153.9733\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 132.3097 - val_loss: 148.8740\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 125.6854 - val_loss: 150.0489\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 1s 112us/step - loss: 132.7463 - val_loss: 158.8577\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 132.2182 - val_loss: 148.6631\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 128.8831 - val_loss: 190.7496\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 131.9614 - val_loss: 166.3739\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 137.6363 - val_loss: 164.9564\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 135.8521 - val_loss: 167.1251\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 129.7397 - val_loss: 165.2927\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 132.6506 - val_loss: 162.9417\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 129.0812 - val_loss: 151.0937\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 126.0178 - val_loss: 165.8549\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 132.8378 - val_loss: 152.3222\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 127.9923 - val_loss: 148.1579\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 129.6892 - val_loss: 172.0188\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 129.5860 - val_loss: 153.5895\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 125.4610 - val_loss: 157.4677\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 130.6053 - val_loss: 147.1561\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 131.9742 - val_loss: 169.6483\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 130.7824 - val_loss: 159.9537\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 129.7863 - val_loss: 152.5912\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 127.9979 - val_loss: 147.8317\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 128.0730 - val_loss: 152.9605\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 129.2305 - val_loss: 156.7960\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 125.6831 - val_loss: 147.2150\n",
      "Epoch 355/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 129.3559 - val_loss: 162.4985\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 124.5144 - val_loss: 148.9075\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 124.9602 - val_loss: 162.8478\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 128.2857 - val_loss: 148.5691\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 128.6780 - val_loss: 161.6317\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 125.0281 - val_loss: 157.9437\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 121.2210 - val_loss: 154.2079\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 124.6106 - val_loss: 148.5582\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 125.0702 - val_loss: 150.9609\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 1s 132us/step - loss: 128.5111 - val_loss: 158.4366\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 127.6403 - val_loss: 151.1282\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 125.3163 - val_loss: 167.1363\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 126.5550 - val_loss: 162.4871\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 129.7874 - val_loss: 155.0947\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 128.3664 - val_loss: 169.9121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 126.2322 - val_loss: 162.5282\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 126.9788 - val_loss: 154.8922\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 121.5320 - val_loss: 147.4802\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 122.9340 - val_loss: 158.2785\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 122.4258 - val_loss: 169.6798\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 126.1091 - val_loss: 158.7359\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 125.1232 - val_loss: 156.8424\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 124.4075 - val_loss: 162.9312\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 125.5565 - val_loss: 157.7419\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 127.7533 - val_loss: 153.6072\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 124.0062 - val_loss: 150.0475\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 120.6768 - val_loss: 155.7166\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 126.6302 - val_loss: 171.3890\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 118.7799 - val_loss: 156.4903\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 125.4731 - val_loss: 155.6467\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 121.3892 - val_loss: 153.0066\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 124.2620 - val_loss: 159.1329\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 121.8661 - val_loss: 152.9032\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 119.3268 - val_loss: 147.0837\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 124.6082 - val_loss: 155.6139\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 123.3356 - val_loss: 150.9225\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 122.3745 - val_loss: 154.8749\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 126.3583 - val_loss: 153.4452\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 121.5666 - val_loss: 148.7274\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 117.4376 - val_loss: 147.2013\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 1s 97us/step - loss: 120.1599 - val_loss: 159.0425\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 120.1949 - val_loss: 171.8360\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 124.2762 - val_loss: 158.2937\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 122.2953 - val_loss: 158.5796\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 126.5988 - val_loss: 155.7143\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 122.0421 - val_loss: 174.4204\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 120.0193 - val_loss: 154.4535\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 122.0001 - val_loss: 195.2681\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 122.6945 - val_loss: 143.9701\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 121.3248 - val_loss: 152.9013\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 121.6812 - val_loss: 151.2507\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 116.1644 - val_loss: 160.2457\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 115.3546 - val_loss: 158.3892\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 123.8691 - val_loss: 152.4348\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 124.0966 - val_loss: 150.1512\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 118.1678 - val_loss: 185.4888\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 125.6041 - val_loss: 152.1100\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 121.4116 - val_loss: 145.0770\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 115.4747 - val_loss: 156.1851\n",
      "Epoch 414/1000\n",
      "7500/7500 [==============================] - 1s 101us/step - loss: 116.8643 - val_loss: 161.6554\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 117.3812 - val_loss: 153.5666\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 118.0434 - val_loss: 151.1846\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 122.2065 - val_loss: 150.6306\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 1s 100us/step - loss: 121.6596 - val_loss: 157.8026\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 117.9480 - val_loss: 146.7519\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 118.9162 - val_loss: 154.2850\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 1s 100us/step - loss: 117.3821 - val_loss: 151.1689\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 119.3374 - val_loss: 150.9208\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 114.8207 - val_loss: 154.6821\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 115.3706 - val_loss: 157.0835\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 118.4907 - val_loss: 183.5752\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 114.7421 - val_loss: 153.8054\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 118.4336 - val_loss: 161.3220\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 121.8304 - val_loss: 159.1913\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 120.0284 - val_loss: 150.7017\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 1s 109us/step - loss: 116.4973 - val_loss: 162.0269\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 1s 128us/step - loss: 117.7234 - val_loss: 164.6609\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 1s 124us/step - loss: 117.4523 - val_loss: 153.5095\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 114.0017 - val_loss: 160.0998\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 116.9323 - val_loss: 160.9383\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 113.8260 - val_loss: 162.7246\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 115.2607 - val_loss: 158.0633\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 120.8978 - val_loss: 165.9194\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 120.6737 - val_loss: 162.3708\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 115.4898 - val_loss: 151.5346\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 119.2249 - val_loss: 155.0825\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 109.5408 - val_loss: 151.7522\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 114.5307 - val_loss: 154.5454\n",
      "Epoch 443/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 75us/step - loss: 114.2992 - val_loss: 166.5107\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 115.9881 - val_loss: 185.7375\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 115.5008 - val_loss: 151.1358\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 116.0597 - val_loss: 167.7203\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 113.3945 - val_loss: 165.2467\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 115.3146 - val_loss: 152.2665\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 119.6809 - val_loss: 154.6479\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 112.2992 - val_loss: 158.4854\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 114.8069 - val_loss: 158.6116\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 110.2763 - val_loss: 165.6214\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 112.4827 - val_loss: 154.4827\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 114.4398 - val_loss: 150.2932\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 111.7262 - val_loss: 159.6333\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 117.6727 - val_loss: 166.9710\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 118.0446 - val_loss: 179.5950\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 116.6856 - val_loss: 163.3893\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 1s 99us/step - loss: 109.1044 - val_loss: 164.1659\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 106.9621 - val_loss: 162.6280\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 112.1809 - val_loss: 164.0798\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 115.7968 - val_loss: 151.9230\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 110.7065 - val_loss: 161.4622\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 113.9778 - val_loss: 158.4484\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 113.1164 - val_loss: 151.4888\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 111.6561 - val_loss: 155.4644\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 114.0194 - val_loss: 150.8668\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 107.8074 - val_loss: 182.4378\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 111.4572 - val_loss: 171.2242\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 111.3655 - val_loss: 163.0173\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 120.5375 - val_loss: 191.3060\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 109.5569 - val_loss: 172.8392\n",
      "Epoch 473/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 112.5739 - val_loss: 160.4009\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 116.5313 - val_loss: 153.3807\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 108.7197 - val_loss: 159.1598\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 107.9289 - val_loss: 156.9145\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 110.4800 - val_loss: 155.6935\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 113.2356 - val_loss: 151.9579\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 1s 112us/step - loss: 106.4109 - val_loss: 161.7329\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 1s 107us/step - loss: 118.5686 - val_loss: 157.1752\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 112.2908 - val_loss: 155.4130\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 108.8222 - val_loss: 156.8343\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 111.1286 - val_loss: 160.6102\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 110.7868 - val_loss: 160.5080\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 111.8019 - val_loss: 168.1320\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 110.5524 - val_loss: 147.4935\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 112.3339 - val_loss: 175.8015\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 108.0606 - val_loss: 155.6342\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 107.8417 - val_loss: 163.9954\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 113.4607 - val_loss: 159.8818\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 108.6963 - val_loss: 170.9663\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 113.4181 - val_loss: 209.7227\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 107.8968 - val_loss: 165.2576\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 106.6121 - val_loss: 169.7465\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 112.9400 - val_loss: 160.9500\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 110.6041 - val_loss: 153.8276\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 110.8842 - val_loss: 150.0762\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 1s 106us/step - loss: 106.9283 - val_loss: 154.9885\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 1s 97us/step - loss: 105.4127 - val_loss: 163.2969\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 1s 103us/step - loss: 108.3416 - val_loss: 153.2663\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 110.8889 - val_loss: 157.4012\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 106.3455 - val_loss: 154.2305\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 110.9889 - val_loss: 166.0352\n",
      "Epoch 00503: early stopping\n",
      "Final score (RMSE): 11.241545677185059\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "model.add(Dropout(0.01)) # Dropout Layer\n",
    "model.add(Dense(25, activation='relu')) # Hidden 2\n",
    "model.add(Dense(10, \n",
    "                kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01),activation='relu')) # Hidden 3 w/regularization\n",
    "model.add(Dense(1)) # Output\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=100, verbose=1, mode='auto')\n",
    "checkpoint = ModelCheckpoint(filepath=filename_checkpoint, verbose=0, save_best_only=True)\n",
    "\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpoint],verbose=1,epochs=1000)\n",
    "model.load_weights(filename_checkpoint)\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "#chart_regression(pred.flatten(),y_test)\n",
    "#chart_regression(pred.flatten(),y_test,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "extract_and_encode_features(df_test)\n",
    "\n",
    "ids_test = df_test['id']\n",
    "df_test.drop('id',1,inplace=True)\n",
    "\n",
    "names_test = df_test['name']\n",
    "df_test.drop('name',1,inplace=True)\n",
    "\n",
    "x_submit = df_test.as_matrix().astype(np.float32)\n",
    "pred_submit = model.predict(x_submit)\n",
    "\n",
    "df_submit = pd.DataFrame({'id': ids_test,'cost': pred_submit[:,0]})\n",
    "df_submit = df_submit[['id', 'cost']]\n",
    "df_submit.to_csv(filename_submit, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
