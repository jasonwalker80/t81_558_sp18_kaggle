{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n",
    "\n",
    "**Kaggle Assignment: **\n",
    "\n",
    "**Student Name: Jason Walker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## Description\n",
    "This is one of the projects from the course T81-855: Applications of Deep Learning at Washington University in St. Louis. All students must create a Kaggle account and submit a solution. Once you have submitted your solution entry log into Blackboard (at WUSTL) and submit a single file telling me your Kaggle name on the leaderboard (you do not need to register to Kaggle with your real name). This competition will be visible to the public, so there may be non-student submissions as well as student.\n",
    "\n",
    "The data set for this competition consists of a number of input columns that should be used to predict a stores sales. This is a regression problem. The inputs are a mixture of discrete and category values. The data set is from a simulation.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The evaluation pages describes how submissions will be scored and how students should format their submissions. The scores are in RMSE.\n",
    "Submission Format\n",
    "\n",
    "For every store in the dataset, submission files should contain a sales volume.\n",
    "\n",
    "The file should contain a header and have the following format:\n",
    "\n",
    "```\n",
    "100000,1.23\n",
    "100001,1.123\n",
    "100002,3.332\n",
    "100003,1.53\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The data contains data and costs for various office supplies. The data came from a simulation and do not directly correspond to any real-world items. See how well you can predict the cost of an item using the provided data. Feature engineering will likely help you. The *name* column may seem useless at first glance; however, it contains information that you can parse to help your predictions.\n",
    "File descriptions\n",
    "```\n",
    "    id - The identifier/primary key.\n",
    "    name - The name of this item.\n",
    "    manufacturer - The manufacturer.\n",
    "    pack - The number of items in this pack.\n",
    "    weight - The weight of a pack of these items.\n",
    "    height - The height of a pack of these items.\n",
    "    width - The width of a pack of these items.\n",
    "    length - The length of a pack of these items.\n",
    "    cost - The cost for this item pack. This is what you are to predict (the target). \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions\n",
    "\n",
    "You will see these at the top of every module and assignment.  These are simply a set of reusable functions that we will make use of.  Each of them will be explained as the semester progresses.  They are explained in greater detail as the course progresses.  Class 4 contains a complete overview of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n",
    "        \n",
    "# This function submits an assignment.  You can submit an assignment as much as you like, only the final\n",
    "# submission counts.  The paramaters are as follows:\n",
    "# data - Pandas dataframe output.\n",
    "# key - Your student key that was emailed to you.\n",
    "# no - The assignment class number, should be 1 through 1.\n",
    "# source_file - The full path to your Python or IPYNB file.  This must have \"_class1\" as part of its name.  \n",
    "# .             The number must match your assignment number.  For example \"_class2\" for class assignment #2.\n",
    "def submit(data,key,no,source_file=None):\n",
    "    if source_file is None and '__file__' not in globals(): raise Exception('Must specify a filename when a Jupyter notebook.')\n",
    "    if source_file is None: source_file = __file__\n",
    "    suffix = '_class{}'.format(no)\n",
    "    if suffix not in source_file: raise Exception('{} must be part of the filename.'.format(suffix))\n",
    "    with open(source_file, \"rb\") as image_file:\n",
    "        encoded_python = base64.b64encode(image_file.read()).decode('ascii')\n",
    "    ext = os.path.splitext(source_file)[-1].lower()\n",
    "    if ext not in ['.ipynb','.py']: raise Exception(\"Source file is {} must be .py or .ipynb\".format(ext))\n",
    "    r = requests.post(\"https://api.heatonresearch.com/assignment-submit\",\n",
    "        headers={'x-api-key':key}, json={'csv':base64.b64encode(data.to_csv(index=False).encode('ascii')).decode(\"ascii\"),\n",
    "        'assignment': no, 'ext':ext, 'py':encoded_python})\n",
    "    if r.status_code == 200:\n",
    "        print(\"Success: {}\".format(r.text))\n",
    "    else: print(\"Failure: {}\".format(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kaggle Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers\n",
    "\n",
    "path = './data'\n",
    "\n",
    "filename_test = os.path.join(path,\"test.csv\")\n",
    "filename_train = os.path.join(path,\"train.csv\")\n",
    "filename_sample = os.path.join(path,\"sample.csv\")\n",
    "filename_submit = os.path.join(path,\"submit.csv\")\n",
    "filename_checkpoint = os.path.join(path,\"checkpoint.hdf5\")\n",
    "\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "\n",
    "np.random.seed(42) # Uncomment this line to get the same shuffle each time\n",
    "df_train = df_train.reindex(np.random.permutation(df_train.index))\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Encode Features\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def extract_and_encode_features(df):\n",
    "    color_regex='(?P<color>red|blue|green|yellow|orange|pink|black|brown|white)'\n",
    "    df['color'] = df.name.str.extract(color_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    quality_regex='(?P<quality>generic|medium\\shigh\\squality|high\\squality)'\n",
    "    df['quality'] = df.name.str.extract(quality_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    size_regex='(?P<size>tiny|small|medium|large)'\n",
    "    df['size'] = df.name.str.extract(size_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    item_regex='(?P<item>paperclips|paperweights|ink\\spens|pencils|stapler|tablets|thumbtacks|post\\sit\\snotes)'\n",
    "    df['item'] = df.name.str.extract(item_regex, flags=re.IGNORECASE, expand=False)\n",
    "    \n",
    "    for column in ['pack','weight','height','width','length']:\n",
    "        missing_median(df,column)\n",
    "    \n",
    "    #df.insert(1,'surface_area',(df['height']*df['width']*df['length']).astype(int))\n",
    "    \n",
    "    ## encode numeric features\n",
    "    #for column in ['pack','weight','height','width','length','surface_area']:\n",
    "    #    encode_numeric_zscore(df,column)\n",
    "\n",
    "    # encode text/categorical features\n",
    "    for column in ['manufacturer','color','quality','size','item']:\n",
    "        encode_text_dummy(df,column)\n",
    "  \n",
    "extract_and_encode_features(df_train)\n",
    "\n",
    "ids_train = df_train['id']\n",
    "df_train.drop('id',1,inplace=True)\n",
    "\n",
    "names_train = df_train['name']\n",
    "df_train.drop('name',1,inplace=True)\n",
    "\n",
    "x,y = to_xy(df_train,'cost')\n",
    "\n",
    "# Cross-Validate\n",
    "kf = KFold(5)\n",
    "\n",
    "# Used before KFold\n",
    "#x_train, x_test, y_train, y_test = train_test_split(    \n",
    "#    x, y, test_size=0.25, random_state=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 32.560638427734375\n",
      "['pack', 'weight', 'height', 'width', 'length', 'manufacturer-6% Solution', 'manufacturer-Deep Office Supplies', 'manufacturer-Duck Lake', 'manufacturer-Offices-R-Us', 'manufacturer-WizBang', 'color-Black', 'color-Blue', 'color-Brown', 'color-Green', 'color-Pink', 'color-Red', 'color-White', 'quality-Generic', 'quality-High Quality', 'quality-Medium High Quality', 'size-Large', 'size-Medium', 'size-Small', 'size-Tiny', 'item-Ink Pens', 'item-Paperclips', 'item-Paperweights', 'item-Pencils', 'item-Post It Notes', 'item-Stapler', 'item-Tablets', 'item-Thumbtacks']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item-Post It Notes</th>\n",
       "      <td>-59.312168</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Thumbtacks</th>\n",
       "      <td>-56.623314</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Pencils</th>\n",
       "      <td>-50.400471</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperclips</th>\n",
       "      <td>-43.599960</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Red</th>\n",
       "      <td>-35.857452</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Green</th>\n",
       "      <td>-24.303213</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>-10.113589</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Blue</th>\n",
       "      <td>-9.778607</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>-8.088337</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>width</th>\n",
       "      <td>-6.066093</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Generic</th>\n",
       "      <td>-4.613498</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Ink Pens</th>\n",
       "      <td>-4.261536</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Offices-R-Us</th>\n",
       "      <td>-1.761803</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Tiny</th>\n",
       "      <td>-1.449806</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Duck Lake</th>\n",
       "      <td>-0.532333</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack</th>\n",
       "      <td>0.019380</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>0.034990</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Deep Office Supplies</th>\n",
       "      <td>0.109180</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-6% Solution</th>\n",
       "      <td>0.238405</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Small</th>\n",
       "      <td>0.369865</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Brown</th>\n",
       "      <td>1.451031</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-WizBang</th>\n",
       "      <td>1.946654</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Medium</th>\n",
       "      <td>3.852479</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Medium High Quality</th>\n",
       "      <td>5.394495</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperweights</th>\n",
       "      <td>6.588786</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Large</th>\n",
       "      <td>6.606421</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-High Quality</th>\n",
       "      <td>7.698106</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Black</th>\n",
       "      <td>15.785252</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-White</th>\n",
       "      <td>29.429951</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Pink</th>\n",
       "      <td>40.224407</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Stapler</th>\n",
       "      <td>48.437527</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Tablets</th>\n",
       "      <td>159.170486</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         coef  positive\n",
       "item-Post It Notes                 -59.312168     False\n",
       "item-Thumbtacks                    -56.623314     False\n",
       "item-Pencils                       -50.400471     False\n",
       "item-Paperclips                    -43.599960     False\n",
       "color-Red                          -35.857452     False\n",
       "color-Green                        -24.303213     False\n",
       "height                             -10.113589     False\n",
       "color-Blue                          -9.778607     False\n",
       "length                              -8.088337     False\n",
       "width                               -6.066093     False\n",
       "quality-Generic                     -4.613498     False\n",
       "item-Ink Pens                       -4.261536     False\n",
       "manufacturer-Offices-R-Us           -1.761803     False\n",
       "size-Tiny                           -1.449806     False\n",
       "manufacturer-Duck Lake              -0.532333     False\n",
       "pack                                 0.019380      True\n",
       "weight                               0.034990      True\n",
       "manufacturer-Deep Office Supplies    0.109180      True\n",
       "manufacturer-6% Solution             0.238405      True\n",
       "size-Small                           0.369865      True\n",
       "color-Brown                          1.451031      True\n",
       "manufacturer-WizBang                 1.946654      True\n",
       "size-Medium                          3.852479      True\n",
       "quality-Medium High Quality          5.394495      True\n",
       "item-Paperweights                    6.588786      True\n",
       "size-Large                           6.606421      True\n",
       "quality-High Quality                 7.698106      True\n",
       "color-Black                         15.785252      True\n",
       "color-White                         29.429951      True\n",
       "color-Pink                          40.224407      True\n",
       "item-Stapler                        48.437527      True\n",
       "item-Tablets                       159.170486      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [ 80.02005768]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAD8CAYAAADExYYgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nF2ZmZtZQ7lyYmZlZQ7lzYWZmZg31/6lPpHWSQn7sAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1ca47ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple function to evaluate the coefficients of a regression\n",
    "%matplotlib inline    \n",
    "from IPython.display import display, HTML    \n",
    "\n",
    "def report_coef(names,coef,intercept):\n",
    "    r = pd.DataFrame( { 'coef': coef, 'positive': coef>=0  }, index = names )\n",
    "    r = r.sort_values(by=['coef'])\n",
    "    display(r)\n",
    "    print(\"Intercept: {}\".format(intercept))\n",
    "    r['coef'].plot(kind='barh', color=r['positive'].map({True: 'b', False: 'r'}))\n",
    "    \n",
    "# Create linear regression\n",
    "regressor = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# Fit/train linear regression\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "print(names)\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_[0,:],\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 9087.6060 - val_loss: 4727.5546\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 5128.4472 - val_loss: 4195.2830\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 4887.6776 - val_loss: 4074.6876\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 4543.0523 - val_loss: 3867.0386\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 4498.9929 - val_loss: 3880.6009\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 4258.6341 - val_loss: 3662.8730\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 4238.9946 - val_loss: 3772.1742\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 4192.3410 - val_loss: 4715.5497\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 4141.6892 - val_loss: 3346.6887\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 3993.4467 - val_loss: 3300.3614\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 3900.4058 - val_loss: 3448.9807\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 3618.5531 - val_loss: 2944.9423\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 3614.7667 - val_loss: 2733.2258\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 3554.8437 - val_loss: 2706.8252\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 3176.4234 - val_loss: 2486.2250\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 3086.9135 - val_loss: 2374.4459\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 2689.1465 - val_loss: 2688.8374\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 2770.8416 - val_loss: 3528.9427\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 2267.8829 - val_loss: 1742.0209\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 2248.9624 - val_loss: 1371.9757\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1708.4007 - val_loss: 1130.8294\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 1501.1363 - val_loss: 951.3466\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 1228.7636 - val_loss: 778.7280\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 1148.1139 - val_loss: 657.3637\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 1088.1352 - val_loss: 933.1470\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 976.9064 - val_loss: 1001.2484\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 892.3105 - val_loss: 604.6940\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 779.6163 - val_loss: 753.5734\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 812.5175 - val_loss: 501.4225\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 658.6303 - val_loss: 624.3127\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 771.0361 - val_loss: 655.7054\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 654.7260 - val_loss: 531.8567\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 757.2571 - val_loss: 518.7539\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 612.3918 - val_loss: 527.6585\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 727.3447 - val_loss: 407.2150\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 549.2121 - val_loss: 571.7631\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 561.8611 - val_loss: 398.7943\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 561.3859 - val_loss: 908.3802\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 626.761 - 0s 57us/step - loss: 625.4815 - val_loss: 744.6140\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 600.7209 - val_loss: 1457.3123\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 829.3258 - val_loss: 422.2273\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 539.2828 - val_loss: 384.1400\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 540.7391 - val_loss: 528.0225\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 522.8187 - val_loss: 443.6435\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 495.3808 - val_loss: 364.6817\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 533.9729 - val_loss: 352.4302\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 575.1783 - val_loss: 348.7507\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 457.7349 - val_loss: 394.5340\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 488.0577 - val_loss: 340.8510\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 491.3803 - val_loss: 843.8707\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 506.1872 - val_loss: 376.5544\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 525.7175 - val_loss: 363.6385\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 442.5910 - val_loss: 310.8657\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 429.1020 - val_loss: 331.6939\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 485.1523 - val_loss: 364.9624\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 563.9901 - val_loss: 353.4040\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 466.0256 - val_loss: 375.3513\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 388.3616 - val_loss: 520.8846\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 403.3303 - val_loss: 295.8109\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 576.3137 - val_loss: 300.1013\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 426.2260 - val_loss: 501.3648\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 453.1015 - val_loss: 321.2735\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 499.5016 - val_loss: 366.9023\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 412.6268 - val_loss: 354.2039\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 443.5149 - val_loss: 289.0082\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 726.1566 - val_loss: 1523.9564\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 655.5948 - val_loss: 341.0514\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 431.8507 - val_loss: 315.2972\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 419.3638 - val_loss: 348.5121\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 415.3572 - val_loss: 303.6087\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 422.8028 - val_loss: 285.9072\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 445.6917 - val_loss: 357.4117\n",
      "Epoch 73/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 68us/step - loss: 401.3186 - val_loss: 379.3324\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 406.9899 - val_loss: 311.5777\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 362.3116 - val_loss: 395.4599\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 374.2895 - val_loss: 424.0518\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 474.3877 - val_loss: 289.8211\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 368.6504 - val_loss: 575.1914\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 372.4552 - val_loss: 320.0837\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 354.8838 - val_loss: 560.1346\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 440.0429 - val_loss: 754.7149\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 399.0787 - val_loss: 279.3262\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 356.2339 - val_loss: 373.0372\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 376.3567 - val_loss: 276.6571\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 414.7935 - val_loss: 315.8015\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 512.4113 - val_loss: 298.6141\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 380.5720 - val_loss: 488.8863\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 335.2268 - val_loss: 340.3727\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 338.1195 - val_loss: 251.9489\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 389.0505 - val_loss: 384.9503\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 340.4231 - val_loss: 452.0840\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 387.4919 - val_loss: 416.4302\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 367.9403 - val_loss: 518.0555\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 290.5606 - val_loss: 261.1143\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 575.0374 - val_loss: 295.0987\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 354.4027 - val_loss: 254.3189\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 321.7680 - val_loss: 367.9397\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 322.7771 - val_loss: 274.7043\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 384.4461 - val_loss: 482.4654\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 334.4423 - val_loss: 441.6921\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 325.3711 - val_loss: 387.3933\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 319.3189 - val_loss: 266.2191\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 384.3208 - val_loss: 524.0823\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 353.6514 - val_loss: 267.6629\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 319.4397 - val_loss: 294.9882\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 442.3432 - val_loss: 488.5827\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 424.5583 - val_loss: 242.2201\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 319.8533 - val_loss: 270.2150\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 331.4604 - val_loss: 248.5345\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 348.0107 - val_loss: 244.9493\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 313.4825 - val_loss: 446.8628\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 335.6595 - val_loss: 278.0942\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 325.0214 - val_loss: 408.9064\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 297.5913 - val_loss: 299.0541\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 326.3398 - val_loss: 321.9466\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 327.7496 - val_loss: 269.9458\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 306.0584 - val_loss: 423.5805\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 288.5729 - val_loss: 317.1369\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 299.4890 - val_loss: 426.2790\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 323.3910 - val_loss: 366.3789\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 343.1230 - val_loss: 236.4264\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 283.5287 - val_loss: 584.9045\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 460.3120 - val_loss: 280.9038\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 297.4181 - val_loss: 236.6417\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 299.1995 - val_loss: 251.8629\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 315.6275 - val_loss: 223.1132\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 459.1092 - val_loss: 225.8817\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 277.8081 - val_loss: 249.9446\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 283.8984 - val_loss: 312.4668\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 319.1701 - val_loss: 249.5608\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 284.0663 - val_loss: 354.3156\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 265.3398 - val_loss: 218.2459\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 292.4043 - val_loss: 299.9290\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 356.6474 - val_loss: 272.0659\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 329.6550 - val_loss: 337.9265\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 275.7981 - val_loss: 218.4256\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 304.0001 - val_loss: 239.4164\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 320.1751 - val_loss: 371.8419\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 344.1726 - val_loss: 230.0510\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 308.8054 - val_loss: 213.1158\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 324.7959 - val_loss: 270.1570\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 265.6488 - val_loss: 202.6770\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 266.1799 - val_loss: 214.3535\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 274.8082 - val_loss: 209.2229\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 266.9804 - val_loss: 224.6854\n",
      "Epoch 146/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 57us/step - loss: 318.1251 - val_loss: 285.6510\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 283.1911 - val_loss: 490.4321\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 316.8413 - val_loss: 218.8380\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 305.2237 - val_loss: 268.9140\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 276.1543 - val_loss: 213.4450\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 326.5162 - val_loss: 279.4381\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 262.1422 - val_loss: 296.2502\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 264.0475 - val_loss: 230.2173\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 329.2454 - val_loss: 216.0988\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 293.8819 - val_loss: 268.1872\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 284.7253 - val_loss: 371.4396\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 315.3818 - val_loss: 392.7206\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 297.3185 - val_loss: 252.8427\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 273.2758 - val_loss: 240.1410\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 258.9028 - val_loss: 231.5055\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 316.3946 - val_loss: 402.2844\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 281.1329 - val_loss: 301.6969\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 275.0930 - val_loss: 348.0313\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 258.2936 - val_loss: 216.1249\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 263.8027 - val_loss: 332.8164\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 317.4295 - val_loss: 249.6540\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 348.2565 - val_loss: 486.3857\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 271.9258 - val_loss: 436.1702\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 274.8930 - val_loss: 296.4832\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 309.0830 - val_loss: 505.6034\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 247.4360 - val_loss: 230.8609\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 273.9836 - val_loss: 235.7200\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 267.0090 - val_loss: 494.0168\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 253.9580 - val_loss: 222.6316\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 244.5957 - val_loss: 199.4690\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 256.7892 - val_loss: 351.7656\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 290.3957 - val_loss: 216.9498\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 246.5054 - val_loss: 264.3301\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 313.1242 - val_loss: 330.0867\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 221.3259 - val_loss: 197.2859\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 258.7892 - val_loss: 195.4268\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 275.0329 - val_loss: 207.5235\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 256.5887 - val_loss: 221.7132\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 298.4832 - val_loss: 216.0947\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 266.9933 - val_loss: 217.9424\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 268.1917 - val_loss: 356.3168\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 290.3724 - val_loss: 209.3418\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 251.2568 - val_loss: 210.0221\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 236.1497 - val_loss: 478.4034\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 262.5455 - val_loss: 214.6850\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 327.6915 - val_loss: 290.3700\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 252.6088 - val_loss: 208.9761\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 247.2576 - val_loss: 210.0924\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 236.4992 - val_loss: 209.3073\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 263.8531 - val_loss: 278.4394\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 242.7417 - val_loss: 208.6423\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 247.7248 - val_loss: 247.1502\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 280.0332 - val_loss: 336.2442\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 267.0126 - val_loss: 340.5048\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 295.8420 - val_loss: 390.7126\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 289.9927 - val_loss: 199.6882\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 268.9940 - val_loss: 188.2344\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 221.1882 - val_loss: 189.9574\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 359.5148 - val_loss: 235.2132\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 229.6806 - val_loss: 215.6532\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 251.9427 - val_loss: 234.0131\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 265.6169 - val_loss: 212.7601\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 299.7501 - val_loss: 197.3905\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 287.0346 - val_loss: 221.0911\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 216.5845 - val_loss: 278.2351\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 258.6063 - val_loss: 1126.9633\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 243.3153 - val_loss: 191.5794\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 249.8513 - val_loss: 234.0739\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 262.388 - 0s 57us/step - loss: 261.2092 - val_loss: 194.6199\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 228.7360 - val_loss: 209.7408\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 264.7081 - val_loss: 235.8595\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 246.3515 - val_loss: 238.5175\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 257.7240 - val_loss: 198.7732\n",
      "Epoch 219/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 236.9322 - val_loss: 219.8030\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 324.3984 - val_loss: 309.9208\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 259.7252 - val_loss: 197.8519\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 266.1147 - val_loss: 186.1478\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 231.6332 - val_loss: 202.1465\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 250.1246 - val_loss: 197.0097\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 220.6025 - val_loss: 222.6516\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 251.7815 - val_loss: 282.5534\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 234.2268 - val_loss: 201.1676\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 269.4091 - val_loss: 203.3496\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 245.3788 - val_loss: 312.2922\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 239.3601 - val_loss: 237.9677\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 247.5914 - val_loss: 247.0534\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 259.5987 - val_loss: 195.8343\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 256.1967 - val_loss: 245.8526\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 235.0345 - val_loss: 235.7424\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 245.1980 - val_loss: 323.3822\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 249.0326 - val_loss: 178.5537\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 276.0027 - val_loss: 325.6897\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 244.6994 - val_loss: 189.3147\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 226.2053 - val_loss: 690.1995\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 268.0429 - val_loss: 227.4409\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 247.7949 - val_loss: 225.0077\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 216.4544 - val_loss: 246.0452\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 233.2486 - val_loss: 457.5496\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 286.0821 - val_loss: 284.3405\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 254.5802 - val_loss: 200.5847\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 247.0487 - val_loss: 323.8330\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 228.0531 - val_loss: 205.9187\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 257.1950 - val_loss: 207.7296\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 221.9379 - val_loss: 211.1674\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 243.3251 - val_loss: 276.0759\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 210.3284 - val_loss: 278.6717\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 209.7576 - val_loss: 219.3477\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 218.0893 - val_loss: 184.4701\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 279.7244 - val_loss: 327.0611\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 250.7685 - val_loss: 244.3471\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 227.6805 - val_loss: 273.2124\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 231.8813 - val_loss: 185.8157\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 232.7712 - val_loss: 231.0839\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 235.1127 - val_loss: 189.1875\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 225.4198 - val_loss: 249.4881\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 210.7209 - val_loss: 305.0474\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 211.5427 - val_loss: 202.3725\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 237.7259 - val_loss: 236.4716\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 231.5017 - val_loss: 238.3572\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 222.7765 - val_loss: 196.7217\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 220.2964 - val_loss: 194.3886\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 306.4053 - val_loss: 182.3918\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 219.3617 - val_loss: 209.7512\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 242.5471 - val_loss: 275.6642\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 229.5779 - val_loss: 172.9740\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 203.7944 - val_loss: 279.0986\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 245.4092 - val_loss: 369.0434\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 213.8385 - val_loss: 279.9004\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 224.3722 - val_loss: 192.9401\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 208.3640 - val_loss: 232.0691\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 257.4201 - val_loss: 215.4103\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 263.4386 - val_loss: 418.3166\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 251.8260 - val_loss: 267.3329\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 267.7510 - val_loss: 185.9129\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 217.9093 - val_loss: 424.1483\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 221.0724 - val_loss: 176.8653\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 235.7653 - val_loss: 199.1411\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 220.1042 - val_loss: 222.3909\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 191.9695 - val_loss: 185.5672\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 205.3096 - val_loss: 209.7671\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 311.2639 - val_loss: 276.9442\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 213.1405 - val_loss: 195.1403\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 199.6933 - val_loss: 176.4715\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 218.2854 - val_loss: 189.2369\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 210.8712 - val_loss: 193.0073\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 290.0781 - val_loss: 188.0800\n",
      "Epoch 292/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 298.5787 - val_loss: 195.3978\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 217.2188 - val_loss: 205.7595\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 244.0246 - val_loss: 230.1474\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 214.9462 - val_loss: 183.3504\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 215.6185 - val_loss: 213.8110\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 205.1339 - val_loss: 243.7920\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 206.8719 - val_loss: 225.0199\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 298.7170 - val_loss: 220.1860\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 251.8332 - val_loss: 187.0698\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 228.7013 - val_loss: 221.4558\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 204.2670 - val_loss: 176.6289\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 223.3021 - val_loss: 285.2029\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 222.5769 - val_loss: 203.5260\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 314.9621 - val_loss: 279.4665\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 217.5916 - val_loss: 222.0205\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 202.6664 - val_loss: 225.0730\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 213.5053 - val_loss: 169.4909\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 195.2900 - val_loss: 203.5752\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 217.7282 - val_loss: 206.1527\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 237.5517 - val_loss: 196.2684\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 184.4974 - val_loss: 166.2076\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 205.0023 - val_loss: 183.7599\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 280.3881 - val_loss: 228.0341\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 210.8854 - val_loss: 183.4440\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 221.5110 - val_loss: 173.7712\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 252.7690 - val_loss: 269.0250\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 221.2717 - val_loss: 280.5657\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 228.7859 - val_loss: 211.9549\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 211.5047 - val_loss: 182.3259\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 258.6675 - val_loss: 300.1250\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 222.3587 - val_loss: 437.8050\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 216.1531 - val_loss: 178.3520\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 229.2267 - val_loss: 245.4142\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 212.5351 - val_loss: 230.7833\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 255.0835 - val_loss: 179.7517\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 226.6765 - val_loss: 179.3420\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 209.2081 - val_loss: 187.1955\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 180.7896 - val_loss: 188.5735\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 211.9416 - val_loss: 209.3216\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 211.5971 - val_loss: 319.4820\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 231.8147 - val_loss: 222.0861\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 212.0160 - val_loss: 203.7303\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 203.7429 - val_loss: 186.8717\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 218.0201 - val_loss: 183.2185\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 203.8811 - val_loss: 174.4072\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 215.8059 - val_loss: 219.1258\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 234.2919 - val_loss: 169.8760\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 200.0918 - val_loss: 215.4876\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 197.1485 - val_loss: 190.5635\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 208.3148 - val_loss: 183.1445\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 218.9235 - val_loss: 172.6069\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 207.5656 - val_loss: 181.6504\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 194.0048 - val_loss: 183.4726\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 217.5331 - val_loss: 253.2356\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 259.5719 - val_loss: 339.5478\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 211.8513 - val_loss: 283.8516\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 200.3479 - val_loss: 313.7231\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 196.3968 - val_loss: 183.0149\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 359.5072 - val_loss: 361.0569\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 250.3832 - val_loss: 182.9602\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 242.6124 - val_loss: 191.5379\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 250.5259 - val_loss: 270.8588\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 210.8872 - val_loss: 177.7654\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 211.6754 - val_loss: 169.6309\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 215.9896 - val_loss: 172.8660\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 235.2916 - val_loss: 202.9858\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 206.3183 - val_loss: 169.1467\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 218.9869 - val_loss: 202.7426\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 228.7683 - val_loss: 204.4787\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 183.9566 - val_loss: 205.7712\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 206.0422 - val_loss: 166.1927\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 218.4827 - val_loss: 171.0386\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 198.8489 - val_loss: 186.2334\n",
      "Epoch 365/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 57us/step - loss: 224.2450 - val_loss: 180.1033\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 220.1621 - val_loss: 236.2144\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 199.6925 - val_loss: 167.6273\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 264.0801 - val_loss: 178.0069\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 190.4530 - val_loss: 193.9652\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 206.2294 - val_loss: 197.1260\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 197.8840 - val_loss: 191.9187\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 195.2237 - val_loss: 267.7173\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 227.6661 - val_loss: 374.4054\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 186.3244 - val_loss: 178.1505\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 206.0294 - val_loss: 210.0875\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 210.3285 - val_loss: 180.7198\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 322.6562 - val_loss: 182.9248\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 202.7310 - val_loss: 164.7834\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 207.1018 - val_loss: 212.0617\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 195.7018 - val_loss: 202.9928\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 177.2797 - val_loss: 224.6938\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 190.4086 - val_loss: 168.0128\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 202.1029 - val_loss: 170.5474\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 229.1741 - val_loss: 163.5415\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 256.1370 - val_loss: 161.4922\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 180.1140 - val_loss: 185.5285\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 213.7541 - val_loss: 217.5223\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 180.9131 - val_loss: 204.2661\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 197.6943 - val_loss: 166.4217\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 190.5068 - val_loss: 184.3653\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 195.6583 - val_loss: 169.6241\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 264.0446 - val_loss: 169.0432\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 190.3293 - val_loss: 161.0699\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 187.2695 - val_loss: 188.2592\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 211.8293 - val_loss: 211.1708\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 198.0393 - val_loss: 198.2738\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 184.7150 - val_loss: 174.6933\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 220.3368 - val_loss: 450.6376\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 221.9959 - val_loss: 193.6577\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 179.2337 - val_loss: 163.1057\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 181.2546 - val_loss: 159.6907\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 207.6158 - val_loss: 193.3069\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 203.5328 - val_loss: 250.5054\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 194.9671 - val_loss: 177.6373\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 195.9110 - val_loss: 214.7231\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 205.1289 - val_loss: 189.6859\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 202.3445 - val_loss: 206.7607\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 189.6411 - val_loss: 212.0853\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 221.8594 - val_loss: 175.1662\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 238.1270 - val_loss: 175.0618\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 198.4594 - val_loss: 176.2371\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 174.3241 - val_loss: 205.6703\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 190.0695 - val_loss: 320.9774\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 203.5928 - val_loss: 245.2347\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 196.4607 - val_loss: 238.9756\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 195.1372 - val_loss: 182.4849\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 189.0126 - val_loss: 162.6300\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 218.4909 - val_loss: 161.5666\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 328.0034 - val_loss: 352.5552\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 200.5211 - val_loss: 161.3654\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 188.8657 - val_loss: 168.8521\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 196.3986 - val_loss: 240.0462\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 192.2642 - val_loss: 157.3150\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 208.1466 - val_loss: 247.2576\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 186.9897 - val_loss: 203.1866\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 204.8803 - val_loss: 204.4053\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 217.9138 - val_loss: 203.5563\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 183.6178 - val_loss: 176.6237\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 199.6821 - val_loss: 262.0081\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 204.2509 - val_loss: 168.8345\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 193.6729 - val_loss: 184.0331\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 188.4983 - val_loss: 279.4734\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 261.2464 - val_loss: 178.5942\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 185.9082 - val_loss: 318.4613\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 210.6933 - val_loss: 162.4706\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 182.3613 - val_loss: 166.5423\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 208.7336 - val_loss: 189.0637\n",
      "Epoch 438/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 192.5283 - val_loss: 202.7875\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 235.4482 - val_loss: 158.3509\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 179.2529 - val_loss: 156.3183\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 195.6247 - val_loss: 159.0178\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 207.5427 - val_loss: 174.6154\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 173.4024 - val_loss: 176.7418\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 188.2973 - val_loss: 191.8959\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 257.9399 - val_loss: 488.8249\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 201.3995 - val_loss: 179.0217\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 191.7180 - val_loss: 247.1483\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 194.9774 - val_loss: 187.8500\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 211.6014 - val_loss: 169.6004\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 193.2393 - val_loss: 415.7491\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 178.6422 - val_loss: 172.1014\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 212.2023 - val_loss: 245.3708\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 180.8027 - val_loss: 160.7185\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 168.3274 - val_loss: 184.2427\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 206.5825 - val_loss: 190.2484\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 191.5765 - val_loss: 165.9011\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 168.9407 - val_loss: 166.9613\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 190.6472 - val_loss: 281.6479\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 228.4397 - val_loss: 212.8788\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.9925 - val_loss: 161.8761\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 175.3748 - val_loss: 172.0842\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 171.3826 - val_loss: 156.2126\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 206.4686 - val_loss: 270.1551\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 177.4273 - val_loss: 172.8215\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 204.5109 - val_loss: 163.1851\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 170.8370 - val_loss: 229.7839\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 189.9121 - val_loss: 208.6320\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 211.9694 - val_loss: 182.1898\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 198.6887 - val_loss: 291.1996\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 185.2254 - val_loss: 154.5000\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 188.8931 - val_loss: 169.5921\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 228.3780 - val_loss: 296.1039\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 241.9284 - val_loss: 163.7424\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 249.0486 - val_loss: 252.2223\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 168.0294 - val_loss: 199.3199\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 179.4895 - val_loss: 183.3373\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.4263 - val_loss: 185.8940\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 178.2497 - val_loss: 156.1326\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 167.2602 - val_loss: 163.3218\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 195.6003 - val_loss: 178.4576\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 194.7843 - val_loss: 168.6496\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 181.6642 - val_loss: 153.7700\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 169.2235 - val_loss: 167.3221\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 185.3708 - val_loss: 185.4069\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 246.4053 - val_loss: 202.1846\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 186.0343 - val_loss: 198.0213\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 210.4070 - val_loss: 304.5420\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 191.9493 - val_loss: 163.3021\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 164.4743 - val_loss: 293.5801\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 206.1286 - val_loss: 163.8776\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 177.5848 - val_loss: 211.9381\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 160.1725 - val_loss: 165.4880\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 195.5027 - val_loss: 163.2436\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 183.7520 - val_loss: 237.1222\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 186.0067 - val_loss: 197.7165\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 172.9794 - val_loss: 162.7620\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 186.0837 - val_loss: 231.0216\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 191.2358 - val_loss: 223.6778\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 219.4211 - val_loss: 176.1414\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 194.6814 - val_loss: 175.4328\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 179.9978 - val_loss: 155.8547\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 175.5162 - val_loss: 184.1499\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 184.3456 - val_loss: 162.2306\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 181.6818 - val_loss: 164.3758\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 172.2728 - val_loss: 307.3890\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 176.1961 - val_loss: 266.3137\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 202.7063 - val_loss: 553.3037\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 302.1844 - val_loss: 172.6142\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 173.1840 - val_loss: 189.3879\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.5872 - val_loss: 167.4577\n",
      "Epoch 511/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 198.7578 - val_loss: 192.4123\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.7174 - val_loss: 163.8523\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 169.2323 - val_loss: 158.4869\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 243.2781 - val_loss: 162.5664\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 178.1369 - val_loss: 308.6779\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 187.2251 - val_loss: 167.6770\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 171.0627 - val_loss: 155.6382\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 182.2967 - val_loss: 193.2447\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 199.4049 - val_loss: 161.9259\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 176.2448 - val_loss: 264.5257\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 178.6441 - val_loss: 246.6167\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 185.4140 - val_loss: 181.0436\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 166.4757 - val_loss: 188.8473\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 179.0168 - val_loss: 173.0887\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 171.7460 - val_loss: 188.5628\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 199.1028 - val_loss: 168.7435\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 210.0466 - val_loss: 175.7372\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 175.6169 - val_loss: 179.6646\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 174.4193 - val_loss: 193.5672\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 185.9726 - val_loss: 249.8911\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 167.9837 - val_loss: 168.4505\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 309.6322 - val_loss: 195.4866\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 184.6119 - val_loss: 171.0381\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 178.7282 - val_loss: 156.5915\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 174.4823 - val_loss: 503.3583\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 235.8818 - val_loss: 261.4010\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 189.5728 - val_loss: 173.0364\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 184.6787 - val_loss: 163.2195\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 182.5078 - val_loss: 158.8379\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 172.7565 - val_loss: 168.9750\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 179.0577 - val_loss: 198.5693\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 230.5741 - val_loss: 264.9808\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 194.5573 - val_loss: 173.7722\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 172.1128 - val_loss: 200.4475\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 207.7960 - val_loss: 193.6444\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 172.8335 - val_loss: 166.0460\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 189.3718 - val_loss: 182.9688\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 392.5071 - val_loss: 160.8647\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 176.2173 - val_loss: 170.9061\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 170.3439 - val_loss: 162.2923\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 171.4601 - val_loss: 153.2296\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 192.3870 - val_loss: 186.6643\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 183.6238 - val_loss: 182.5493\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.4867 - val_loss: 388.4338\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 190.5524 - val_loss: 162.2323\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.1978 - val_loss: 178.8989\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.1534 - val_loss: 174.1862\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 184.7727 - val_loss: 163.5174\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 197.1707 - val_loss: 247.2877\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 173.3610 - val_loss: 335.0866\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 173.3560 - val_loss: 178.8627\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 172.3748 - val_loss: 240.6984\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 179.8381 - val_loss: 155.0410\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 165.8329 - val_loss: 166.5717\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 447.0855 - val_loss: 262.8263\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 247.2555 - val_loss: 204.9875\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 188.4759 - val_loss: 156.8346\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 164.3353 - val_loss: 203.9931\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 171.2062 - val_loss: 158.0418\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 179.2650 - val_loss: 179.5409\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 176.2958 - val_loss: 162.8737\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 250.6179 - val_loss: 194.1965\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.3280 - val_loss: 181.1841\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 168.0419 - val_loss: 163.7069\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 164.6828 - val_loss: 156.6618\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 168.8208 - val_loss: 162.9149\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 157.3651 - val_loss: 156.4368\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 314.2868 - val_loss: 211.8979\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 179.0398 - val_loss: 182.4532\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 169.4114 - val_loss: 157.9801\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 177.5527 - val_loss: 166.9944\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.9541 - val_loss: 170.9223\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 164.2884 - val_loss: 153.2733\n",
      "Epoch 584/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 187.0675 - val_loss: 166.9737\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.4996 - val_loss: 162.9884\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.5834 - val_loss: 154.6911\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 164.3343 - val_loss: 198.0332\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 168.4636 - val_loss: 159.0792\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 223.6023 - val_loss: 423.1310\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 194.6413 - val_loss: 156.7971\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.4374 - val_loss: 173.2523\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 159.6609 - val_loss: 148.9716\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 165.3512 - val_loss: 169.1673\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 171.3644 - val_loss: 152.3149\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.6570 - val_loss: 153.8881\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 188.3953 - val_loss: 149.1543\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.5250 - val_loss: 161.7919\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 195.8084 - val_loss: 170.9865\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 185.4923 - val_loss: 160.5825\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 178.7463 - val_loss: 157.4299\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 165.0428 - val_loss: 151.1920\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 245.7196 - val_loss: 160.9649\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 167.2003 - val_loss: 154.6349\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 165.7193 - val_loss: 157.0124\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 168.0015 - val_loss: 155.3293\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 164.4882 - val_loss: 173.9898\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 161.3514 - val_loss: 365.3526\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 162.5621 - val_loss: 179.6202\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 170.0087 - val_loss: 194.1645\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 157.9304 - val_loss: 161.3476\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 210.7066 - val_loss: 171.2209\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 181.7808 - val_loss: 254.5560\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 210.7010 - val_loss: 192.1020\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 177.0862 - val_loss: 160.4785\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 215.3995 - val_loss: 171.0499\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 155.7868 - val_loss: 155.7701\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.5353 - val_loss: 172.2982\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.2190 - val_loss: 190.2688\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 168.8442 - val_loss: 193.2348\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.3951 - val_loss: 152.3195\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 192.8075 - val_loss: 217.2624\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 184.1715 - val_loss: 346.4479\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 177.8418 - val_loss: 157.6386\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 223.6127 - val_loss: 156.8383\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.7826 - val_loss: 240.8884\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.9654 - val_loss: 195.2515\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 160.3848 - val_loss: 264.3030\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.9625 - val_loss: 194.5713\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 244.5194 - val_loss: 201.0498\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 156.1146 - val_loss: 210.3643\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 232.7560 - val_loss: 239.3012\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.1775 - val_loss: 176.8468\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 161.1247 - val_loss: 196.0512\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 185.9053 - val_loss: 160.9284\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 237.1697 - val_loss: 303.4021\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.5187 - val_loss: 192.6344\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.6957 - val_loss: 224.1434\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 165.4403 - val_loss: 174.9047\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 160.6896 - val_loss: 165.4958\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.7567 - val_loss: 154.5821\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 189.5504 - val_loss: 155.3139\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 160.0644 - val_loss: 167.6494\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 220.3555 - val_loss: 152.3105\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 150.0104 - val_loss: 180.0111\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 159.4646 - val_loss: 160.0834\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 210.2632 - val_loss: 152.6611\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 164.4027 - val_loss: 475.5735\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.7170 - val_loss: 155.4953\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 179.5874 - val_loss: 642.6054\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 199.8524 - val_loss: 164.7158\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.9700 - val_loss: 153.8298\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 162.5738 - val_loss: 204.7997\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 162.8952 - val_loss: 179.2774\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 176.2965 - val_loss: 152.5481\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 155.0519 - val_loss: 155.3405\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.1829 - val_loss: 189.0071\n",
      "Epoch 657/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 55us/step - loss: 263.5421 - val_loss: 198.1430\n",
      "Epoch 658/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.7321 - val_loss: 154.4872\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 169.6435 - val_loss: 168.1144\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.2153 - val_loss: 153.2468\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 159.9566 - val_loss: 153.1263\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.0122 - val_loss: 245.9407\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 210.7789 - val_loss: 823.2889\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 242.7163 - val_loss: 154.8929\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 155.9374 - val_loss: 151.7250\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.6115 - val_loss: 150.6870\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 173.4957 - val_loss: 154.3263\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.1482 - val_loss: 168.8115\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 153.9393 - val_loss: 153.6854\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.9415 - val_loss: 217.8888\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 173.3624 - val_loss: 156.4361\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 176.3386 - val_loss: 164.2141\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 165.2874 - val_loss: 159.1654\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 159.3164 - val_loss: 163.7653\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 170.0723 - val_loss: 238.4529\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 245.0525 - val_loss: 243.6795\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 199.5188 - val_loss: 199.2849\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.7844 - val_loss: 150.1800\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 168.1098 - val_loss: 149.9250\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.2166 - val_loss: 261.2657\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 189.2299 - val_loss: 169.6160\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 162.7793 - val_loss: 153.4477\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 168.8830 - val_loss: 201.2782\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 174.5137 - val_loss: 176.7077\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.6229 - val_loss: 166.6135\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.7128 - val_loss: 175.2967\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.0703 - val_loss: 189.1701\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.7606 - val_loss: 149.5239\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 201.2119 - val_loss: 253.2363\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 169.4352 - val_loss: 264.6185\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 155.1403 - val_loss: 288.7693\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 158.4482 - val_loss: 183.8207\n",
      "Epoch 00692: early stopping\n",
      "Fold score (RMSE): 12.90379524230957\n",
      "Fold #2\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 5113.2991 - val_loss: 5030.7001\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 4646.2252 - val_loss: 4804.4526\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 4537.5356 - val_loss: 4836.4386\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 4242.2041 - val_loss: 4516.8235\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 4147.4921 - val_loss: 4606.4092\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 3993.0477 - val_loss: 4329.8254\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 3977.0432 - val_loss: 4211.3176\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 3850.7505 - val_loss: 4547.7834\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 3799.0374 - val_loss: 4501.0281\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 3774.6358 - val_loss: 4145.6733\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 3597.7774 - val_loss: 3507.4098\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 3486.3549 - val_loss: 3627.4754\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 3337.1113 - val_loss: 3198.2437\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 3034.7217 - val_loss: 2990.2389\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 3041.6953 - val_loss: 2762.0370\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 2846.2848 - val_loss: 2546.9293\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 2674.4323 - val_loss: 2354.9171\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 2178.2339 - val_loss: 2006.6190\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 2290.6361 - val_loss: 2346.5984\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 2003.4570 - val_loss: 1623.3512\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1726.0059 - val_loss: 1833.0950\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 1583.0532 - val_loss: 1249.1513\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 1380.5279 - val_loss: 1129.0240\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 1350.6582 - val_loss: 1051.8128\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 1198.9462 - val_loss: 1067.8126\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 1311.3210 - val_loss: 882.5621\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 1088.6767 - val_loss: 950.9002\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 988.8545 - val_loss: 797.3045\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 945.7792 - val_loss: 722.6143\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 892.0499 - val_loss: 607.7675\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 720.1164 - val_loss: 535.8057\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 748.0351 - val_loss: 500.5952\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 843.0593 - val_loss: 530.6643\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 705.3351 - val_loss: 491.6279\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 671.5096 - val_loss: 644.1298\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 692.9989 - val_loss: 1131.4961\n",
      "Epoch 37/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 55us/step - loss: 639.1530 - val_loss: 467.3266\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 672.6253 - val_loss: 579.2982\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 565.9465 - val_loss: 452.3116\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 625.0120 - val_loss: 434.1672\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 562.9003 - val_loss: 420.6458\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 615.8435 - val_loss: 457.6825\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 622.3603 - val_loss: 1015.6860\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 608.4495 - val_loss: 1133.2287\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 619.2786 - val_loss: 696.4123\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 593.0878 - val_loss: 424.5651\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 500.1758 - val_loss: 437.9411\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 537.7079 - val_loss: 578.1271\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 556.1008 - val_loss: 376.5700\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 598.2839 - val_loss: 682.1535\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 535.7042 - val_loss: 398.8948\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 506.2785 - val_loss: 492.5068\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 541.5551 - val_loss: 491.1321\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 487.5277 - val_loss: 396.6356\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 534.0002 - val_loss: 479.8696\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 426.7432 - val_loss: 363.9891\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 464.6126 - val_loss: 401.2443\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 462.8880 - val_loss: 422.6591\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 492.8908 - val_loss: 376.2439\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 476.6632 - val_loss: 475.9199\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 431.6603 - val_loss: 381.0559\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 488.1482 - val_loss: 589.0154\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 424.9869 - val_loss: 459.8645\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 482.6417 - val_loss: 364.7764\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 448.9909 - val_loss: 793.6896\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 435.5501 - val_loss: 391.7297\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 529.2992 - val_loss: 322.7449\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 373.5706 - val_loss: 307.6106\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 382.1856 - val_loss: 344.0699\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 392.5158 - val_loss: 617.0137\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 460.5202 - val_loss: 325.8899\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 488.0808 - val_loss: 599.2505\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 391.3977 - val_loss: 437.1081\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 355.2538 - val_loss: 303.8745\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 397.9860 - val_loss: 314.5575\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 371.1728 - val_loss: 351.6032\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 504.2789 - val_loss: 293.9760\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 361.6634 - val_loss: 295.4300\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 405.7103 - val_loss: 338.9931\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 327.8893 - val_loss: 483.3376\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 389.8908 - val_loss: 401.1726\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 385.5760 - val_loss: 277.4922\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 397.2678 - val_loss: 321.7885\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 431.8240 - val_loss: 293.7276\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 441.5286 - val_loss: 284.9938\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 356.0057 - val_loss: 628.6042\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 376.6440 - val_loss: 294.8161\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 443.8286 - val_loss: 278.7269\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 336.2884 - val_loss: 821.2103\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 391.9708 - val_loss: 270.5532\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 333.2599 - val_loss: 285.5462\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 343.0880 - val_loss: 313.6925\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 381.2801 - val_loss: 293.1915\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 331.7951 - val_loss: 947.2237\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 323.4265 - val_loss: 280.1354\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 318.7260 - val_loss: 842.2527\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 439.1232 - val_loss: 284.3683\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 388.4635 - val_loss: 323.4433\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 363.9660 - val_loss: 906.8266\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 341.8016 - val_loss: 318.0274\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 390.1644 - val_loss: 349.2173\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 353.1148 - val_loss: 261.3849\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 309.9219 - val_loss: 267.1964\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 305.2906 - val_loss: 301.5426\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 301.0543 - val_loss: 329.7083\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 315.0447 - val_loss: 281.3153\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 342.4198 - val_loss: 252.9239\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 326.9697 - val_loss: 307.9992\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 343.0191 - val_loss: 259.0419\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 308.6854 - val_loss: 549.2544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 376.2172 - val_loss: 384.6584\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 287.3201 - val_loss: 378.3219\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 298.5592 - val_loss: 723.6209\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 376.3027 - val_loss: 271.8724\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 296.8859 - val_loss: 246.7394\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 293.9923 - val_loss: 515.5186\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 395.8083 - val_loss: 304.1520\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 274.5014 - val_loss: 259.3057\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 275.4803 - val_loss: 304.0764\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 315.3175 - val_loss: 354.3127\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 311.6425 - val_loss: 247.1090\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 350.0964 - val_loss: 268.8225\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 314.3879 - val_loss: 368.9939\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 310.9821 - val_loss: 245.6068\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 319.2565 - val_loss: 422.0620\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 283.8020 - val_loss: 306.4244\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 283.9675 - val_loss: 245.9803\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 282.8246 - val_loss: 245.5381\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 288.9821 - val_loss: 353.6475\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 356.4722 - val_loss: 279.5008\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 286.4524 - val_loss: 234.2096\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 290.5809 - val_loss: 242.4330\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 304.8059 - val_loss: 348.6855\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 316.9210 - val_loss: 266.9103\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 254.4868 - val_loss: 699.9862\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 336.7135 - val_loss: 275.7726\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 282.8286 - val_loss: 301.2325\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 297.4236 - val_loss: 240.7670\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 265.2623 - val_loss: 249.7063\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 282.1555 - val_loss: 457.4173\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 357.0389 - val_loss: 269.1027\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 274.8491 - val_loss: 243.5158\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 259.4102 - val_loss: 265.5572\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 268.5512 - val_loss: 226.2820\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 329.8060 - val_loss: 313.5618\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 405.7710 - val_loss: 505.2468\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 289.5277 - val_loss: 416.4169\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 283.9306 - val_loss: 277.9379\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 264.8652 - val_loss: 220.4197\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 267.1196 - val_loss: 228.3145\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 309.3215 - val_loss: 283.5985\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 333.4601 - val_loss: 271.7273\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 279.7627 - val_loss: 274.5172\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 257.5717 - val_loss: 300.9608\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 314.2500 - val_loss: 380.4648\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 256.7544 - val_loss: 286.8115\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 308.6760 - val_loss: 258.1725\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 263.5587 - val_loss: 297.9152\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 303.3742 - val_loss: 400.4985\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 268.6424 - val_loss: 265.6693\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 249.8468 - val_loss: 230.3664\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 289.1715 - val_loss: 224.4332\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 265.1458 - val_loss: 282.5193\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 279.7738 - val_loss: 838.4522\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 253.7133 - val_loss: 387.8907\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 289.4944 - val_loss: 309.3019\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 246.4592 - val_loss: 409.9071\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 266.9025 - val_loss: 272.2257\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 249.3580 - val_loss: 498.9552\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 257.7182 - val_loss: 442.5236\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 324.9245 - val_loss: 399.8760\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 281.8691 - val_loss: 246.6718\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 262.7752 - val_loss: 275.6968\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 338.8171 - val_loss: 253.6608\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 272.2710 - val_loss: 247.0493\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 280.0109 - val_loss: 231.8526\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 285.4692 - val_loss: 224.8915\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 273.2606 - val_loss: 278.0326\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 258.9337 - val_loss: 269.8910\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 257.6225 - val_loss: 288.9675\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 224.8643 - val_loss: 241.0421\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 231.8539 - val_loss: 217.8143\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 250.0909 - val_loss: 232.6629\n",
      "Epoch 184/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 253.5382 - val_loss: 408.0337\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 278.2610 - val_loss: 282.5854\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 273.2298 - val_loss: 227.6192\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 276.4243 - val_loss: 228.3135\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 236.7369 - val_loss: 220.5367\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 320.3091 - val_loss: 234.9492\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 259.6817 - val_loss: 220.2104\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 244.4428 - val_loss: 237.8504\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 271.5388 - val_loss: 245.9422\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 234.3894 - val_loss: 245.5417\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 260.2153 - val_loss: 236.8199\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 331.8933 - val_loss: 238.5727\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 249.1821 - val_loss: 253.0706\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 247.1078 - val_loss: 371.3112\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 280.1255 - val_loss: 392.8107\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 253.3710 - val_loss: 484.8256\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 257.4805 - val_loss: 402.3680\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 236.0203 - val_loss: 243.1131\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 310.1960 - val_loss: 283.6250\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 259.2081 - val_loss: 205.3521\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 226.9092 - val_loss: 218.7543\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 260.3439 - val_loss: 312.6498\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 277.9071 - val_loss: 345.8465\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 278.4995 - val_loss: 270.7708\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 226.6919 - val_loss: 223.3661\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 213.1868 - val_loss: 231.3346\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 271.8962 - val_loss: 202.6004\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 226.6427 - val_loss: 372.5799\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 252.8038 - val_loss: 219.7163\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 230.8501 - val_loss: 219.1565\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 256.0191 - val_loss: 250.6325\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 243.9290 - val_loss: 324.2237\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 279.0204 - val_loss: 212.2842\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 237.0580 - val_loss: 233.7032\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 261.5414 - val_loss: 271.7385\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 236.5415 - val_loss: 216.7284\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 261.8378 - val_loss: 234.3972\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 242.8403 - val_loss: 264.2120\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 280.6772 - val_loss: 292.2366\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 230.5642 - val_loss: 198.4229\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 240.9225 - val_loss: 199.3151\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 232.6181 - val_loss: 205.5651\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 252.2579 - val_loss: 217.9881\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 234.6612 - val_loss: 364.5797\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 239.6545 - val_loss: 202.1200\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 228.7993 - val_loss: 217.3524\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 227.7407 - val_loss: 241.6681\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 255.1085 - val_loss: 230.1755\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 246.6258 - val_loss: 271.2493\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 223.9953 - val_loss: 255.3460\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 215.2028 - val_loss: 501.2073\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 222.0794 - val_loss: 342.0693\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 239.0782 - val_loss: 211.7942\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 289.6093 - val_loss: 293.4644\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 227.2649 - val_loss: 321.1958\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 234.9197 - val_loss: 217.0585\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 305.5032 - val_loss: 364.2334\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 254.1058 - val_loss: 211.0285\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 213.2317 - val_loss: 296.1770\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 223.5369 - val_loss: 275.1924\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 300.2742 - val_loss: 218.8766\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 212.3152 - val_loss: 199.0119\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 229.2917 - val_loss: 570.8723\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 228.9528 - val_loss: 352.8946\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 256.1250 - val_loss: 208.4261\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 228.4828 - val_loss: 266.1280\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 264.3314 - val_loss: 200.6408\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 213.0174 - val_loss: 260.8409\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 235.6441 - val_loss: 197.6844\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 255.1337 - val_loss: 278.9243\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 221.6366 - val_loss: 236.1699\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 227.3989 - val_loss: 300.8432\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 242.1672 - val_loss: 209.5540\n",
      "Epoch 257/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 272.6449 - val_loss: 204.4891\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 215.5849 - val_loss: 214.3077\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 226.4479 - val_loss: 191.5425\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 238.5146 - val_loss: 255.5872\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 237.0043 - val_loss: 191.7207\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 211.9546 - val_loss: 198.3355\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 230.0069 - val_loss: 208.8580\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 248.3023 - val_loss: 204.5367\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 234.5563 - val_loss: 210.3413\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 263.6496 - val_loss: 321.8028\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 217.6479 - val_loss: 272.7993\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 219.8286 - val_loss: 215.7011\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 229.6948 - val_loss: 200.0262\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 244.2176 - val_loss: 323.2706\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 248.5292 - val_loss: 191.6482\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 230.0775 - val_loss: 308.1372\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 253.0264 - val_loss: 523.5344\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 220.7936 - val_loss: 240.1424\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 206.2703 - val_loss: 191.4541\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 244.3974 - val_loss: 257.5454\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 211.1335 - val_loss: 420.4504\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 223.3152 - val_loss: 201.2389\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 340.4507 - val_loss: 340.9356\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 228.1977 - val_loss: 218.6560\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 223.7766 - val_loss: 204.5183\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 206.8834 - val_loss: 207.1501\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 217.2265 - val_loss: 199.6615\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 251.9629 - val_loss: 246.4420\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 226.7669 - val_loss: 215.4933\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 197.5104 - val_loss: 332.2762\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 225.8366 - val_loss: 238.8879\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 219.2245 - val_loss: 209.4710\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 302.7014 - val_loss: 199.1693\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 266.1567 - val_loss: 186.9241\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 219.5536 - val_loss: 191.0022\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 191.3389 - val_loss: 196.0438\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 198.8479 - val_loss: 224.6458\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 225.1567 - val_loss: 213.8771\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 226.6926 - val_loss: 205.2002\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 213.5277 - val_loss: 210.8943\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 233.1774 - val_loss: 180.5749\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 205.0323 - val_loss: 183.3153\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 238.9717 - val_loss: 199.4414\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 238.3928 - val_loss: 232.2468\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 216.7505 - val_loss: 201.5226\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 214.8249 - val_loss: 284.2660\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 210.2222 - val_loss: 287.0143\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 218.3393 - val_loss: 384.1510\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 220.1074 - val_loss: 181.1619\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 191.7071 - val_loss: 232.9068\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 198.6004 - val_loss: 186.5682\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 199.6445 - val_loss: 267.4831\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 215.6371 - val_loss: 221.3738\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 254.9271 - val_loss: 248.9654\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 260.4687 - val_loss: 335.1289\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 198.8451 - val_loss: 213.3898\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 221.3486 - val_loss: 286.9205\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 270.7823 - val_loss: 207.6037\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 232.6151 - val_loss: 329.3167\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 202.1273 - val_loss: 182.8461\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 205.2127 - val_loss: 417.9819\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 239.5438 - val_loss: 224.2212\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 222.2267 - val_loss: 311.6384\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 209.7560 - val_loss: 191.4512\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 233.8794 - val_loss: 243.0342\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 219.7875 - val_loss: 227.2706\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 252.7083 - val_loss: 245.0170\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 231.7178 - val_loss: 237.0879\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 234.8136 - val_loss: 303.8507\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 181.506 - 0s 57us/step - loss: 180.5658 - val_loss: 180.8648\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 197.3739 - val_loss: 214.9065\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 211.0380 - val_loss: 368.4214\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 224.6750 - val_loss: 326.8724\n",
      "Epoch 330/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 218.2926 - val_loss: 226.6255\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 219.2079 - val_loss: 194.2979\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 239.2312 - val_loss: 245.4986\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 225.9116 - val_loss: 275.5523\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 231.1828 - val_loss: 240.1334\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 240.2768 - val_loss: 209.0450\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 226.4918 - val_loss: 250.9124\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 248.8531 - val_loss: 188.9410\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 189.1142 - val_loss: 206.8520\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 210.0842 - val_loss: 208.8792\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 185.3145 - val_loss: 205.2108\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 197.7723 - val_loss: 207.5114\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 198.5915 - val_loss: 193.4753\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 251.6533 - val_loss: 187.9869\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 199.4012 - val_loss: 188.8436\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 244.9204 - val_loss: 192.1271\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 191.5394 - val_loss: 191.6917\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 201.1298 - val_loss: 213.7716\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 212.2604 - val_loss: 256.1283\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 236.2901 - val_loss: 205.6951\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 192.4520 - val_loss: 208.3025\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 220.9255 - val_loss: 285.0274\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 207.0754 - val_loss: 179.9945\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 207.1871 - val_loss: 219.4334\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 202.0240 - val_loss: 190.6551\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 232.9321 - val_loss: 196.1182\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 204.1062 - val_loss: 177.8316\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 216.0414 - val_loss: 207.7418\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 208.0823 - val_loss: 177.1319\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 208.8695 - val_loss: 186.2056\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 208.1869 - val_loss: 258.1487\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 234.4685 - val_loss: 213.4164\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.0266 - val_loss: 245.6475\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 195.5888 - val_loss: 225.7208\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 222.6546 - val_loss: 187.9549\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 205.8208 - val_loss: 224.1545\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 193.1978 - val_loss: 232.2682\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 181.5650 - val_loss: 549.1210\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 203.4394 - val_loss: 224.1445\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 187.7882 - val_loss: 196.2833\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 212.8123 - val_loss: 195.8060\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 201.7899 - val_loss: 196.3462\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 197.8163 - val_loss: 238.7003\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 196.3053 - val_loss: 177.2791\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 222.3213 - val_loss: 176.5713\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 219.3626 - val_loss: 185.3171\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 199.7374 - val_loss: 339.4056\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 191.6059 - val_loss: 190.9773\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 199.2522 - val_loss: 289.6806\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 200.3317 - val_loss: 231.7878\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 196.7468 - val_loss: 180.2399\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 204.8528 - val_loss: 200.2290\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 232.4866 - val_loss: 442.7004\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 203.8342 - val_loss: 183.5355\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 222.2778 - val_loss: 186.7848\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 195.2776 - val_loss: 187.8401\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 210.2249 - val_loss: 254.3411\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 216.2309 - val_loss: 180.1829\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 195.4623 - val_loss: 221.8480\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 188.7272 - val_loss: 238.5279\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 201.6503 - val_loss: 507.7585\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 184.3140 - val_loss: 191.3148\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 237.1708 - val_loss: 185.2019\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 203.4002 - val_loss: 226.3680\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 210.2033 - val_loss: 241.6529\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 178.6225 - val_loss: 184.8702\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 203.3193 - val_loss: 194.6432\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 191.8782 - val_loss: 543.6526\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 198.6857 - val_loss: 177.7959\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 250.4095 - val_loss: 255.6355\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.7276 - val_loss: 217.8639\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 204.2124 - val_loss: 188.4334\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 203.5116 - val_loss: 185.9684\n",
      "Epoch 403/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 196.9828 - val_loss: 214.6124\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 211.5487 - val_loss: 218.9691\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 197.8334 - val_loss: 208.7081\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 200.2758 - val_loss: 217.8769\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 184.2838 - val_loss: 175.3165\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 191.8864 - val_loss: 177.5871\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 206.8814 - val_loss: 283.4382\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 196.8880 - val_loss: 240.6018\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 177.6893 - val_loss: 186.2683\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 195.4847 - val_loss: 373.5797\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 196.3714 - val_loss: 184.9190\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 209.1510 - val_loss: 181.5402\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 184.5927 - val_loss: 196.0553\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 196.9138 - val_loss: 185.6306\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 174.6960 - val_loss: 211.6740\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 176.5128 - val_loss: 184.4502\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 196.4425 - val_loss: 644.9559\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 259.9388 - val_loss: 217.3034\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 180.0238 - val_loss: 185.7559\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 183.4854 - val_loss: 222.3804\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 185.6043 - val_loss: 181.2768\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 186.1722 - val_loss: 174.4399\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 207.6025 - val_loss: 183.3401\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 221.4955 - val_loss: 265.6038\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 215.5935 - val_loss: 201.0787\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 225.1236 - val_loss: 206.5341\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 169.7650 - val_loss: 299.6153\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 189.5403 - val_loss: 209.9318\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 205.5342 - val_loss: 174.7164\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 187.6060 - val_loss: 618.5963\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 234.0961 - val_loss: 174.8423\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 196.3827 - val_loss: 177.4793\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 189.3378 - val_loss: 419.0286\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 241.4674 - val_loss: 176.1026\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 183.7437 - val_loss: 188.7835\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 174.4936 - val_loss: 184.3030\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 200.0640 - val_loss: 181.4891\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 207.3031 - val_loss: 173.3557\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 187.2424 - val_loss: 338.4114\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 201.5574 - val_loss: 166.9298\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 188.7755 - val_loss: 207.9748\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 251.4883 - val_loss: 166.1171\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 165.9444 - val_loss: 205.4890\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 174.8946 - val_loss: 166.9998\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 174.0197 - val_loss: 167.5921\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 195.9660 - val_loss: 256.8455\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 225.1092 - val_loss: 202.5092\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 201.0540 - val_loss: 204.2729\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 180.5987 - val_loss: 170.6894\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 191.5237 - val_loss: 216.0841\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 189.7177 - val_loss: 166.2271\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 171.5806 - val_loss: 192.6962\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 165.0570 - val_loss: 172.1500\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 184.2892 - val_loss: 196.8363\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 178.7825 - val_loss: 206.2567\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 196.9057 - val_loss: 181.5958\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 241.0858 - val_loss: 174.5462\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 169.4204 - val_loss: 216.9329\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 217.5185 - val_loss: 172.3818\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 181.5441 - val_loss: 172.0472\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 168.8073 - val_loss: 205.3719\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 185.4074 - val_loss: 174.8249\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 189.3019 - val_loss: 186.1122\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 176.7817 - val_loss: 282.7207\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 223.5045 - val_loss: 203.4044\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 218.1450 - val_loss: 405.5605\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 188.3541 - val_loss: 167.5373\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 185.2081 - val_loss: 175.6944\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 194.6540 - val_loss: 172.6644\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 174.3811 - val_loss: 200.0455\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 282.6060 - val_loss: 173.0084\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 180.0152 - val_loss: 171.0368\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 237.0960 - val_loss: 250.2370\n",
      "Epoch 476/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 55us/step - loss: 192.5547 - val_loss: 198.9354\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 321.1726 - val_loss: 172.5170\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 170.2719 - val_loss: 169.7718\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 179.3199 - val_loss: 173.1252\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 179.0072 - val_loss: 182.7882\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 203.8601 - val_loss: 192.0631\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 188.9975 - val_loss: 168.1413\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 170.9413 - val_loss: 215.8539\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 192.6350 - val_loss: 196.3061\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 205.4880 - val_loss: 203.1111\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 204.9953 - val_loss: 195.9920\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 177.5756 - val_loss: 198.6765\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 197.2961 - val_loss: 178.8398\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 172.6434 - val_loss: 163.3013\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 164.4239 - val_loss: 235.5684\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 193.8690 - val_loss: 188.0310\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 195.5868 - val_loss: 169.0315\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 184.6105 - val_loss: 169.2259\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.5764 - val_loss: 199.9907\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 167.7916 - val_loss: 200.1451\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.6243 - val_loss: 174.5366\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 208.2734 - val_loss: 223.8593\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 177.7770 - val_loss: 247.0784\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 174.5342 - val_loss: 169.4050\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 176.3909 - val_loss: 384.3254\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 193.0457 - val_loss: 264.3434\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 179.2185 - val_loss: 172.0488\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 196.0901 - val_loss: 238.8736\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 343.5481 - val_loss: 174.4178\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 169.7599 - val_loss: 168.1264\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 174.1174 - val_loss: 162.7489\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 172.5296 - val_loss: 184.4138\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 205.3972 - val_loss: 173.7453\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.3746 - val_loss: 188.5139\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 177.3592 - val_loss: 473.6950\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 223.9107 - val_loss: 181.5651\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 172.8573 - val_loss: 186.8587\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 178.9376 - val_loss: 171.8595\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 211.5032 - val_loss: 174.2624\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 202.6879 - val_loss: 224.2035\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 173.2132 - val_loss: 166.0112\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 172.1461 - val_loss: 217.0356\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 179.1361 - val_loss: 191.7366\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 186.5377 - val_loss: 334.1589\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 168.3155 - val_loss: 182.3829\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 186.1364 - val_loss: 183.9041\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 172.4799 - val_loss: 178.3882\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 189.1669 - val_loss: 338.1538\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 421.2017 - val_loss: 184.8303\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 201.3141 - val_loss: 164.1213\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 163.0703 - val_loss: 192.0915\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 167.3526 - val_loss: 180.9733\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 167.4309 - val_loss: 170.2096\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.9452 - val_loss: 212.1254\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.7468 - val_loss: 173.3395\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.5365 - val_loss: 220.0211\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 212.5944 - val_loss: 591.1772\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 193.2395 - val_loss: 190.9784\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 193.2033 - val_loss: 233.3573\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 209.9774 - val_loss: 235.6333\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 168.0301 - val_loss: 176.2131\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 165.8602 - val_loss: 225.0741\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 251.1377 - val_loss: 170.0878\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 179.2680 - val_loss: 207.5516\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.4393 - val_loss: 168.4149\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 203.6116 - val_loss: 197.6303\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 183.8109 - val_loss: 167.9781\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.8262 - val_loss: 179.3878\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 176.0577 - val_loss: 218.0193\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 418.6441 - val_loss: 240.2440\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 235.5996 - val_loss: 237.5413\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 189.0030 - val_loss: 187.9878\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 162.3387 - val_loss: 161.7178\n",
      "Epoch 549/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.9960 - val_loss: 250.2346\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 179.8378 - val_loss: 172.3825\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 171.4253 - val_loss: 299.3378\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 192.4546 - val_loss: 192.7131\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 283.5285 - val_loss: 269.3007\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 165.5485 - val_loss: 169.7191\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 169.6773 - val_loss: 190.4981\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 159.3413 - val_loss: 162.7338\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 162.7658 - val_loss: 258.2960\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 173.3347 - val_loss: 164.6564\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.1047 - val_loss: 165.0996\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 195.4973 - val_loss: 226.4039\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 187.9977 - val_loss: 191.5157\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 174.9380 - val_loss: 197.8641\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 172.1941 - val_loss: 197.2397\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 435.0605 - val_loss: 399.5875\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 282.6763 - val_loss: 221.4295\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 180.7890 - val_loss: 187.0385\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 170.5646 - val_loss: 164.9859\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 183.9520 - val_loss: 179.0416\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 156.6717 - val_loss: 190.4406\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 205.7515 - val_loss: 166.4003\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 173.5299 - val_loss: 163.4148\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 188.2609 - val_loss: 163.0311\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 201.6258 - val_loss: 273.2913\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 168.7550 - val_loss: 203.6439\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 165.0150 - val_loss: 173.3549\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 188.6606 - val_loss: 175.3436\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 165.5056 - val_loss: 189.5204\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 168.7689 - val_loss: 275.9287\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 172.4512 - val_loss: 189.4403\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 208.9728 - val_loss: 444.2442\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 194.7447 - val_loss: 207.5984\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 180.9936 - val_loss: 231.3347\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 202.7625 - val_loss: 188.9375\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 206.9107 - val_loss: 169.4514\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.6057 - val_loss: 161.7009\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 180.9440 - val_loss: 183.0925\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 188.9998 - val_loss: 164.5747\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 170.9373 - val_loss: 217.6850\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 200.6433 - val_loss: 294.0971\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 195.6968 - val_loss: 173.0357\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 334.1758 - val_loss: 642.5407\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 247.9820 - val_loss: 286.5159\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 232.7745 - val_loss: 231.6653\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 202.0282 - val_loss: 312.2181\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 175.7132 - val_loss: 166.0439\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 163.1217 - val_loss: 215.1883\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 186.5819 - val_loss: 185.8405\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 169.5075 - val_loss: 160.7109\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 209.9900 - val_loss: 211.6019\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 166.6671 - val_loss: 254.7890\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 202.8618 - val_loss: 190.5623\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 187.8807 - val_loss: 285.5366\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.8767 - val_loss: 165.8581\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 360.9622 - val_loss: 212.1932\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 173.8303 - val_loss: 176.6614\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 164.5358 - val_loss: 212.2205\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 183.3866 - val_loss: 190.7969\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 168.1513 - val_loss: 183.9598\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 204.3098 - val_loss: 192.0755\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 162.9803 - val_loss: 159.3564\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 159.6996 - val_loss: 268.3523\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 207.5291 - val_loss: 237.0561\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 171.7472 - val_loss: 162.9180\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 213.6610 - val_loss: 164.6341\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 171.0465 - val_loss: 205.4134\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.2610 - val_loss: 162.9363\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 194.9927 - val_loss: 213.9572\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 178.3250 - val_loss: 160.8709\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 165.1769 - val_loss: 176.8157\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 169.4715 - val_loss: 190.5687\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 165.4429 - val_loss: 172.7762\n",
      "Epoch 622/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 185.4536 - val_loss: 180.4371\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 167.6222 - val_loss: 267.9542\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 315.5324 - val_loss: 220.9932\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 154.4305 - val_loss: 164.5652\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 167.1875 - val_loss: 177.8337\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 158.9212 - val_loss: 211.7529\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 217.7220 - val_loss: 168.9799\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 175.7554 - val_loss: 167.2476\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.3162 - val_loss: 171.1337\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 162.2097 - val_loss: 177.6595\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 167.4303 - val_loss: 192.7985\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.4235 - val_loss: 223.7895\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 182.3586 - val_loss: 188.4633\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 162.1761 - val_loss: 222.3732\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 369.0608 - val_loss: 219.3782\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 166.0384 - val_loss: 197.5980\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 167.0362 - val_loss: 162.7998\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 165.9172 - val_loss: 164.5669\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.8729 - val_loss: 162.6115\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 173.5983 - val_loss: 177.8497\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 158.8779 - val_loss: 188.9088\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 186.4692 - val_loss: 175.6424\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 172.9784 - val_loss: 161.5021\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 186.3940 - val_loss: 158.9767\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 183.4065 - val_loss: 227.4391\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.1431 - val_loss: 191.0535\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 150.3568 - val_loss: 161.9437\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 281.3542 - val_loss: 248.6437\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.2000 - val_loss: 164.1406\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.3753 - val_loss: 169.0775\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 166.5274 - val_loss: 205.9145\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 253.8985 - val_loss: 227.4834\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 169.6319 - val_loss: 251.8744\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 166.823 - 1s 64us/step - loss: 163.4403 - val_loss: 160.1946\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 161.8168 - val_loss: 178.8513\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 181.8245 - val_loss: 414.8049\n",
      "Epoch 658/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 169.6215 - val_loss: 312.3952\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 171.8625 - val_loss: 180.3977\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 164.7691 - val_loss: 191.7129\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 198.6970 - val_loss: 260.2957\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 183.8922 - val_loss: 157.7284\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 206.0295 - val_loss: 230.9863\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 167.7297 - val_loss: 154.1671\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 164.1701 - val_loss: 258.2079\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 156.1801 - val_loss: 257.9714\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 176.8324 - val_loss: 173.7363\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.0060 - val_loss: 216.4483\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 245.0124 - val_loss: 349.4438\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 252.9002 - val_loss: 157.1394\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.5607 - val_loss: 159.0287\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 160.4215 - val_loss: 168.1768\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 173.1136 - val_loss: 219.1192\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 172.6961 - val_loss: 178.4418\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 168.1816 - val_loss: 160.4501\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 174.3968 - val_loss: 304.0370\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 205.8202 - val_loss: 188.4229\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 188.2996 - val_loss: 222.3989\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 160.7514 - val_loss: 156.1957\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.9863 - val_loss: 179.5676\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 181.7008 - val_loss: 196.7602\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 201.9363 - val_loss: 165.2385\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 232.1833 - val_loss: 185.8829\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 169.3647 - val_loss: 166.7673\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 186.5165 - val_loss: 214.8308\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 172.2674 - val_loss: 236.9410\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.2537 - val_loss: 331.8853\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 198.8999 - val_loss: 161.4920\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 184.4667 - val_loss: 205.8734\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 201.3232 - val_loss: 239.7329\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 165.3032 - val_loss: 180.8047\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 184.9556 - val_loss: 169.0917\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 155.1614 - val_loss: 221.3584\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 152.3178 - val_loss: 158.7825\n",
      "Epoch 695/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 172.6662 - val_loss: 207.3698\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 157.2455 - val_loss: 163.3132\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 293.0223 - val_loss: 240.3844\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.7773 - val_loss: 164.0393\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 154.3539 - val_loss: 181.0124\n",
      "Epoch 700/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 179.1508 - val_loss: 164.2606\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 159.5731 - val_loss: 181.9548\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 168.2317 - val_loss: 176.8116\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.0563 - val_loss: 185.8753\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 176.6508 - val_loss: 189.3484\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 221.3118 - val_loss: 454.4557\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 245.4145 - val_loss: 174.6062\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 166.8315 - val_loss: 154.0038\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 153.2186 - val_loss: 157.9268\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 164.7676 - val_loss: 195.7122\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 158.1911 - val_loss: 158.7050\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 279.1362 - val_loss: 204.8765\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 181.8449 - val_loss: 171.3501\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 273.6350 - val_loss: 176.3220\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 163.5563 - val_loss: 164.9177\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 155.2853 - val_loss: 380.4797\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 157.8199 - val_loss: 160.8152\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.9607 - val_loss: 164.8330\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 148.6093 - val_loss: 162.9225\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.9480 - val_loss: 214.1333\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 175.0029 - val_loss: 162.8097\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 269.6355 - val_loss: 201.6823\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 188.7094 - val_loss: 159.5438\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 157.8183 - val_loss: 215.1858\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 216.4235 - val_loss: 159.0026\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 166.9494 - val_loss: 193.7905\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 167.6472 - val_loss: 175.1741\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 289.4363 - val_loss: 177.6229\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 182.3297 - val_loss: 226.3579\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 175.2859 - val_loss: 155.5756\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.1644 - val_loss: 193.2402\n",
      "Epoch 731/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 158.8050 - val_loss: 157.1991\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 157.9617 - val_loss: 206.1350\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 178.6649 - val_loss: 254.5976\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 291.8106 - val_loss: 254.4100\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 202.9249 - val_loss: 160.3968\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 174.5324 - val_loss: 297.9559\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.9714 - val_loss: 161.8731\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 162.4577 - val_loss: 186.0102\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.6059 - val_loss: 216.5891\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 181.4493 - val_loss: 158.1986\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 187.3772 - val_loss: 412.6475\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 193.5905 - val_loss: 166.8418\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.9293 - val_loss: 166.3235\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 161.6725 - val_loss: 164.8521\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 164.5171 - val_loss: 183.3309\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 213.1458 - val_loss: 212.7449\n",
      "Epoch 747/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 170.8210 - val_loss: 156.4618\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 183.1382 - val_loss: 187.6115\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.0302 - val_loss: 160.7623\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.4209 - val_loss: 167.1877\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 153.6746 - val_loss: 169.8379\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 239.1397 - val_loss: 194.2468\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 160.1465 - val_loss: 161.7586\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 180.6987 - val_loss: 178.4091\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.3629 - val_loss: 172.7358\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 178.7687 - val_loss: 179.8374\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 167.4652 - val_loss: 184.1662\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 165.2271 - val_loss: 158.7132\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 169.1385 - val_loss: 594.0695\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 214.0648 - val_loss: 193.5174\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 172.5812 - val_loss: 158.5170\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 206.2715 - val_loss: 679.3675\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 170.7616 - val_loss: 247.7044\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 164.8367 - val_loss: 230.8312\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 154.3309 - val_loss: 191.6178\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 208.7389 - val_loss: 321.3947\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.2944 - val_loss: 176.9703\n",
      "Epoch 768/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 164.1221 - val_loss: 165.4024\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 173.7241 - val_loss: 339.3930\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.2126 - val_loss: 155.9121\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 195.6129 - val_loss: 327.8736\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 434.4585 - val_loss: 409.8568\n",
      "Epoch 773/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 217.2327 - val_loss: 171.3619\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 172.2747 - val_loss: 197.6577\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.9041 - val_loss: 325.4372\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 244.2897 - val_loss: 270.2405\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.7553 - val_loss: 167.7124\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.7238 - val_loss: 155.9517\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 171.4088 - val_loss: 233.1592\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.1904 - val_loss: 161.6927\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 160.0180 - val_loss: 161.7216\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 156.4445 - val_loss: 163.7054\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 160.1270 - val_loss: 175.3569\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 171.6513 - val_loss: 169.4865\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.1733 - val_loss: 158.9295\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 217.2300 - val_loss: 192.2928\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.5362 - val_loss: 171.6664\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 160.0624 - val_loss: 151.9680\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.9705 - val_loss: 225.0967\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.4894 - val_loss: 222.8079\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 153.2966 - val_loss: 150.4136\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 160.5375 - val_loss: 153.9819\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 165.1879 - val_loss: 164.4558\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 316.5466 - val_loss: 242.2818\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 193.5888 - val_loss: 172.0056\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 160.0615 - val_loss: 172.3049\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 165.5550 - val_loss: 160.5061\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 201.8796 - val_loss: 410.5120\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 277.6415 - val_loss: 198.5069\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 159.4461 - val_loss: 158.0071\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 155.6924 - val_loss: 165.5790\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 154.9765 - val_loss: 172.2014\n",
      "Epoch 803/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 159.3196 - val_loss: 154.1996\n",
      "Epoch 804/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 173.5661 - val_loss: 173.1031\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 179.2053 - val_loss: 190.8988\n",
      "Epoch 806/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 159.9806 - val_loss: 228.4620\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 195.9312 - val_loss: 180.8682\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 173.5534 - val_loss: 174.9416\n",
      "Epoch 809/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 173.7445 - val_loss: 213.2564\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 374.3902 - val_loss: 231.7351\n",
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 238.2346 - val_loss: 292.7545\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 195.9751 - val_loss: 173.0062\n",
      "Epoch 813/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 167.4782 - val_loss: 164.4768\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 166.9130 - val_loss: 159.4254\n",
      "Epoch 815/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 207.3907 - val_loss: 170.4548\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 167.2192 - val_loss: 278.9122\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 160.7216 - val_loss: 207.7018\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.8903 - val_loss: 213.2633\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 209.2475 - val_loss: 225.1213\n",
      "Epoch 820/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 243.9905 - val_loss: 246.0848\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 158.6371 - val_loss: 200.2873\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.1518 - val_loss: 325.9695\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 171.8180 - val_loss: 157.0844\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 157.5245 - val_loss: 153.4570\n",
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 196.4895 - val_loss: 173.4872\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 173.1174 - val_loss: 165.2283\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 160.0843 - val_loss: 161.5511\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 192.0967 - val_loss: 251.7430\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 185.2558 - val_loss: 162.7888\n",
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.4132 - val_loss: 177.4466\n",
      "Epoch 831/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 164.0337 - val_loss: 177.9056\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 279.5069 - val_loss: 172.1265\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 165.5486 - val_loss: 153.3130\n",
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 171.7833 - val_loss: 189.7951\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 157.8834 - val_loss: 157.0802\n",
      "Epoch 836/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 158.1978 - val_loss: 319.2660\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 168.8733 - val_loss: 163.2547\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 198.2363 - val_loss: 151.9562\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 176.1245 - val_loss: 477.5826\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 171.2075 - val_loss: 202.8341\n",
      "Epoch 841/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 156.8666 - val_loss: 193.0103\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 165.2415 - val_loss: 240.9594\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 247.5098 - val_loss: 171.4209\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 247.9237 - val_loss: 507.7744\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 182.0022 - val_loss: 157.6811\n",
      "Epoch 846/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 152.9254 - val_loss: 154.3207\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.5894 - val_loss: 261.5667\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 157.8825 - val_loss: 176.5684\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 158.0840 - val_loss: 159.1388\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 155.1605 - val_loss: 157.4126\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 164.3682 - val_loss: 195.9761\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 149.3480 - val_loss: 156.7016\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 152.2563 - val_loss: 162.2024\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.6411 - val_loss: 156.4551\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 153.3753 - val_loss: 185.4718\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 228.1633 - val_loss: 301.7670\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 257.7825 - val_loss: 178.3987\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 165.3880 - val_loss: 160.3315\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 171.1850 - val_loss: 160.0936\n",
      "Epoch 860/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 158.0747 - val_loss: 153.9101\n",
      "Epoch 861/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 159.3585 - val_loss: 208.7795\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 165.2234 - val_loss: 194.4265\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.7307 - val_loss: 164.4565\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 153.0974 - val_loss: 170.2894\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 167.8977 - val_loss: 186.1299\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 169.5493 - val_loss: 163.0704\n",
      "Epoch 867/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 153.3895 - val_loss: 158.6956\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 165.5904 - val_loss: 179.6915\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 355.5550 - val_loss: 211.1686\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 238.9592 - val_loss: 168.5669\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 162.1319 - val_loss: 181.5916\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 182.7972 - val_loss: 171.7200\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 165.1282 - val_loss: 156.2748\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 150.0709 - val_loss: 206.5490\n",
      "Epoch 875/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 156.6055 - val_loss: 165.5922\n",
      "Epoch 876/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 252.6755 - val_loss: 172.7858\n",
      "Epoch 877/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 145.2080 - val_loss: 166.3224\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 147.3048 - val_loss: 153.6347\n",
      "Epoch 879/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.4090 - val_loss: 155.2136\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.0817 - val_loss: 236.9399\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 150.5434 - val_loss: 162.4309\n",
      "Epoch 882/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 199.0483 - val_loss: 207.9960\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.9848 - val_loss: 182.1102\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 177.7327 - val_loss: 165.2527\n",
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.3798 - val_loss: 151.3885\n",
      "Epoch 886/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 166.7918 - val_loss: 159.2638\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 171.5748 - val_loss: 190.5481\n",
      "Epoch 888/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.1595 - val_loss: 177.3926\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 155.0206 - val_loss: 152.0641\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.5065 - val_loss: 188.6595\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.6137 - val_loss: 150.0036\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.1134 - val_loss: 218.2523\n",
      "Epoch 893/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 158.0587 - val_loss: 186.2161\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 253.1492 - val_loss: 176.5141\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 149.9026 - val_loss: 176.2048\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.8022 - val_loss: 161.2742\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.4303 - val_loss: 176.6743\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 147.2838 - val_loss: 210.8950\n",
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 202.4644 - val_loss: 159.5680\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.8935 - val_loss: 191.7117\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 151.8719 - val_loss: 162.4215\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 162.4029 - val_loss: 198.2806\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 158.8747 - val_loss: 166.9291\n",
      "Epoch 904/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 184.9554 - val_loss: 175.0693\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 305.7626 - val_loss: 495.6468\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 194.2750 - val_loss: 160.6570\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.1172 - val_loss: 185.5880\n",
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 156.9903 - val_loss: 201.0618\n",
      "Epoch 909/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 157.5837 - val_loss: 250.5436\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 158.2231 - val_loss: 159.7946\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 175.0934 - val_loss: 188.9738\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 160.4859 - val_loss: 153.3553\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.8957 - val_loss: 158.1004\n",
      "Epoch 914/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 285.0894 - val_loss: 222.5065\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 164.9235 - val_loss: 156.7537\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 155.0690 - val_loss: 182.9538\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 152.5966 - val_loss: 164.7834\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 191.8643 - val_loss: 184.6973\n",
      "Epoch 919/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 156.0708 - val_loss: 157.6918\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 147.8256 - val_loss: 166.3628\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 181.6514 - val_loss: 215.1368\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.8696 - val_loss: 161.6128\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 191.0976 - val_loss: 415.0877\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 253.0109 - val_loss: 161.2434\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 150.2832 - val_loss: 159.0628\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.9916 - val_loss: 155.6789\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 145.2281 - val_loss: 160.9145\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 212.5082 - val_loss: 172.8472\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.6324 - val_loss: 157.5639\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 165.8349 - val_loss: 149.4727\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 189.6710 - val_loss: 206.6680\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 171.2289 - val_loss: 165.7651\n",
      "Epoch 933/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.4678 - val_loss: 248.2098\n",
      "Epoch 934/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 151.3032 - val_loss: 179.2165\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 160.5005 - val_loss: 153.3450\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 171.2034 - val_loss: 657.8571\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 174.7308 - val_loss: 161.4629\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.4425 - val_loss: 205.6692\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 150.4593 - val_loss: 177.3185\n",
      "Epoch 940/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 155.3016 - val_loss: 169.0647\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 156.7491 - val_loss: 168.0429\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 256.1330 - val_loss: 176.3451\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 150.1288 - val_loss: 193.1142\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 155.3799 - val_loss: 157.7209\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 148.6155 - val_loss: 215.1229\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 166.0267 - val_loss: 157.6731\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 152.6218 - val_loss: 175.8037\n",
      "Epoch 948/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 217.5913 - val_loss: 333.0700\n",
      "Epoch 949/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 286.2708 - val_loss: 158.1004\n",
      "Epoch 950/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 147.2581 - val_loss: 209.9575\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 156.5436 - val_loss: 180.6460\n",
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 154.8923 - val_loss: 149.8595\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.3055 - val_loss: 152.0778\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 159.7756 - val_loss: 171.1204\n",
      "Epoch 955/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 206.6995 - val_loss: 270.9350\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 183.7377 - val_loss: 175.4959\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 146.8409 - val_loss: 184.6646\n",
      "Epoch 958/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 160.3770 - val_loss: 186.0283\n",
      "Epoch 959/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 165.8202 - val_loss: 161.2387\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 152.8442 - val_loss: 201.9356\n",
      "Epoch 961/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 167.2834 - val_loss: 350.9897\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 220.4220 - val_loss: 187.4225\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 171.4768 - val_loss: 199.0580\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 157.3326 - val_loss: 177.0170\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 179.6029 - val_loss: 230.9270\n",
      "Epoch 966/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 305.0017 - val_loss: 157.2055\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 162.1451 - val_loss: 196.0921\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 158.2025 - val_loss: 173.0424\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 179.0964 - val_loss: 159.4849\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 162.0983 - val_loss: 185.4558\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.0509 - val_loss: 172.8573\n",
      "Epoch 972/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.8728 - val_loss: 176.2566\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 149.5223 - val_loss: 166.2361\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 178.1843 - val_loss: 183.4339\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 267.9624 - val_loss: 171.9864\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 160.5432 - val_loss: 168.5833\n",
      "Epoch 977/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 169.9613 - val_loss: 206.9615\n",
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 187.7053 - val_loss: 174.2618\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 182.8944 - val_loss: 182.6610\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 167.5981 - val_loss: 152.4873\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 172.0252 - val_loss: 177.4496\n",
      "Epoch 982/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 356.3415 - val_loss: 440.5579\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 324.0030 - val_loss: 295.1060\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 239.6949 - val_loss: 259.1635\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 197.9449 - val_loss: 225.2011\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 190.1020 - val_loss: 177.1879\n",
      "Epoch 987/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 190.3714 - val_loss: 300.4269\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 192.0688 - val_loss: 181.8448\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.9871 - val_loss: 182.9689\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 167.5209 - val_loss: 199.6411\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 177.5185 - val_loss: 168.7337\n",
      "Epoch 992/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 176.4469 - val_loss: 219.1063\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 191.5394 - val_loss: 170.2369\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 180.9462 - val_loss: 185.4239\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 171.8285 - val_loss: 159.2909\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 169.3538 - val_loss: 187.4768\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 185.2118 - val_loss: 158.8310\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 167.6016 - val_loss: 162.6894\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 159.0991 - val_loss: 170.6971\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 158.1015 - val_loss: 163.8131\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 158.3380 - val_loss: 169.5677\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 180.1647 - val_loss: 172.2536\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 167.4969 - val_loss: 194.2327\n",
      "Epoch 1004/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 171.4819 - val_loss: 166.4151\n",
      "Epoch 1005/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 165.0106 - val_loss: 176.0944\n",
      "Epoch 1006/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.3986 - val_loss: 200.7609\n",
      "Epoch 1007/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 169.8042 - val_loss: 456.8832\n",
      "Epoch 1008/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 180.5895 - val_loss: 231.4319\n",
      "Epoch 1009/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 166.1936 - val_loss: 217.8518\n",
      "Epoch 1010/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.7291 - val_loss: 164.2538\n",
      "Epoch 1011/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 180.5038 - val_loss: 217.2904\n",
      "Epoch 1012/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 170.6882 - val_loss: 161.6374\n",
      "Epoch 1013/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 148.2059 - val_loss: 169.6199\n",
      "Epoch 1014/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 172.7682 - val_loss: 166.2804\n",
      "Epoch 1015/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 166.1562 - val_loss: 210.3398\n",
      "Epoch 1016/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 156.2347 - val_loss: 215.3293\n",
      "Epoch 1017/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 184.2787 - val_loss: 177.2965\n",
      "Epoch 1018/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.9919 - val_loss: 254.0399\n",
      "Epoch 1019/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 167.7788 - val_loss: 155.2437\n",
      "Epoch 1020/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.4839 - val_loss: 157.0035\n",
      "Epoch 1021/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 226.2790 - val_loss: 164.6829\n",
      "Epoch 1022/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 156.1700 - val_loss: 182.5902\n",
      "Epoch 1023/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.2194 - val_loss: 170.0690\n",
      "Epoch 1024/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 160.9538 - val_loss: 228.8456\n",
      "Epoch 1025/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 181.2984 - val_loss: 161.3709\n",
      "Epoch 1026/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 176.2864 - val_loss: 203.3164\n",
      "Epoch 1027/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 154.3244 - val_loss: 170.6174\n",
      "Epoch 1028/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.8695 - val_loss: 171.5795\n",
      "Epoch 1029/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 157.6096 - val_loss: 163.0176\n",
      "Epoch 1030/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 154.4990 - val_loss: 159.9876\n",
      "Epoch 01030: early stopping\n",
      "Fold score (RMSE): 11.976305961608887\n",
      "Fold #3\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 141us/step - loss: 10451.2461 - val_loss: 4965.0042\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 4816.1329 - val_loss: 4696.7260\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 4493.2393 - val_loss: 4468.6137\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 4391.0557 - val_loss: 4540.9216\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 4275.9021 - val_loss: 4214.7215\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 4148.1112 - val_loss: 4346.6670\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 4005.9924 - val_loss: 4523.1919\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 4040.3941 - val_loss: 4026.4553\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 3952.4362 - val_loss: 3952.2441\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 3861.4669 - val_loss: 4727.8402\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 3802.4863 - val_loss: 4475.5348\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 3715.7605 - val_loss: 3692.8630\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 3678.5260 - val_loss: 3735.5359\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 3527.8308 - val_loss: 3633.8665\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 3472.9684 - val_loss: 3560.9754\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 3313.7933 - val_loss: 3262.3764\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 3272.5494 - val_loss: 3264.2099\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 3261.0479 - val_loss: 3162.3118\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 3142.3315 - val_loss: 3038.3413\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 2831.7999 - val_loss: 3531.1580\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 2838.6952 - val_loss: 2469.5459\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 2505.2400 - val_loss: 2243.1025\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 2250.4852 - val_loss: 2092.6112\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 2428.4820 - val_loss: 3789.6546\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 1986.9686 - val_loss: 1673.0782\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 1666.5480 - val_loss: 1291.7313\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 1462.0859 - val_loss: 1181.8119\n",
      "Epoch 28/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 1306.8118 - val_loss: 1022.6727\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 1162.9849 - val_loss: 900.1195\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1093.9382 - val_loss: 1333.9789\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 1012.8427 - val_loss: 776.5664\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 959.6779 - val_loss: 658.5132\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 849.1231 - val_loss: 673.1585\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 942.8097 - val_loss: 526.3064\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 876.6846 - val_loss: 930.2483\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 816.7774 - val_loss: 697.0327\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 690.9469 - val_loss: 588.8138\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 700.1594 - val_loss: 527.1981\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 782.1639 - val_loss: 636.2663\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 661.9519 - val_loss: 517.8150\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 691.3655 - val_loss: 423.4982\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 616.2693 - val_loss: 494.5823\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 586.6249 - val_loss: 461.0097\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 631.2087 - val_loss: 515.3933\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 556.5985 - val_loss: 594.1912\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 585.8906 - val_loss: 426.7575\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 552.4859 - val_loss: 384.5174\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 502.9658 - val_loss: 385.8654\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 577.5513 - val_loss: 405.8596\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 576.3214 - val_loss: 1755.0691\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 616.8208 - val_loss: 373.6539\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 494.1412 - val_loss: 353.1072\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 438.7951 - val_loss: 349.7188\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 557.0643 - val_loss: 380.1934\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 480.3413 - val_loss: 389.6384\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 725.7663 - val_loss: 437.5270\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 553.4333 - val_loss: 316.4890\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 480.4926 - val_loss: 307.9756\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 413.3000 - val_loss: 321.9063\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 457.0002 - val_loss: 620.9965\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 446.9835 - val_loss: 398.4758\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 505.7716 - val_loss: 346.3240\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 429.9307 - val_loss: 287.5167\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 426.4162 - val_loss: 686.2597\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 425.0733 - val_loss: 349.3452\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 413.3629 - val_loss: 453.9370\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 367.6160 - val_loss: 701.6165\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 500.7095 - val_loss: 729.1969\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 365.6693 - val_loss: 318.2732\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 376.4109 - val_loss: 325.4316\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 463.2090 - val_loss: 412.7214\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 398.8664 - val_loss: 291.6312\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 432.6212 - val_loss: 510.5800\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 575.4224 - val_loss: 306.2089\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 341.5053 - val_loss: 267.5626\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 362.9882 - val_loss: 332.3739\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 358.5774 - val_loss: 318.1336\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 345.8149 - val_loss: 258.3801\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 426.9396 - val_loss: 676.1702\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 345.6407 - val_loss: 271.9338\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 376.3760 - val_loss: 338.2894\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 415.1202 - val_loss: 496.4845\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 367.5591 - val_loss: 288.3311\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 340.1894 - val_loss: 321.5643\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 321.8317 - val_loss: 260.9942\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 367.1512 - val_loss: 292.2237\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 319.2622 - val_loss: 298.8836\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 390.4010 - val_loss: 263.3915\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 309.8979 - val_loss: 471.5002\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 325.0419 - val_loss: 290.0060\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 381.2039 - val_loss: 448.7612\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 386.1318 - val_loss: 302.6968\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 340.7585 - val_loss: 295.0944\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 349.6622 - val_loss: 283.1467\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 397.8468 - val_loss: 634.7963\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 344.0366 - val_loss: 633.8114\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 312.3195 - val_loss: 243.2993\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 318.0722 - val_loss: 354.2572\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 350.6079 - val_loss: 261.8022\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 368.9679 - val_loss: 330.8146\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 291.5383 - val_loss: 269.2174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 351.0400 - val_loss: 450.4835\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 335.6866 - val_loss: 642.7327\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 323.3059 - val_loss: 390.2115\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 326.7360 - val_loss: 294.8577\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 298.3902 - val_loss: 235.8666\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 311.8617 - val_loss: 277.3381\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 294.5713 - val_loss: 233.5249\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 301.4492 - val_loss: 274.1356\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 352.6720 - val_loss: 259.3732\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 315.9945 - val_loss: 496.8208\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 345.6613 - val_loss: 296.4994\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 300.5653 - val_loss: 277.1932\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 370.9953 - val_loss: 335.8243\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 344.2928 - val_loss: 313.6204\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 341.7264 - val_loss: 527.3330\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 305.5621 - val_loss: 239.7456\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 342.9800 - val_loss: 316.1724\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 278.0864 - val_loss: 423.6744\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 286.4178 - val_loss: 377.7299\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 319.2182 - val_loss: 263.6214\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 274.5439 - val_loss: 229.3164\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 341.6589 - val_loss: 405.5977\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 312.7820 - val_loss: 301.0560\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 280.1333 - val_loss: 222.5037\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 314.2168 - val_loss: 294.8800\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 310.4614 - val_loss: 247.1704\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 306.7510 - val_loss: 238.3457\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 310.1855 - val_loss: 361.4781\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 285.7587 - val_loss: 239.5964\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 307.5043 - val_loss: 273.0708\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 253.5494 - val_loss: 248.9729\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 314.2209 - val_loss: 283.7721\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 283.5730 - val_loss: 233.1729\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 334.0682 - val_loss: 235.6756\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 350.7129 - val_loss: 341.7546\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 259.7222 - val_loss: 270.6545\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 271.7245 - val_loss: 233.3359\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 284.0074 - val_loss: 294.9951\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 359.7946 - val_loss: 265.5336\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 270.5032 - val_loss: 250.1525\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 259.2036 - val_loss: 223.5030\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 326.0029 - val_loss: 393.2696\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 295.2641 - val_loss: 257.9086\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 293.7508 - val_loss: 254.2231\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 264.1693 - val_loss: 270.2730\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 292.7100 - val_loss: 227.5007\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 297.5237 - val_loss: 247.9293\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 344.9590 - val_loss: 261.4655\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 285.8025 - val_loss: 294.3522\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 260.0100 - val_loss: 247.4184\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 293.7632 - val_loss: 223.8154\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 274.7353 - val_loss: 215.7548\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 261.5107 - val_loss: 210.4156\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 267.0568 - val_loss: 235.5992\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 286.0571 - val_loss: 307.7940\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 250.1686 - val_loss: 405.1744\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 251.1678 - val_loss: 207.8033\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 266.8580 - val_loss: 242.6151\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 272.4657 - val_loss: 271.3002\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 250.9241 - val_loss: 225.3593\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 296.1598 - val_loss: 241.2287\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 281.9169 - val_loss: 226.1686\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 260.9639 - val_loss: 211.3924\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 261.7735 - val_loss: 223.6670\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 257.8591 - val_loss: 207.1378\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 282.2813 - val_loss: 262.2211\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 279.6684 - val_loss: 220.4322\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 279.6542 - val_loss: 1206.2989\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 300.3616 - val_loss: 217.2245\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 244.6761 - val_loss: 384.1050\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 244.3410 - val_loss: 204.7779\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 255.6671 - val_loss: 226.8364\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 259.4932 - val_loss: 227.9229\n",
      "Epoch 175/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 57us/step - loss: 335.6811 - val_loss: 214.7058\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 254.5865 - val_loss: 274.8959\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 268.5845 - val_loss: 206.9293\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 244.7014 - val_loss: 202.1812\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 248.6864 - val_loss: 206.3750\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 250.2904 - val_loss: 202.0115\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 261.9191 - val_loss: 214.0660\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 289.5404 - val_loss: 201.8960\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 268.4101 - val_loss: 300.6155\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 275.4613 - val_loss: 290.7912\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 236.6161 - val_loss: 212.1904\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 224.7105 - val_loss: 295.6825\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 219.7075 - val_loss: 299.2986\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 289.4683 - val_loss: 278.7135\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 302.5437 - val_loss: 186.8323\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 249.5880 - val_loss: 251.9806\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 245.7694 - val_loss: 401.1978\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 282.7169 - val_loss: 242.7714\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 231.5301 - val_loss: 210.1492\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 226.4987 - val_loss: 211.5589\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 238.3750 - val_loss: 335.0546\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 234.0190 - val_loss: 206.0544\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 277.6500 - val_loss: 240.8915\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 305.2705 - val_loss: 622.4620\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 248.8482 - val_loss: 199.8193\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 285.1948 - val_loss: 540.1104\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 235.6177 - val_loss: 235.4930\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 228.0086 - val_loss: 283.1594\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 284.7068 - val_loss: 185.4774\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 238.2788 - val_loss: 197.8587\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 284.5898 - val_loss: 284.3496\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 250.2912 - val_loss: 195.5466\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 236.9586 - val_loss: 206.5290\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 212.4195 - val_loss: 220.4816\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 267.9704 - val_loss: 312.1044\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 297.8565 - val_loss: 250.0517\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 253.0387 - val_loss: 221.4598\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 223.5349 - val_loss: 173.5910\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 248.5090 - val_loss: 217.2309\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 216.9186 - val_loss: 224.1727\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 266.3008 - val_loss: 264.7143\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 217.3356 - val_loss: 200.7995\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 243.1158 - val_loss: 183.2570\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 286.8706 - val_loss: 304.6077\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 259.1144 - val_loss: 182.9895\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 251.0386 - val_loss: 205.6043\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 288.1578 - val_loss: 301.0339\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 284.6940 - val_loss: 276.6866\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 232.5237 - val_loss: 171.8676\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 249.6386 - val_loss: 280.1948\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 234.1395 - val_loss: 288.1759\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 238.3972 - val_loss: 204.6883\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 225.6021 - val_loss: 246.1249\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 279.4531 - val_loss: 191.4982\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 213.0965 - val_loss: 265.7681\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 232.4952 - val_loss: 283.8625\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 249.6835 - val_loss: 253.3747\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 255.5181 - val_loss: 189.8327\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 258.1028 - val_loss: 216.9744\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 212.6701 - val_loss: 195.9713\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 214.6111 - val_loss: 181.4304\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 216.2650 - val_loss: 226.0060\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 265.8329 - val_loss: 192.4159\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 271.0441 - val_loss: 251.0003\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 264.4339 - val_loss: 176.6784\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 196.2857 - val_loss: 243.1276\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 209.3965 - val_loss: 230.2913\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 275.8430 - val_loss: 197.4098\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 242.3312 - val_loss: 171.5640\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 217.9232 - val_loss: 324.0915\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 466.9878 - val_loss: 306.1093\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 326.2350 - val_loss: 252.3353\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 305.6740 - val_loss: 229.2689\n",
      "Epoch 248/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 55us/step - loss: 287.7200 - val_loss: 258.7669\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 250.3247 - val_loss: 226.3026\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 239.3987 - val_loss: 181.5336\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 216.5182 - val_loss: 426.2592\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 241.0181 - val_loss: 203.9668\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 216.4759 - val_loss: 176.5054\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 235.0262 - val_loss: 188.8322\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 237.2159 - val_loss: 224.1687\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 231.7727 - val_loss: 201.3633\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 224.4953 - val_loss: 232.3355\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 206.7645 - val_loss: 387.5074\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 249.2012 - val_loss: 198.4474\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 223.7260 - val_loss: 180.3964\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 301.2800 - val_loss: 205.9310\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 245.1152 - val_loss: 210.5760\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 276.2126 - val_loss: 622.9523\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 256.8377 - val_loss: 246.3289\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 243.3789 - val_loss: 182.5033\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 206.7005 - val_loss: 183.6705\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 229.5652 - val_loss: 222.4371\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 209.4656 - val_loss: 203.6924\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 256.2456 - val_loss: 263.4505\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 197.7617 - val_loss: 183.9438\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 221.3343 - val_loss: 197.8537\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 232.5700 - val_loss: 198.6215\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 277.0668 - val_loss: 469.8520\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 250.1421 - val_loss: 163.9038\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 258.9998 - val_loss: 185.7326\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 250.7404 - val_loss: 255.9347\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 226.9717 - val_loss: 196.2074\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 219.4321 - val_loss: 232.9787\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 222.6893 - val_loss: 411.2611\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 256.1914 - val_loss: 295.3802\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 212.1399 - val_loss: 214.0952\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 216.1532 - val_loss: 181.8783\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 205.2953 - val_loss: 172.1518\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 209.0690 - val_loss: 250.1594\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 239.9280 - val_loss: 238.2325\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 263.7649 - val_loss: 175.5435\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 221.5587 - val_loss: 184.6476\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 229.4890 - val_loss: 180.3910\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 314.2186 - val_loss: 174.5986\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 201.3580 - val_loss: 162.6306\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 252.8826 - val_loss: 218.4919\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 198.3138 - val_loss: 171.9560\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 212.1004 - val_loss: 202.6565\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 217.6514 - val_loss: 189.6570\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 274.5134 - val_loss: 1662.4920\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 379.5137 - val_loss: 437.3291\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 224.2484 - val_loss: 233.7548\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 206.6214 - val_loss: 319.1286\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 263.7055 - val_loss: 231.7290\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 208.7014 - val_loss: 446.0129\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 257.4865 - val_loss: 1196.9656\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 213.7712 - val_loss: 190.2988\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 190.7754 - val_loss: 166.0285\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 193.9077 - val_loss: 176.6215\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 202.0199 - val_loss: 539.5733\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 462.6506 - val_loss: 189.6729\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 226.0829 - val_loss: 360.1403\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 247.6178 - val_loss: 187.2260\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 241.4503 - val_loss: 397.5276\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 250.0549 - val_loss: 228.6307\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 181.6625 - val_loss: 208.3327\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 202.5660 - val_loss: 180.5393\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 266.2332 - val_loss: 316.5978\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 217.0929 - val_loss: 179.2933\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 209.5007 - val_loss: 247.7284\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 241.9650 - val_loss: 192.1786\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 197.7915 - val_loss: 191.3226\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 244.7891 - val_loss: 176.5249\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 213.7349 - val_loss: 179.4647\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 218.2158 - val_loss: 162.7830\n",
      "Epoch 321/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 56us/step - loss: 205.3194 - val_loss: 171.5119\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 190.7663 - val_loss: 188.7531\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 192.6488 - val_loss: 167.9484\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 218.2724 - val_loss: 212.5192\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 204.3265 - val_loss: 179.3770\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 328.0348 - val_loss: 194.7310\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 207.1272 - val_loss: 376.5830\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 197.4186 - val_loss: 177.4740\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 186.5222 - val_loss: 169.2847\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 215.2316 - val_loss: 186.9337\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 195.1912 - val_loss: 166.1164\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 204.0971 - val_loss: 163.6874\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 405.3284 - val_loss: 181.2118\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 214.7649 - val_loss: 180.0522\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 216.5196 - val_loss: 230.0493\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 208.3167 - val_loss: 165.7442\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 187.6318 - val_loss: 206.0959\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 189.7375 - val_loss: 182.8584\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 216.4064 - val_loss: 257.1197\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 197.6742 - val_loss: 166.9792\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 244.7539 - val_loss: 203.1589\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 289.1860 - val_loss: 188.1644\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 199.0424 - val_loss: 175.3151\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 181.3610 - val_loss: 185.4310\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 204.0139 - val_loss: 216.3998\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 190.9127 - val_loss: 213.6613\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 191.5817 - val_loss: 185.9128\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 193.9618 - val_loss: 240.6583\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 187.4673 - val_loss: 163.0567\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 243.3571 - val_loss: 682.5399\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 220.5536 - val_loss: 190.3611\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 196.1945 - val_loss: 176.8640\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 211.6273 - val_loss: 677.5927\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 338.5759 - val_loss: 296.1505\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 197.6033 - val_loss: 225.8439\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 216.4983 - val_loss: 217.4377\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 245.3371 - val_loss: 649.4416\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 227.1252 - val_loss: 270.6404\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 208.2813 - val_loss: 393.3618\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 191.4737 - val_loss: 158.9583\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 177.6332 - val_loss: 161.0708\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 214.3809 - val_loss: 188.8622\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 186.5106 - val_loss: 191.5567\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 225.6481 - val_loss: 203.0885\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 239.4534 - val_loss: 163.6544\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 220.5724 - val_loss: 176.0517\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 212.2890 - val_loss: 244.7733\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 181.8565 - val_loss: 171.2295\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 217.7720 - val_loss: 181.3169\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 210.7285 - val_loss: 184.2887\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 287.0164 - val_loss: 182.9870\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 205.9723 - val_loss: 194.1727\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 242.3254 - val_loss: 240.2309\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 198.3661 - val_loss: 185.8761\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 186.5276 - val_loss: 182.9485\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 233.6350 - val_loss: 169.0811\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.8415 - val_loss: 159.0694\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 245.8885 - val_loss: 172.7204\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 186.1203 - val_loss: 161.5537\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 260.4848 - val_loss: 170.8119\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 186.3827 - val_loss: 220.1433\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 176.5592 - val_loss: 182.3408\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 193.4035 - val_loss: 173.6193\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 332.1730 - val_loss: 162.4475\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 183.6886 - val_loss: 177.1594\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 195.1622 - val_loss: 311.0819\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 193.3151 - val_loss: 179.3823\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 265.6247 - val_loss: 180.4084\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 180.8458 - val_loss: 171.5749\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 199.9004 - val_loss: 178.1543\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 196.5286 - val_loss: 197.6772\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 214.6723 - val_loss: 169.5003\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 204.0080 - val_loss: 180.0206\n",
      "Epoch 394/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 276.1320 - val_loss: 214.3963\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 259.4567 - val_loss: 379.1693\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 233.3254 - val_loss: 192.1327\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 191.4924 - val_loss: 154.4947\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 244.5585 - val_loss: 174.0255\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 189.0993 - val_loss: 214.6554\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 171.9313 - val_loss: 175.9444\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 196.3257 - val_loss: 223.9691\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 235.4149 - val_loss: 193.7731\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 193.3786 - val_loss: 175.1758\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 184.1724 - val_loss: 225.7842\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 226.4193 - val_loss: 181.5033\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 235.5576 - val_loss: 167.3392\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 179.4986 - val_loss: 163.9814\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 174.5034 - val_loss: 171.1600\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 188.2670 - val_loss: 161.1236\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 247.1095 - val_loss: 253.3096\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 227.2935 - val_loss: 203.1792\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 363.7957 - val_loss: 223.5100\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 277.5542 - val_loss: 178.9833\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 205.4593 - val_loss: 163.7386\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 247.7782 - val_loss: 174.6527\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 250.3335 - val_loss: 180.0917\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 237.1832 - val_loss: 262.4222\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 266.4712 - val_loss: 167.4329\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 196.8112 - val_loss: 157.6759\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 204.9021 - val_loss: 192.7517\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 246.6566 - val_loss: 177.8578\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 262.2580 - val_loss: 192.2677\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 235.0221 - val_loss: 168.2038\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 210.0756 - val_loss: 157.4574\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 208.5714 - val_loss: 232.2595\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 225.9909 - val_loss: 204.4146\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 224.5599 - val_loss: 214.6053\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 205.8068 - val_loss: 180.5839\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 191.5972 - val_loss: 189.6620\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 250.7515 - val_loss: 277.2041\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 237.9911 - val_loss: 198.5071\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 266.3961 - val_loss: 207.2147\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 193.6265 - val_loss: 161.1455\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 196.5867 - val_loss: 159.6924\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 201.7662 - val_loss: 185.4091\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 177.4977 - val_loss: 192.5707\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 197.6768 - val_loss: 166.9240\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 205.6771 - val_loss: 193.9864\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 237.9136 - val_loss: 206.7625\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 420.2791 - val_loss: 183.4737\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 305.6735 - val_loss: 255.1969\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 234.3898 - val_loss: 199.9147\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 247.7048 - val_loss: 193.7063\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 304.5645 - val_loss: 180.0966\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 203.7350 - val_loss: 211.5812\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 236.1584 - val_loss: 221.5844\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 214.5874 - val_loss: 225.2962\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 212.9121 - val_loss: 172.2850\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 201.3898 - val_loss: 252.2107\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 238.2710 - val_loss: 191.8829\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 218.4250 - val_loss: 188.4193\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 221.8499 - val_loss: 174.0156\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 249.8465 - val_loss: 160.6107\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 212.3308 - val_loss: 164.2178\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 243.6023 - val_loss: 173.3220\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 191.3144 - val_loss: 188.1973\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 214.8585 - val_loss: 197.4996\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 294.4023 - val_loss: 335.7552\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 208.9874 - val_loss: 225.5856\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 226.1677 - val_loss: 183.4119\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 195.8516 - val_loss: 215.3840\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 202.8410 - val_loss: 166.4719\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 202.6180 - val_loss: 292.7226\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 223.9023 - val_loss: 204.2491\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 230.5325 - val_loss: 185.5066\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 199.0447 - val_loss: 273.4831\n",
      "Epoch 467/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 56us/step - loss: 231.1421 - val_loss: 206.5229\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 256.7078 - val_loss: 215.2306\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 224.2070 - val_loss: 162.8252\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 193.6768 - val_loss: 178.3525\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 229.7104 - val_loss: 165.0190\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 207.6498 - val_loss: 197.9960\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 237.9861 - val_loss: 180.6192\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 196.5276 - val_loss: 226.9290\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 272.7939 - val_loss: 497.4510\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 199.2582 - val_loss: 163.9680\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 186.0614 - val_loss: 333.3911\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 197.3724 - val_loss: 159.8916\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 292.6469 - val_loss: 391.1596\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 219.8174 - val_loss: 231.1392\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 270.6837 - val_loss: 190.2904\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 187.1723 - val_loss: 177.5761\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 222.0118 - val_loss: 317.0024\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 189.1489 - val_loss: 224.4296\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 226.9269 - val_loss: 210.3147\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 203.7256 - val_loss: 215.4346\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 200.3175 - val_loss: 234.3085\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.8983 - val_loss: 200.6115\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 186.8692 - val_loss: 248.7070\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 220.6362 - val_loss: 189.3357\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 251.6319 - val_loss: 159.5754\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 189.5260 - val_loss: 222.8226\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 211.3956 - val_loss: 166.5229\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 203.4003 - val_loss: 176.3232\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 225.0616 - val_loss: 162.8575\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 211.2079 - val_loss: 159.1025\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 194.1070 - val_loss: 239.3648\n",
      "Epoch 00497: early stopping\n",
      "Fold score (RMSE): 14.736985206604004\n",
      "Fold #4\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 144us/step - loss: 6119.7831 - val_loss: 5131.6303\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 4734.8213 - val_loss: 4699.2218\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 4541.1899 - val_loss: 4839.2043\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 4429.9688 - val_loss: 4291.3189\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 4197.8193 - val_loss: 4193.4879\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 4091.6305 - val_loss: 4164.2931\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 4105.0282 - val_loss: 4546.6013\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 3992.3171 - val_loss: 3853.6916\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 3993.3624 - val_loss: 4099.4426\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 3934.7810 - val_loss: 3757.2148\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 3810.9810 - val_loss: 3571.8147\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 3568.1363 - val_loss: 3439.1526\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 3507.1317 - val_loss: 3298.2058\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 3292.3026 - val_loss: 3644.3655\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 3229.8493 - val_loss: 3842.1016\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 3005.6363 - val_loss: 3039.2865\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 2999.7374 - val_loss: 2581.5630\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 2770.3767 - val_loss: 2269.4715\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 2332.7879 - val_loss: 1965.4439\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 2153.2210 - val_loss: 3051.7075\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 1951.6043 - val_loss: 2406.2818\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 2013.5294 - val_loss: 1426.0835\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 1826.7215 - val_loss: 1419.3089\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 1565.8985 - val_loss: 1462.0194\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 1401.6022 - val_loss: 1199.3146\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 1302.7527 - val_loss: 857.8717\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 1117.5531 - val_loss: 752.2404\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 1043.2275 - val_loss: 639.6423\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 976.0263 - val_loss: 603.5922\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 878.1182 - val_loss: 526.9824\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 821.9331 - val_loss: 534.4192\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 834.6738 - val_loss: 477.5040\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 854.2645 - val_loss: 403.0564\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 694.4651 - val_loss: 384.3717\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 715.9322 - val_loss: 567.7239\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 717.3016 - val_loss: 366.7532\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 636.9686 - val_loss: 354.0170\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 651.7768 - val_loss: 365.4103\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 679.5298 - val_loss: 377.1928\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 607.7512 - val_loss: 736.7649\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 567.6515 - val_loss: 530.7242\n",
      "Epoch 42/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 56us/step - loss: 575.4707 - val_loss: 342.5309\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 535.4070 - val_loss: 465.5931\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 534.1892 - val_loss: 354.6426\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 516.2673 - val_loss: 313.6011\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 564.7174 - val_loss: 707.5023\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 715.5784 - val_loss: 1291.9303\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 587.9113 - val_loss: 309.5868\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 550.7847 - val_loss: 981.4338\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 662.3737 - val_loss: 353.6287\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 469.6992 - val_loss: 298.8264\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 473.5478 - val_loss: 266.9041\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 461.6104 - val_loss: 285.8367\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 472.6180 - val_loss: 432.5399\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 460.1993 - val_loss: 332.9031\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 444.7511 - val_loss: 264.3967\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 484.7223 - val_loss: 336.4836\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 452.5015 - val_loss: 488.3187\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 483.8062 - val_loss: 407.3292\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 480.7983 - val_loss: 355.7344\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 448.9793 - val_loss: 485.3386\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 462.3606 - val_loss: 480.0045\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 447.6583 - val_loss: 719.5204\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 436.9722 - val_loss: 271.0951\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 497.5759 - val_loss: 584.9711\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 407.0116 - val_loss: 299.1505\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 424.6955 - val_loss: 300.8316\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 405.8130 - val_loss: 352.5458\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 392.2697 - val_loss: 237.4275\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 414.6533 - val_loss: 252.3241\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 454.5667 - val_loss: 314.7380\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 350.0268 - val_loss: 273.8541\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 386.1823 - val_loss: 277.4203\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 334.4765 - val_loss: 243.0490\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 405.3299 - val_loss: 300.7186\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 415.5438 - val_loss: 660.7679\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 416.9122 - val_loss: 387.0524\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 486.4860 - val_loss: 437.2833\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 353.9302 - val_loss: 274.2156\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 429.8392 - val_loss: 327.6825\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 371.5204 - val_loss: 233.3133\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 361.1983 - val_loss: 256.5681\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 334.1050 - val_loss: 320.5744\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 359.2878 - val_loss: 260.5987\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 421.6123 - val_loss: 298.5832\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 517.8092 - val_loss: 521.5972\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 359.9757 - val_loss: 340.3397\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 346.2913 - val_loss: 271.0816\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 413.2601 - val_loss: 307.5286\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 363.1129 - val_loss: 414.7795\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 356.5098 - val_loss: 662.8453\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 371.4315 - val_loss: 384.2165\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 386.1317 - val_loss: 407.0945\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 491.8983 - val_loss: 289.3808\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 315.1801 - val_loss: 213.7441\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 452.2641 - val_loss: 287.6298\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 410.2021 - val_loss: 416.2912\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 338.7677 - val_loss: 303.9086\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 364.8290 - val_loss: 217.6237\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 379.1625 - val_loss: 305.2550\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 325.5548 - val_loss: 340.0769\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 347.3507 - val_loss: 282.5740\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 335.3920 - val_loss: 198.1973\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 329.7354 - val_loss: 262.1378\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 332.8164 - val_loss: 240.9838\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 378.7544 - val_loss: 217.0632\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 348.9417 - val_loss: 270.7462\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 325.9364 - val_loss: 217.6521\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 330.7876 - val_loss: 746.2308\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 426.5564 - val_loss: 420.9133\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 315.8663 - val_loss: 316.2634\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 370.0542 - val_loss: 239.1484\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 320.3729 - val_loss: 201.6085\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 336.1267 - val_loss: 242.8866\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 308.6578 - val_loss: 204.6493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 330.6084 - val_loss: 220.8807\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 340.2438 - val_loss: 200.2689\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 340.9479 - val_loss: 342.7731\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 301.9552 - val_loss: 282.9543\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 302.7757 - val_loss: 282.8657\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 292.2616 - val_loss: 254.4255\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 304.6955 - val_loss: 253.6522\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 306.9836 - val_loss: 198.6478\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 346.4393 - val_loss: 312.6858\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 331.0415 - val_loss: 220.8912\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 309.6480 - val_loss: 209.7409\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 356.7663 - val_loss: 303.3133\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 279.6419 - val_loss: 197.4874\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 280.9654 - val_loss: 185.0160\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 303.0828 - val_loss: 191.3156\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 329.7587 - val_loss: 255.6997\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 278.0770 - val_loss: 240.2833\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 333.0358 - val_loss: 176.6777\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 264.6682 - val_loss: 182.5037\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 290.4585 - val_loss: 276.3532\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 436.1204 - val_loss: 310.6409\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 293.9091 - val_loss: 279.0308\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 314.9734 - val_loss: 353.2743\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 298.6237 - val_loss: 258.5739\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 296.6718 - val_loss: 182.8121\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 289.7034 - val_loss: 272.4776\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 348.8929 - val_loss: 174.0099\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 338.1391 - val_loss: 194.2029\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 300.8591 - val_loss: 258.0971\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 269.0427 - val_loss: 277.8627\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 284.9030 - val_loss: 371.2463\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 368.9268 - val_loss: 254.4858\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 271.5342 - val_loss: 231.6600\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 247.4503 - val_loss: 173.7304\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 274.2770 - val_loss: 199.3012\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 284.0105 - val_loss: 540.8094\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 284.9129 - val_loss: 232.1757\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 305.4170 - val_loss: 173.4910\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 285.3643 - val_loss: 305.4668\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 295.8486 - val_loss: 175.7908\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 346.5366 - val_loss: 447.6416\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 277.1080 - val_loss: 284.2879\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 264.8196 - val_loss: 249.2859\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 285.9519 - val_loss: 396.4804\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 314.1520 - val_loss: 308.4983\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 318.4667 - val_loss: 453.0456\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 354.2135 - val_loss: 183.7589\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 247.0817 - val_loss: 192.1003\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 288.0742 - val_loss: 275.1984\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 310.6571 - val_loss: 192.2897\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 280.1736 - val_loss: 181.5864\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 282.2438 - val_loss: 207.9429\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 247.9199 - val_loss: 183.7924\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 292.5422 - val_loss: 199.2431\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 304.7363 - val_loss: 265.7504\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 274.3670 - val_loss: 289.4948\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 303.4167 - val_loss: 309.8038\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 276.1115 - val_loss: 266.0941\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 313.8924 - val_loss: 179.7964\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 258.2197 - val_loss: 172.8541\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 257.2004 - val_loss: 282.0413\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 277.8804 - val_loss: 358.4785\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 275.3505 - val_loss: 182.4633\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 282.4861 - val_loss: 239.3346\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 251.7227 - val_loss: 164.0005\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 251.8326 - val_loss: 164.7981\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 272.7116 - val_loss: 205.7425\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 254.1488 - val_loss: 220.5062\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 237.0528 - val_loss: 194.3776\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 240.1565 - val_loss: 184.9262\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 376.6458 - val_loss: 174.5812\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 258.431 - 1s 70us/step - loss: 254.4661 - val_loss: 298.9504\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 269.3215 - val_loss: 192.3104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 284.2972 - val_loss: 232.7180\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 288.2974 - val_loss: 170.4893\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 280.2725 - val_loss: 188.6924\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 291.6082 - val_loss: 196.3799\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 300.4624 - val_loss: 174.9130\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 311.1239 - val_loss: 296.8689\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 259.4921 - val_loss: 168.4080\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 251.8570 - val_loss: 177.9089\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 258.6835 - val_loss: 324.2690\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 259.6443 - val_loss: 197.1859\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 303.0453 - val_loss: 198.1925\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 254.1500 - val_loss: 217.1490\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 254.5400 - val_loss: 180.6160\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 240.5708 - val_loss: 204.9523\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 266.1474 - val_loss: 239.4051\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 264.8039 - val_loss: 157.2626\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 259.1185 - val_loss: 164.8355\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 278.2642 - val_loss: 155.6568\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 287.4693 - val_loss: 364.3351\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 358.2898 - val_loss: 174.7938\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 248.7894 - val_loss: 258.7739\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 257.1947 - val_loss: 170.6587\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 281.4157 - val_loss: 167.1850\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 245.1600 - val_loss: 167.1483\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 228.1776 - val_loss: 155.6769\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 268.2081 - val_loss: 166.4796\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 236.7068 - val_loss: 168.8235\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 239.0969 - val_loss: 251.8870\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 233.9493 - val_loss: 161.6006\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 264.4273 - val_loss: 232.1415\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 256.1246 - val_loss: 161.6164\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 242.6607 - val_loss: 178.4521\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 267.2770 - val_loss: 374.2318\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 262.6034 - val_loss: 307.6220\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 271.0822 - val_loss: 154.6612\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 242.1718 - val_loss: 245.7739\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 252.1939 - val_loss: 176.2206\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 238.1621 - val_loss: 169.9224\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 230.1523 - val_loss: 180.8487\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 226.7477 - val_loss: 152.6369\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 239.7431 - val_loss: 155.4247\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 303.3639 - val_loss: 223.1409\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 220.0945 - val_loss: 156.3245\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 225.0543 - val_loss: 160.5562\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 244.9305 - val_loss: 162.6967\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 233.5031 - val_loss: 187.3293\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 250.4085 - val_loss: 217.3738\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 289.9623 - val_loss: 166.4545\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 276.6887 - val_loss: 279.0242\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 294.2687 - val_loss: 179.2119\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 241.9003 - val_loss: 385.8515\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 306.4011 - val_loss: 179.4786\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 237.9867 - val_loss: 231.5941\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 235.2272 - val_loss: 207.9749\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 239.3436 - val_loss: 168.2810\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 227.8213 - val_loss: 177.5840\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 237.0748 - val_loss: 206.8045\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 234.1040 - val_loss: 237.2271\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 225.8249 - val_loss: 164.6647\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 319.3447 - val_loss: 169.7954\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 224.8705 - val_loss: 159.1791\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 230.7274 - val_loss: 186.0839\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 287.8489 - val_loss: 263.3154\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 242.1382 - val_loss: 172.6160\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 229.6147 - val_loss: 251.6662\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 256.6494 - val_loss: 154.3777\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 270.9798 - val_loss: 179.0771\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 227.6072 - val_loss: 158.0811\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 218.2980 - val_loss: 292.4046\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 232.0562 - val_loss: 159.0022\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 267.2579 - val_loss: 205.0603\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 242.3097 - val_loss: 150.0751\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 223.0952 - val_loss: 161.9454\n",
      "Epoch 262/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 55us/step - loss: 224.0831 - val_loss: 529.9748\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 364.6000 - val_loss: 197.5314\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 281.3900 - val_loss: 152.0352\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 216.5158 - val_loss: 157.7811\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 242.2910 - val_loss: 299.0766\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 233.6545 - val_loss: 156.2546\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 268.6489 - val_loss: 151.9459\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 276.3896 - val_loss: 198.6162\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 199.9125 - val_loss: 156.2047\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 215.3750 - val_loss: 191.7865\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 230.7738 - val_loss: 262.1562\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 242.8080 - val_loss: 156.2749\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 210.1658 - val_loss: 190.0186\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 296.4452 - val_loss: 158.3603\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 304.6914 - val_loss: 166.6525\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 202.6398 - val_loss: 151.4091\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 196.2082 - val_loss: 157.8094\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 270.8119 - val_loss: 201.3257\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 213.2646 - val_loss: 185.7046\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 229.8691 - val_loss: 157.7284\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 205.7222 - val_loss: 208.5888\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 201.5595 - val_loss: 191.2863\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 296.1233 - val_loss: 323.3842\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 212.1511 - val_loss: 194.7267\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 211.2975 - val_loss: 183.8540\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 224.6136 - val_loss: 166.9054\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 213.7632 - val_loss: 144.7659\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 223.4360 - val_loss: 149.0644\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 200.2751 - val_loss: 146.7984\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 217.7140 - val_loss: 185.1728\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 269.5480 - val_loss: 179.4425\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 225.6017 - val_loss: 149.5601\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 212.1500 - val_loss: 184.8346\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 222.2817 - val_loss: 270.3844\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 312.2259 - val_loss: 211.1409\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 229.7712 - val_loss: 283.3311\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 236.0876 - val_loss: 215.2392\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 213.9095 - val_loss: 172.6273\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 233.1983 - val_loss: 153.1108\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 227.0459 - val_loss: 161.0523\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 238.8183 - val_loss: 169.6383\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 226.1210 - val_loss: 219.4259\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 278.9822 - val_loss: 151.1921\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 202.3310 - val_loss: 232.9100\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 206.3303 - val_loss: 148.7520\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 197.1610 - val_loss: 147.9692\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 271.6740 - val_loss: 157.3396\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 220.9403 - val_loss: 153.8506\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 262.0341 - val_loss: 175.0805\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 189.1621 - val_loss: 167.6934\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 199.2277 - val_loss: 182.7618\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 211.1109 - val_loss: 167.3554\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 196.0328 - val_loss: 159.2677\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 253.8900 - val_loss: 260.1775\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 204.1024 - val_loss: 150.2622\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 226.2520 - val_loss: 264.4184\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 247.7329 - val_loss: 158.4982\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 314.8178 - val_loss: 154.0362\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 204.1529 - val_loss: 153.2059\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 191.5972 - val_loss: 147.4649\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 217.4196 - val_loss: 167.0559\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 228.5633 - val_loss: 230.2066\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 215.7699 - val_loss: 172.9502\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 244.1412 - val_loss: 196.2046\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 285.6540 - val_loss: 147.5262\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 274.8943 - val_loss: 169.5714\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 248.7454 - val_loss: 201.0433\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 204.7893 - val_loss: 198.1553\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 231.9401 - val_loss: 292.1185\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 242.4792 - val_loss: 169.5203\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 215.8869 - val_loss: 152.3726\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 240.8956 - val_loss: 184.0504\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 383.7486 - val_loss: 160.2924\n",
      "Epoch 335/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 199.7835 - val_loss: 154.8967\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 193.9675 - val_loss: 210.6713\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 233.3151 - val_loss: 158.2471\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 200.8341 - val_loss: 157.8760\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 243.8524 - val_loss: 311.2494\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 265.9857 - val_loss: 164.8373\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 193.6872 - val_loss: 151.6626\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 207.3625 - val_loss: 141.7482\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 194.3516 - val_loss: 171.8050\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 215.0109 - val_loss: 153.3559\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 260.3603 - val_loss: 312.3899\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 260.0368 - val_loss: 174.4017\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 200.8975 - val_loss: 156.6071\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 206.2549 - val_loss: 155.8479\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 222.3187 - val_loss: 180.7791\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 201.1272 - val_loss: 149.8376\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 186.6349 - val_loss: 180.1195\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 430.3006 - val_loss: 248.2046\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 252.3431 - val_loss: 195.9875\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 197.0264 - val_loss: 161.0696\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 205.6125 - val_loss: 151.3791\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 205.1975 - val_loss: 156.2680\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 195.5129 - val_loss: 147.7580\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 297.4877 - val_loss: 376.1149\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 200.5459 - val_loss: 222.2310\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 197.2219 - val_loss: 143.7212\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 516.5719 - val_loss: 563.0160\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 326.1553 - val_loss: 212.5188\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 273.5053 - val_loss: 172.5123\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 240.5139 - val_loss: 448.7069\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 318.2321 - val_loss: 153.3235\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 200.5315 - val_loss: 159.3862\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 203.4808 - val_loss: 261.0036\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 211.9996 - val_loss: 143.8027\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 202.9185 - val_loss: 154.6118\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 209.5055 - val_loss: 189.6672\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 260.3953 - val_loss: 229.2448\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 206.6610 - val_loss: 154.1968\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 196.6306 - val_loss: 152.9426\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 207.6238 - val_loss: 139.7264\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 192.6375 - val_loss: 177.8160\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 231.8874 - val_loss: 179.2586\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 210.5737 - val_loss: 180.4503\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 210.9940 - val_loss: 143.8743\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 211.0304 - val_loss: 155.9777\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 217.4275 - val_loss: 149.3294\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 205.8576 - val_loss: 157.0261\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 254.6590 - val_loss: 157.0637\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 191.9049 - val_loss: 207.5442\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 191.1753 - val_loss: 167.1098\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 189.6094 - val_loss: 140.3457\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 417.0923 - val_loss: 142.5647\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 186.1775 - val_loss: 135.1464\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 182.7017 - val_loss: 169.1787\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 214.5888 - val_loss: 162.5233\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 226.3738 - val_loss: 139.0622\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 195.2661 - val_loss: 174.4469\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 193.5044 - val_loss: 151.3389\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 191.5278 - val_loss: 158.3724\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 210.1286 - val_loss: 270.2147\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 223.6281 - val_loss: 153.9317\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 217.5563 - val_loss: 159.7039\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 187.6232 - val_loss: 148.6276\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 225.1405 - val_loss: 280.0201\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 234.4509 - val_loss: 175.7033\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 214.9015 - val_loss: 168.3458\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 222.9177 - val_loss: 170.6468\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 196.0819 - val_loss: 156.3760\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 233.8907 - val_loss: 448.0991\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 198.4594 - val_loss: 139.0715\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 193.9859 - val_loss: 171.1273\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 273.8105 - val_loss: 344.8678\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 392.6635 - val_loss: 148.6723\n",
      "Epoch 408/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 186.1871 - val_loss: 278.0249\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 249.0784 - val_loss: 140.0868\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 204.8170 - val_loss: 249.4131\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 215.6593 - val_loss: 175.4982\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 205.0973 - val_loss: 150.3753\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 290.0747 - val_loss: 426.3700\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 259.2305 - val_loss: 180.7258\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 202.6267 - val_loss: 168.3071\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 237.2462 - val_loss: 176.8270\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 212.3787 - val_loss: 148.4750\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 343.7310 - val_loss: 145.6974\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 205.7106 - val_loss: 157.4336\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 192.7070 - val_loss: 143.5235\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 180.5674 - val_loss: 143.4524\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 184.1792 - val_loss: 150.5239\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 201.4984 - val_loss: 149.2660\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 232.8927 - val_loss: 1371.2698\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 232.1825 - val_loss: 162.3663\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 214.7789 - val_loss: 143.6337\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 188.2127 - val_loss: 152.7719\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 252.9742 - val_loss: 138.7474\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 225.4514 - val_loss: 152.4857\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 201.6248 - val_loss: 195.8284\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 189.6963 - val_loss: 160.1034\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 241.3144 - val_loss: 256.9439\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 210.3252 - val_loss: 228.3194\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 209.8684 - val_loss: 137.3008\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 201.0322 - val_loss: 146.5460\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 186.2123 - val_loss: 153.8047\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 195.8239 - val_loss: 175.7962\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 186.6406 - val_loss: 142.1702\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 406.0279 - val_loss: 160.5035\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 198.2921 - val_loss: 250.0906\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 198.0998 - val_loss: 142.2383\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 186.9314 - val_loss: 149.0439\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 223.6756 - val_loss: 200.8733\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 205.6456 - val_loss: 153.9794\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 195.9257 - val_loss: 162.0820\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 187.0431 - val_loss: 134.4971\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 262.1689 - val_loss: 147.7640\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 199.9998 - val_loss: 189.1520\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 195.1566 - val_loss: 173.0355\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 201.9194 - val_loss: 282.5483\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 207.2592 - val_loss: 143.4739\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 200.8286 - val_loss: 164.9318\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 253.3300 - val_loss: 555.9161\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 198.4907 - val_loss: 144.6652\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 184.7972 - val_loss: 153.6769\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 332.1111 - val_loss: 278.5763\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 215.2699 - val_loss: 157.1287\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 202.9794 - val_loss: 235.7544\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 201.3940 - val_loss: 167.7459\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 202.7426 - val_loss: 169.0331\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 197.4382 - val_loss: 142.5463\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 263.6252 - val_loss: 174.9194\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 230.3185 - val_loss: 179.7019\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 189.7486 - val_loss: 190.9740\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 356.0232 - val_loss: 337.3420\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 300.1550 - val_loss: 196.8883\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 217.3590 - val_loss: 157.4826\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 180.7386 - val_loss: 137.3390\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 200.7634 - val_loss: 229.3481\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 198.5751 - val_loss: 153.0461\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 190.2126 - val_loss: 208.5277\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 222.4836 - val_loss: 181.4272\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 181.6184 - val_loss: 154.6428\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 209.0126 - val_loss: 204.4027\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 197.7660 - val_loss: 193.4348\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 223.9663 - val_loss: 166.3508\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 192.5942 - val_loss: 203.1126\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 193.9644 - val_loss: 197.4148\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 192.5422 - val_loss: 202.6765\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 243.9982 - val_loss: 148.9702\n",
      "Epoch 481/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 199.7257 - val_loss: 144.0248\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 192.3003 - val_loss: 157.8435\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 186.5265 - val_loss: 150.1597\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 223.9995 - val_loss: 189.8273\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 244.9795 - val_loss: 260.3800\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 190.2266 - val_loss: 144.1174\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 188.0077 - val_loss: 177.5103\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 200.9115 - val_loss: 147.1779\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 308.8105 - val_loss: 181.5599\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 196.6328 - val_loss: 187.4588\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 181.9874 - val_loss: 148.2416\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 203.6993 - val_loss: 159.8374\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 178.2842 - val_loss: 145.2594\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 196.4522 - val_loss: 220.6646\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 202.5635 - val_loss: 142.5821\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 182.7775 - val_loss: 150.1972\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 216.5272 - val_loss: 164.8044\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 246.2955 - val_loss: 149.3841\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 192.7403 - val_loss: 132.6515\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 199.4730 - val_loss: 199.7132\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 202.5666 - val_loss: 166.4836\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 202.2642 - val_loss: 177.4946\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 181.6905 - val_loss: 148.9469\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 178.7975 - val_loss: 200.0372\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 226.1713 - val_loss: 169.5390\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 180.0452 - val_loss: 134.5109\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 230.9567 - val_loss: 160.0066\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 198.5747 - val_loss: 134.4459\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 195.3331 - val_loss: 137.0090\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 174.8532 - val_loss: 140.0538\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 186.1082 - val_loss: 175.3661\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 201.3233 - val_loss: 140.9819\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 200.8893 - val_loss: 133.1033\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 187.8112 - val_loss: 170.1220\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 184.9108 - val_loss: 152.1980\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 181.9666 - val_loss: 140.1194\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 426.7072 - val_loss: 165.5573\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 227.5833 - val_loss: 171.3300\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 191.2176 - val_loss: 140.6785\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 238.9957 - val_loss: 228.4883\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 201.2550 - val_loss: 181.1970\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 217.5406 - val_loss: 186.0581\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 215.3857 - val_loss: 361.7877\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 205.8376 - val_loss: 138.1034\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 195.4703 - val_loss: 141.0406\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 218.8654 - val_loss: 173.5208\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 188.2198 - val_loss: 134.0617\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 242.2989 - val_loss: 151.9448\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 248.5755 - val_loss: 185.1941\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 188.7009 - val_loss: 162.0393\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 189.2660 - val_loss: 187.8267\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 267.4818 - val_loss: 136.2302\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 182.9956 - val_loss: 147.1617\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 175.7357 - val_loss: 147.1426\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 192.9750 - val_loss: 138.7889\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 193.3969 - val_loss: 173.4172\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 188.5878 - val_loss: 144.0696\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 203.9365 - val_loss: 152.9974\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 206.2298 - val_loss: 142.8686\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 192.3731 - val_loss: 155.3055\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 193.0471 - val_loss: 208.4989\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 223.2909 - val_loss: 162.3545\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 192.2951 - val_loss: 139.1763\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 224.5092 - val_loss: 136.6699\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 207.9799 - val_loss: 158.7016\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 185.3371 - val_loss: 181.5538\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 196.1349 - val_loss: 144.1718\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 225.0892 - val_loss: 169.2688\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 190.5908 - val_loss: 164.8089\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 199.3132 - val_loss: 142.5181\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 213.3608 - val_loss: 360.0683\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 205.7245 - val_loss: 169.0065\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 189.5337 - val_loss: 150.3055\n",
      "Epoch 554/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 56us/step - loss: 199.0654 - val_loss: 157.2436\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 227.3846 - val_loss: 185.6161\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 214.3596 - val_loss: 145.8538\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 183.7961 - val_loss: 164.3659\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 190.5362 - val_loss: 134.0280\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 198.1565 - val_loss: 187.8700\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 192.9125 - val_loss: 145.6003\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 180.3793 - val_loss: 201.8546\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 187.5969 - val_loss: 141.9591\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 181.5353 - val_loss: 196.9597\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 204.9778 - val_loss: 257.8199\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 176.9682 - val_loss: 290.7057\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 206.9650 - val_loss: 133.4376\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 224.3979 - val_loss: 257.6004\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 281.0170 - val_loss: 133.6489\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 168.7777 - val_loss: 143.2293\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 178.7292 - val_loss: 177.7353\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 207.2194 - val_loss: 134.8943\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 181.5774 - val_loss: 198.7710\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 304.5550 - val_loss: 154.0785\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 204.3233 - val_loss: 215.9865\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 225.8518 - val_loss: 132.9577\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 214.2016 - val_loss: 150.7254\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 191.7956 - val_loss: 161.1646\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 232.7957 - val_loss: 134.4633\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 206.1798 - val_loss: 149.8667\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 216.6148 - val_loss: 166.5548\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 206.4294 - val_loss: 188.4348\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 173.9269 - val_loss: 131.7854\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 193.5775 - val_loss: 138.0647\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 201.6600 - val_loss: 146.5224\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 215.9760 - val_loss: 144.2939\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 182.5811 - val_loss: 145.1031\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 197.5058 - val_loss: 200.1911\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 179.4315 - val_loss: 163.6363\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 189.6298 - val_loss: 352.5364\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 204.3771 - val_loss: 168.8587\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 190.8356 - val_loss: 253.6620\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 244.7921 - val_loss: 146.1255\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 259.9508 - val_loss: 301.0983\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 178.9870 - val_loss: 141.7622\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 179.7161 - val_loss: 130.8789\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 227.1419 - val_loss: 234.3256\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 183.3483 - val_loss: 159.7158\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 197.6659 - val_loss: 140.8263\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 227.7631 - val_loss: 200.6328\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 211.2797 - val_loss: 150.1136\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 180.0775 - val_loss: 199.2917\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 231.8457 - val_loss: 506.7087\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 256.4977 - val_loss: 195.5669\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.4992 - val_loss: 161.1669\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 184.9541 - val_loss: 139.6022\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 242.9406 - val_loss: 133.7830\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 171.1049 - val_loss: 128.9948\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 216.3962 - val_loss: 168.2772\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 180.0967 - val_loss: 160.3075\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 185.3395 - val_loss: 131.4994\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 178.8981 - val_loss: 150.8129\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 184.2152 - val_loss: 139.0378\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 222.9899 - val_loss: 184.1345\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 187.7946 - val_loss: 138.5861\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 186.3772 - val_loss: 140.5573\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 194.6062 - val_loss: 136.6195\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 196.8483 - val_loss: 156.9918\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 271.2080 - val_loss: 186.2597\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 261.1253 - val_loss: 175.0909\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 236.3515 - val_loss: 159.2041\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 198.4862 - val_loss: 153.5025\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 182.7170 - val_loss: 144.7997\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.5504 - val_loss: 140.6017\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 260.4227 - val_loss: 225.0493\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 184.2013 - val_loss: 181.8798\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.9902 - val_loss: 146.7934\n",
      "Epoch 627/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 250.4086 - val_loss: 149.4798\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 174.4930 - val_loss: 138.7691\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 172.9949 - val_loss: 187.7209\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 172.7551 - val_loss: 145.6340\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 339.9187 - val_loss: 298.6485\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 285.0294 - val_loss: 196.3085\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 208.6254 - val_loss: 147.1905\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 218.2617 - val_loss: 138.9683\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 191.5586 - val_loss: 195.3133\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 222.9406 - val_loss: 304.9269\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 287.0242 - val_loss: 153.0407\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 255.2183 - val_loss: 143.9161\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 195.6270 - val_loss: 144.8717\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 173.4937 - val_loss: 137.4315\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 173.1683 - val_loss: 136.4752\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 169.7472 - val_loss: 148.9699\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 176.8636 - val_loss: 137.6372\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 192.6979 - val_loss: 139.1446\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 213.5827 - val_loss: 272.3572\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 189.7180 - val_loss: 127.8978\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 177.1695 - val_loss: 186.0447\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 188.0265 - val_loss: 130.9092\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 260.2649 - val_loss: 186.8730\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 207.2069 - val_loss: 156.4864\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 208.5961 - val_loss: 144.5437\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 303.9349 - val_loss: 138.2770\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 176.8665 - val_loss: 132.8580\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 187.3743 - val_loss: 175.0465\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 188.7078 - val_loss: 126.7994\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 178.1807 - val_loss: 135.7703\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 182.1040 - val_loss: 152.1789\n",
      "Epoch 658/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 221.5224 - val_loss: 210.2226\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 170.8151 - val_loss: 155.8442\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 176.4202 - val_loss: 128.1092\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 197.3170 - val_loss: 135.4321\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 180.5170 - val_loss: 155.9016\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 183.0685 - val_loss: 131.6351\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 221.7621 - val_loss: 141.7118\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 194.2350 - val_loss: 159.0148\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 223.4918 - val_loss: 184.3115\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 194.2829 - val_loss: 140.3624\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 169.4504 - val_loss: 128.3068\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 257.6580 - val_loss: 148.4953\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 167.1449 - val_loss: 149.2092\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 161.0960 - val_loss: 145.0487\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 202.0908 - val_loss: 127.8438\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 170.2381 - val_loss: 136.5133\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 184.8264 - val_loss: 131.5060\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 187.6089 - val_loss: 127.8935\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 172.3124 - val_loss: 141.9326\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 166.2313 - val_loss: 129.9094\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 231.7937 - val_loss: 515.3117\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 227.7821 - val_loss: 129.0495\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 201.9938 - val_loss: 894.9246\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 242.0169 - val_loss: 151.4872\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 177.9247 - val_loss: 171.5490\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 225.4693 - val_loss: 152.4203\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 217.9523 - val_loss: 149.2743\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 213.6156 - val_loss: 202.6094\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 216.9116 - val_loss: 140.8211\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.6712 - val_loss: 133.9511\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.5323 - val_loss: 131.3932\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 196.9031 - val_loss: 126.5917\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 175.3020 - val_loss: 128.9989\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 166.0860 - val_loss: 130.5753\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 200.6084 - val_loss: 176.1692\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 196.3823 - val_loss: 592.4516\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 442.9997 - val_loss: 212.5050\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 251.0747 - val_loss: 218.8555\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 193.6995 - val_loss: 134.2648\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 193.6137 - val_loss: 158.3192\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 183.1793 - val_loss: 134.2456\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 173.8795 - val_loss: 139.5927\n",
      "Epoch 700/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 57us/step - loss: 185.0199 - val_loss: 124.8162\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 165.0668 - val_loss: 131.0164\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 162.1593 - val_loss: 123.7104\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 178.0710 - val_loss: 152.5851\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 201.1521 - val_loss: 135.9301\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 319.6342 - val_loss: 336.5116\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 221.5618 - val_loss: 167.3669\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 188.7189 - val_loss: 130.0960\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 170.1062 - val_loss: 132.8046\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 215.4594 - val_loss: 167.8345\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 172.0671 - val_loss: 147.5041\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 163.1562 - val_loss: 143.5930\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 207.5959 - val_loss: 150.0712\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 167.9938 - val_loss: 134.2026\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 178.7435 - val_loss: 138.7615\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 217.0727 - val_loss: 196.9253\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 211.1363 - val_loss: 163.9835\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 217.3297 - val_loss: 204.1055\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 190.6034 - val_loss: 137.4972\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 194.6358 - val_loss: 177.5592\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 201.3510 - val_loss: 132.6301\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 183.9331 - val_loss: 198.3694\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 194.6224 - val_loss: 142.7008\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 197.7209 - val_loss: 131.2341\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 169.8395 - val_loss: 129.7125\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 197.8161 - val_loss: 135.5895\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 180.4555 - val_loss: 131.6828\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 199.0095 - val_loss: 128.6077\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 184.5019 - val_loss: 162.9507\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 182.6741 - val_loss: 147.5982\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 207.2345 - val_loss: 311.9934\n",
      "Epoch 731/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 185.6052 - val_loss: 147.9086\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 177.5594 - val_loss: 140.9912\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 324.3025 - val_loss: 151.3735\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 162.8120 - val_loss: 132.9804\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 165.6366 - val_loss: 124.3260\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 168.8468 - val_loss: 156.3738\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 200.2463 - val_loss: 145.7339\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 182.4377 - val_loss: 145.2402\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 283.3700 - val_loss: 159.8269\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 180.5544 - val_loss: 150.0074\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.8486 - val_loss: 155.1525\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 174.4863 - val_loss: 183.0342\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 180.0834 - val_loss: 124.1672\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 176.9299 - val_loss: 128.2683\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 169.6195 - val_loss: 171.3062\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 207.2817 - val_loss: 143.0670\n",
      "Epoch 747/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 175.7398 - val_loss: 126.2351\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 163.8345 - val_loss: 137.0029\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 312.8977 - val_loss: 135.0847\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 166.2157 - val_loss: 125.0041\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 169.2734 - val_loss: 127.5957\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 162.1785 - val_loss: 139.3888\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 179.1151 - val_loss: 295.6601\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 166.2504 - val_loss: 125.9854\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 188.2277 - val_loss: 135.2264\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 168.4838 - val_loss: 126.7301\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 184.5625 - val_loss: 139.5370\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 172.7775 - val_loss: 134.8656\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 185.6316 - val_loss: 141.7244\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 250.4208 - val_loss: 247.9926\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 282.6092 - val_loss: 174.0321\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 271.8771 - val_loss: 158.9041\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 195.4184 - val_loss: 136.3016\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 196.0567 - val_loss: 133.8076\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 186.0606 - val_loss: 166.9131\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 218.7143 - val_loss: 252.8199\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 203.0917 - val_loss: 275.1266\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 183.5213 - val_loss: 220.1420\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 179.1139 - val_loss: 179.2924\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 173.6813 - val_loss: 125.1248\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 180.0455 - val_loss: 128.9775\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 180.7751 - val_loss: 126.5969\n",
      "Epoch 773/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 55us/step - loss: 175.2737 - val_loss: 136.2378\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 169.3500 - val_loss: 130.4971\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 218.6320 - val_loss: 132.0642\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 205.1543 - val_loss: 226.3217\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 183.4070 - val_loss: 128.9832\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 163.7820 - val_loss: 139.4821\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 167.9872 - val_loss: 125.2236\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 156.3649 - val_loss: 148.2883\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 240.1287 - val_loss: 152.8827\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 165.5037 - val_loss: 130.9518\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 186.3596 - val_loss: 132.7576\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 167.0789 - val_loss: 133.4062\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 165.8481 - val_loss: 147.7536\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 160.0640 - val_loss: 125.6786\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 341.3562 - val_loss: 138.4585\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 245.2707 - val_loss: 147.3563\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 194.4068 - val_loss: 155.8465\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 174.4255 - val_loss: 132.7192\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 168.0919 - val_loss: 130.4793\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 174.2091 - val_loss: 124.6327\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 158.4607 - val_loss: 162.8110\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 174.1261 - val_loss: 232.9723\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 176.3667 - val_loss: 140.0660\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 161.7157 - val_loss: 130.8755\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 181.8156 - val_loss: 124.2484\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 173.0250 - val_loss: 142.4918\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 250.7062 - val_loss: 316.4837\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 242.4127 - val_loss: 178.8348\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 188.7859 - val_loss: 133.5486\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 173.9266 - val_loss: 127.9717\n",
      "Epoch 00802: early stopping\n",
      "Fold score (RMSE): 10.530600547790527\n",
      "Fold #5\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 15237.2552 - val_loss: 4888.0460\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 5378.7909 - val_loss: 4059.5993\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 5063.3765 - val_loss: 3798.3516\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 4899.0091 - val_loss: 3736.9157\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 4581.7889 - val_loss: 3651.5650\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 4472.0248 - val_loss: 3417.1189\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 4365.8161 - val_loss: 3332.2586\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 4135.9256 - val_loss: 3404.3383\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 4097.3638 - val_loss: 3498.0466\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 3951.7357 - val_loss: 3223.1656\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 3836.2792 - val_loss: 3331.9678\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 3721.6219 - val_loss: 3012.4343\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 3838.7893 - val_loss: 2859.8152\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 3432.7416 - val_loss: 4380.4892\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 3398.4783 - val_loss: 3287.8797\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 3255.4470 - val_loss: 2797.2454\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 2937.8806 - val_loss: 2150.7104\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 2926.6387 - val_loss: 2848.3771\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 2555.0027 - val_loss: 1839.6259\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 2457.1119 - val_loss: 1676.7463\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 2112.4680 - val_loss: 1378.2637\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 2018.4389 - val_loss: 1415.0574\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1737.6453 - val_loss: 1459.5533\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 1795.3753 - val_loss: 979.7975\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1388.0039 - val_loss: 1016.5548\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1197.5739 - val_loss: 1088.3863\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 1314.3368 - val_loss: 710.7537\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 997.3465 - val_loss: 641.8813\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 962.1679 - val_loss: 620.1607\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 915.7417 - val_loss: 692.9583\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 987.5032 - val_loss: 3177.7549\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 839.3062 - val_loss: 538.9457\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 995.9481 - val_loss: 873.1307\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 732.9788 - val_loss: 522.7197\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 771.0050 - val_loss: 481.5731\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 759.1107 - val_loss: 413.9888\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 740.0500 - val_loss: 668.1183\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 678.6776 - val_loss: 529.3083\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 681.3560 - val_loss: 399.6058\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 643.8675 - val_loss: 394.8395\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 687.1004 - val_loss: 376.1952\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 665.2716 - val_loss: 708.0535\n",
      "Epoch 43/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 66us/step - loss: 579.5187 - val_loss: 391.1043\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 533.0719 - val_loss: 743.3650\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 652.7924 - val_loss: 363.5605\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 545.7218 - val_loss: 334.2523\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 475.6481 - val_loss: 323.3740\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 593.7055 - val_loss: 335.5636\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 502.6638 - val_loss: 311.8110\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 481.9155 - val_loss: 339.8681\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 518.5355 - val_loss: 343.0354\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 436.3002 - val_loss: 719.0870\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 514.9232 - val_loss: 346.8038\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 511.9401 - val_loss: 626.6616\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 514.7776 - val_loss: 286.9749\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 525.8207 - val_loss: 293.7141\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 410.6762 - val_loss: 283.5948\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 402.2932 - val_loss: 341.9462\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 489.7548 - val_loss: 292.6072\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 446.2676 - val_loss: 316.8810\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 433.5254 - val_loss: 331.8941\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 480.5650 - val_loss: 376.6313\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 435.9220 - val_loss: 348.8938\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 420.8942 - val_loss: 370.8201\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 424.7248 - val_loss: 243.6312\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 422.7050 - val_loss: 261.0937\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 413.8056 - val_loss: 248.5445\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 375.9392 - val_loss: 296.9391\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 390.4798 - val_loss: 364.5947\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 368.9427 - val_loss: 601.0830\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 464.8426 - val_loss: 259.0807\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 398.0491 - val_loss: 246.7414\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 440.5331 - val_loss: 246.4109\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 383.0228 - val_loss: 386.8719\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 432.8759 - val_loss: 305.1677\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 389.8660 - val_loss: 369.2110\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 432.6196 - val_loss: 590.5967\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 459.7782 - val_loss: 445.0826\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 355.0487 - val_loss: 354.1906\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 353.4287 - val_loss: 285.4032\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 362.5113 - val_loss: 247.9612\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 416.7054 - val_loss: 246.9375\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 354.6067 - val_loss: 266.5029\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 374.4967 - val_loss: 259.7686\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 360.2930 - val_loss: 270.1295\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 357.5990 - val_loss: 265.4472\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 325.6874 - val_loss: 226.4223\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 381.6216 - val_loss: 228.2101\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 360.9854 - val_loss: 301.8677\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 344.8090 - val_loss: 271.5572\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 342.4038 - val_loss: 267.3931\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 360.9370 - val_loss: 288.9124\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 381.6671 - val_loss: 467.0755\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 376.5231 - val_loss: 514.9414\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 390.4900 - val_loss: 239.2922\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 345.4112 - val_loss: 205.2844\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 422.2776 - val_loss: 417.4894\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 293.4706 - val_loss: 277.4592\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 455.8709 - val_loss: 209.9426\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 318.0384 - val_loss: 205.3155\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 370.7362 - val_loss: 258.6482\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 312.0652 - val_loss: 315.8523\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 332.5570 - val_loss: 253.0388\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 326.9571 - val_loss: 445.2084\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 436.2120 - val_loss: 1215.3780\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 421.5123 - val_loss: 277.3275\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 305.6610 - val_loss: 220.8768\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 316.5082 - val_loss: 428.3669\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 355.8805 - val_loss: 290.8289\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 291.8590 - val_loss: 239.4285\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 316.5990 - val_loss: 299.9856\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 302.6470 - val_loss: 220.3349\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 333.2467 - val_loss: 238.6324\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 311.2739 - val_loss: 407.3566\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 349.1269 - val_loss: 224.7630\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 297.5816 - val_loss: 213.4751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 303.3643 - val_loss: 200.4073\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 279.2296 - val_loss: 208.4546\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 294.0148 - val_loss: 207.7133\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 338.8406 - val_loss: 188.5180\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 327.0331 - val_loss: 451.1933\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 298.4565 - val_loss: 199.3454\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 297.8372 - val_loss: 212.5485\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 288.4605 - val_loss: 329.9680\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 314.6357 - val_loss: 203.7836\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 290.7279 - val_loss: 525.8084\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 332.8103 - val_loss: 271.6534\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 314.6483 - val_loss: 192.9176\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 280.7824 - val_loss: 519.4078\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 307.3275 - val_loss: 190.7915\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 342.4658 - val_loss: 416.6229\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 327.3812 - val_loss: 187.4481\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 286.0597 - val_loss: 262.9295\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 342.2070 - val_loss: 257.9171\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 337.8027 - val_loss: 189.0032\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 287.6610 - val_loss: 320.8488\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 286.8812 - val_loss: 183.7922\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 301.0612 - val_loss: 200.4568\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 276.9989 - val_loss: 214.9908\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 353.2091 - val_loss: 175.8800\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 358.4298 - val_loss: 226.9862\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 324.6859 - val_loss: 208.5163\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 281.8113 - val_loss: 219.2166\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 267.7747 - val_loss: 190.2287\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 295.9538 - val_loss: 192.4981\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 282.6425 - val_loss: 239.2332\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 316.9537 - val_loss: 176.5144\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 271.6042 - val_loss: 179.2512\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 303.2366 - val_loss: 198.9533\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 340.1472 - val_loss: 381.4786\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 274.5957 - val_loss: 327.1612\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 272.3558 - val_loss: 275.3205\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 269.4621 - val_loss: 199.9662\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 301.1097 - val_loss: 235.3800\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 294.3329 - val_loss: 259.0556\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 269.1694 - val_loss: 221.8205\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 260.2388 - val_loss: 240.3190\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 253.1326 - val_loss: 199.0395\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 283.3334 - val_loss: 239.0798\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 275.3836 - val_loss: 184.6708\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 320.7894 - val_loss: 200.5427\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 294.6543 - val_loss: 490.5217\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 301.3328 - val_loss: 200.7666\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 219.3691 - val_loss: 226.9632\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 288.8083 - val_loss: 336.5559\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 281.0231 - val_loss: 174.0995\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 266.0562 - val_loss: 209.8095\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 252.7313 - val_loss: 194.6865\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 304.4970 - val_loss: 174.8939\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 290.8333 - val_loss: 203.4060\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 280.0999 - val_loss: 341.2433\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 527.3164 - val_loss: 348.8433\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 372.7801 - val_loss: 363.7134\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 338.0866 - val_loss: 263.7568\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 327.0776 - val_loss: 190.5137\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 304.1372 - val_loss: 200.4758\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 342.4084 - val_loss: 225.3053\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 318.4065 - val_loss: 193.1360\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 287.5535 - val_loss: 287.2754\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 267.582 - 0s 57us/step - loss: 267.3270 - val_loss: 173.0948\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 287.8239 - val_loss: 293.8942\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 297.0124 - val_loss: 292.1009\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 274.7543 - val_loss: 175.0991\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 268.1309 - val_loss: 182.4843\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 281.1177 - val_loss: 256.4398\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 249.2293 - val_loss: 197.9363\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 319.4928 - val_loss: 173.4469\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 321.3714 - val_loss: 173.6569\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 278.7371 - val_loss: 272.2262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 250.8159 - val_loss: 170.6979\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 266.4237 - val_loss: 200.5782\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 257.1043 - val_loss: 211.3023\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 256.0356 - val_loss: 215.1753\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 276.8350 - val_loss: 181.7793\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 379.2979 - val_loss: 277.6853\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 417.2015 - val_loss: 261.0164\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 265.3008 - val_loss: 234.2851\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 235.6953 - val_loss: 169.0624\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 245.0921 - val_loss: 187.2460\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 226.6827 - val_loss: 158.9210\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 289.2377 - val_loss: 240.3493\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 634.6390 - val_loss: 188.5305\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 281.4985 - val_loss: 243.3563\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 258.1072 - val_loss: 190.6092\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 230.0367 - val_loss: 289.5335\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 313.7988 - val_loss: 201.5789\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 275.3012 - val_loss: 186.7399\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 280.2399 - val_loss: 270.0669\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 240.4340 - val_loss: 178.8669\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 237.4469 - val_loss: 221.8960\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 323.6478 - val_loss: 215.2903\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 233.5692 - val_loss: 185.0964\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 231.4444 - val_loss: 171.9617\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 274.4500 - val_loss: 249.6538\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 243.1363 - val_loss: 213.7641\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 381.5395 - val_loss: 462.6676\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 255.3046 - val_loss: 191.1685\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 304.8979 - val_loss: 203.6715\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 272.9768 - val_loss: 199.9484\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 243.1998 - val_loss: 196.6592\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 283.3893 - val_loss: 159.0728\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 267.6002 - val_loss: 162.6639\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 246.5730 - val_loss: 185.1419\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 239.9780 - val_loss: 156.9009\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 261.8576 - val_loss: 157.1382\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 290.3776 - val_loss: 158.8050\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 281.7291 - val_loss: 317.6617\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 412.3142 - val_loss: 168.7705\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 251.8235 - val_loss: 177.4116\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 218.5284 - val_loss: 156.6920\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 249.1051 - val_loss: 181.2709\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 280.6368 - val_loss: 310.3821\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 244.8985 - val_loss: 189.9414\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 287.8961 - val_loss: 342.2857\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 251.0266 - val_loss: 186.4283\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 237.5829 - val_loss: 192.5019\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 255.8560 - val_loss: 189.1155\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 225.1623 - val_loss: 202.8027\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 281.0031 - val_loss: 296.6240\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 291.0897 - val_loss: 247.2915\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 257.9914 - val_loss: 159.9737\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 254.2749 - val_loss: 157.2258\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 277.5831 - val_loss: 214.5959\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 223.5403 - val_loss: 315.2610\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 258.2509 - val_loss: 153.7312\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 304.3687 - val_loss: 149.0415\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 206.3851 - val_loss: 165.6743\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 269.8058 - val_loss: 326.1113\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 234.6372 - val_loss: 182.9628\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 211.9246 - val_loss: 154.4744\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 236.3365 - val_loss: 277.9846\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 231.0347 - val_loss: 175.7826\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 231.2410 - val_loss: 256.4521\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 301.0547 - val_loss: 190.9238\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 272.6415 - val_loss: 160.8080\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 256.8848 - val_loss: 248.4326\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 219.5414 - val_loss: 183.8662\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 809.2895 - val_loss: 321.6003\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 356.9532 - val_loss: 259.4369\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 313.6645 - val_loss: 240.5112\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 287.0482 - val_loss: 169.8912\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 260.7159 - val_loss: 172.0204\n",
      "Epoch 263/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 294.2128 - val_loss: 178.9618\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 262.2457 - val_loss: 184.2750\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 263.0855 - val_loss: 736.0339\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 320.1837 - val_loss: 174.2123\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 260.1659 - val_loss: 162.7452\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 246.4268 - val_loss: 156.3244\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 268.7860 - val_loss: 174.2113\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 251.7035 - val_loss: 195.1298\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 250.4296 - val_loss: 166.8403\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 314.3732 - val_loss: 170.0479\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 258.7704 - val_loss: 181.4644\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 232.3587 - val_loss: 159.2804\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 236.1356 - val_loss: 193.4864\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 244.9539 - val_loss: 311.1556\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 268.7477 - val_loss: 733.3296\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 282.6417 - val_loss: 212.8091\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 222.6055 - val_loss: 174.0040\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 240.4127 - val_loss: 324.7187\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 260.7712 - val_loss: 173.9965\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 252.8655 - val_loss: 157.8296\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 236.9520 - val_loss: 200.9548\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 263.0851 - val_loss: 166.9940\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 234.9986 - val_loss: 195.8650\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 246.7792 - val_loss: 162.2394\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 234.4736 - val_loss: 176.5753\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 252.7471 - val_loss: 226.1340\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 246.1303 - val_loss: 253.8723\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 223.7333 - val_loss: 319.0181\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 295.6590 - val_loss: 160.2075\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 246.5759 - val_loss: 360.3327\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 240.0213 - val_loss: 228.0007\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 241.1254 - val_loss: 152.7695\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 213.2276 - val_loss: 156.6276\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 259.7555 - val_loss: 341.3784\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 233.3615 - val_loss: 149.9512\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 215.3539 - val_loss: 268.6462\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 215.7519 - val_loss: 174.1214\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 305.0407 - val_loss: 284.5611\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 224.4137 - val_loss: 157.8344\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 244.2986 - val_loss: 147.8158\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 237.6636 - val_loss: 144.3557\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 382.9303 - val_loss: 160.3500\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 218.2164 - val_loss: 165.3260\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 243.1816 - val_loss: 237.4482\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 216.2559 - val_loss: 224.3840\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 229.0549 - val_loss: 170.7786\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 210.6953 - val_loss: 153.2576\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 231.5938 - val_loss: 145.2271\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 249.6623 - val_loss: 152.8417\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 196.2262 - val_loss: 147.2064\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 225.2299 - val_loss: 147.5477\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 257.5394 - val_loss: 165.5436\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 222.8648 - val_loss: 242.5021\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 269.9473 - val_loss: 171.1476\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 202.0013 - val_loss: 169.2079\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 206.9718 - val_loss: 143.7616\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 236.9256 - val_loss: 154.6587\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 239.7484 - val_loss: 181.2738\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 228.1547 - val_loss: 374.3025\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 258.4109 - val_loss: 169.5176\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 220.6995 - val_loss: 143.6815\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 200.2043 - val_loss: 157.1719\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 240.8255 - val_loss: 250.1997\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 251.5274 - val_loss: 160.1971\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 197.0364 - val_loss: 159.9094\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 210.1934 - val_loss: 182.5847\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 207.4977 - val_loss: 192.0227\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 209.2335 - val_loss: 184.7154\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 217.3805 - val_loss: 146.4799\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 213.3014 - val_loss: 250.8607\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 220.6336 - val_loss: 211.2706\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 227.3354 - val_loss: 142.0230\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 230.9818 - val_loss: 163.0646\n",
      "Epoch 336/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 214.1609 - val_loss: 169.1652\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 202.7111 - val_loss: 176.4627\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 214.5856 - val_loss: 156.0571\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 228.8404 - val_loss: 228.6073\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 232.0522 - val_loss: 144.6087\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 227.4078 - val_loss: 141.0219\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 194.8645 - val_loss: 183.4460\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 192.1028 - val_loss: 143.9101\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 234.6500 - val_loss: 173.8829\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 259.1835 - val_loss: 186.8392\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 212.6507 - val_loss: 143.8576\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 201.8158 - val_loss: 171.6128\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 232.3336 - val_loss: 156.0119\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 201.8155 - val_loss: 274.5794\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 221.6904 - val_loss: 224.7387\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 203.8769 - val_loss: 142.3748\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 220.2658 - val_loss: 163.3000\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 201.2970 - val_loss: 146.7208\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 219.9538 - val_loss: 172.3092\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 250.9676 - val_loss: 140.5959\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 190.8112 - val_loss: 142.6319\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 228.2819 - val_loss: 142.2734\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 226.0211 - val_loss: 234.6498\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 211.6798 - val_loss: 284.4473\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 217.6878 - val_loss: 158.9375\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 198.0332 - val_loss: 152.1055\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 199.7768 - val_loss: 203.0959\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 211.7176 - val_loss: 303.9251\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 226.5149 - val_loss: 168.3671\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 222.4555 - val_loss: 156.8594\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 222.0234 - val_loss: 170.9185\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 205.6946 - val_loss: 170.9201\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.4139 - val_loss: 144.3018\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 195.4481 - val_loss: 148.6421\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 197.3429 - val_loss: 159.6151\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 234.4233 - val_loss: 312.9988\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 206.8900 - val_loss: 204.5944\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 217.5381 - val_loss: 150.6081\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 238.9490 - val_loss: 142.0083\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 238.9980 - val_loss: 144.6303\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 213.6414 - val_loss: 145.6728\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 246.1737 - val_loss: 248.9433\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 205.5182 - val_loss: 185.7145\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 204.4872 - val_loss: 186.8867\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 200.3909 - val_loss: 146.1667\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 210.6552 - val_loss: 171.8835\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 213.0051 - val_loss: 164.2881\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 252.6820 - val_loss: 360.6827\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 233.2309 - val_loss: 279.2340\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 215.1281 - val_loss: 1860.3773\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 287.4598 - val_loss: 362.1153\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 190.2067 - val_loss: 157.8761\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 191.1727 - val_loss: 143.0190\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 208.8374 - val_loss: 141.0588\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 190.5224 - val_loss: 145.8731\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 224.0697 - val_loss: 192.7781\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 243.2002 - val_loss: 544.8488\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 211.0697 - val_loss: 140.6713\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 180.3333 - val_loss: 161.2284\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 199.7311 - val_loss: 209.5446\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 199.2980 - val_loss: 186.7098\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 222.7490 - val_loss: 137.3368\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 203.7225 - val_loss: 275.4288\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 190.3778 - val_loss: 141.2492\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 199.3649 - val_loss: 153.8668\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 199.0252 - val_loss: 168.8646\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 197.2991 - val_loss: 195.0362\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 241.2767 - val_loss: 193.7988\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 212.5880 - val_loss: 153.2767\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 209.3262 - val_loss: 186.3876\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 184.8138 - val_loss: 156.3069\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 188.2820 - val_loss: 147.6920\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 217.5756 - val_loss: 172.2232\n",
      "Epoch 409/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 189.5982 - val_loss: 223.5427\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 210.6459 - val_loss: 171.9331\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 191.7775 - val_loss: 137.6152\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 227.5709 - val_loss: 146.0837\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 189.2655 - val_loss: 210.8279\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 207.6368 - val_loss: 139.5763\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 219.8391 - val_loss: 364.7662\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 284.0499 - val_loss: 150.6107\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 197.2691 - val_loss: 137.0518\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 191.4281 - val_loss: 163.1944\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 191.9273 - val_loss: 137.2526\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 172.9321 - val_loss: 136.1331\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 197.8446 - val_loss: 144.3957\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 204.5496 - val_loss: 212.4701\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 187.7171 - val_loss: 148.3342\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 183.8498 - val_loss: 141.9224\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 244.5303 - val_loss: 313.4064\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 190.8396 - val_loss: 134.8978\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 199.4661 - val_loss: 153.0326\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 260.6486 - val_loss: 143.7483\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 187.4778 - val_loss: 152.4188\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 201.8955 - val_loss: 165.9002\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 203.8343 - val_loss: 135.2575\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 233.3871 - val_loss: 516.0129\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 193.0385 - val_loss: 282.5552\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 184.5505 - val_loss: 147.5753\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 288.1503 - val_loss: 166.2755\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 194.3978 - val_loss: 164.4770\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 196.5301 - val_loss: 144.6322\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 197.2110 - val_loss: 164.1650\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 233.5111 - val_loss: 203.2299\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 182.3487 - val_loss: 159.9010\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 186.2918 - val_loss: 146.6158\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 195.5446 - val_loss: 165.4761\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 202.2699 - val_loss: 198.0043\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 191.9006 - val_loss: 165.6526\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 199.7706 - val_loss: 241.8773\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 189.0790 - val_loss: 235.8688\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 249.5440 - val_loss: 147.4737\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 211.2429 - val_loss: 183.5897\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 191.8139 - val_loss: 184.3824\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 215.1452 - val_loss: 160.4463\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 178.1594 - val_loss: 191.6773\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 231.2367 - val_loss: 134.9034\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 174.9831 - val_loss: 177.0637\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 198.6844 - val_loss: 134.7473\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 226.5126 - val_loss: 132.6452\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 184.3579 - val_loss: 143.8718\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 222.7675 - val_loss: 405.3733\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 212.1008 - val_loss: 144.9142\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 210.0329 - val_loss: 141.8785\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 225.6014 - val_loss: 139.7249\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 187.4066 - val_loss: 134.4664\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 183.2500 - val_loss: 159.3259\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 206.6145 - val_loss: 192.2974\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 183.5562 - val_loss: 133.2137\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 167.6422 - val_loss: 189.8966\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 200.2477 - val_loss: 150.0253\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 211.2400 - val_loss: 146.9358\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 256.4403 - val_loss: 140.7455\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 179.7286 - val_loss: 142.3679\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 211.7804 - val_loss: 135.7270\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 196.7032 - val_loss: 139.3867\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 200.1186 - val_loss: 140.5774\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.6386 - val_loss: 144.1039\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 201.7465 - val_loss: 202.7412\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 291.9534 - val_loss: 147.8417\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 172.0105 - val_loss: 180.1645\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 175.9291 - val_loss: 219.8016\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 203.3433 - val_loss: 244.5660\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 180.9062 - val_loss: 157.6195\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 194.7458 - val_loss: 149.3731\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 186.8831 - val_loss: 149.4335\n",
      "Epoch 482/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 55us/step - loss: 284.5790 - val_loss: 181.0564\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 295.7732 - val_loss: 243.3333\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 236.2131 - val_loss: 163.9238\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 218.6311 - val_loss: 203.6891\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 213.2354 - val_loss: 168.8732\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 241.4776 - val_loss: 198.1552\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 221.2945 - val_loss: 142.4323\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 202.8567 - val_loss: 338.3459\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 222.6904 - val_loss: 135.4874\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 210.7894 - val_loss: 157.0705\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 206.4886 - val_loss: 168.7740\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 208.1362 - val_loss: 215.7654\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 222.7412 - val_loss: 202.2920\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 186.3482 - val_loss: 139.8966\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 226.5056 - val_loss: 190.5944\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 198.2362 - val_loss: 134.5821\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 216.1651 - val_loss: 159.5663\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 228.0182 - val_loss: 148.7493\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 202.7464 - val_loss: 343.3847\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 196.5305 - val_loss: 134.0428\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 199.1979 - val_loss: 134.7624\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 196.3204 - val_loss: 167.8772\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 185.5315 - val_loss: 138.6153\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 209.6637 - val_loss: 150.0708\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 212.2192 - val_loss: 250.6989\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 196.4405 - val_loss: 183.2377\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 219.0046 - val_loss: 139.0084\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 190.1743 - val_loss: 155.4601\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 175.7244 - val_loss: 236.0319\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 183.3984 - val_loss: 143.2967\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 209.2773 - val_loss: 148.9288\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 186.6094 - val_loss: 140.1222\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 187.2933 - val_loss: 160.1934\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 255.2274 - val_loss: 235.0626\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 206.7368 - val_loss: 139.8239\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 185.8007 - val_loss: 141.2440\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 192.6567 - val_loss: 149.1801\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 201.8580 - val_loss: 136.1694\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 189.5015 - val_loss: 142.6547\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 191.5631 - val_loss: 136.5118\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 253.2072 - val_loss: 129.2456\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 207.1231 - val_loss: 193.3546\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 189.8427 - val_loss: 153.4258\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 179.6142 - val_loss: 143.9188\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 180.7658 - val_loss: 169.5116\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 198.3203 - val_loss: 197.0126\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 180.7963 - val_loss: 136.9709\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 183.8055 - val_loss: 220.2800\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 260.1554 - val_loss: 133.1137\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 184.9575 - val_loss: 210.9656\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 192.7702 - val_loss: 128.9128\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 194.7291 - val_loss: 139.2325\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 195.8928 - val_loss: 153.2659\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 186.2897 - val_loss: 157.1696\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 182.0741 - val_loss: 138.0068\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 193.6437 - val_loss: 145.0916\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 253.0591 - val_loss: 151.6464\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 215.1308 - val_loss: 193.5730\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 181.2564 - val_loss: 139.2046\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 192.6139 - val_loss: 137.8162\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 185.9245 - val_loss: 155.7786\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 202.6207 - val_loss: 150.5494\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 205.1090 - val_loss: 173.5280\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 215.3727 - val_loss: 302.8093\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 220.5005 - val_loss: 152.7223\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 211.9818 - val_loss: 217.3512\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 203.7707 - val_loss: 153.7395\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 184.7208 - val_loss: 136.2662\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 192.8797 - val_loss: 133.3035\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 186.0907 - val_loss: 141.9292\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 203.7868 - val_loss: 135.7458\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 188.0267 - val_loss: 156.9952\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 190.0047 - val_loss: 162.7754\n",
      "Epoch 555/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 243.6618 - val_loss: 166.5235\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 183.7218 - val_loss: 132.6357\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 172.4459 - val_loss: 196.5880\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 199.9521 - val_loss: 148.1248\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 173.5342 - val_loss: 203.8592\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 170.3446 - val_loss: 148.0184\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 179.2436 - val_loss: 141.4069\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 179.1313 - val_loss: 135.0692\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 192.3773 - val_loss: 149.3904\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 269.8275 - val_loss: 144.5545\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 169.1019 - val_loss: 140.4848\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 172.2320 - val_loss: 129.4478\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 183.6316 - val_loss: 157.8065\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 238.2227 - val_loss: 130.8500\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 176.8668 - val_loss: 134.9372\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 176.2095 - val_loss: 138.7373\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 197.4266 - val_loss: 140.7597\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 189.6668 - val_loss: 156.2615\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 188.6018 - val_loss: 218.0828\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 202.5597 - val_loss: 306.9780\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 212.2214 - val_loss: 145.1092\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 182.0179 - val_loss: 142.9486\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 170.3407 - val_loss: 162.0044\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 195.4560 - val_loss: 437.4618\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.7582 - val_loss: 130.8400\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 244.9694 - val_loss: 2443.6810\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 340.9737 - val_loss: 145.2195\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 190.7678 - val_loss: 148.4874\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 227.4287 - val_loss: 142.1086\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 208.1302 - val_loss: 163.9192\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 199.1491 - val_loss: 152.5897\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 186.5520 - val_loss: 161.1471\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 184.4368 - val_loss: 190.2776\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 196.3123 - val_loss: 151.9658\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 183.5279 - val_loss: 144.2128\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 186.4797 - val_loss: 137.8315\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 181.1358 - val_loss: 207.5514\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 198.6660 - val_loss: 151.3911\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 206.4126 - val_loss: 156.8707\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 193.6244 - val_loss: 161.9199\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 210.4162 - val_loss: 144.3030\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 195.6374 - val_loss: 158.3037\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 231.4720 - val_loss: 148.9043\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 196.4178 - val_loss: 146.4809\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 190.9513 - val_loss: 137.7060\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 188.4187 - val_loss: 200.6972\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 180.3260 - val_loss: 229.5582\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 202.9598 - val_loss: 170.0925\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 172.4359 - val_loss: 145.6443\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 193.3425 - val_loss: 136.3198\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 190.9009 - val_loss: 137.4315\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 194.3672 - val_loss: 175.4044\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 186.6378 - val_loss: 134.5501\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 208.0039 - val_loss: 233.6953\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 193.9986 - val_loss: 143.0260\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 216.0337 - val_loss: 188.3908\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 196.0041 - val_loss: 196.8254\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 223.9951 - val_loss: 140.1328\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 258.3517 - val_loss: 193.8565\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 200.1445 - val_loss: 135.4190\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 200.9474 - val_loss: 136.2125\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 208.7027 - val_loss: 149.6641\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 177.3801 - val_loss: 154.7564\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 173.5430 - val_loss: 132.8837\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 305.4022 - val_loss: 136.3012\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 172.2566 - val_loss: 193.2423\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 183.1357 - val_loss: 156.1227\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 201.7229 - val_loss: 176.1690\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 177.0194 - val_loss: 129.5002\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 193.1463 - val_loss: 132.6622\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 167.4022 - val_loss: 195.9100\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 189.8159 - val_loss: 129.2652\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 186.1359 - val_loss: 149.9029\n",
      "Epoch 628/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 196.4917 - val_loss: 145.3059\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 201.8019 - val_loss: 173.6750\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 183.9776 - val_loss: 143.1315\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 183.4236 - val_loss: 187.7093\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 225.1083 - val_loss: 245.7738\n",
      "Epoch 00632: early stopping\n",
      "Fold score (RMSE): 15.040975570678711\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=filename_checkpoint, verbose=0, save_best_only=True)\n",
    "\n",
    "# Turn off KFold\n",
    "#if (0):\n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "    \n",
    "for train, test in kf.split(x):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "    model.add(Dropout(0.01)) # Dropout Layer\n",
    "    model.add(Dense(25, activation='relu')) # Hidden 2\n",
    "    model.add(Dense(10, \n",
    "                kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01),activation='relu')) # Hidden 3 w/regularization\n",
    "    model.add(Dense(1)) # Output\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=100, verbose=1, mode='auto')\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpoint],verbose=1,epochs=10000)\n",
    "    \n",
    "    pred = model.predict(x_test)\n",
    "    \n",
    "    oos_y.append(y_test)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure this fold's RMSE\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "    print(\"Fold score (RMSE): {}\".format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final, out of sample score (RMSE): 12.102914810180664\n"
     ]
    }
   ],
   "source": [
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))\n",
    "print(\"Final, out of sample score (RMSE): {}\".format(score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#pred = model.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "#score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "#print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "#chart_regression(pred.flatten(),y_test)\n",
    "#chart_regression(pred.flatten(),y_test,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(filename_checkpoint)\n",
    "\n",
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "extract_and_encode_features(df_test)\n",
    "\n",
    "ids_test = df_test['id']\n",
    "df_test.drop('id',1,inplace=True)\n",
    "\n",
    "names_test = df_test['name']\n",
    "df_test.drop('name',1,inplace=True)\n",
    "\n",
    "x_submit = df_test.as_matrix().astype(np.float32)\n",
    "pred_submit = model.predict(x_submit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = [n if n > 0 else 0 for n in pred_submit[:,0]]\n",
    "df_submit = pd.DataFrame({'id': ids_test,'cost': cost})\n",
    "df_submit = df_submit[['id', 'cost']]\n",
    "df_submit.to_csv(filename_submit, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
