{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n",
    "\n",
    "**Kaggle Assignment: **\n",
    "\n",
    "**Student Name: Jason Walker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## Description\n",
    "This is one of the projects from the course T81-855: Applications of Deep Learning at Washington University in St. Louis. All students must create a Kaggle account and submit a solution. Once you have submitted your solution entry log into Blackboard (at WUSTL) and submit a single file telling me your Kaggle name on the leaderboard (you do not need to register to Kaggle with your real name). This competition will be visible to the public, so there may be non-student submissions as well as student.\n",
    "\n",
    "The data set for this competition consists of a number of input columns that should be used to predict a stores sales. This is a regression problem. The inputs are a mixture of discrete and category values. The data set is from a simulation.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The evaluation pages describes how submissions will be scored and how students should format their submissions. The scores are in RMSE.\n",
    "Submission Format\n",
    "\n",
    "For every store in the dataset, submission files should contain a sales volume.\n",
    "\n",
    "The file should contain a header and have the following format:\n",
    "\n",
    "```\n",
    "100000,1.23\n",
    "100001,1.123\n",
    "100002,3.332\n",
    "100003,1.53\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The data contains data and costs for various office supplies. The data came from a simulation and do not directly correspond to any real-world items. See how well you can predict the cost of an item using the provided data. Feature engineering will likely help you. The *name* column may seem useless at first glance; however, it contains information that you can parse to help your predictions.\n",
    "File descriptions\n",
    "```\n",
    "    id - The identifier/primary key.\n",
    "    name - The name of this item.\n",
    "    manufacturer - The manufacturer.\n",
    "    pack - The number of items in this pack.\n",
    "    weight - The weight of a pack of these items.\n",
    "    height - The height of a pack of these items.\n",
    "    width - The width of a pack of these items.\n",
    "    length - The length of a pack of these items.\n",
    "    cost - The cost for this item pack. This is what you are to predict (the target). \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions\n",
    "\n",
    "You will see these at the top of every module and assignment.  These are simply a set of reusable functions that we will make use of.  Each of them will be explained as the semester progresses.  They are explained in greater detail as the course progresses.  Class 4 contains a complete overview of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n",
    "        \n",
    "# This function submits an assignment.  You can submit an assignment as much as you like, only the final\n",
    "# submission counts.  The paramaters are as follows:\n",
    "# data - Pandas dataframe output.\n",
    "# key - Your student key that was emailed to you.\n",
    "# no - The assignment class number, should be 1 through 1.\n",
    "# source_file - The full path to your Python or IPYNB file.  This must have \"_class1\" as part of its name.  \n",
    "# .             The number must match your assignment number.  For example \"_class2\" for class assignment #2.\n",
    "def submit(data,key,no,source_file=None):\n",
    "    if source_file is None and '__file__' not in globals(): raise Exception('Must specify a filename when a Jupyter notebook.')\n",
    "    if source_file is None: source_file = __file__\n",
    "    suffix = '_class{}'.format(no)\n",
    "    if suffix not in source_file: raise Exception('{} must be part of the filename.'.format(suffix))\n",
    "    with open(source_file, \"rb\") as image_file:\n",
    "        encoded_python = base64.b64encode(image_file.read()).decode('ascii')\n",
    "    ext = os.path.splitext(source_file)[-1].lower()\n",
    "    if ext not in ['.ipynb','.py']: raise Exception(\"Source file is {} must be .py or .ipynb\".format(ext))\n",
    "    r = requests.post(\"https://api.heatonresearch.com/assignment-submit\",\n",
    "        headers={'x-api-key':key}, json={'csv':base64.b64encode(data.to_csv(index=False).encode('ascii')).decode(\"ascii\"),\n",
    "        'assignment': no, 'ext':ext, 'py':encoded_python})\n",
    "    if r.status_code == 200:\n",
    "        print(\"Success: {}\".format(r.text))\n",
    "    else: print(\"Failure: {}\".format(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kaggle Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwalker/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers\n",
    "\n",
    "path = './data'\n",
    "\n",
    "filename_test = os.path.join(path,\"test.csv\")\n",
    "filename_train = os.path.join(path,\"train.csv\")\n",
    "filename_sample = os.path.join(path,\"sample.csv\")\n",
    "filename_submit = os.path.join(path,\"submit.csv\")\n",
    "filename_checkpoint = os.path.join(path,\"checkpoint.hdf5\")\n",
    "\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "\n",
    "np.random.seed(42) # Uncomment this line to get the same shuffle each time\n",
    "df_train = df_train.reindex(np.random.permutation(df_train.index))\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Encode Features\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def extract_and_encode_features(df):\n",
    "    color_regex='(?P<color>red|blue|green|yellow|orange|pink|black|brown|white)'\n",
    "    df['color'] = df.name.str.extract(color_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    quality_regex='(?P<quality>generic|high\\squality)'\n",
    "    df['quality'] = df.name.str.extract(quality_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    size_regex='(?P<size>tiny|small|medium|large)'\n",
    "    df['size'] = df.name.str.extract(size_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    item_regex='(?P<item>paperclips|paperweights|ink\\spens|pencils|stapler|tablets|thumbtacks|post\\sit\\snotes)'\n",
    "    df['item'] = df.name.str.extract(item_regex, flags=re.IGNORECASE, expand=False)\n",
    "    \n",
    "    for column in ['pack','weight','height','width','length']:\n",
    "        missing_median(df,column)\n",
    "    \n",
    "    #df.insert(1,'surface_area',(df['height']*df['width']*df['length']).astype(int))\n",
    "    \n",
    "    ## encode numeric features\n",
    "    #for column in ['pack','weight','height','width','length','surface_area']:\n",
    "    #    encode_numeric_zscore(df,column)\n",
    "\n",
    "    # encode text/categorical features\n",
    "    for column in ['manufacturer','color','quality','size','item']:\n",
    "        encode_text_dummy(df,column)\n",
    "  \n",
    "extract_and_encode_features(df_train)\n",
    "\n",
    "ids_train = df_train['id']\n",
    "df_train.drop('id',1,inplace=True)\n",
    "\n",
    "names_train = df_train['name']\n",
    "df_train.drop('name',1,inplace=True)\n",
    "\n",
    "x,y = to_xy(df_train,'cost')\n",
    "\n",
    "# Used before KFold\n",
    "#x_train, x_test, y_train, y_test = train_test_split(    \n",
    "#    x, y, test_size=0.25, random_state=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 32.5279541015625\n",
      "['pack', 'weight', 'height', 'width', 'length', 'manufacturer-6% Solution', 'manufacturer-Deep Office Supplies', 'manufacturer-Duck Lake', 'manufacturer-Offices-R-Us', 'manufacturer-WizBang', 'color-Black', 'color-Blue', 'color-Brown', 'color-Green', 'color-Pink', 'color-Red', 'color-White', 'quality-Generic', 'quality-High Quality', 'size-Large', 'size-Medium', 'size-Small', 'size-Tiny', 'item-Ink Pens', 'item-Paperclips', 'item-Paperweights', 'item-Pencils', 'item-Post It Notes', 'item-Stapler', 'item-Tablets', 'item-Thumbtacks']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item-Post It Notes</th>\n",
       "      <td>-60.369431</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Thumbtacks</th>\n",
       "      <td>-57.847595</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Pencils</th>\n",
       "      <td>-50.215904</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperclips</th>\n",
       "      <td>-43.970024</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Red</th>\n",
       "      <td>-39.540787</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Green</th>\n",
       "      <td>-27.516720</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Blue</th>\n",
       "      <td>-13.250540</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>-10.317136</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>-8.247688</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>width</th>\n",
       "      <td>-6.184750</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Ink Pens</th>\n",
       "      <td>-3.713280</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Generic</th>\n",
       "      <td>-3.706334</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Tiny</th>\n",
       "      <td>-2.535614</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Offices-R-Us</th>\n",
       "      <td>-1.422012</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Brown</th>\n",
       "      <td>-1.184677</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Small</th>\n",
       "      <td>-0.868019</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Deep Office Supplies</th>\n",
       "      <td>-0.601757</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack</th>\n",
       "      <td>0.019117</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>0.034992</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-6% Solution</th>\n",
       "      <td>0.206511</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Duck Lake</th>\n",
       "      <td>0.396286</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-WizBang</th>\n",
       "      <td>1.421118</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Medium</th>\n",
       "      <td>2.418503</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Large</th>\n",
       "      <td>5.069157</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperweights</th>\n",
       "      <td>7.354851</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-High Quality</th>\n",
       "      <td>8.346462</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Black</th>\n",
       "      <td>12.828798</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-White</th>\n",
       "      <td>27.035027</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Pink</th>\n",
       "      <td>37.166756</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Stapler</th>\n",
       "      <td>48.908066</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Tablets</th>\n",
       "      <td>159.851807</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         coef  positive\n",
       "item-Post It Notes                 -60.369431     False\n",
       "item-Thumbtacks                    -57.847595     False\n",
       "item-Pencils                       -50.215904     False\n",
       "item-Paperclips                    -43.970024     False\n",
       "color-Red                          -39.540787     False\n",
       "color-Green                        -27.516720     False\n",
       "color-Blue                         -13.250540     False\n",
       "height                             -10.317136     False\n",
       "length                              -8.247688     False\n",
       "width                               -6.184750     False\n",
       "item-Ink Pens                       -3.713280     False\n",
       "quality-Generic                     -3.706334     False\n",
       "size-Tiny                           -2.535614     False\n",
       "manufacturer-Offices-R-Us           -1.422012     False\n",
       "color-Brown                         -1.184677     False\n",
       "size-Small                          -0.868019     False\n",
       "manufacturer-Deep Office Supplies   -0.601757     False\n",
       "pack                                 0.019117      True\n",
       "weight                               0.034992      True\n",
       "manufacturer-6% Solution             0.206511      True\n",
       "manufacturer-Duck Lake               0.396286      True\n",
       "manufacturer-WizBang                 1.421118      True\n",
       "size-Medium                          2.418503      True\n",
       "size-Large                           5.069157      True\n",
       "item-Paperweights                    7.354851      True\n",
       "quality-High Quality                 8.346462      True\n",
       "color-Black                         12.828798      True\n",
       "color-White                         27.035027      True\n",
       "color-Pink                          37.166756      True\n",
       "item-Stapler                        48.908066      True\n",
       "item-Tablets                       159.851807      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [ 84.28936005]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAD8CAYAAADExYYgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzsnXeYnWW1vu8HCL2JBAUFgghIDyRB\nQg1FRKQqgoh0QeQIiAeVI/wQUJSmHCkKyKEKiCBgBA/FkFBCDSEVKQcIgiAmAtFAkPb8/njXTr7Z\n2XtmTzIzKbPu65prvv1+b/0mV76133c9a8k2SZIkSZIkXcVCc3sCSZIkSZIsWKRxkSRJkiRJl5LG\nRZIkSZIkXUoaF0mSJEmSdClpXCRJkiRJ0qWkcZEkSZIkSZeSxkWSJEmSJF1KGhdJkiRJknQpaVwk\nSZIkSdKlLDK3J5D0DJIesL2FpH7AFrav7YYxHgYWA1YAlgD+Grf2tD2pSZuXgA1sv1FX/iNgiu3/\nbme8LwBP2H6ys3NdccUV3a9fv842S5Ik6dU89thjU2z37aheGhe9BNtbxGU/4CtAlxsXtj8NIOlg\nYKDtb3b1GHV8AfgA6LRx0a9fP0aNGtX1M0qSJFmAkfRCK/XSuOglSJpme2ngDGBdSWOAK4HzomwI\nZdfhQtsXSxoCnAq8CvQHbgLGA8dSdiX2tP1sJ8a/BNg02l5v+7TK7RMkbQ8Y2M/2c3Vt1wIuAFYE\n3gS+BnwE2AXYUtIpwJ7AXsDhwLvAeNtfbXV+yVxEmtszSJLeRQ/kFEvjovdxAnC87V0BJB0BTLU9\nSNJiwEhJd0bdjYF1gdeA54BLbW8m6VjgaOBbnRnX9muSFgGGS7rR9hNx7/Xo91DgZxRDocolwNds\nPytpS+AC2ztJ+iNwo+1bYi3fBVa3/Y6k5Tv3WJIkSZKuIo2LZCdgI0l7x+flgLWAd4BHbb8CIOlZ\noGZ0jAe26+Q4+0k6jPJvbhVgPaBmXFwXv6+h7KLMIIyEzYHfaeY33Gb/bicCv5b0e+CW+pthSB0B\nsNpqq3Vy+kmSJEmrpHGRCDja9h1tCsuxyL8rRR9UPn8ALCJpYeCxKBtq++SGA5RjjWOBzWy/IenX\nwOKVKu3t0Yni2Nm/hbV8FtgW2AM4SdIGtt+fMYh9CWUXhIEDB3b/vmCSJEkvJY2L3se/gGUqn+8A\nviHpbtvvSlqbmSqPdokXdysv/WVj3H9KWpliBNxeub8vcA6wHzCybozXJb0iaS/bN0taCNjQ9tjq\nWsLQ+bjtuyXdD+wPLBl1knmZHjj/TZKkZ8k4F70ISQ8A4yi7Di9IOg64lHI8MVrSBOBi5tzo3Jpy\nDDIu+vx4jPEicBV1BgSwpKRHgG8A/9mgvy8DR0r6CzAF2DXKrwO+H86pnwSulTQOGA2caTsNiyRJ\nkrmAnN8aeh1x5DHDqbOL+/44cA+wqe2pkpYG+tp+XtKIGHe2NKCdlbhKWsT2e43uDRw40ClFTZIk\n6RySHrM9sKN6eSzSi+ghOepKlKOIaQC2pwHTwmF0IHCNpOnAYOA7wG7R1wPA1207jJAxwGaUI5VD\nbT9St5a+wEVAzTPzW7ZHhix1FUo8jymUmB7JvExKUduSX/iSBYA8FumdnADcZ7u/7XOBwwg5KjAI\nOFzSGlF3Y4oxsSFwALC27c0oxylHN+h7LMUYeV7S5ZJ2A7B9IzAK2D/GnU6RlA6yvQHFwKjupCwV\ngb+OAi5rMM7PgXNjzl+M+dQYAOxhOw2LJEmSuUDuXCTQhXJU2+9L2plipOwAnCtpgO1TGoy7XcSm\nWJISMnwi8Ie4d130d6+kZRvErdgRWK8iT11WUs1RdWgYL21IKWqSJEnPkMZFAl0sR3Vx5HkEeETS\nXcDlwCl1fS8O/ILiQ/FiHGe0J0+t/7wQMLjeiAhj481Gi0wpapIkSc+QxyK9k2Zy1D4AktaWtFQr\nHdl+P445+ts+WdIqkjatVOkP1GLRV8etGRJTwulzb9qyb8xlK8qRzdS6+3cCMxw7JbUiiU3mRez8\nqf4kyQJA7lz0TsYB70kaC1xB8V/oR5GjCpjMrCG42yV2HqYBNwDnSFoFeDv6OjKqXQFcVHHo/BXl\neGUS8CjwLUlfBNYAVpP0KMUXY2qDY5FjgAtDeroIsLyk3Tsz5yRJkqR7SClq0iXUjAvb57RYfxaZ\nqKRJFEXJjZRcJtNsH9NifyPohMw1pahJkiSdp1Upah6LJO0i6cAIhjVW0tWSVpc0LMqGSZrFM1JS\nf0kPRZ2bJX0oykdI+rGkeygKlPZ4nBIYC0mTJK0oqZ+kP0v6laSJku6UtETd2AtJulLSj7roESTd\njbTg/CRJAqRxkbSDpPWBE4HtbdckqRcAV9neiJJo7LwGTa8Cvhd1xgM/qNxb3va2tn/abFzbQ4B1\nom09a1HicKwPvEGRodZYJOb0tO2TWltlkiRJ0tWkcZG0x/aUlOZTAGy/RvGVuDbuXw1sVW0gaTmK\nAXFPFF0JbFOpcn0HYw6P4F7LAj9pcP9522Pi+jGKr0iNi4EJtk9v1LGkIySNkjRq8uTJHUwjSZIk\nmV3SuEjaQ7SfsZQW7tfzJpREY5LGxM9plfvbhfLkQNtvNGhflca+T1un5AcosTMWpwG2L7E90PbA\nvn37dnLaSZIkSaukcZG0xzBgH0kfBpC0AuUF/uW4vz9wf7VBSEZfl7R1FB1AyTVCXb02EtYumu//\nAH8EbpCUSqj5hbkt/UwZaZJ0OfkfcNIU2xMlnQ7cI2lFSlbTLwGXSfoORWZ6SIOmB1Ekp0tSVB+H\nSDqWcNAEkHQxsKbtHePz0ZQonatKGhEhwes5Dlgq6n8L6NNgzj+Lo5mrJe1v+4PZXX+SJEkye6QU\nNWmJOZWaShpEccTcLD4/TNk52zxChl8H3AI8DNzaxLio9j+JEt1zyuysJ6WoSZIknSelqElL9KDU\n9HFgbUlLxM7CW5TMpxvG/S0oRy4ACzeSm0q6QtLeko6hZD4dLml43NtJ0oOSRku6IaJ+JvMDc1s+\nmpLSJOly0rjoxfSk1DR2McZQEpptTtmheAjYIqJ5yvaLUb09uSm2zwNepjh/bhdHNicBO9relJJ9\n9duz9VCSJEmSOSZ9Lno3s0hNJQ0GvhD3rwbOqjZoIjW9oVKlPanpSMoOxRLAg8AzwPcpvhsPVOq1\nJzdtxObAesDISFy2aPTfBmVW1CRJkh4hjYveTbdKTanLlkoxIL5OSVp2IcWoWC9+j6z0US83bROF\nswEC7rK9X3uVMitqkiRJz5DHIr2bnpaaPkDZZehr+++Rmn0ysAdtdy5aoZph9SFgS0m1cOFLSlq7\nk/0lc4u5LR9NSWmSdDm5c9GLqZOavk9xujyGdqSmoRoZBpxdlZq2ON7rkiYDa0kaT9mVWJkSjXNs\nVJsIPN9Cd5cA/yvplfC7eAK4XdJbcf8k4OlW5pUkSZJ0LSlFTTrFnEpSo2wSISOVtA5wp+3V4940\n251Weki6giJhvbGV+ilFTZIk6TwpRU06RQ9KUutZFni9Qd9Lx7ijJY2XtEezuTZo+8OQrea/7/mB\nlI8myQJHHoskVUnqlrGbsAJFBXKV7SslHUqRpO5Z1/Qq4Gjb90R+kB8A34p7y9vetp1hh6tIOz4B\n7NPg/tvAXrb/GVLThyQNpTiA1s+1upazgOWAQ5zbckmSJHOF/GaXwNzJfrpdROHcELigQdArAT+W\nNA74E/Ax4CNN5lrj/8Wcvt7IsFBmRU2SJOkR0rhIYO5kPy2d2s8Cr1J2JKrsD/QFBtjuH3UW72Cu\njwID6nczKmNlVtQkSZIeII2LBOZi9lNJKwFrAC/U3VoO+LvtdyVtB6zezlxr3A6cAdwmaRmS+YOU\njybJAkf6XMyjSOpHJPCSNBA40PYxkoYA79juVFyIehWGpIMpio1vAlsDI2hfknqbpAuAaqKwRtlP\nF6VkP71F0tvAk8BRtv9SN6XhMVYf4ATbr9bdvwb4g6SXKdlYn6RkRf0DcDowIWSto4GDa41s3xCG\nxVBJu9ie3pnnlCRJksw5aVzMB9geRcmXATAEmEbng0611/9FTW5tX7sIY+QTtk+ptBtDCYpFpd45\nwB3AEZHt9BDg95IG1NKf2+7XzlyWjt9TgMHNpK+STqXkRKn5Xhxc6eMy4LJ2F50kSZJ0G3ks0sVI\nOlHSU5L+JOk6ScdH+YjYgUDSihHrAUn9JN0XksvRkrZo0OcQSbfGbsaRwHHhw7C1pOcl9Yl6y0qa\nVPvciTmfUpnnoJB5PijpbEkTKlVXkXS7pGdClVHfz5KUgFrH2X4fwPblFGNox1jrhEr948N4QNLh\nkh4Neenvoq/6/htmRZV0mKRzK/UOl/SzzjyDZC6SktMkWeBI46ILkTSA4qewCSX516AWmv0d+Exk\n89yXxllIAbA9CbgIODd8GO6jHGd8Pqp8Gfid7XcbNF+i4lg5BpjFuTK4HDjS9mBKBM0q/WOOGwL7\nSlq17v4ngb/Y/mdd+Shmddis5ybbgyI765+Bw5pVrM+KCvwG2L1iVB0S60iSJEnmAmlcdC1bAzfb\nfitesENbaNMH+JVKOOwb6PglXM+lzAy/3d5LdXrFsbI/0Mi5cnlgmYo/x7V1VYbZnmr7beAJZjpZ\nzuiCxkqOVr5ubhA7OOMpDqTrt9AGANtvAncDu0r6FNDH9vhZJpFS1CRJkh4hjYuup5nb+nvMfN6L\nV8qPo8gsNwYGUtKFtz6YPRLoJ2lbYGHbEyStWtmlOLIT3XVkBNRnK6332fk/YPUGSo1NKbsX1WcA\nbZ/DFcA3bW8InFp3rxUupTh2NjWwUoqaJEnSM6Rx0bXcC+wlaYl4we5WuTcJGBDXe1fKlwNeCWfH\nA4CFOxijmg20xlXAdcRL1faLlV2KZs6as2D7deBfkmpOml9ur36D9m9Sgmn9TCXlOpIOpETbHEkx\nolaS9GFJiwG7VpovA7wSRxv7tzBcm+dg+2FgVeArlGeRzC+k5DRJFjjSuOhCbI+mRKYcA/wOuK9y\n+xzgHEkPAGsCS0X5L4CDJD0ErE0En2qHP1AMmDGVGBPXAB8iXqrh1Dk+nCPvlPTRTizjMOASSQ9S\ndjKmdqItwH8B04GnJP0V+DawhwvvUnw9HgZupchLa74V/y/K74ryKt9XCQFe5RKKdPaxStlvgZFh\nJCVJkiRzicyK2o20I6McAhxve9dG7WZjnL0pL/AD4vMkZmYd/TGwtO1jWuxradvT4voEYGXbTZOP\nqUHW08q9j1ICW/3C9iWdWlTbfiYR66krP4XK85V0K8XZdVhHfWZW1CRJks6jFrOiZpyLHqQSyOoM\nYN1QbVxJUYicQYlhsRhwoe2Lwwg5lXKc0B+4CRhPyTS6BCWR2LeAzwG7NBn2XkpALCT9kqJgWYKS\nn+MHUT6JsuOyHbCCpHeAD4BXgLclPRp9fcv2yHiprwL0A6aEI+gJtsdJepzi1Hoa8B/ABbYvVQnG\ntU+s7+bK2NNsL62SwfQCYFvgecqu2mWVFOpHS9qN4gD7JcpRy5HA+5IOApYHXgJ+rhKca6rtaq6T\nZF6lVXlpfhFKkvmGNC66kWrAqTpOoLJzIekIystwUPgijJR0Z9TdGFgXeI0SBfNS25tJOpaSkfTo\nDqaxK8UgATjR9mvhDzFM0ka2x8W9f0a/BwL72N5V0rWUXYf7VVKu3xFzgeI/spXt6bHDsXUYKe8B\nW0adrYBfS9oJWAvYjHLUMlTSNrbvrczzCxRjZUNgJYoctRoIa4rtTSUdFc/ua5Iuou3OxXjgs7b/\nGgZPkiRJMhdIn4t5g52AA2Mn42Hgw5SXMcCjtl+x/W/gWaBmdIynvIybMTz6Wxb4SZTtI2k0Jbz3\n+rSVvV5X+T04rnekZCwdQ5HVLltRggythNa+j5IRdSvgNmDpCILVz/ZTsb6dYtzRwKcq66uxFXCD\n7Q9s/w0YXnf/pvj9WDvrHglcIelwGjjGphQ1SZKkZ8idi3kDUXYh7mhTWI5FqvLPDyqfPwAWiV2I\nmlPj0EpysO2qPgqS1gCOBwbZfl3SFbSVe7rB9ULA4Pr8HCrb2FXH00cpMtrnKA6ZKwKHV+Yl4Ce2\nL26y/lqd9qitu5EEtkzaPlLSpylBxcZI6m/7H5X7l1AcQRk4cGDusSdJknQTuXMxd6iXk94BfEMz\nw3ivLWmphi3r6CjraIVlKQbBVEkfofhpVNm38vvBuL4T+GatgqT+TebwDiW52D7AQ5SdjOOZqZa5\nAzhU0tLRz8dUsqFWuR/4oqSFYn5D2llLjTbPUdKath+O5zCFIk1N5nVSgpokCxy5czF3GAe8J2ks\nJXjUzylb/aNVtgUmU5w1uwzbY8PZciJlh2FkXZXFJD1MMTj3i7JjgAsljaO8qO9kphFSz33ADrbf\nknQf8PEow/adktYFHpS0HvAI8FVK6PMavwN2ACYAT1OOhzqSwf4BuFHSHsDRlJwra1F2QYYBYzto\nnyRJknQDKUVNmko9u2msNqnf6+4tbXuapA9TDJAtw/+iXcnr7JBS1CRJks7TqhQ1j0WSWZC0lKTb\nIgjXBEn7KrK6Stq9Elr8KUnPR5sBku6R9JikOySt3Inxdotdk1ck/YuSTv6HwJGSLgnlzFWSlpT0\nW5WsrddLelgzM83upJLJdbSkG2pHMMl8QGZATZIFjjQuEmz3q9u12Bl42fbGtjegBMKq1R1aSX42\nlhJ1tA9wPrC37QEUCenpnZjC/cDmtpeh5Fq51fYVcW8AJUDYV4CjgNdtb0QxPgZASWEPnATsGNll\nR1EigyZJkiRzgfS5SBoxnmI0nEl50d+num+Rkr5LybR6oaQNgA2Au6LewpQAXK3yceD62O1YlBJE\nq0ZV8roVxT+FSNBWi9GxOUVWOzLGX5SZTqnVOR8BHAGw2mqrdWJ6SZIkSWdI4yKZBdtPSxpAifr5\nk0pALwAk7UCJklmLgClgou3BdfVWpThdAlzUThK184Gf2R4a8ttTKveqktdm++QC7rK9X5P7tXWl\nFDVJkqQHyGORZBYkrQK8ZfvXlIRrm1burU5JtrZPZUfhKaCvpMFRp4+k9TuRnXU54K9xfVA79e6n\nyF0J1cmGUf4QsKWkT8a9JSWt3YklJ3OTlJ8myQJHGhe9FEmXxgu6ERsCj0RkzhOBH1XuHUyJIHpz\nOHUa+B9KGvkzQ147Hbi2Sd9LSnqp8vNtyk7F/SGVnQJs2CR89y8oRsw44HsUSe9U25NjXtfFvYco\nUUCTJEmSuUBKUZM5QtI04Blgi8gz8jlKuPGXOpP1VdIISs6QpvrQiEbax/bbktakxLJYO4J4dYqU\noiZJknSelKImM+gBaen/UkJuQwnAVctTUhv7MkmPSno8Al4haQlJv6nJSimZWmttJklaUVI/SRMq\n45wAPBe7I2MpuUr+JOnPkgZJuknSM5KqOy3JvE5KT5NkgSONi95Bd0tLfwN8WdLiwEaU6Jo1TgTu\ntj2IktL9bJXQ5t+g+HVsFH0PaGEd/wYusb0xRW76dKRVvwj4PSXF+wbAwRGIK0mSJJkLpFqkd9Ct\n0lLb4yT1o+xa/LHu9k7A7pKOj8+LA6tRlCbnVdqPo/MMraxvou1XYi3PUcKV/6NaOaWoSZIkPUMa\nF72AHpKWDqUoS4ZQHD5nNAO+GKnXq31B20ysjXiPtrtri9fdr2aIrc8eO8u/7ZSiJkmS9Ax5LNIL\n6CFp6WXAabbH15XfARytsCYkbRLl9wL7R9kGlOOUel4FVpL0YUmLAS07iCbzESk9TZIFjjQu5gEk\n9Y08GY9L2rqTbftL2qWDap2Rlv4x1BdVaekYYIsY71xJ36q0X1zSpbZfsv1zST+l7IIsJulGSpju\nPsC4cM78YbS7jeIbMZ1iaLwFrF6dtO13gdMoPhy3Ak+2/mSSJEmSuUVKUecBJH0Z+Jzt9gJINWt7\nMCWj6Tc70UaUv/0HLdRd2Pb7lc9fAr5kex9JCwGPAu/UjlAkPQh8y/bDjXuc0U8/iv/HBvH56xQ5\na6efweyQUtQkSZLOk1LUOSAkkE9GoKkJkq6RtKOkkSF13Cx+HojdhgckrRNtDw5J5O1R96xKv9Mq\n13tLukJSf+AsYJfYOVhC0i8ljZI0UdKplTaDYqyxkh6RtBzlm/2+0XZfSadUnCeJ+feLnz9L+gUw\nGlhVTTKJhhT0ZEn3U3YhqowkdjGA9YEJwL8kfSiOLtYFHq/KSOM51uSukyX9oMFjXxZ4vfL874t5\njZZU2zUZoiKhvTH+PtdUjlt2ibL7JZ0n6dbO/dWTuUbKUJNkgSMdOpvzScqL9QjKt/OvUBJn7Q58\nHzgQ2Mb2e5J2BH4MfDHa9gc2oTgZPiXpfNsvNhrE9hhJJ1PZfZB0ou3XVIJGDZO0EeVI4HpgX9uP\nSlqWcpRQ3/aUdta0DnCI7aPUNpPom5K+R8kkelrUfdv2Vg3m+7Kk9yStRjEyHgQ+BgwGpgLjbL+j\nysvC9tdibqtTfDCuoDh6rhlHNcsASwKfjiZ/Bz4TwbLWosTNqFnKm1CMmpcphs6WkkYBF1P+Hs9L\nmhFnI0mSJOl50rhozvM150RJE4Fhti1pPNCPkg/jynj5meJXUGOY7anR9gmKL0FD46IJ+6jIJhcB\nVqZk/DTwiu1HAWz/M/rvzJpesP1QXHeUSfT6dvqp7V5sAfyMYlxsQTEuHmjUQCUGxg3AN22/EMci\nz0Z8DSTtS1Fy7Ex5lhfErs77QDVPyCO2X4o2Yyh/i2nAc7Zr2VSvIySndXNIKWqSJEkPkMcizamX\nNlZlj4tQHBOHh8/AbrSVSVbbvs9MI67q4FIvqwRA0hrA8cAOEWDqtqgrOpZuQvvyzfoMo3dV1B/r\n2T6svq6kVStHGkfGvQcoxsSGlGORhyg7F1tQDI9GXATcZPtPTe4PZaYU9jiKUmRjyo7FopV6jZ5t\nSxaW7UtsD7Q9sG/fvq00SZIkSWaDNC5mn2omz4NbbPOqpHXDEXKvJnWWpbzYp0r6CPC5KH8SWEXS\nIABJy0haBPgX5VihxiRCaippU2CNJuO0lEm0ifx0JEUW+prt922/BixPMTAerO9D0n8Ay9g+o8lc\noBw5PRvXy1F2aT4ADqAE8WqPJ4FPxG4IwL4d1E/mJVKGmiQLHGlczD5nUQJSjaT9l9+iwC9UMn5e\nQZFU3k2TiJe2azkzJgI3Av8X5e9QXprnq8hD76LsSgwH1qs5dAK/A1aII4NvAE83GadhJlFJ7wOr\nULKUjpX07TCGqowHVow21bKptqdUylYK59LjKZlO63dA1pT0N0kvU3xWvhblvwAOkvQQ5UikuuOy\nc4O1TAeOAm4PJ9RXKUc0SZIkyVwgpajdjOYjmWmUTbNdU42sREmdPtJ2I4VHR/2fAkyzfc6c1Gk2\nv7rypW1Pi/VfCDxj+9xm/aQUNUmSpPMopaizohYkplGvt8pM22D77xQHyG+qcLCkCyrj3yppSFzv\nHGONlTSswbM/XNL/Slqi/l6Tv9UtKhlZJ4YjZv39FWN9tWys10p6C5hOOWK5uJVxknmAlKEmyQJH\nrzIugk8CP6eEm/4UMyWmx1MkplDO8LexvQlF6vnjSvv+lOOJDSkv/lWbDWR7TLS/PnwWpgMnhtW3\nEbCtpI0kLUpRZxwbGT93pBwFVNu2p96AIjO9Kub8JjNlpptSMoh+u1L3bdtb2f5NB31i+znKv5OV\nmtWR1Bf4FSWHyMbUGS2Svklxet2zEmK8Iw6NjKwDgWNUyXIavii3ASfbvk3STsDfgKUokta/MFO6\nmiRJkvQwvVGK2pHEFHq3zLQRHU1kc+DemhQ0HDxrHAC8RDEs3u3EmMdIqjm9rgqsRcly2gcYBvyH\n7Xvi/k7x83h8Xjrq39tmESlFTZIk6RF6o3HRkcQUZspM9woFwogm7WdXZjrI9uuSrqB7Zab7Neln\nhsyU5llOa3P+BGWdf29n/PbmP4Gy2/Nx4PkmderHHELZvRls+y1JIypjvQc8BnwWqBkXAn5iu92j\nkMyKmiRJ0jP0xmORVujNMtMZxHHHRcAFLp6/k4D+khYKw2SzqPog5YhnjWi3QqWbx4GvA0NVsrO2\nwnLA62FYfIqyMzJj2sChFGXLCVF2B3Boxa/kY+GMmswPpFN5kixwpHHRmFZlpsCMl7Ap2UMfozWZ\n6WWUeBGrUb6ld7vMtFJlWc3Mz/FnzUytfiawlKTX4sjoT8AUIudHzPd5iuz0HIrz6NExz/eACZL+\nTN2xi+37KTs2t0n6i0ro8SonSXqp9gP0jXmMo+wivU4lY2ooXL4MbCfpKNt3UlQtD8bx1o20NcqS\nJEmSHiSlqF2A5j+56ZXAfbYvDWfSJSnG0a22t5Z0DXAGJcbGrcDOjfwlJO1Hyaeyj+0PJH0ceNP2\n6/V1K20mxXqntFNnBHC87W7TiqYUNUmSpPOot0pRlXLTduWmKgnPtgH+B0pwLttvUHxOFg3DZQng\nXeA7wHntOGKuzMxImth+qWZYSNpP0viY/5lN/k4TKp+Pj7XvTVF6XFN5niMkDWyvX0nTJJ0ez/Yh\nlWOnJEmSZC6wwBkXQcpNm8tNPwFMBi4Pw+pSSUvZ/hfl2OVxytHHVIrj6e/bmc9vgd3CCPippE0A\nwrfiTGB7yrMcJGnPDtYGgO0bYy37V54nLfS7FPBQPNt7gcNbGS9JkiTpehZU4+J52+PjG/UMuSnF\nV6Bf1FkOuCG+PZ9LSeNdY5jtqbbfBmpy086wj6TRlBf1+hRZ6DrUyU1tv9fJfpvJTccAB9XNs5mh\nsgjFKfSXFSPlhJjTWfFC/0+Kr8PJkr4m6beSTqrvyCU76TrAf1F2PoZJ2gEYBIywPTnWeA0zk5LN\nCe31+w7lCAeK30u/+saSjogdpVGTJ0/ugukkSZIkjVhQjYvOyE17W1bTl4CXbD8c9W4kFCiVNWwS\nl08DB9reB9hAJe5HG2z/2/b/2v4OZfdnT1rLUtreOpvRXr/veqYDUfVvVp1rZkVNkiTpARZU46IV\neqXc1PbfgBdrPibADpTdmSo/pBzX9GGmWuYDiuPnDCRtGkcVxDPZCHgBeJhyHLSipIWB/ZgZk6LG\nq5TEZh+WtBgly2qN+mdSo5V+kyRJkrlMbzYuOiU3DU6gc1lNa3LTHstq2uI6jqY4TI6j+C7M8DcJ\nH4ZHbb8cjp41eadjbVVWAv4R9RLqAAAgAElEQVQQR0vjKLsRF9h+hXJUMhwYC4yu990IJ9HTKAbD\nrRTjC0mXUiSwF9UcOittav0+Q/ELadNvOIl+pcVnkCRJknQTKUVN5jtUIpveGs6f1fIhFAnrro3a\nVUkpapIkSedRb5WiJvMGkr4r6Zi4PlfS3XG9g6RftyOjrcpOD5P0dJT9SpWMrMA2KrLe50K+CiU2\nx9ax43FcDy43SZIkqZDGRdJd3AtsHdcDgaUl9aFIgsfTvoy2Jjv9fxRVzGeY9chn5ehrV4pRAeXY\n6r7wLzm3y1eUJEmStEQaF0l38RgwQNIyFPXNgxQjY2tgOu3LaKHkLbnH9mvhn3FD3f1bbH9g+wmg\npYBZKUVNkiTpGXpjVtSkB7D9rkqo70OABygOn9sBa1KcMdvL2gody1mrcuGWctNnVtQkSZKeIXcu\nku7kXkrMj3uB+4AjKcndWpHRPkKRnX4oJLtfbGG8ZhLWJEmSpAdJ4yLpTu6j+EY8aPtV4G2KT0SH\nMlrbf6VIZB+mSFOfoIQkb0goRU4D3ov8IunQmSRJMpfIY5Gk27A9jBKIq/Z57cr13ZRw3vVthlQ+\nXmv7kti5uBm4M+ocXNdm6TAubHuHLlxCkiRJMhvkzkUy19HMTLZXShon6UZJSwJ3SHqLEvF0DeCW\nqP9JSX+KHYrRktas629QJGX7RM+vJkmSJEnjIplXWAe4JPKx/BM4CtjN9pK2F6NEPa0Fx7oGuDAy\noG5BJVqqpC2Ai4A9bD/XkwtIkiRJCmlcJPMKL9oeGde/psSw2E7SwxF+fHtg/ZC2fsz2zQC237b9\nVrRbl6IG2c32X+oHSClqkiRJz5DGRTKvUC8NNfALYG/bGwK/YmZ22Wa8QnEa3aTRzcyKmiRJ0jOk\ncZHMK6wmaXBc7wfcH9dTIjT43gC2/wm8FAnWkLRY+GcAvAF8HvhxOHgmSZIkc4F5zriQ1De2wh+X\ntHXHLdq07S9pl+6aW5Mx349cFhPDwfDbkX68O8eUpJMkPRO5N4ZLWr9y/0uS/ixpeHy+Lhwlj5N0\nmqQdu2AOJ8aax8X6Pz2HXf4ZOCikqSsAvwQWo2SXvYWSgbXGAcAxUfcB4KO1GyF53Q24sAvmlCRJ\nkswG86IUdQfgSdsHzUbb/pQQ039stYEkUbLDftBC3YVtv19XPN12/7i/EnAtsBzwg5Zn3Xn+g+LI\nuLHttyTtBAyVtL7tt4HDgKNsD5f0UWAL2/XhtWeb2GHYFdjU9r8lrQgsOofdfmD7yLpx3gA2sz2l\nWm77GYoPRpXngBFx/y/A+iRJkiRzhXa/YVckgpdKmiDpGkk7ShoZ35o3i3qbRYbKx+P3OlF+sKSb\nJN0e9c+q9D2tcr23pCsk9QfOAnaJb8NLSPplOOFNlHRqpc2gGGuspEckLUcJorRvtN1X0imSjq+0\nmRBr6hff7H8BjAZWVfMsnZMknSzpfuBL7T0v238HjgC+GbsLC0s6W9Kj8Q3/65W5fKdSfmrd875S\nbSWZ9XwPOLrmyGj7Tso3+P0lnUxxhrxI0tmU2BArxTPZOp7z3k2e4TLtzbnCysAU2/+O8afYfrny\nvFaM64GSRsT1KZKulnR3/Fs4PMqHANdTjkWekHSRGuz81P17afTslpJ0W6xlgqR92/tbJfMIaily\ne5Ik8xmtbN9/Evg5sBEliuJXKC+v44HvR50ngW1sbwKcTImsWKM/sC+wIeXFv2qzgWyPifbXR2bL\n6cCJkTt+I0o46I0kLUp5IR0bcsQdKbEQqm2v72Bd6wBXxZzfpP0snW/b3sr2bzrok5A/LkTZxj8M\nmGp7ECVg1OGS1oidhrUoybn6UxJ8bVOZV70kcwaSlgWWsv1s3dCjgPVtnxbX+9v+DrA78Gw8k/sq\n/TR6htObzblurDspBtnTkn4haduOnkuwEcUnYjBwskrmU4CN4zlsSMk98oVmHbTz7HYGXra9se0N\ngNtbnFOSJEnSxbRyLPK87fEAkiYCw2xbRR7YL+osB1wpaS2Kl3+fSvthtqdG+yco2S9f7MQc95F0\nRMx1ZUo2TQOv2H4UZjj5oc59C3rB9kNxvTkzs3RC2eJ/sFK3I0OlntpEdgI2qu0UUJ7TWlG+EyV2\nA8DSUf4XZpVkHgOc0+KYnUnGtQ6Nn2GzOT9fa2h7mqQBlAyn2wHXSzrB9hUdjPn7MBinq/iDbEZx\nwnykFpNC0nUU4/XGJn00e3b3AedIOhO4tWpI1Yh/R0cArLbaah1MNUmSJJldWjEuqtknP6h8/qDS\n/ofAcNt7SepHnH03aP9+pU31Rbh4o4HjG/PxwCDbr0u6gplyxFZepO/RdnemOs6b1aFoP0vnmzGf\nVYE/RNlFti9qMOdPUNb59+j3aNt31NX5LPAT2xfXlfejsSRz5gf7n5LelPSJuiBRmwL3NJl/I5o9\nw4Zzrid8T0YAI8LQPAi4grbPvP7v2mxt7a65wfxmeXYAYfDsAvxE0p2xi1Odc2ZFTZIk6QG6StWw\nHPDXuD64xTavSlo3ztf3alJnWcqLfaqkjwCfi/IngVUkDQIIX4FFmDUr5iTKSxdJm1JCSDeilSyd\n2H4xjhf6NzEs+lKiQ15g28AdwDck9Yn7a0taKsoPrfh1fEzFGRSaSzKrnA2cJ2mJaL8j5dv+tU3W\n14hmz7DZnKvrXCd2qWr0B16I60nAgLiuz2S6h6TFJX0YGAI8GuWbxXHRQpQjtEZrrtHw2cURy1u2\nf03Z6dm0paeQzF2cNl6SLIh0lVrkLMqxyLeBu1tscwJwK+WIZAJle7sNtsdKepwiR3wOGBnl74TD\n3vnxgp1O8RkYDpwgaQzwE+B3wIHx+VHg6UYTsT1Z0sGULJ2LRfFJzerXsUT034fyrf1q4Gdx71LK\n0dFolfOWycCetu+UtC7wYBzDTAO+StnxqEkyLwaeoUgy6zkf+BAwXtL7wN8o4a6ntzDf2pqbPcOG\nc5Z0KfAz209Q/lbnS1o+1vx/xHEDcCrwP5K+T8loWuURyt/y49HvH4GLKUdQZ1B8Lu6lJClrNu9m\nz24C8EwYSP2AbZr1kSRJknQvcn5zmGeIY5FbwyFxgULSKcCHKVLhIRUJ67bAIbZ3ba99C/1Pi+yo\n/WjhGQ4cONCjRo2akyGTJEl6HZIeC5FFu8xzQbSSuU8jWaekESrS0t1VZK1jJD0l6floM0DSPZIe\nk3SHpJUbdL0MdRJW4B/RfpKkH6vIgUdJ2jT6eVbSkVFnaUnDVOTC4yXt0UOPJOkuUoqaJAsk82IQ\nrV6L7UnAvLBrUZN1fh5AJYbINwBsDwWGRvlvgXvCP+N8ytHM5DhuOR04tNah7VPCT+J+SU8Df6LI\nhkdQnEInUZQygyWdS3EO3ZLiFDqR4svyNrBXOLWuCDwkaahz+y1JkmSeIncukkaMB3aUdKakrWtS\n4iqSvkuJTnohRda6AXBX+J+cRPGraIPtaRRnzyMoPhfXh69LjaGV8R+2/S/bk4G3w79DlLwh4yjG\nyceAj7S6KGVW1CRJkh4hdy6SWbD9dL2ss3pf0g6UaKU1p0kBE20Prqs3i3S3HQkrtJU510ugFwH2\nB/oCA2y/G7sdDWXMTdaVUtQkSZIeII2LZBZC1vma7V+rhN0+uHJvdUoq9J0r6pSngL6SBtt+MI5J\n1rY9kSJTrbVdh5JD5JkoqkpYW2E54O9hWGxHCciWzM/kiVaSLJCkcZE0YkPg7Ii98S4lDkktSujB\nFNXHzSEFfdn2LioRPc8L/4xFgP8GJsbuwr8oMtulgPckmVklrK1wDfAHSaOAMZRYHUmSJMk8RkpR\nk6aEfHSa7VbCjyNpEdvv1ZVNAgbanhI7F3fWZ2iNeBotZabtKlKKmiRJ0nlSipo0RdKBKhlFx6pk\nKl09JJ7j4vcsiTck9Zf0UNS5WdKHonxESEjvAY7tYOhlgdejXaPMtPuFxHSCSo4QJO0j6Wdxfayk\nWg6SNVUy1dZkrKdWJKqf6qJHlXQnUkpRk2QBJY2LXoak9YETge0jG+qxwAWUDLEbUY4ezmvQ9Crg\ne1FnPPCDyr3lbW9r+6dNhh0uaQIl98lJlfJqZtp3gTOB7Sm+GIMk7UmJ2Ll11N8a+Iekj1HCnVeT\nk02JjLa/pOSjSZIkSeYSaVz0PrYHbowAVth+jZICvZaX5GrKi3sG4UexvO1aYrQraRteu6OssdtF\nxMwNgQsi3gW0zUw7CBhhe3IcrVwDbGP7b8DSkpYBVo15bkMxNKrGxU3x+zFmZuttQ0pRkyRJeoY0\nLnofrWSU7awjTi1r7MKV6J2n1Vey/SzwKiW9/Yx2lXk140HgEIoq5T6KYTGYyDUT1KSr1cy79eNf\nYnug7YF9+/bteFVJkiTJbJHGRe9jGLCPSmZSJK0APAB8Oe7vT11W0gii9bqk2vHEATRI7277/UrW\n2JPr74f6ZA0ay08fBraVtKKkhSkZYWtj3Es56rgXeBzYDvh3o+BeyXyEnVLUJFlASSnqHKCSYv1W\nYFHgGNv3ddCk2rY/sIrtP3bX/BqM+XHgxxTHypcl/RP4X+AY4DJJ5wMLAz8Lp8gjgfcl3UzZzThb\n0pKUDLWHdGLo4SrZW/sAJ9h+VSXBWG1eV1ASmJmSJfdvwA22fx9V7qMcibwN/D7qPFlpu2SnHkSS\nJEnSraRxMWfsADxp+6DZaNufkiG0ZeOiM5JNSQtHNMxq25uAX9reI3YHLqEEy5ok6SuUkNurR/0T\ngItt1xw3+9MA20Pam4ftfk3KJ9E2j8p3bN8YwbEusf3dSt1nYwlD4vNOdd0dVfEhGQW0O6ckSZKk\ne1mgjkVC3vikpEtDzniNpB0ljZT0jKTN4ucBSY/H73Wi7cGSbpJ0e9Q9q9LvtMr13pKuiJ2Hs4Bd\nwsdgCUm/DIfBiZJOrbQZFGONlfRIOEieBuwbbfeVdIqk4yttJsR6Gkk2d1LJHjpa0g01B8mQZJ4c\nEs0v1T2e7YG3bV8O5QgDOA44NHYj7gRWivn8APgW8DVJwxs8g++G5HOspDOibM14do9Juq8mB5X0\npVjLWEn3tvBnfJCSM6TTSDpD0hMqctmWYnMkc5GUoibJAsuCuHPxScqL9QjgUeArFPXD7sD3gQMp\nKoT3JO1IOSb4YrTtD2xCcQ58StL5tl9sNIjtMZJOpgSI+iaApBNtvxa7AsMkbUTZvr8e2Nf2o5KW\nBd4C6tue0s6a1gEOsX2USjbQk4Adbb8p6XvAtynGChQDYqsGfaxPUVJU1/BPSX+JZ7Y7cKvt/jEf\n0SCAlqTPAXsCn7b9VvhsQNkFOdL2M5I+TQkRvn2s87O2/6qSfKwjdgZuaaFeG2IeewGfsu0Wx0qS\nJEm6gQXRuHje9ngASROBYfGyGU+RKC4HXClpLcoZf59K22E1J0FJT1ByVzQ0Lpqwj6QjKM91ZYoq\nwsArth+F8kKP/juzpqpkc/Pod2T0sSjl236NZrLQZiqRVtQjVXYELrf9FhQpa+ycbAHcUFnXYvF7\nJHCFSnr2m+o7q3B27BatRFljI5rN08A/KT4Zl0q6jeIL04b42xwBsNpqs8QJS5IkSbqIBepYJKjP\nplnNtLkI8ENgeMRd2I22WTWrbauSxupLrWEWTklrUBQNO0Sgqduibqsv7/do+/eojlMv2byrospY\nz/Zh9XUlraqZstAjgYkUH4/qnJelOEo+28L8quPXr2ch4I3KnPrbXhfA9pGUnZZVgTGSPizp8phX\n1d/kO5QdlJMocTSQ9OnKGnYH/gF8qG7sFSgBtN4DNgN+R9lZub1+4ilFTZIk6RkWROOiI5YD/hrX\nB7fY5lVJ60paiLL13ohlKS/2qZI+Anwuyp8EVpE0CEDSMpIWoSTzWqbSfhKwadTZlCLZbMRDwJaS\nPhl1l5S0dn0l2y9WXvQXUSSoS0o6MNotDPwUuKK2C9EidzLTTwNJK8RuzPOSvhRlkrRxXK9p++GQ\npk4BVrV9SMxrl7o5fwD8HFhI0mejXW0NQ4Fn4lmuG32vDmxMMVqWBpYL9c23aOKAmsxDpBQ1SRZY\neqNxcRbwE0kjKbLLVjiBss1+N/BKowq2x1JiMEwELiMCPNl+B9gXOF/SWOAuyq7EcGC9mkMn5Rv3\nCpLGAN8Anm4yzmSKUXSdpHEUY6PDXBouGer2Ar4k6Zno/22KH0rL2L4dGAqMirkeL+lSSjjww2KN\nE4E9osnZ4fw5gRKnYmyDbheLemMoz3c14MZ4NotKeiDG/jfwVeDyqHsj8LU4yloGuDWeyT0UZ9Uk\nSZJkLpBZUZN5CnUyE+vskllRkyRJOo8yK2rSHUhaStJtIS2doCKjHSFpoKTdKz4ST0l6PtoMkHRP\nyFTvkLRyJ8ecFr+HxFg3qkiOr4kjmB1UAn3V6n9GUnvOo8ncpiZDTSlqkiyQpHGRdJadgZdtbxxO\nsTMcJ20PrflIUI4/zpHUBzgf2Nv2AMqR0elzMP4mFJ+K9YBPAFtSjqvWVYmYCiV66OVzMEaSJEky\nB6RxkXSW8cCOks6UtHWj/B6SvgtMt30hJUbHBsBd4SdxEvDxORj/EdsvhfPnGKBf+JNcDXw14lsM\npoQ1r59XZkVNkiTpARbEOBdJN2L7aUkDgF0ojrF3Vu9L2oESxKyWkl3ARNuD6+qtCvwhPl4UipZW\naCYXvjz6e5uSl+S9BnO/hBLsi4EDB6azUZIkSTeRxkXSKSStQslH8uvwhTi4cm91SmTOnW1Pj+Kn\ngL6SBtt+MI5J1rY9kS6Ui9p+WdLLlJ2Rz3RVv0k3kY7kSbJAk8civRCVfCUT4nqgpPPieoikLTpo\nviHwSBxxnAj8iBLl9EcUKe6awARJz0v6Y0hx9wbODJnqGEo0z65g/bq+rgFetP1EF/WfJEmSzAa5\nc9HLiSyiNU3mEGAa8EA79e8A7qh9Von3vRBwi+2do2x1YHfb50ebMcw8JuloPqc0KFs6fo8ARsQY\ni9jerq7qVsCvWhknSZIk6T5y52I+Q9KJIfP8k6TrJB1fk4LG/RUlTYrrfioZSkfHzyw7BrFbcauk\nfsCRwHEhJd06dh/6RL1lVbKu9qnrYnvgnarPhO0XaoaFpIUlnS3pUZVspV+vjDuLrDTuNZSuRv0f\nS7oHOFaVTLKxE7N/lI+WtGZXPfOkC6lKUFOKmiQLLLlzMR8RjpRfpsgxF6GkYH+snSZ/Bz5j+22V\nRG3XUZdfpIbtSZIuohLAStII4POULKVfBn5n+926puvHPJpxGDDV9iBJi1ESrtWcQDeJ9i9TIppu\nKelhinR1D9uTVaKXng4cGm2Wt71tzO+UyjhvAofavlnS4qThnCRJMtdI42L+Ymvg5louEElDO6jf\nB7hAUn+KsmKWHCQdcCnwXYpxcQhweEcNJF1IOZ54x/YgYCdgI0l7R5XlgLWAdwhZabQbQ8la+wYz\npatQQrRXQ67PkvVV0jLAx2zfDGD77SZzy6yoSZIkPUAaF/MfjdzsqxlVq9lUjwNepST3Wogi02x9\nIHtkHK1sCyxse0K9hJSSR+SLlTb/IWlFZvpxCDg6fDVmIGkIjWWlDaWrFd5sUNbS3npKUZMkSXqG\n3Dqev7gX2EvSEvFtfbconwQMiOu9K/WXA16JgFMH0HGitvpMrQBXUY5TLoeG2VbvBhaX9I1KmyUr\n13cA36j4bqwtaal25jBDuhr1+0hav71JR1bWlyTtGW0WU2RtTeYxaplQqz9JkixwpHExH2F7NOVY\nYAwli+p9cescygv8AeA/oThzAn8BDpL0EOVIpNG3/ip/oBgvYyRtHWXXAB+iGBgzCOfOFSM65p7A\ntuEA+ghwJfC98Nl4HHgCGB1OlxdT2TELJ82nKD4dJ1JSzc+OdPUA4BiVrKgPAB9toU2SJEnSDWRW\n1PkYtZNBNI4djre96xyOsTfFufKAuvJJwEDbU9ppOyLm0DT9aLVO+ETsanv3OZlzK2RW1CRJks6j\nzIraO4momQBnAFvHLsRxHUhC75H0W0lPSzpD0v6SHpE0hbIr8sN2xusn6c+SfiVpoqQ7JS1RV2ch\nSVdK+lEH078X+GS0aU+OembM7+naDouk9aNsTKxvrdl5fkk3k1LUJOkVpEPnfEyjgFMVTqCycxG7\nAs0koRsD6wKvAc8Bl9reTNKxwBq2n+5gKmsB+9k+XNJvKQ6ev457i1COVibY7igb6m7AeM3MpNpM\njrpIzG8X4AfAjpQYHT+3fY2kRenYvyRJkiTpJtK46D20Jwl91PYrAJKeBWpGx3igPgpmI56PKJxQ\n4m70q9y7GPhtB4bFNZKmUxxTj6ZtJlWYVY56U4OxHgROlPRx4Cbbz9QPklLUJEmSniGPRXoPNUlo\nTemxhu2aEVGVhH5Q+fwBsEgcqYyJn9Ma9N0sUykU58rtIrBVM/aPOe1p+0VmylFrc93Q9k4Nxpsx\nlu1rgd2B6cAdkravH8T2JbYH2h7Yt2/fdqaTJEmSzAlpXCy41MtKOysJnYHt9ysv+pM7OY//Af4I\n3CCp1Z2yTstRJX0CeM72ecBQYKNOzjPpCVKKmiS9gjQuFlzGAe9JGivpOEq0zaaS0O7E9s8oIcKv\nltThv7nZzKS6LyUb6xjgU5T4HEmSJMlcIKWoyTyFpD8CX7H9Rl35KYTsVtLBwJ22X457k+hAFltP\nSlGTJEk6T0pRk/kS27vUGxYNOBhYpQemk3QFjeSnKUVNkgWaNC6SHkXSdyUdE9fnSro7rneQ9Ota\n5M8om5FenqIgqQX1GkhRmIypxNQ4WiXV+nhJn+r5lSVJkiQ10rhIepp7KdldoRgJS4eT6VbMDGde\nn17+C8AgANs3UpKi1RQm06PJFNubAr8Ejm80sKQjJI2SNGry5Mldv7IkSZIESOMi6XkeAwaoJF77\nNyU+xUCKwXFfpd6M9PKRmKyj9PKNYl+0IaWoSZIkPUMG0Up6FNvvhgPmIZQYGOMogbrWBP5cX70T\nXc8S+yKZR0in8STpdeTORTI3uJdydHEvZbfiSGCM20qXmqWXh8ap4ZMkSZJ5hDQukm6lkkityn3A\nysCDtl8F3qbtkUh76eUBRlBiZtQcOpcHjur62SdJkiSzQ24fJz2O7WFAn8rntSvX/SrXp1MSltXz\nD+AW298EkPTfwFvRZhQwpDvmnXSCzkhM89gkSRY4cuci6TEkfaeS8v3UKGuasl3SoKj7YKSLnxAZ\nT08D9o2di32j+/UiHftzNalrkiRJMndI4yLpESTtRMnCuhnQn6IY2SZurwVcaHt94A1KynaAy4Ej\nbQ+mOGrWQoOfDFwfUtTro+6ngM9G/z+o5VCpm0NKUZMkSXqANC6SnmKn+HmckmfkUxSjAhqkbJe0\nPLCM7Qei/NoO+r/N9r8jBPjfgY/UV0gpapIkSc+QPhdJTyHgJ7YvblMo9WPWlO1LRP3O0F7a96Sn\nST+KJOnV5M5F0lPcARwqaWkASR+TtFKzyrZfB/4lafMo+nLldkpRkyRJ5mHy213S3UjSBNsbSFoX\neFBFSTAN+CrhS1Fhc2DhuD4M+JWkNyny06lR/lFg50iv/pO6wb7fLatIkiRJWiaNi6S7WR+4FcD2\nz4GfN6izQe3C9t6V8om2NwKQdAIlpwjAm8ANNSlqHd+3vXRXTDzpJLOb4TSPUJJkgSOPRZKeYOF6\nqamkNSXdLukxSffVMplKOkVSLfHY0ZKmS3oLOJziEFpjlWj/jKSzou0ZwBIhUb2mR1eYJEmSzCCN\ni6QnaCQ1vQQ42vYASijwXzRotz+wg+0lgRtoe4TSH9gX2JAS82JV2ycA00Oiun99ZylFTZIk6RnS\nuEh6glmkpsAWwA3hN3ExJRz4DFqQog6zPdX228ATwOodTSKlqEmSJD1D+lwkPUG9TPQjwBu2+7fT\npqMD/JSezmuk70SSJEHuXCRzg38Cz0v6EhQ5iaSNqxU6kKK2x7uNonMmSZIkPUd+20vmGEmnANNs\nn9OJZvsDv5R0EiWJ2cqSXgL6Apb0DDOlqOsDZzFTitoelwDjJI1u5HeRJEmSdD9pXCTdiu1JtJWa\nniNpEdvvATvXyiVNArazPUXSOsCdwPq2N4q07VMJKartK4ArKn3uWrn+HvC9blxSUmN2paf15HFK\nkixw5LFI0hRJB0ZW0rGSrpa0uqRhUTZM0moN2vSX9FDUuVnSh6J8hKQfS7oHOLaDoZcFXgc+Hw6f\nSwBbA3dLurUy1gWSDo7rAZLuCWnrHZJWbtRxkiRJ0v2kcZE0JI4iTgS2t70xxSC4ALgqAltdA5zX\noOlVwPeiznjgB5V7y9ve1vZPmww7XNIE4B7gJNvXh9PndNufp8mxSPhYnA/sHdLWy4DTG9RLKWqS\nJEkPkMciSTO2B26MLKPYfk3SYOALcf9qih/EDCQtRzEg7omiKynxKWpcT/vUjkXWBIZJGmF7Wgtz\nXYdy9HJXhBZfGHilvpLtSyg+GQwcODD34pMkSbqJNC6SZgjo6AXc2Rf0mwCSFqbEuwAYavvkNp3a\nz0p6FVgPeKRy6z3a7rYtXpnrRNuDOzmfZE5IX4kkSZqQxyJJM4YB+0j6MICkFYAHmCkJ3R+4v9rA\n9lTgdUlbR9EBlCMO6uq9H1E0+9cbFjHWSsAawAt1t14A1pO0WOyS7BDlTwF9Y2cFSX3iWCdJkiSZ\nC+TORdIQ2xMlnQ78WdJ7FPXGMcBlkr4DTAYOadD0IOAiSUsCzwGHRJr1tYBbJE0GPgAusv2rurbD\nJb1PkaaeYPvVujm9KOm3wDjgGeDxKH9H0t7AeWF0LAL8NzBxzp9EkiRJ0lnk3NpM2qGzMSwqMtNq\n2W8ohsZJtj+Q1Bc41PaZdfUWtl2fgr1bGDhwoEeNGtVxxQWJrpKOdjX5f1CSzDdIesz2wI7q5bFI\nL6WnZKbhnLkZYVgA2J5cMywkDZE0XNK1FHUJkr4q6ZHIbnpx+GggaSdJD0oaLemG2BFB0iRJp0b5\neEWG1SRJkmTukMZFL6SHZabrA2NrhkUTNgNOtL2epHUp2U63DBnq+8D+klYETgJ2tL0pJaDWtyt9\nTInyX1KyrDZad0pRkwS8iPMAAA9ESURBVCRJeoA0Lnons8hMgcHMzDx6NbBVtUETmek2lSodyUxr\n/ZwYOxIvV4ofsf18XO8ADAAejQBaOwCfADanqEdGRvlBtM2EelP8rmVdnYXMipokSdIzpENn76TH\nZKaU3Y6NJS1k+wPbpwOnR0jvNm0rc7vS9n+1mbC0G3CX7f2ajF/LkpoZUpuRvg1JkvQQuXPRO+kx\nmant/6McYfyo4juxOM1Tqg8D9g45KpJWkLQ68BCwpaRPRvmSktaezfUnSZIk3Uh+w+uFVGSm94T0\n83FmU2ZavdmOsuRrwNnA/0l6DZhO8+Rif6QYvS/E3J4HDvf/b+/eg+2s6jOOfx/C1YQENMoELCY4\nRIRySyEVQUw0RZB0AiKGYAejyMUmsdJh2lAvBazcW4S2oIFCiGIY1CoZGkgUEiKBQAiEXKggpUdF\nUhJrCjqkXOTXP9baPW/2Pnufk+Q9+3LO85k5s2/vu971vrOzs2a961krYkVeR2S+pN3ytl8Cnqna\nfzIp9mpmZi3iKKqVpqTYahdwVJ4G/BJg34g4ZxvqMD3vP7PRdh0XRW3XGGkZ/Btk1jEcRbXSNCu2\n2oOHgf0KZdaLqH5a0jO5zGPLO3MzM9seblxYQ02OrVY7Efhhrke9iOoo4BJSo+JPSImSeufiKKqZ\nWRO4cWG9aUVsdYmkjcCkwnHqRVT/GFiaJ+Z6rVHZjqKamTWHB3Rab1qxOurEvM1c4FLSZFn1Iqqn\nbMfxO4/HJZhZB3HPhfWmJaujRsQW4AvAWfmY9SKqjwATJL1N0i7A6aWctZmZbTf3XHQQSQ9FxPsl\njQbeHxHf6WWX7T1OF/Bb0uqlL5LGWJQaW+2j+aRGxQzSrZjLgMWSdgJeB2bkiOrFpMGfG4DHgSHb\ncSwzMyuJo6gdSNIE4MKImNxP5XfRHQe9DBgWEZ/vp2PVxFELny0lnWfpmdG2jaIO5MhpPf4NMusY\njqIOQIUps68APpAjmRdIGiLpakkrc/TzvLz9BEkPSLozRzWvkPTJHOdcq7RiaW+WAZVZMW/MaYv1\neQ6KSr26JF2Zy320MIvm2yV9P9drpaRj8/sXS5ojaTEwL9f/mlynNZJm9XDuXZJGShot6aeSbsvb\nfi/3jpDP76n8fp/m2jAzs/L5tkhnmk2h50LSucBLEXF0nr1yef6PG+Bw4L3Ab0i3J26OiPGS/gKY\nRRrX0Mhk8lLopJVLf5MHYt4n6bCIWJM/ezmXexbw9bzfdcC1EfFgngtjUa4LpOTHcRGxRdLngDHA\nkRHxRh5j0ch7gLMjYrmkW4A/z4+nAgdFREjaq3qnfJ3OBdh//5qpOczMrCTuuRgYTiANfFxNGuD4\nNrqnwF4ZERsi4lXgP4BKo2MtdVYPzZbk8oYDl+f3PiHpcdK4i0PYek6J+YXHY/LzScA/5XIWAMMl\n7Zk/W5AHbVa2+0bl9kiOuzbyy4hYnp9/mxSFfRn4X+BmSR8DXqneyVFUM7PmcM/FwCBgVkQs2urN\nNDbj1cJbbxZevwns3CgOWpnbIpc1BrgQODoiNkuaC+xeKDt6eL4TcEyhEVEpC2pXQt2WG+/V20bu\n8RhPmv/iDGAmaY6OzuLxB2Y2ALjnojP9Ftiz8HoR8LkcxUTSWElD+1JQozholeGkBsFLkvYBTqr6\nfGrh8eH8fDHpP3lyvY6oU/Zi4HxJO+fterstsr+kSu/INOBBScOAERGxkHSrp96xzMysn7nnojOt\nAd6Q9CRpoqnrSLc4HlfqFtgEnNLDfkcC5Cjrh7flgBHxpKQngPWksRvLqzbZTdIjpAbrKElrST0S\nH5JUiaEuA87vofibgbHAGkmvAzeR4q/1/A44R9I3SbdmhgMjgLvUvZz7BdtyfmZmVh5HUQehsqOs\nxehq9euyo6y5YXR3RPxhfv27iBi2reW0bRTVzKyNOYpqNdogynqCpIclPS7pu/lWRiVmekl+f62k\ng/L7wyTdWoionpbLHCtpZNW5jZK0LJ/TOnXPDtpeJP9V/5nZgOPGxeA0G/hJHmdxLXA2OcoKHE26\n5TAmb1tZCfVQ0jTeYyNiPOlWxiyAiBhdHPxZZTKwNjcGvgRMiohxwGOkNUMqfp3fv5E0cBTgy7le\nh+bVVe+PiC7ghR6OcyawKK+YejiwunoDeVVUM7Om8JgLgxRlPUzSx/PrEaQo62vkKCuApOoo68QG\nZS5Rmi58DalRcRxpfMTynBbZle6BnwD/mh9XAR/LzyfRvYYJEbG5wfFWkqYk3wX4YUTUNC4iYg4w\nB9JtkQZlmZnZDnDjwqA5UVYBP4qIaXXqUCn393R/L/scUY2IZZKOB04GviXp6oiY15d9m8pjnMxs\nEPBtkcGpFVHWFcCx6p4a/C2SxvZSfHWUde96GyqtkLoxIm4C/gUY15f6m5lZ+dy4aAOSHsqPoyWd\n2U/HeATYQ9IvgHuBcZK2SLoU+AfgKVKUdR3wTUro1ZI0t3KrJSI2AdOB+ZKeIUVaD+qliL8D9s4D\nNJ+k+zbMO4HquTAmAKtzXPY0UjzXzMxawFHUNlJ2RLTOMaaTYqLFHoHtinP24VhzSbHR7/VWh20s\nt4tC9HV7OIpqZrbtHEXtIC2KiFbX4WuSnpS0QmkGzq16Hor13MbjT5L0k7zdZEm7ApcCU/N5TpU0\nXtJDkp7Ij+/Jx2m4WqqkPSTdK+kcSUMl/Vs+h3WSptIsrY5ydvqfmQ04HtDZXpq52mnRUGBFRHxR\n0lXAOaRbEo309fijgQ8C7waWkOa8+AqFngtJw4Hj8/ogk4DLSLc2zgXG0PNqqcOAO4B5ETEvz4Hx\nQkScnMscUV1heVVUM7OmcM9Fe+uP1U578hpwd36+qo/79/X4d0bEmxHxM1IjpKdxFiOA7+bxHteS\nVlyFxqul3gXcWkiErCX1klwp6QMR8VL1QbwqqplZc7hx0d4qEdFKGmNMRFT+E+81IppvO6zOgzYb\neT26B98Uo6BvkL8jOUq6a2GfhscvfFazgmkPx/8qsCRP6f2ndK+22iiKuhw4KdeLiHgG+CNSI+Ny\nSY2SK+WK8N+O/JnZgOPGRXtpRUS0kS7Sf9gAU4BdtqOM0yXtlMdhHAA8Te15jgB+lZ9PL7zfaLXU\nrwD/DdyQP9sXeCUivg1cg6OoZmYt48ZFC1UiqOmpzqSw2qmkC0hTbO9wRFRp7Y61Oc55IfCWPu56\nE/BBSY8CnwVeqbPdQcCH8vOPs/X36mngAeAeYDPwIGnsxcGSnpa0HriK1NuwHBhS2PdmYAvwXK57\ndUz3C8DueZzIocCj+RbSF+l9zIiZmfUTR1HbQH9HUFXCKqWSlpLqWJPfLEZOG8VEcxkHAOdFxD2S\njgKuiYgJDY47nR2IrdbjKKqZ2bZzFLUDtCiCWlyldFreb52kK/N7Q3IEdV3+7IIcRz0KuD3XcY86\n5/N5YF/SuiJL6hz/atJaI9X77q7uFVCfkDSxTmx1qKRb8rV5QtKUvP8h+TqsztfswOpjlKrV8c2B\n9GdmA46jqO2hmRHUyiql+wJXksZUbAYWSzoF+CWwXx5ciaS9IuJ/JM2kTs9FRURcL+kvqVpXpMrD\nwKmSJpLGXlTMyGUcqrTk+mJgLLWx1ctIq6N+RtJepFshPwbOB66LiNtzo6R4e4W8r6OoZmZN4J6L\n9tQfEdQlubzhwOWkpdWXRsSmHPW8HTie1GA5QNI/SjoReLncUwPSeIjq3ovjgG8BRMRPgZ+TGhfV\nTgBm53NZSkqW7E9qtPyNpL8G3hURW6p3dBTVzKw53HPRnioR1P5epbRGRGyWdDjwEVJvwieAz+zY\n6dQc435JXwXeV3i7r/3jAk6LiKer3v93pfVTTgYWSfpsRNxfQnV75rFKZmZ1ueeiPbQigvoIKQky\nMjdIpgEPSBoJ7BQR3we+THeks7qOfT2Xer4G/FXh9TLgk5DOl9Qb0VNsdREwq9I4knRkfjwAeC4i\nrgcWAIf1oQ5mZtYP3HPRHv4/ggrMJa3oOZoUQRWwCTilzANGxAZJF5FioQIWRsRdudfiVkmVhudF\n+XEu8A1JW4BjerrtkM0B7pG0ISImNjj+QkmbCm/dkMtfS5q8a3pEvJoHhlZug1xOmnDr68CafG26\nSONIpgJ/Jul14L9IA0HrWrVq1a8l/bzRNv1kJLDdC64NUL4mtXxNavma1GrFNXlXXzZyFNWsiSQ9\n1pcY12Dia1LL16SWr0mtdr4mvi1iZmZmpXLjwszMzErlxoVZc81pdQXakK9JLV+TWr4mtdr2mnjM\nhZmZmZXKPRdmZmZWKjcuzJpA0sWSfpXXPlkt6aOFzy6S9KzSKrEfaWU9m03Sifm8n5U0u9X1aRV1\nr1y8WtJj+b23SvqRpJ/lx71bXc/+lNcM2qi0AnTlvR6vgZLr8/dmjaRx9UvuXHWuSUf8lrhxYdY8\n1xYmOFsIIOlg4AzgEOBE4IY8qdmAl8/zn4GTgIOBafl6DFYT83ejEi2cDdwXEQcC9+XXA9lc0r+B\nonrX4CTSkggHktYLurFJdWy2udReE+iA3xI3LsxaawpwR0S8GhH/CTwLjG9xnZplPPBsRDwXEa8B\nd5CuhyVTgNvy89soeSK9dhMRy0gLMhbVuwZTgHmRrAD2kjSqOTVtnjrXpJ62+i1x48KseWbmLtxb\nCl3c+5FWoq14Pr83GAzmc68WpJWJV+XVewH2iYgNkGbUBd7Rstq1Tr1rMNi/O23/W+LGhVlJJP1Y\n0roe/qaQum3fDRwBbAD+vrJbD0UNlgjXYD73asdGxDhSd/8MSce3ukJtbjB/dzrit8Rri5iVJCIm\n9WU7STcBd+eXzwN/UPj4ncALJVetXQ3mc99KRLyQHzdK+gGpO/tFSaPyOkCjgI0trWRr1LsGg/a7\nExEvVp6382+Jey7MmqDqfvCpQGX09wLgDEm7SRpDGqD2aLPr1yIrgQMljZG0K2kw2oIW16npJA2V\ntGflOXAC6fuxAPhU3uxTwF2tqWFL1bsGC4CzcmrkfcBLldsnA12n/Ja458KsOa6SdASpm7ILOA8g\nItZLuhN4irQa7IyI+H3LatlEEfGGpJnAImAIcEtErG9xtVphH+AHaZFfdga+ExH3SloJ3CnpbOAX\nwOktrGO/kzQfmACMlPQ88LfAFfR8DRYCHyUNWnwF+HTTK9wEda7JhE74LfEMnWZmZlYq3xYxMzOz\nUrlxYWZmZqVy48LMzMxK5caFmZmZlcqNCzMzMyuVGxdmZmZWKjcuzMzMrFRuXJiZmVmp/g8qIXne\nIwR7rwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf0dafe7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple function to evaluate the coefficients of a regression\n",
    "%matplotlib inline    \n",
    "from IPython.display import display, HTML    \n",
    "\n",
    "def report_coef(names,coef,intercept):\n",
    "    r = pd.DataFrame( { 'coef': coef, 'positive': coef>=0  }, index = names )\n",
    "    r = r.sort_values(by=['coef'])\n",
    "    display(r)\n",
    "    print(\"Intercept: {}\".format(intercept))\n",
    "    r['coef'].plot(kind='barh', color=r['positive'].map({True: 'b', False: 'r'}))\n",
    "    \n",
    "# Create linear regression\n",
    "regressor = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# Fit/train linear regression\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "print(names)\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_[0,:],\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 32.51896667480469\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item-Thumbtacks</th>\n",
       "      <td>-49.421860</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Post It Notes</th>\n",
       "      <td>-49.344654</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Pencils</th>\n",
       "      <td>-44.134438</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Red</th>\n",
       "      <td>-37.983204</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperclips</th>\n",
       "      <td>-34.972664</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Green</th>\n",
       "      <td>-25.736231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>-17.168795</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Blue</th>\n",
       "      <td>-11.625701</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Generic</th>\n",
       "      <td>-4.687774</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Offices-R-Us</th>\n",
       "      <td>-0.996790</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Deep Office Supplies</th>\n",
       "      <td>-0.785418</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Tiny</th>\n",
       "      <td>-0.344142</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Brown</th>\n",
       "      <td>-0.261662</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Ink Pens</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>width</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-6% Solution</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack</th>\n",
       "      <td>0.020104</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>0.034882</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Small</th>\n",
       "      <td>0.037237</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Duck Lake</th>\n",
       "      <td>0.081102</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-WizBang</th>\n",
       "      <td>1.041263</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Medium</th>\n",
       "      <td>1.653781</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Large</th>\n",
       "      <td>3.429472</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-High Quality</th>\n",
       "      <td>7.357063</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperweights</th>\n",
       "      <td>7.962308</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Black</th>\n",
       "      <td>14.052813</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-White</th>\n",
       "      <td>27.178623</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Pink</th>\n",
       "      <td>38.855175</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Stapler</th>\n",
       "      <td>47.857166</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Tablets</th>\n",
       "      <td>161.930939</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         coef  positive\n",
       "item-Thumbtacks                    -49.421860     False\n",
       "item-Post It Notes                 -49.344654     False\n",
       "item-Pencils                       -44.134438     False\n",
       "color-Red                          -37.983204     False\n",
       "item-Paperclips                    -34.972664     False\n",
       "color-Green                        -25.736231     False\n",
       "height                             -17.168795     False\n",
       "color-Blue                         -11.625701     False\n",
       "quality-Generic                     -4.687774     False\n",
       "manufacturer-Offices-R-Us           -0.996790     False\n",
       "manufacturer-Deep Office Supplies   -0.785418     False\n",
       "size-Tiny                           -0.344142     False\n",
       "color-Brown                         -0.261662     False\n",
       "item-Ink Pens                        0.000000      True\n",
       "width                               -0.000000      True\n",
       "length                              -0.000000      True\n",
       "manufacturer-6% Solution             0.000000      True\n",
       "pack                                 0.020104      True\n",
       "weight                               0.034882      True\n",
       "size-Small                           0.037237      True\n",
       "manufacturer-Duck Lake               0.081102      True\n",
       "manufacturer-WizBang                 1.041263      True\n",
       "size-Medium                          1.653781      True\n",
       "size-Large                           3.429472      True\n",
       "quality-High Quality                 7.357063      True\n",
       "item-Paperweights                    7.962308      True\n",
       "color-Black                         14.052813      True\n",
       "color-White                         27.178623      True\n",
       "color-Pink                          38.855175      True\n",
       "item-Stapler                        47.857166      True\n",
       "item-Tablets                       161.930939      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [ 70.26249695]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAD8CAYAAADExYYgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBFIG6wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf0db709e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Create linear regression\n",
    "regressor = Lasso(random_state=0,alpha=0.01)\n",
    "\n",
    "# Fit/train LASSO\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_,\n",
    "  regressor.intercept_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1e-08, 100000000.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAF6CAYAAAAeZ/GvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xd4W9X9x/H3V7Isr8QZzgDHmSQh\nCYQRswuBEmjYm0KhzEIn3b+Wli5oS1toaWlLS2nLaFkNlBF2gYZAmVlAFs4ezh7O8Jal8/tDDvG2\nHFu+uvLn9Tx6Iulcy99jOf7o3nvuOeacQ0RERNJXwOsCREREJLkU9iIiImlOYS8iIpLmFPYiIiJp\nTmEvIiKS5hT2IiIiaU5hLyIikuYU9iIiImlOYS8iIpLmMrwuoKMKCgrc8OHDvS5DRESkW8yZM2er\nc25AZ17Dd2E/fPhwZs+e7XUZIiIi3cLMVnf2NXQYX0REJM0p7EVERNKcwl5ERCTNKexFRETSnMJe\nREQkzSnsRURE0pzCXkREJM0p7EVERNKcwl5ERCTNKexFRETSnMJeREQkzflubvy6Oti8ufFzWVnQ\nu3f8ftM2gOxs6NULnIMtW5q35+RAXh7EYrB1a/P23Nz4LRqFbduat+flxV+jrg62b2/e3qtXvIZI\nBMrKmrf37h3vQ00N7NzZvD0/H8JhqK6GXbuat/ftC6EQVFXB7t3N2/v1g4wMqKyE8vLm7f37QzAI\nFRXxW1MFBRAIxL+2srJ5+4ABYBb/3lVVzdsHDoz/u2tXvA8NmcW/HuJ9r6lp3B4IxL8/wI4dUFvb\nuD0YjNcP8Z99XV3j9oyMeP8h/t5Fo43bMzOhT5/4/bKy+O+I2d5bKBR/byH+s2nYZhb//hn1/4ui\n0cZtIiKpwndhX1q6je9+9/5Gz320ahRbjtlNkCjHbmiexvNXjmHHsTsIRaIcvbV5+/srx7L72DJC\nO4yjqzY1a5+zchyVx24jc1OIo2LrmrW/t3o8NUdvJbwmzJGhtc3a3147nrojt5KzPItJOWuatb+5\nfjyxSVvJ+yiHw/JXNWt/ffM47JBt9F6YwyH9mrfP3DGWwLgy+s7P5qCC5uslzKgcQ8aoHRR8kM24\ngc3bX42NIrNwN4M+CDNmYPP6Xw0PI7NfFYUfhBg5sHH/I5EQbw3uTyAzxtAPgwwbsKFRe2VlNnNG\n9QJg5AIo7N/409iuXb34YGw2AGMWxhjUr/GnrW3b+7FoQvzXdMJHtfTL39GofePmgSw9JH7/kKWV\n9M5r/GmmdMP+rDw8/glg0spd5GQ1/rSxqrSItUfUgMHRa7YTymj8aWHZmuFsO64aMzhyzcZmP5uS\nNaOomlxFBjEOWtm8ffHa0fDJagI1UcauWd+8fd1YwqdUw3bHyI3NfzcWbzqQ3qfUEF0HQ7et3Ntg\nAI6Ssgn0/2QtvWqrKNq2hrEHBAk0OF538sknU1RUxNq1a3n11Vebvf7UqVMZPHgwK1as4PXXX2/W\nfuaZZ1JQUEBJSQlvv/12s/bzzjuP/Px8FixY0OICVRdffDE5OTm8//77vP/++83aL7vsMkKhELNm\nzWLhwoXN2q+66ioA3nrrLZYsWdKoLRQKcdlllwEwc+ZMVq5c2ag9JyeHiy++GIBXXnmF0tLSRu29\ne/fm/PPPB+DFF19k48bG71///v0566yzAHjmmWfY1uST/uDBg5k6dSoATzzxBLuafBIfMmQIU6ZM\nAWDatGlUNvmkPGLECCZPngzAQw89RCQSadQ+ZswYjj32WADuv/9+mpowYQJHHHEEkUiEhx56qFn7\noYceyqGHHkplZSXTpk1r1l5cXMxBBx3Ezp07efLJJ5u1H3PMMYwdO5atW7fy7LPPNms/4YQTGDly\nJBs3buTFF19s1q7fva773esKvgv7QMARzok1em746CjnTx6Ji9ax4rnmu+5jDoxy2OSR1FbWsPaV\n5u3jJzgOnTyS8q1VbHprQ7P2iRMdh0weyfY1lZS93zwMiw8zDpo8kk0l5ZSXNA/TY4/IYNzkkZT2\n3UXNqliz9slHhRlz3EhW5+6gbn3z9lOPy2HkpHxWZGzHbWvefsaE3gw9uC9L2UxgZ/P284v7sN8B\n/SiJbCSjsnn7ZyYNoH/hABZXrSOztnn71ZMLycuPsah8DeFo4/ZgXZTPTR5OKBMW7V5JONbk6zOi\nfH7ySAAW7VxKmMbteYEG7WUlhAON2/Otjs9PHgPA4rKFZDZp7zdob3vJjg/IsMbtA/ePcOrkUQAs\n2TGHYJP2wUV1nHbCSBywfPoWbE99Lv7PfkOjnHrkUJyDdVvW45p0b3BhlPET98NF69i8fv3HX7fn\nNQYNjjFmdAGRylrKN5Q2agOj3wDHqCF9qMisxba6Ju2Q3weG9c+lvCJCYIdr2ARATjb0zc1k3vTe\nhIesY8tGR9Fwx4ihOkMnInuZc679rVJIcXGx0xK3Io1tL4txxsWVvPNKLmDk9qvl7/c6Pn1O2OvS\nRKSTzGyOc664M6+hj/8iaaBf3wBvv5zHBwvrGDKmiortmXzm0wF+9cJH7K6OtP8CIpLWFPYiaWTi\n+BBrS7J55MkaTrm+lD/PXM4xN77DUVMq2LCx+SkaEekZFPYiaeiSc8O8+PsRPP3l49j5vwN579Vc\nCgvh09dUUFHhr1N3ItJ5CnuRNHZIUR8Wv1bAWZdU4Jwx7b5c+vSP8pNftXCNpIikLYW9SJrLyzOm\nP5LLsuWO8UdUUleTwc9udXzn8Q/ZvLu6/RcQEd9T2Iv0EKNGBFj4Xg7/mRHhmh9t4Im5pUz6/HyG\njqni7fei7b+AiPiW766zF5HOOeXEEKecOIpVWwcz+ZQIa5Zmc+xRjiNOrGTafdkMH67p/0TSjfbs\nRXqo4QW5LH6zD1d9uQoLOGa9ls2IkXDFF1qYE1lEfE1hL9KD5eTAfX/MZvUq45iTq8EZ/5pewzX3\nz2LZ5vJmaxGIiD8p7EWEoiLjrVeyeeOtKD/8XRnvrtjGUZetpv/gCPf+ow6fTbQpIk1oulwRaWbL\n7hqOOK6W1fPjixgVjqrhgb9lcPKJQY8rE+l5NF2uiCTFgF5hPnqvF1+/sYZARox1yzOZclKQU86t\nwG87CCKisBeRVmRlwW9/EaZ0TYBPnVsNON4q2cl5f3qLd5eVsan5atAikqIU9iLSpv32gxefzGbW\nbPjTX6Ks3V7JlIt3UlgU5dvfr6WiwusKRaQ9CnsRSUjxJOPKE4p4/TsnUVAzkGgkyG9+kcmgwgh/\n+HMdUc3LI5KyFPYi0iG54Qw+mpvDD26JkBGOUrEzg69+KYOjp5QTiWplPZFUpLAXkQ4Lh+GnPwxR\nujrIhZfVgjmW1K7nlDtm8sRbm5g9W4P4RFKJwl5E9tmgQfDYg2EWLoB/3d2bmIPLroAjjjDOPL+W\nVau8rlBEQGEvIl1g/Hhj6sGDeeWbkxkY6APAc0+FGHVAjC/eUEdZmccFivRwCnsR6TKZGQGWLgjz\ns19GCWfFiEWNu/8Y5MRzd1FZW+d1eSI9lsJeRLpUZibc9N0ga1cHueKaOiwA6/os5fjbZvDraet4\n6CFHTOP4RLqVwl5EkmLAAHjg7yFWrzJeunME++dnc9M3s7j8cmPcxAgzZ3pdoUjPobAXkaQqKoJJ\nw/vx+PXHMSjcG4AlHwU48UT45Kl1LFrkbX0iPYHCXkS6RThsLP8oxK/viJGdDeCY8arxmW9sYWt5\njdfliaQ1hb2IdJtQCL71jQClq4Nc/8UYoRBsKVzMCbfN4Cu/LuWmH8TYtcvrKkXSj8JeRLpdv37w\nlz8F2bg+yMxfHEbxsL7c8+s8bv15gP2Lotx2W4yqKq+rFEkfCnsR8Uy/fnDAoF78/oKjKOyVB0Bl\nTYzvfjfAkGFRHnxQM/GJdAWFvYh4rk8fWLo4g7/9zdG/T/zPUtmuKD95cDWvlWymrs5poR2RTlDY\ni0hKyMiAa6811q4KcvuvHYMHG32KV3HVfbM44orljJ0Q5emnwWlnX6TDzPnsf05xcbGbPXu212WI\nSJLFYhCJRbn/jdV89YLBRCpDuJoQEw+LcsftQU4+2esKRbqHmc1xzhV35jW0Zy8iKSkQgHBGkMuP\nHMnVl2QRiGZAIMb8hTGmTIHPXhPxukQR31DYi0hKy82Fu/8UYEmJcdFFDhfJwDKivLj+I777+Ics\nXVPN/PleVymS2hT2IuILI0fCtEeDvD/P+OxnHZdfGeWxOWspPm8TEyc6Pn1pjGXLvK5SJDXpnL2I\n+NaCVeUccXCY2jpHrDYDw7jmGsfNPwlQWOh1dSJdQ+fsRaRHmzAsj8cfDTFqeBBiASxcy9/vhTMv\nqSAS1dJ6Inso7EXEt8zgjDNg8YIg//wnDOqfAbEApQWLmXzbDP741Hp+/GOnKXilx1PYi4jvBYNw\n+eWwanmQxx5zPPSzIeSEM/juTTFuucUoGhbj9tudpuCVHkvn7EUkLVVUOAqHxthZFiCYW020PJuC\ngVF+eWuQa6/1ujqRxOmcvYhIK3JzjWUlQb7xdbDaMBaIUVYR4ef3buKDtWVAfOIekZ4gqWFvZlPN\nrMTMlpnZjS20DzWzGWY2z8w+NLPTk1mPiPQsBQVwxx3GimUBrrwSqM0kMHEZ59z1Fmf9YDHjNAWv\n9BBJC3szCwJ3AacB44FLzWx8k81+AExzzh0GXAL8KVn1iEjPVVQE990bYH1pgLm/PZLrjx/BjH/1\nY9maOs49Fw4vjvLqq15XKZI8ydyzPxJY5pxb4ZyrBR4FzmmyjQN619/PB9YnsR4R6eEGDoTeWSG+\nNWU8hxcWEKsME8iu5cNFUaZMgXPO09J6kp6SGfaFwNoGj0vrn2voJ8DlZlYKPA/ckMR6REQACIdh\n5owg//kPjB+dQaw6k2BuNW9uXckvX1jMzsoIixd7XaVI10lm2FsLzzU9M3YpcL9zbghwOvBPM2tW\nk5ldb2azzWz2li1bklCqiPQ0ZnDKKfDBvADTpsVDf8rFu7h75gomXFLChAmOSz+jKXglPSTt0jsz\nOwb4iXPuU/WPvwfgnPtFg20WAlOdc2vrH68AjnbObW7tdXXpnYgk03srtnPq5DAV5RAtzyLgAlxz\nLfz4R6YpeMUTqX7p3SxgtJmNMLNM4gPwpjfZZg1wMoCZjQOyAO26i4hnDivqx01fzSXbZePqgljv\nCv76N8exJ9YSi2nYvvhT0sLeOVcHfAV4CVhMfNT9QjO7xczOrt/sW8B1ZvYB8AhwlfPbLD8iklZC\nIfi//4M1qwJ873uOQFUOBuweXsLUO1/nmTmbuPVWTcEr/qIZ9ERE2rBhA9z5e8fY01Zz9/+WUfKf\nQnbOHEd+3xg/vCnAl74E2dleVynprCsO4yvsRUQSVFEdpWhYjJ07jUBONXVleQwcFOOWmwNcdx0E\nNCepJEGqn7MXEUkruVlB/vtSiCknBqgryyOYU8v2mipu/t1uVm0t97o8kVZleF2AiIifHHoovPRi\ngJkz4f++k8Gs9zJxJ83l1N9t5ORhw3jvnnGceEKAIUP4+HbwwdC7d/uvLZIsCnsRkX0weTK8+06A\n116D0YeM4zcvB7jvzkwqV1bx4fwsYpHgx9ve9IfNnDY1wLI5efzh9jBFRdbow8Dpp0PfvvE5+q2l\nGUpEOknn7EVEukA0CgdOiLKsJIgFYoR61WJZNYQG76DP5BKC2RF2vHkA1SsGQE0mdRVhotUhAL77\nt5UccnCQd6b348G7cykawscfCAoL4atfjR8Z2LEDMjIgL8/jzkq36opz9tqzFxHpAsEgLPggyL/+\nBR99FKC0NIu1a8McNzmXC6/tzYr1VVzwq/0/3t4CMcJ9qsmdUMojHy3hkcXG9hnjqOtdyUfbgpRs\nyKRuRpi6ikw2D5vP0EGZzHhgMM8/nE+v3o4hQ2Bo/QeCv/wl/v1LSqC2Nr7wT36+jhLIXtqzFxHp\nBnV1MH8+lJbC2rV7/z3zzBinnBnhzbk1nH1C4xP7Fowx8sylZE5cya5tGZTNGIeLBHExi0/4U5tJ\nIBrk3FvnM6h3Fv/900g+mBl/jZwcR+EQOPgg49//jr/eyy9DdfXe0wcFBfpA4Ae69E5EJE04B1u3\n7v0QsOcDwbnnwpFHOl76b4SzTw8RqW2czkd/bgnhMetYszTMhlcPwMxBIPbxB4KsXnUUX72Egb3C\nvPabg9hQsvccQGbYceKJjhdfMMyMP/0pfjpiz6DCAw7o7p+CtESH8UVE0oQZDBgQvx12WLNWpp6c\nSU01bNnS+OjAGWeMYfjwMbzwguNLr8C6Uhp9IDj75hLIzeTDN/PYUV5H1vDNWCgav/A6Bu9W7GTi\nzasZ0CvMrNuOonxzfIagzLBj21bT+IA0obAXEfEJMxg4MH47/PDGbaedZqxcAbFY/AjBng8DJ500\nlt694enBcMdKx5q1sG713g8EX/1hOdWhgbz2WD/q6iBnSBl1Uajd0JeZb0Y441MhD3oqXU1hLyKS\nRgKBvR8IJk3a+/w558A558QDvuEHgkMPHU0wCE+G4fEsWLs2i/mLYtQCr7xdrbBPEwp7EZEepuEH\ngj3OOy9+A+OV1xyfmhrB9d0B9PKoSulKmi5XREQa+eQJGZzwszfZlb/R61KkiyjsRUSkkUAA+m4v\n4tEbD2TzZn9dsSUtU9iLiEgzkXX9qdrUixdm1HpdinQBhb2IiDQzcUwmAP9+KuJxJdIVFPYiItLM\nhWeGAZj1rsZxpwOFvYiINDPxoPiCPptLQ/hsolVpgcJeRESaCQah//61kBFld7nS3u8U9iIi0qLT\nL6wmZ/RGNlaUe12KdJLCXkREWvTD7wXpN3U+7y7d5XUp0kkKexERadHIgjy2/ftIvv/5fK9LkU7S\nMEsREWlRIGBENvZhXVWQaDR+Hl/8SXv2IiLSqv6DorhYgPkLo16XIp2gsBcRkVYVHxUP+WnP1nhc\niXSGwl5ERFp18fnxs70vvqjL7/xMYS8iIq067aT4tLm1wWqPK5HO0AA9ERFpVf/+xmHnlcJ+W4D+\nXpcj+0h79iIi0qYrvlDFxopylq7Sojh+pbAXEZE2DbR+bHjgeO68W4P0/EphLyIibeoX6A3Af17S\nhfZ+pbAXEZE2TT42BDjWLNMwL79S2IuISJuysyEnv46a8gx2aZp8X1LYi4hIu0aOiwDGf16r9boU\n2QcKexERaddpp8UA2FK72+NKZF8o7EVEpF1fvCqb/S59hx2Z27wuRfaBwl5ERNo1YmiQEfuHePzB\nsNelyD5Q2IuISEKqFw1l7mPD2LhR8+T7jcJeREQSEq7LAeDp/2hyHb9R2IuISELOOcsAePzJOo8r\nkY5S2IuISEIuOD0LgA/maHIdv1HYi4hIQkaNDBDIiLF9k8LebxT2IiKSEDMYeXAlGf3LiURjXpcj\nHaCwFxGRhP3kd7sZdPmblGzU5Dp+orAXEZGEHT8hnx2vjeP2O6JelyId0GbYm1nQzG7vrmJERCS1\n7Z+fTeWiQp55NMfrUqQD2gx751wUmGRm1k31iIhICsvIMCwWpGxDmDpdgecbiRzGnwc8bWafNbPz\n99ySXZiIiKSm/YdFwBlzP1Ta+0UiYd8P2AZ8Ejir/nZmMosSEZHUdezx8fP1D/9bM+n5RbsXSzrn\nru6OQkRExB8uvSCTR/4KHyzSnr1ftLtnb2ZDzOxJM9tsZpvM7N9mNqQ7ihMRkdRz8vGZhPtVkjlk\nu9elSIISOYx/HzAd2B8oBJ6pf05ERHqgnBy4+s7FVAxZidMCeL6QSNgPcM7d55yrq7/dDwxIcl0i\nIpLC8ssH8c6tR/OfGRGvS5EEJBL2W83s8vpr7oNmdjnxAXsiItJD5VTnE92Vw4OPaZCeHyQS9tcA\nFwMbgQ3AhfXPtcvMpppZiZktM7MbW9nmYjNbZGYLzezhRAsXERHvnHJcNgBvvqGJWP2gzdH4ZhYE\nLnDOnd3RF67/2ruAU4BSYJaZTXfOLWqwzWjge8BxzrkyMxvY0e8jIiLd74jDMsAc61Zmel2KJCCR\nGfTO2cfXPhJY5pxb4ZyrBR5t4bWuA+5yzpXVf7/N+/i9RESkG2VkQO/+EWorMygr0yi9VJfI8Zc3\nzeyPZna8mR2+55bA1xUCaxs8Lq1/rqExwBgze9PM3jGzqS29kJldb2azzWz2li1bEvjWIiKSbOMP\nqwWLsWRtldelSDvanVQHOLb+31saPOeIz6jXlpbm02/68S8DGA2cCAwB3jCzg5xzOxp9kXP3APcA\nFBcX6yOkiEgKuPpqWLhtDat2ZXEUWhgnlbW36l0A+LNz7qQmt/aCHuJ78kUNHg8B1rewzdPOuYhz\nbiVQQjz8RUQkxV11UQ4DT13EnOVa2z7VtXfOPgZ8ZR9fexYw2sxGmFkmcAnxyXkaego4CcDMCogf\n1l+xj99PRES6UWZGgJpXJ/GHrw31uhRpRyLn7F82s2+bWZGZ9dtza++LnHN1xD8ovAQsBqY55xaa\n2S1mtmd0/0vANjNbBMwA/s85p2v4RUR8omZTPtU7w6wt1RnWVGaunbkOzWxlC08759zI5JTUtuLi\nYjd79mwvvrWIiDRx8jkV/Hd6Lr+5u4pvfj7b63LSkpnNcc4Vd+Y12t2zd86NaOHmSdCLiEhqueC8\neIw8NT3qcSXSlkRWvcsxsx+Y2T31j0ebmdazFxERLjgtC4CF8xK5uEu8kuiqd7XsvQSvFPhZ0ioS\nERHfGDTIyOpdCznVXpcibUgk7Ec5524DIgDOuSpavoZeRER6oAtu2ELWMYupjuhQfqpKJOxrzSyb\n+glxzGwUoGWOREQEgEsvDFIXMZ6ZWe51KdKKRML+x8CLQJGZPQS8CnwnqVWJiIhvjBvUh82PHcGd\nd2gFvFTV7ogK59zLZjYXOJr44fuvOee2Jr0yERHxhYG9siDm+ODdsNelSCsSGj5ZP9HNc0muRURE\nfCgvD8J5dZSXhYhEIBTyuiJpSsdcRESk04aOioAz3p4d8boUaYHCXkREOu2Ek2IAPDhN47dTUath\n33Ae/JZu3VmkiIiktisuip+vr87SiPxU1NY5+znEL7czYChQVn+/D7AGGJH06kRExBeOOSLE4V+b\nRWh0ABjsdTnSRKt79g3mwH8JOMs5V+Cc6w+cCTzRXQWKiEjqC4Vg7H69eO7vA6is9LoaaSqRc/ZH\nOOee3/PAOfcCMDl5JYmIiB9FSwew6a2hvDhD5+1TTSJhv7V+IZzhZjbMzG4CtOa8iIg0MmJQfFGc\nhx/XiPxUk0jYXwoMAJ6svw0ALklmUSIi4j9XXBgP+3ff1oVeqSaRSXVOds59reETZnYR8FhyShIR\nET8aNzaIBWNsXJ3pdSnSRCIfv76X4HMiItKDmUGfgRHqagOUlzuvy5EGWt2zN7PTgNOBQjP7fYOm\n3kBdsgsTERH/OXFqNdMfj7FuV5SxeXlelyP12jqMvx6YDZxN/Jr7PXYD30hmUSIi4k8332zMHfhf\nFm06hLH7K+xTRath75z7APjAzB52zkUAzKwvUOScK+uuAkVExD/GF/Zi9xvj+MX8bM7T8mkpI5EB\nei+b2dn1274PbDGzmc65bya3NBER8ZtgwIgsH8zcsjDOxc/ji/cSGaCX75zbBZwP3OecmwRMSW5Z\nIiLiV7m5RrQ2yMrVMa9LkXqJhH2Gme0HXAw8m+R6RETE5yYdGQXgkSerPK5E9kgk7G8hPj/+Mufc\nLDMbCSxNblkiIuJXl14YP0M8XefsU0a75+ydc4/RYAId59wK4IJkFiUiIv519qlhwLF5a9TrUqSe\n5jQUEZEu1auXMax4O+GRm7wuReop7EVEpMt97dZtVI9ZQlm55mBLBQp7ERHpcoMD/Vj921P5zZ80\nSC8VdCjszUyj8UVEpF0j+vaG2gye/Lf2KVNBR9+FwqRUISIiaWXSwfGV71YtCXlciUDHw35eUqoQ\nEZG0EgpBTp8IlTtC1NZ6XY20GvZmdo+ZnWdmvfY855y7pnvKEhERvxsxJgIYr72ltPdaW3v29wKH\nAM+b2atm9l0zO6Sb6hIREZ/71NT4v3OWlHtbiLQe9s65d5xzP3HOHU98qtw1wLfMbJ6Z3WtmF3db\nlSIi4jvXXR6mz7FLqczZ4XUpPV4iq97hnNsGPFJ/w8wmAVOTWJeIiPjcgaMzOOzMjcxd0s/rUnq8\nhAfomdkpe+475+Y4536enJJERCRdVLw7hud/Op7ycud1KT1aR0bj/yppVYiISFoKVOQBxtMv1Xhd\nSo+m2Q5ERCRpzjrTAHj4MU2b66U2z9mb2X2AAwwYamb37mnTZXgiItKez16YxU1fh3mzgl6X0qO1\nN0Dv/gb3PwE8kLxSREQk3RQVBgiGomwuzfS6lB6tzbB3zs3cc9/Mdjd8LCIikoih46pYuzKDuqiR\nEdTZYy905KeuKZBERKTDvvydSgrOncOyLZpcxysJh71z7uhkFiIiIunpvFNziJaHefDfWu7WKzqe\nIiIiSTW8fy5lr07g3jvzvC6lx0poBj0REZF9FQgYFgmxcUUA58DM64p6Hu3Zi4hI0u03rBYXDbBk\nedTrUnqkhMLezIaZ2ZT6+9kNl70VERFpz1HHxgC479FqjyvpmdoNezO7Dngc+Ev9U0OAp5JZlIiI\npJcrLw0B8PLLHhfSQyWyZ/9l4DhgF4BzbikwMJlFiYhIejnlhDAWjBHou9vrUnqkRAbo1Tjnaq1+\nRIWZZRCfQldERCQhmZlw6W/ms7ZmOzDY63J6nET27Gea2feB7Pplbh8DnkluWSIikm5G9enD3H8e\nwLz5Ea9L6XESCfsbgS3AfODzwPPAD5JZlIiIpJ9etX2oWFDEX/6h5W67W3ur3gWBB5xzlwN/7Z6S\nREQkHU0uzgHgv6/oqu/u1uZP3DkXBQaY2T4tV2RmU82sxMyWmdmNbWx3oZk5Myvel+8jIiKpr/iQ\nEARirF0e8rqUHieRAXqrgDfNbDpQsedJ59wdbX1R/VGBu4BTgFJglplNd84tarJdL+CrwLsdK11E\nRPwkEIBe/SLs3ppJdbUjK0tT6XWXRI6lrAeerd+2V4Nbe44EljnnVjjnaoFHgXNa2O6nwG2AZloQ\nEUlzYybEB+e9+4HO23endvfsnXM3w8d74M45l+gahYXA2gaPS4GjGm5gZocBRc65Z83s2629kJld\nD1wPMHTo0AS/vYiIpJrLr4CWf2EcAAAU3klEQVQPF5SzfHsVk8nyupweI5EZ9A4ys3nAAmChmc0x\nswkJvHZLx2c+vj7fzALAb4FvtfdCzrl7nHPFzrniAQMGJPCtRUQkFX3hs9kM+/wbbLHtXpfSoyRy\nGP8e4JvOuWHOuWHEwzmRkfmlQFGDx0OInxLYoxdwEPCama0Cjgama5CeiEj6ygoFsbkT+M3X9ve6\nlB4lkbDPdc7N2PPAOfcakJvA180CRpvZiPrR/JcA0xu8zk7nXIFzbrhzbjjwDnC2c252RzogIiL+\nUlvany1LerFzpyZj7S6JhP0KM/uhmQ2vv/0AWNneFznn6oCvAC8Bi4FpzrmFZnaLmZ3dubJFRMSv\nhg8HMB59WuOyu0siYX8NMAB4ov5WAFydyIs75553zo1xzo1yzv28/rkfOeemt7DtidqrFxFJfxee\nFwTgsSe0tn13Mef8dRiluLjYzZ6tzwQiIn61vczRvx8MKKph8xqNyG+Pmc1xznVqPFsio/FfNrM+\nDR73NbOXOvNNRUSk5+rX1wjn1VFVqz377pLIYfwC59yOPQ+cc2VoPXsREemEky7aTua4UmrrYl6X\n0iMkEvYxM/t4JhszG4bWsxcRkU644Rsx8iatYPbSXV6X0iMkMjf+TcD/zGxm/eMTqJ/NTkREZF+M\nKchnze9O5ecby3nuQa+rSX+JTJf7opkdTnzSGwO+4ZzbmvTKREQkbQ0ryIYYvPmyBuh1h0QG6B0H\nVDnnngXyge/XH8oXERHZJ6GQEc6tY9fWED67KMyXEjln/2eg0swOAf4PWA38I6lViYhI2hsyMoKL\nBZi/uM7rUtJeImFf5+IX458D/N45dyeJLXErIiLSquMnx0fi/+1BLXebbImE/W4z+x5wOfCcmQWB\nUHLLEhGRdHft5ZkArN2maXOTLZGw/zRQA1zrnNtIfJ3625NalYiIpL1jizMZccZSwgds8LqUtJfI\naPyNwB0NHq9B5+xFRKSTAgE4+exK3vgfRKMQDHpdUfpKZM9eREQkKapK9qfkn4fw3zdrvS4lrSns\nRUTEM/v1zgHg/ocV9snUatib2bfNrKg7ixERkZ7lusvDALz1hvY9k6mtn24h8JaZvW5mXzSzgu4q\nSkREeoYxozKwYIz1q3WRVzK1GvbOuW8AQ4EfAhOBD83sBTO7wsx0nb2IiHSJPgMi1FZkUFWlqfSS\npc3jJi5upnPui0AR8DvgG8Cm7ihORETS37EnVwNGSWml16WkrURWvcPMDgYuIX7N/Tbg+8ksSkRE\neo5vf8uYl/0aK3eP5lByvS4nLbUa9mY2GriUeMhHgUeBU51zK7qpNhER6QGOm5iH257PP/4B5x3u\ndTXpqa09+5eAR4BPO+fmd1M9IiLSw4SCAapmH8ALL4fjJ4uly7V1zv5TwAtNg97MjjezUcktS0RE\nepKsYAY1uzPZui3mdSlpqa2w/y2wq4Xnq9BnLxER6UKHFUcBuH+aFsVJhrbCfrhz7sOmTzrnZgPD\nk1aRiIj0OJdeHJ8Y/6mndfldMrQV9llttGV3dSEiItJzXXRmFuBYWqKZ9JKhrZ/qLDO7rumTZnYt\nMCd5JYmISE+TlWX0G1pJsH9LZ4+ls9oajf914Ekzu4y94V4MZALnJbswERHpWb5z1zrueWsZVbWf\nIjtT6912pbamy93knDsWuBlYVX+72Tl3TP0a9yIiIl1mWF5fNjx+OPc9VuF1KWmn3Rn0nHMzgBnd\nUIuIiPRgBw7uRdXSMH//awVfuszratKLRkKIiEhKOHh0FhgsWZDpdSlpR2EvIiIpIRiEnPwI5WUZ\nOF2B16UU9iIikjKGHRCBWIB359Z6XUpaUdiLiEjKOPmU+C79069oJr2upLAXEZGU8aWrw4QLt1MZ\n3ul1KWlFYS8iIilj3OgQx33jQ3b11hXeXUlhLyIiKSX24Wj+8ZVDiEQ0Sq+rKOxFRCS17OhNrCqT\nZ1+u8bqStKGwFxGRlHLSCfFo+uejdR5Xkj4U9iIiklKu+2x80dX33tX8+F1FYS8iIillv4FBAqEo\nm9eGvC4lbSjsRUQk5ew/qppo1BGNaZBeV1DYi4hIyrn8C1X0PmY5SzaUe11KWlDYi4hIyrn20jB5\nh6zhtXm7vS4lLSjsRUQk5YwsyGPDvcfz21vyvC4lLbS7nr2IiEh3CwSMQDTE8ve13G1X0J69iIik\npMFFtcQiQdZvinpdiu8p7EVEJCUdcUwMgHv+oRXwOkthLyIiKenKy+Jnmp973uNC0oDCXkREUtLp\nJ4WxjCgVUc2R31kaoCciIikpI8M4+SvL2R0uA47yuhxf0569iIikrE8eH2T5wjArSyNel+JrCnsR\nEUlZoR392frsofz5Pg3S6wyFvYiIpKzJk3IAePYZxVVn6KcnIiIpq3hiJhaIsXqpVsDrjKSGvZlN\nNbMSM1tmZje20P5NM1tkZh+a2atmNiyZ9YiIiL+YQW7fOip3hojFvK7Gv5IW9mYWBO4CTgPGA5ea\n2fgmm80Dip1zE4HHgduSVY+IiPjTAeMi4IzX3tQlePsqmXv2RwLLnHMrnHO1wKPAOQ03cM7NcM5V\n1j98BxiSxHpERMSHLv1MfJf+/VUVHlfiX8kM+0JgbYPHpfXPteZa4IWWGszsejObbWazt2zZ0oUl\niohIqvvcZ3IYesMrxAbq7/++SmbYWwvPuRY3NLscKAZub6ndOXePc67YOVc8YMCALixRRERSXb/8\nIH137c8Df9Ryt/sqmTPolQJFDR4PAdY33cjMpgA3AZOdczohIyIizVQuKmTp7F7U1joyM1val5S2\nJHPPfhYw2sxGmFkmcAkwveEGZnYY8BfgbOfc5iTWIiIiPja4IAguwKNPa3KdfZG0sHfO1QFfAV4C\nFgPTnHMLzewWMzu7frPbgTzgMTN738ymt/JyIiLSg513bnxv/pHHtbb9vjDnWjyNnrKKi4vd7Nmz\nvS5DRES60a7dMfJ7G4NGVLNxRbbX5XQrM5vjnCvuzGtoBj0REUl5vXsFyMiKsmNr0OtSfElhLyIi\nvjBpyk6CBbuIRDWVXkcp7EVExBe+/7MaBlz8LiUbd3tdiu8o7EVExBcO7N+HjQ8fzR2/1yC9jlLY\ni4iIL4wuzKZ2Qx+efijH61J8R2EvIiK+YGZkZsUo25TpdSm+o7AXERHf2H94BFcXYMWaOq9L8RWF\nvYiI+MZxn4iPxP/zfZpdvSMU9iIi4hufuyIEwPsLIx5X4i8KexER8Y0Tjg7Tf+ImQkO2eV2Kryjs\nRUTEN8zg4u+UUtanFJ/N9u4phb2IiPhK1aIhvPPjE5i3oNbrUnxDYS8iIr5SkJkLGL//q8I+UQp7\nERHxlc99NguA/81UhCVKPykREfGVsSMzsGCMdStDXpfiGwp7ERHxnfwBEap3ZxCNapReIhT2IiLi\nO8XH1QDGewuqvC7FFxT2IiLiOzfcAP1Onc+a3bu8LsUXFPYiIuI7U4/Lo/cB23h9rvbsE6GwFxER\n38nMCLDjucP5562DvC7FFxT2IiLiS5mE2L0pm8oqDdJrj8JeRER86ZBJUcC491Edym+Pwl5ERHzp\nkoviEfbvJ2MeV5L6FPYiIuJLl52XDTgWzMvwupSUp7AXERFfCoeNPoWV1KK17dujsBcREd+66qbN\n9D1nFtWRqNelpDSFvYiI+NaxE3PYVTKIZ2eWe11KSlPYi4iIbx3QP5+yVybwxz94XUlqU9iLiIhv\nHTo2C8wx7+0sr0tJaQp7ERHxLTPI7h1h93Ytd9sWhb2IiPja0FF1uGiADxdrVH5rFPYiIuJrJ34y\nPqnO3x+q8biS1KWwFxERX/v6F8JgMbbVVnhdSsrStEMiIuJrB44K8YmfziSnKBfQKngt0Z69iIj4\nXv72Ih6/vQinBfBapLAXERHfq1vXj7IFg3hpZrXXpaQkhb2IiPhe8aHxS+/ufbDO40pSk8JeRER8\n78vXZAGO995SrLVEPxUREfG9/QYGCYRibFid6XUpKUlhLyIiaaFg/1pqqwNEoxql15TCXkRE0sLF\n11SSVbSN+at0vX1TCnsREUkLX74+xKBL3mPFzh1el5JyFPYiIpIWRg/sxZZHjuGXP8r2upSUoxn0\nREQkLQQDRnRHHnNf0X5sU/qJiIhI2igYGCVaE2T7jpjXpaQUhb2IiKSNSUdFAePuBzSTXkMKexER\nSRtXfCZ+dvqZ57Rn35DCXkRE0sa5nwpjwSgbNyvsG9IAPRERSRvBoHH4+euoCZcD470uJ2Voz15E\nRNLKZddVsyN/PVt3alGcPRT2IiKSVurWDKD0rin87eFKr0tJGQp7ERFJK584JAeAh/8Z9LiS1KGw\nFxGRtHJMcRgCMVaUhLwuJWUo7EVEJO3k9qmjokxj0PdIatib2VQzKzGzZWZ2YwvtYTP7V337u2Y2\nPJn1iIhIzzBybARcgDfeq/G6lJSQtLA3syBwF3Aa8esfLjWzptdBXAuUOecOAH4L/CpZ9YiISM9x\nwUXx6+xffbvK40pSQzKPcRwJLHPOrQAws0eBc4BFDbY5B/hJ/f3HgT+amTnnXBLrEhGRNPfFK7O5\n67W5PP1uL+47KIPKnY3j7oDi3Xzyio3Mf60P77/cl6rde9st6BhzxG4mf2YT817uy/wZfaku3zvY\nLxhyjD16J5+4aAuznuvPwjfyqa3c254RjjH+uJ0cfe5W3nmqgMVv5ROp3rtvnZkdY8IJOzjijG38\nb9pAlrzXi7rave1ZuVEOPqmMw04t47WHB3XJzyOZYV8IrG3wuBQ4qrVtnHN1ZrYT6A9sbbiRmV0P\nXA8wdOjQZNUrIiJpYmC/DC7+NDzxtwCblmQRizQemb+r3FF7+EbWzc1j87IsXMN2g91VdVRN3Mia\nOX3YtiwLV9fgQLg5dkeyKB+3kVVz+lO2PAsXbdAecFS4KnYcsJEVsweyc2UWLmp7vzzgqMjIYNuw\njSybvR+7VmVBrEF70FGZtZNN+2+k5L0hXfLzsGTtRJvZRcCnnHOfq3/8WeBI59wNDbZZWL9Naf3j\n5fXbbGvtdYuLi93s2bOTUrOIiEiqMbM5zrnizrxGMgfolQJFDR4PAda3to2ZZQD5wPYk1iQiItLj\nJDPsZwGjzWyEmWUClwDTm2wzHbiy/v6FwH91vl5ERKRrJe2cff05+K8ALwFB4F7n3EIzuwWY7Zyb\nDvwd+KeZLSO+R39JsuoRERHpqZI644Bz7nng+SbP/ajB/WrgomTWICIi0tNpBj0REZE0p7AXERFJ\ncwp7ERGRNKewFxERSXMKexERkTSnsBcREUlzCnsREZE0p7AXERFJcwp7ERGRNJe0Ve+Sxcx2AyVe\n15FEBTRZ4jfNpHP/0rlvoP75nfrnX2Odc7068wJJnS43SUo6u9RfKjOz2eqfP6Vz30D98zv1z7/M\nrNPruuswvoiISJpT2IuIiKQ5P4b9PV4XkGTqn3+lc99A/fM79c+/Ot033w3QExERkY7x4569iIiI\ndIDCXkREJM0p7EVERNKcwl5ERCTNpVXYm9lQM5tuZvea2Y1e19PVzCxgZj83sz+Y2ZVe19PVzCzX\nzOaY2Zle19LVzOxcM/urmT1tZqd6XU9XqH+/Hqjv12Ve19PV0vE9ayjN/7+l+9/KDmddyoR9fdGb\nzWxBk+enmlmJmS1LoFNjgOecc9cA45NW7D7oov6dAxQCEaA0WbV2VBf1DeC7wLTkVLnvuqJ/zrmn\nnHPXAVcBn05iuZ3Swb6eDzxe36+zu73YfdCR/vnlPdtjH35PU/L/W2s62L+U/FvZlg72r+NZ55xL\niRtwAnA4sKDBc0FgOTASyAQ+qO/YwcCzTW4Dgf7ADOC/wNVe9ykJ/bsR+Hz91z7udZ+6uG9TgEuI\n/2E90+s+dXX/Gnzdb4DDve5TF/X1e8Ch9ds87HXtXd0/v7xn+/jepez/ty7qX0r+rezC/nU461Jm\nbnzn3OtmNrzJ00cCy5xzKwDM7FHgHOfcL4Bmh57M7NvAj+tf63HgvuRWnbgu6l8pUFv/MJq8ajum\ni/p2EpBL/Be5ysyed87Fklp4grqofwb8EnjBOTc3uRXvu470lfge0xDgfVLoKGFbOtI/M1uMD96z\nPTr43uWRov/fWtPB/q0lBf9WtqWD/YvQwaxLmbBvRSHxN22PUuCoNrZ/EfiJmX0GWJXEurpKR/v3\nBPAHMzseeD2ZhXWBDvXNOXcTgJldBWxN9T88dPy9u4H43lS+mR3gnLs7mcV1sdb6+nvgj2Z2BvCM\nF4V1kdb65+f3bI8W++ac+wr46v9ba1p77+7EP38r29Ja/+6mg1mX6mFvLTzX6pR/zrkFwIXJK6fL\ndbR/lcC1ySunS3Wobx9v4Nz9XV9KUnT0vfs98XD0oxb76pyrAK7u7mKSoLX++fk926PN31Mf/X9r\nTWvvnZ/+Vraltf51OOtS/dBbKVDU4PEQYL1HtSRDOvcvnfsG6d+/htK9r+ncv3TuG6h/CUv1sJ8F\njDazEWaWSXxAyXSPa+pK6dy/dO4bpH//Gkr3vqZz/9K5b6D+Jc7rEYgNRh0+Amxg76US19Y/fzqw\nhPiIxJu8rlP961l96wn960l9Tef+pXPf1L/O90+r3omIiKS5VD+MLyIiIp2ksBcREUlzCnsREZE0\np7AXERFJcwp7ERGRNKewFxERSXMKexH5mJmtMrOCzm4jIqlFYS8iIpLmFPYiPZSZPWVmc8xsoZld\n36RtuJl9ZGYPmNmHZva4meU02OQGM5trZvPN7MD6rznSzN4ys3n1/47t1g6JSKsU9iI91zXOuUlA\nMfBVM+vfpH0scI9zbiKwC/hSg7atzrnDgT8D365/7iPgBOfcYcCPgFuTWr2IJExhL9JzfdXMPgDe\nIb6y1ugm7Wudc2/W338Q+ESDtifq/50DDK+/nw88ZmYLgN8CE5JRtIh0nMJepAcysxOBKcAxzrlD\ngHlAVpPNmi6c0fBxTf2/USCj/v5PgRnOuYOAs1p4PRHxiMJepGfKB8qcc5X159yPbmGboWZ2TP39\nS4H/JfCa6+rvX9UlVYpIl1DYi/RMLwIZZvYh8T3yd1rYZjFwZf02/Yifn2/LbcAvzOxNINiVxYpI\n52iJWxFpxsyGA8/WH5IXEZ/Tnr2IiEia0569iIhImtOevYiISJpT2IuIiKQ5hb2IiEiaU9iLiIik\nOYW9iIhImvt/FbpnkpxyYC0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1b921d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lasso = Lasso(random_state=42)\n",
    "alphas = np.logspace(-8, 8, 10)\n",
    "\n",
    "scores = list()\n",
    "scores_std = list()\n",
    "\n",
    "n_folds = 3\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso.alpha = alpha\n",
    "    this_scores = cross_val_score(lasso, x, y, cv=n_folds, n_jobs=1)\n",
    "    scores.append(np.mean(this_scores))\n",
    "    scores_std.append(np.std(this_scores))\n",
    "\n",
    "scores, scores_std = np.array(scores), np.array(scores_std)\n",
    "\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "plt.semilogx(alphas, scores)\n",
    "\n",
    "# plot error lines showing +/- std. errors of the scores\n",
    "std_error = scores_std / np.sqrt(n_folds)\n",
    "\n",
    "plt.semilogx(alphas, scores + std_error, 'b--')\n",
    "plt.semilogx(alphas, scores - std_error, 'b--')\n",
    "\n",
    "# alpha=0.2 controls the translucency of the fill color\n",
    "plt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2)\n",
    "\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(scores), linestyle='--', color='.5')\n",
    "plt.xlim([alphas[0], alphas[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 32.519569396972656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/scipy/linalg/basic.py:40: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number/precision: 1.2776476057685393e-11 / 5.960464477539063e-08\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item-Post It Notes</th>\n",
       "      <td>-56.841797</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Thumbtacks</th>\n",
       "      <td>-55.589020</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Pencils</th>\n",
       "      <td>-49.602299</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperclips</th>\n",
       "      <td>-41.850784</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Red</th>\n",
       "      <td>-38.117607</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Green</th>\n",
       "      <td>-25.877810</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Blue</th>\n",
       "      <td>-11.786979</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>-9.410345</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>-7.374828</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>width</th>\n",
       "      <td>-5.693603</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Generic</th>\n",
       "      <td>-4.691457</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Ink Pens</th>\n",
       "      <td>-4.656077</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Tiny</th>\n",
       "      <td>-1.190912</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Offices-R-Us</th>\n",
       "      <td>-0.914563</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Deep Office Supplies</th>\n",
       "      <td>-0.701630</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Brown</th>\n",
       "      <td>-0.428727</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Small</th>\n",
       "      <td>-0.085723</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack</th>\n",
       "      <td>0.019621</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>0.034968</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-6% Solution</th>\n",
       "      <td>0.116749</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Duck Lake</th>\n",
       "      <td>0.270160</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-WizBang</th>\n",
       "      <td>1.214188</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Medium</th>\n",
       "      <td>2.258845</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Large</th>\n",
       "      <td>4.766972</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperweights</th>\n",
       "      <td>5.014147</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-High Quality</th>\n",
       "      <td>7.386119</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Black</th>\n",
       "      <td>14.024263</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-White</th>\n",
       "      <td>27.147795</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Pink</th>\n",
       "      <td>38.815361</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Stapler</th>\n",
       "      <td>45.628017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Tablets</th>\n",
       "      <td>157.815704</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         coef  positive\n",
       "item-Post It Notes                 -56.841797     False\n",
       "item-Thumbtacks                    -55.589020     False\n",
       "item-Pencils                       -49.602299     False\n",
       "item-Paperclips                    -41.850784     False\n",
       "color-Red                          -38.117607     False\n",
       "color-Green                        -25.877810     False\n",
       "color-Blue                         -11.786979     False\n",
       "height                              -9.410345     False\n",
       "length                              -7.374828     False\n",
       "width                               -5.693603     False\n",
       "quality-Generic                     -4.691457     False\n",
       "item-Ink Pens                       -4.656077     False\n",
       "size-Tiny                           -1.190912     False\n",
       "manufacturer-Offices-R-Us           -0.914563     False\n",
       "manufacturer-Deep Office Supplies   -0.701630     False\n",
       "color-Brown                         -0.428727     False\n",
       "size-Small                          -0.085723     False\n",
       "pack                                 0.019621      True\n",
       "weight                               0.034968      True\n",
       "manufacturer-6% Solution             0.116749      True\n",
       "manufacturer-Duck Lake               0.270160      True\n",
       "manufacturer-WizBang                 1.214188      True\n",
       "size-Medium                          2.258845      True\n",
       "size-Large                           4.766972      True\n",
       "item-Paperweights                    5.014147      True\n",
       "quality-High Quality                 7.386119      True\n",
       "color-Black                         14.024263      True\n",
       "color-White                         27.147795      True\n",
       "color-Pink                          38.815361      True\n",
       "item-Stapler                        45.628017      True\n",
       "item-Tablets                       157.815704      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [ 78.86076355]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAD8CAYAAADExYYgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a26a4e748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Create linear regression\n",
    "regressor = Ridge(alpha=1)\n",
    "\n",
    "# Fit/train Ridge\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_[0,:],\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 33.101341247558594\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item-Pencils</th>\n",
       "      <td>-40.795601</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Red</th>\n",
       "      <td>-35.561138</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Thumbtacks</th>\n",
       "      <td>-33.184319</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Green</th>\n",
       "      <td>-24.621284</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperweights</th>\n",
       "      <td>-23.317530</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Large</th>\n",
       "      <td>-15.308939</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Post It Notes</th>\n",
       "      <td>-12.960384</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Ink Pens</th>\n",
       "      <td>-11.629585</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Blue</th>\n",
       "      <td>-11.232443</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperclips</th>\n",
       "      <td>-7.472833</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Generic</th>\n",
       "      <td>-4.679661</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Medium</th>\n",
       "      <td>-4.627986</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Offices-R-Us</th>\n",
       "      <td>-1.101483</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Deep Office Supplies</th>\n",
       "      <td>-0.770265</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Brown</th>\n",
       "      <td>-0.215902</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack</th>\n",
       "      <td>0.013878</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>0.036338</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Duck Lake</th>\n",
       "      <td>0.148932</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-6% Solution</th>\n",
       "      <td>0.426581</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-WizBang</th>\n",
       "      <td>1.185131</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>width</th>\n",
       "      <td>1.982397</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>2.681512</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Stapler</th>\n",
       "      <td>2.830680</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>3.379771</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Small</th>\n",
       "      <td>6.011306</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-High Quality</th>\n",
       "      <td>7.231806</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Black</th>\n",
       "      <td>13.037317</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Tiny</th>\n",
       "      <td>16.263184</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-White</th>\n",
       "      <td>25.214830</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Pink</th>\n",
       "      <td>35.919052</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Tablets</th>\n",
       "      <td>126.973938</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         coef  positive\n",
       "item-Pencils                       -40.795601     False\n",
       "color-Red                          -35.561138     False\n",
       "item-Thumbtacks                    -33.184319     False\n",
       "color-Green                        -24.621284     False\n",
       "item-Paperweights                  -23.317530     False\n",
       "size-Large                         -15.308939     False\n",
       "item-Post It Notes                 -12.960384     False\n",
       "item-Ink Pens                      -11.629585     False\n",
       "color-Blue                         -11.232443     False\n",
       "item-Paperclips                     -7.472833     False\n",
       "quality-Generic                     -4.679661     False\n",
       "size-Medium                         -4.627986     False\n",
       "manufacturer-Offices-R-Us           -1.101483     False\n",
       "manufacturer-Deep Office Supplies   -0.770265     False\n",
       "color-Brown                         -0.215902     False\n",
       "pack                                 0.013878      True\n",
       "weight                               0.036338      True\n",
       "manufacturer-Duck Lake               0.148932      True\n",
       "manufacturer-6% Solution             0.426581      True\n",
       "manufacturer-WizBang                 1.185131      True\n",
       "width                                1.982397      True\n",
       "length                               2.681512      True\n",
       "item-Stapler                         2.830680      True\n",
       "height                               3.379771      True\n",
       "size-Small                           6.011306      True\n",
       "quality-High Quality                 7.231806      True\n",
       "color-Black                         13.037317      True\n",
       "size-Tiny                           16.263184      True\n",
       "color-White                         25.214830      True\n",
       "color-Pink                          35.919052      True\n",
       "item-Tablets                       126.973938      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [ 14.47364044]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAD8CAYAAADExYYgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzMzMWur/AaLMQWZzOEo4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a26b3f6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Create linear regression\n",
    "regressor = ElasticNet(alpha=0.01, l1_ratio=0.1)\n",
    "\n",
    "# Fit/train LASSO\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_,\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 5659.2567 - val_loss: 4107.3521\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 4779.3982 - val_loss: 4016.6760\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 4744.6900 - val_loss: 3766.0933\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 4407.7363 - val_loss: 3726.1054\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 4390.4114 - val_loss: 3598.3143\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 4279.8354 - val_loss: 3572.0842\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 4246.6302 - val_loss: 3479.0055\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 4034.8926 - val_loss: 3195.6319\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 3981.7770 - val_loss: 3389.9014\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 3666.8847 - val_loss: 2683.5883\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 3741.5122 - val_loss: 4201.3279\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 3404.5395 - val_loss: 2776.7649\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 3003.8237 - val_loss: 3377.8045\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 2814.8209 - val_loss: 3054.9885\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 2310.4501 - val_loss: 1395.5146\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 2020.6620 - val_loss: 1363.8951\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 1690.6782 - val_loss: 1031.1212\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 1063.1730 - val_loss: 776.8578\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 1340.1247 - val_loss: 1108.0854\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 1304.3986 - val_loss: 1157.4431\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 1184.8575 - val_loss: 911.6436\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 1034.3336 - val_loss: 746.4589\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 920.1501 - val_loss: 1336.1240\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 1029.1467 - val_loss: 937.7246\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 878.1173 - val_loss: 830.9437\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 815.9878 - val_loss: 582.5163\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 629.3501 - val_loss: 426.2885\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 631.6635 - val_loss: 823.6201\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 804.0534 - val_loss: 627.9225\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 701.3764 - val_loss: 697.4798\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 524.1786 - val_loss: 457.0281\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 489.5541 - val_loss: 384.9645\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 556.6016 - val_loss: 452.7769\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 655.1479 - val_loss: 473.3120\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 656.5258 - val_loss: 389.0418\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 489.0325 - val_loss: 362.9085\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 549.4442 - val_loss: 547.4789\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 593.4842 - val_loss: 376.2775\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 723.1850 - val_loss: 1005.7291\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 512.7578 - val_loss: 550.0371\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 586.7535 - val_loss: 342.0204\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 438.3960 - val_loss: 334.5297\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 391.5945 - val_loss: 322.7529\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 401.0965 - val_loss: 333.3260\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 521.3960 - val_loss: 425.1121\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 417.8856 - val_loss: 351.2577\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 572.1939 - val_loss: 448.4051\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 685.1117 - val_loss: 340.6831\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 509.2139 - val_loss: 624.3439\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 582.4748 - val_loss: 495.5118\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 424.7290 - val_loss: 402.0109\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 408.9260 - val_loss: 364.7910\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 443.3361 - val_loss: 309.2761\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 395.7323 - val_loss: 1085.4509\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 484.5899 - val_loss: 295.6664\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 381.6646 - val_loss: 297.5304\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 530.7311 - val_loss: 506.1497\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 411.4864 - val_loss: 477.3331\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 508.3070 - val_loss: 315.5623\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 382.9074 - val_loss: 605.7028\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 598.1974 - val_loss: 398.0267\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 370.9709 - val_loss: 368.8024\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 374.6867 - val_loss: 414.8961\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 330.5105 - val_loss: 269.5011\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 389.2400 - val_loss: 575.1166\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 376.4517 - val_loss: 258.8083\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 354.9585 - val_loss: 267.1639\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 367.9117 - val_loss: 367.9575\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 354.5512 - val_loss: 237.2036\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 439.2925 - val_loss: 298.2447\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 421.5175 - val_loss: 286.0385\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 350.7591 - val_loss: 415.5244\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 276.3982 - val_loss: 284.5272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 381.8733 - val_loss: 881.9468\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 340.3723 - val_loss: 249.7436\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 311.8190 - val_loss: 280.8514\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 351.3682 - val_loss: 249.7712\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 307.6452 - val_loss: 469.1609\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 355.7113 - val_loss: 244.2583\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 434.9029 - val_loss: 342.3024\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 364.3188 - val_loss: 256.2639\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 359.3189 - val_loss: 265.2827\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 270.8316 - val_loss: 225.7603\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 327.3165 - val_loss: 364.1777\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 340.1595 - val_loss: 294.7070\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 303.4044 - val_loss: 341.8192\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 275.6191 - val_loss: 235.5137\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 279.1783 - val_loss: 342.7534\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 268.1333 - val_loss: 366.9964\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 282.8096 - val_loss: 246.4598\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 318.1492 - val_loss: 494.6871\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 404.7629 - val_loss: 417.3099\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 337.2799 - val_loss: 261.4747\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 298.7399 - val_loss: 224.5775\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 262.1766 - val_loss: 223.7285\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 397.0564 - val_loss: 211.6010\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 286.5635 - val_loss: 573.3307\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 285.2219 - val_loss: 294.6331\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 317.8071 - val_loss: 223.2828\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 316.0024 - val_loss: 468.4256\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 293.4990 - val_loss: 545.3061\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 296.1943 - val_loss: 225.8682\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 382.0465 - val_loss: 351.2266\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 304.8552 - val_loss: 207.3920\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 273.9010 - val_loss: 230.5849\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 302.3668 - val_loss: 426.8142\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 302.9784 - val_loss: 789.8969\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 333.4123 - val_loss: 231.9548\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 276.7119 - val_loss: 355.6054\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 249.2944 - val_loss: 383.5488\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 265.3472 - val_loss: 213.3777\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 260.3456 - val_loss: 507.3135\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 331.9445 - val_loss: 232.8515\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 305.0103 - val_loss: 667.7762\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 257.3275 - val_loss: 236.9913\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 281.0412 - val_loss: 218.2005\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 302.3539 - val_loss: 233.0876\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 590.1003 - val_loss: 253.7629\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 250.7708 - val_loss: 457.5341\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 234.0303 - val_loss: 249.1234\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 237.0348 - val_loss: 239.7644\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 409.7131 - val_loss: 205.3458\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 225.9678 - val_loss: 208.3355\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 231.4882 - val_loss: 239.1292\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 302.8700 - val_loss: 447.0495\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 258.4381 - val_loss: 212.1083\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 275.0072 - val_loss: 245.2584\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 266.4190 - val_loss: 376.6271\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 250.8722 - val_loss: 199.2322\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 262.9978 - val_loss: 216.8832\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 237.2796 - val_loss: 245.2196\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 294.1671 - val_loss: 330.9139\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 230.4183 - val_loss: 579.3594\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 368.3393 - val_loss: 215.6978\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 287.2959 - val_loss: 208.8314\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 215.1302 - val_loss: 245.9097\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 227.3416 - val_loss: 353.8781\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 301.7470 - val_loss: 278.8518\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 264.4430 - val_loss: 278.1430\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 369.3947 - val_loss: 219.4746\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 322.6689 - val_loss: 392.8800\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 269.1340 - val_loss: 343.1902\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 257.1601 - val_loss: 334.2152\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 286.5402 - val_loss: 257.2007\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 291.8378 - val_loss: 246.3332\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 229.7338 - val_loss: 226.7071\n",
      "Epoch 147/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 260.1289 - val_loss: 240.9845\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 261.1529 - val_loss: 204.5672\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 255.3661 - val_loss: 351.4143\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 298.3389 - val_loss: 242.1885\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 258.0382 - val_loss: 196.3030\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 246.2309 - val_loss: 292.8368\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 269.2809 - val_loss: 196.3768\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 241.9543 - val_loss: 877.5647\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 258.4714 - val_loss: 365.3691\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 323.4356 - val_loss: 218.1078\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 221.1689 - val_loss: 206.9814\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 209.1083 - val_loss: 223.8483\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 231.6991 - val_loss: 204.9874\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 232.9219 - val_loss: 533.7490\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 238.5694 - val_loss: 198.3310\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 242.9486 - val_loss: 249.9988\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 215.1942 - val_loss: 211.2320\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 290.5108 - val_loss: 758.7094\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 385.3334 - val_loss: 237.2860\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 303.8529 - val_loss: 201.4818\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 223.0353 - val_loss: 631.8562\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 233.2977 - val_loss: 203.8998\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 231.0972 - val_loss: 350.9730\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 270.4596 - val_loss: 298.5966\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 338.3472 - val_loss: 326.0339\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 201.8135 - val_loss: 179.6847\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 261.7933 - val_loss: 257.4017\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 240.7950 - val_loss: 210.1746\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 190.8526 - val_loss: 244.1649\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 240.0687 - val_loss: 221.8190\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 302.8028 - val_loss: 187.0804\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 223.4486 - val_loss: 240.1773\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 195.8464 - val_loss: 216.0419\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 314.6804 - val_loss: 243.2692\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 250.1561 - val_loss: 262.7622\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 237.7780 - val_loss: 238.6398\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 284.6302 - val_loss: 203.9026\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 236.1755 - val_loss: 226.6067\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 280.5372 - val_loss: 289.0672\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 270.4432 - val_loss: 178.2908\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 211.8191 - val_loss: 198.1497\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 226.3282 - val_loss: 179.1627\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 240.2234 - val_loss: 184.1056\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 211.8463 - val_loss: 256.3319\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 223.4138 - val_loss: 187.5705\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 235.2637 - val_loss: 452.1807\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 241.1084 - val_loss: 192.9678\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 203.7874 - val_loss: 183.9468\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 227.9191 - val_loss: 251.6964\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 431.3728 - val_loss: 224.9992\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 204.3118 - val_loss: 188.1758\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 202.0592 - val_loss: 204.0375\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 230.7073 - val_loss: 188.3682\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 410.2286 - val_loss: 272.0318\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 283.4095 - val_loss: 378.1584\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 251.1779 - val_loss: 230.4098\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 269.3275 - val_loss: 344.0602\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 209.0199 - val_loss: 211.3893\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 216.3947 - val_loss: 176.1272\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 257.7813 - val_loss: 242.7900\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 219.1782 - val_loss: 179.0494\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 246.2524 - val_loss: 224.9360\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 227.6488 - val_loss: 264.4959\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 192.9843 - val_loss: 684.2486\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 213.7461 - val_loss: 181.3852\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 185.0514 - val_loss: 222.6393\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 193.6453 - val_loss: 167.9311\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 216.3333 - val_loss: 503.2661\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 197.7807 - val_loss: 197.2319\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 221.9804 - val_loss: 180.5361\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 283.4253 - val_loss: 435.9460\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 209.1060 - val_loss: 312.1541\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 329.9866 - val_loss: 222.8552\n",
      "Epoch 220/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 76us/step - loss: 211.2625 - val_loss: 181.6720\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 194.2549 - val_loss: 172.4646\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 188.3251 - val_loss: 167.8340\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 270.6398 - val_loss: 272.0801\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 205.0109 - val_loss: 164.1966\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 216.2264 - val_loss: 206.1613\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.1683 - val_loss: 161.9350\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 227.6050 - val_loss: 189.6599\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 357.5911 - val_loss: 182.5592\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 179.2983 - val_loss: 205.2813\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 197.2874 - val_loss: 247.6800\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 214.5653 - val_loss: 167.7531\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 194.5723 - val_loss: 161.5626\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 189.4488 - val_loss: 159.9239\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 1s 134us/step - loss: 198.2021 - val_loss: 179.7749\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 223.6707 - val_loss: 235.3571\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 271.5545 - val_loss: 164.9705\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 221.9100 - val_loss: 166.0980\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 1s 142us/step - loss: 201.5091 - val_loss: 182.9786\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 1s 130us/step - loss: 289.6278 - val_loss: 428.6141\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 231.1462 - val_loss: 194.7216\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 208.4365 - val_loss: 205.3077\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 182.3155 - val_loss: 170.5693\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 214.1766 - val_loss: 168.7522\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 185.4790 - val_loss: 177.3192\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 310.527 - 1s 63us/step - loss: 297.7719 - val_loss: 179.9204\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 187.4459 - val_loss: 191.3219\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 208.7447 - val_loss: 202.1957\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 182.4445 - val_loss: 158.5609\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 187.0727 - val_loss: 222.8448\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 180.7143 - val_loss: 161.9219\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 170.2415 - val_loss: 163.9079\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 198.2935 - val_loss: 157.0644\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 207.7811 - val_loss: 268.8240\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 205.6451 - val_loss: 260.3312\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 334.8095 - val_loss: 222.3643\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 217.7691 - val_loss: 181.0861\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.5000 - val_loss: 163.7398\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 185.3864 - val_loss: 179.6775\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 1s 145us/step - loss: 176.2704 - val_loss: 168.7715\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 194.2595 - val_loss: 203.6103\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 252.2966 - val_loss: 183.5604\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 175.4952 - val_loss: 301.5175\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 162.1041 - val_loss: 180.7019\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 285.5269 - val_loss: 231.6275\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 192.6443 - val_loss: 171.0390\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 193.9407 - val_loss: 173.6447\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 1s 142us/step - loss: 191.9977 - val_loss: 182.8352\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 212.8465 - val_loss: 182.3079\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 196.3988 - val_loss: 321.8888\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 185.4047 - val_loss: 178.1529\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 251.1848 - val_loss: 164.7463\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 185.0805 - val_loss: 178.9387\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 182.7997 - val_loss: 251.7605\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 189.4118 - val_loss: 174.9334\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 173.1792 - val_loss: 165.7218\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 178.7918 - val_loss: 215.6397\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 186.0863 - val_loss: 274.6825\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 169.2835 - val_loss: 182.1987\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 180.5876 - val_loss: 566.9802\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 359.5931 - val_loss: 389.0292\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 206.3360 - val_loss: 206.5239\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 231.9926 - val_loss: 179.9264\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 198.9358 - val_loss: 186.4801\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 203.0202 - val_loss: 468.6989\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 207.7341 - val_loss: 241.3418\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 196.4188 - val_loss: 195.5104\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 231.2784 - val_loss: 206.8166\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 191.9570 - val_loss: 350.7609\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 257.0686 - val_loss: 387.5611\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 201.2162 - val_loss: 170.7046\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 214.0881 - val_loss: 220.3958\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 187.3944 - val_loss: 171.6010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 193.6149 - val_loss: 196.2476\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 189.9553 - val_loss: 270.0288\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 188.4434 - val_loss: 174.6447\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 179.5154 - val_loss: 327.0443\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 201.9131 - val_loss: 368.8547\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 200.5616 - val_loss: 210.2083\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 183.3984 - val_loss: 189.1323\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 189.6512 - val_loss: 205.0889\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 178.2010 - val_loss: 186.4284\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 183.0690 - val_loss: 160.0569\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 1s 148us/step - loss: 262.5040 - val_loss: 161.0775\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 179.2195 - val_loss: 155.4059\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 173.8995 - val_loss: 177.1612\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 258.4253 - val_loss: 175.4279\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 170.6546 - val_loss: 258.2963\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 167.6228 - val_loss: 262.8890\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 174.4423 - val_loss: 234.7621\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 254.8325 - val_loss: 266.1643\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 259.7985 - val_loss: 172.1417\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 155.2082 - val_loss: 178.4732\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 181.6448 - val_loss: 260.5816\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 178.5923 - val_loss: 174.7380\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 1s 147us/step - loss: 185.6864 - val_loss: 158.3379\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 194.8861 - val_loss: 156.7146\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 179.6759 - val_loss: 157.7280\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 204.4895 - val_loss: 161.0334\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 187.3742 - val_loss: 180.0614\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 184.7331 - val_loss: 201.5603\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 214.5120 - val_loss: 168.1629\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 208.2785 - val_loss: 160.6908\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 157.2433 - val_loss: 159.3131\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 176.3536 - val_loss: 185.1947\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 173.6984 - val_loss: 488.6118\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 204.8990 - val_loss: 156.3136\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 163.6154 - val_loss: 305.6638\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 179.6993 - val_loss: 179.8442\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 157.8571 - val_loss: 153.3846\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 198.7503 - val_loss: 158.7910\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 178.4902 - val_loss: 171.9481\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 173.6259 - val_loss: 166.3891\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 192.2275 - val_loss: 172.0001\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 172.8808 - val_loss: 264.7324\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 195.6934 - val_loss: 224.0127\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 165.1615 - val_loss: 153.4249\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 178.3257 - val_loss: 244.6247\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 196.2654 - val_loss: 206.1182\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 219.8981 - val_loss: 155.9298\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 157.6240 - val_loss: 167.1165\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 1s 124us/step - loss: 188.8362 - val_loss: 225.2249\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 203.7661 - val_loss: 155.3108\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 248.3649 - val_loss: 154.6684\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 163.0125 - val_loss: 162.7074\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 173.2760 - val_loss: 231.1458\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 208.7394 - val_loss: 322.3671\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 186.7620 - val_loss: 184.6890\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 199.4440 - val_loss: 166.1461\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 1s 142us/step - loss: 155.9554 - val_loss: 153.2786\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 172.0012 - val_loss: 169.6173\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 153.2233 - val_loss: 162.0264\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 218.1425 - val_loss: 221.9113\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 205.0051 - val_loss: 366.9900\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 185.6346 - val_loss: 257.9415\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 146.7277 - val_loss: 158.4763\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 178.3083 - val_loss: 256.4560\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.1232 - val_loss: 162.4740\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 172.5110 - val_loss: 815.9762\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 269.6948 - val_loss: 168.6468\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 174.0541 - val_loss: 155.3284\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 158.4585 - val_loss: 220.7381\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 160.6107 - val_loss: 292.3848\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 166.0126 - val_loss: 223.8124\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 169.6301 - val_loss: 156.8466\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 190.9612 - val_loss: 182.7569\n",
      "Epoch 366/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 85us/step - loss: 155.0305 - val_loss: 148.3629\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 172.4777 - val_loss: 220.5127\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 167.5395 - val_loss: 245.0039\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 179.5156 - val_loss: 183.8823\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 155.7823 - val_loss: 215.0990\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 202.1413 - val_loss: 276.1087\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 173.2266 - val_loss: 190.8855\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 206.1829 - val_loss: 573.6679\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 222.0837 - val_loss: 190.6337\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 204.0261 - val_loss: 289.0225\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 213.2360 - val_loss: 222.5732\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 174.9040 - val_loss: 207.8144\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 164.2069 - val_loss: 170.6146\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 164.7949 - val_loss: 164.4332\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 166.9334 - val_loss: 169.7265\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 160.3811 - val_loss: 183.8929\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 182.9350 - val_loss: 258.1909\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 191.7265 - val_loss: 149.7440\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.3098 - val_loss: 155.0896\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 154.6327 - val_loss: 276.1604\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 168.4121 - val_loss: 204.6950\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 180.7171 - val_loss: 148.4967\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 167.7500 - val_loss: 223.9147\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 176.1674 - val_loss: 151.6427\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 155.6246 - val_loss: 202.5610\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 199.8642 - val_loss: 194.1236\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 174.4855 - val_loss: 163.2568\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 164.3730 - val_loss: 150.0487\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 259.5042 - val_loss: 2033.6062\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 218.7994 - val_loss: 146.8886\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.3913 - val_loss: 149.6779\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.6874 - val_loss: 154.4923\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 152.3836 - val_loss: 149.3045\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 158.8259 - val_loss: 164.1218\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 172.2893 - val_loss: 150.1628\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 154.3567 - val_loss: 174.7648\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 275.6053 - val_loss: 153.2619\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.1553 - val_loss: 155.0003\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 182.3566 - val_loss: 240.4623\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 164.6376 - val_loss: 165.7954\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 165.6723 - val_loss: 264.8528\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 159.0001 - val_loss: 287.7266\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 195.3497 - val_loss: 153.8958\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 179.2611 - val_loss: 170.0545\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 160.8630 - val_loss: 184.4674\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 164.7886 - val_loss: 215.5037\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 163.9255 - val_loss: 146.9788\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 247.2901 - val_loss: 221.6812\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 161.6801 - val_loss: 165.4077\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 146.8674 - val_loss: 160.0264\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 174.8556 - val_loss: 171.6971\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 161.8043 - val_loss: 156.8954\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 161.2288 - val_loss: 230.5875\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 161.6224 - val_loss: 163.6454\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 179.4997 - val_loss: 155.3992\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 156.7103 - val_loss: 386.6616\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 196.6306 - val_loss: 325.8571\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 202.7875 - val_loss: 150.7243\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 173.4635 - val_loss: 148.0129\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 150.9926 - val_loss: 160.8786\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 158.1259 - val_loss: 189.7789\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.4608 - val_loss: 149.4644\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 180.6944 - val_loss: 152.6271\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.8061 - val_loss: 186.3590\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 183.8972 - val_loss: 146.3487\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 158.6528 - val_loss: 145.5738\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 174.9186 - val_loss: 170.1343\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 194.5501 - val_loss: 166.7094\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.0394 - val_loss: 150.8735\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 170.0233 - val_loss: 156.1222\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 162.8461 - val_loss: 239.3278\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 165.1934 - val_loss: 155.7811\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 161.7258 - val_loss: 195.6747\n",
      "Epoch 439/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 183.4374 - val_loss: 147.5091\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 177.6049 - val_loss: 154.1775\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 161.5027 - val_loss: 151.9420\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.7662 - val_loss: 216.2058\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 158.5940 - val_loss: 164.2382\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 169.0392 - val_loss: 167.7677\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 183.9790 - val_loss: 173.3128\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 155.3738 - val_loss: 143.3705\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.9799 - val_loss: 161.6793\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 163.6178 - val_loss: 153.1338\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 150.4054 - val_loss: 141.7688\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.1146 - val_loss: 150.7072\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 178.2264 - val_loss: 157.4627\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 151.7069 - val_loss: 212.9767\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 163.0353 - val_loss: 155.5198\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 203.7937 - val_loss: 150.2497\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.7257 - val_loss: 170.0641\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 161.6658 - val_loss: 198.5109\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 146.6554 - val_loss: 187.6314\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 167.0521 - val_loss: 149.6983\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 190.5404 - val_loss: 252.0092\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 151.9866 - val_loss: 291.7996\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 148.9435 - val_loss: 141.5566\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 148.7417 - val_loss: 152.6008\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 270.9268 - val_loss: 174.2936\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.9891 - val_loss: 173.8749\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.6982 - val_loss: 143.7771\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 142.0738 - val_loss: 145.3033\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.8769 - val_loss: 168.5520\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 148.6324 - val_loss: 197.8208\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.5727 - val_loss: 144.7745\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 168.3217 - val_loss: 178.3067\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 227.3468 - val_loss: 198.1672\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 166.9640 - val_loss: 355.0108\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 160.5029 - val_loss: 189.8346\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 170.4849 - val_loss: 196.6502\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 155.0901 - val_loss: 168.6437\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 152.5500 - val_loss: 179.4077\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.4433 - val_loss: 159.3107\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 157.3924 - val_loss: 151.6290\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 169.4129 - val_loss: 177.4158\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 152.1065 - val_loss: 149.8879\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 374.9827 - val_loss: 220.2346\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 144.4772 - val_loss: 203.3230\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.8886 - val_loss: 149.3631\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.2362 - val_loss: 204.4238\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.2930 - val_loss: 168.6187\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 202.1812 - val_loss: 237.9781\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 159.1305 - val_loss: 141.1921\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 157.7924 - val_loss: 145.0964\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 147.7575 - val_loss: 166.6133\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 165.0934 - val_loss: 157.0683\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 152.7132 - val_loss: 152.1729\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 162.8351 - val_loss: 166.3046\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.9359 - val_loss: 177.2754\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.6324 - val_loss: 155.9174\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 175.6548 - val_loss: 222.7986\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 156.5436 - val_loss: 142.0105\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.4867 - val_loss: 151.2325\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 152.7811 - val_loss: 140.6377\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.0100 - val_loss: 154.7032\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 149.3568 - val_loss: 169.2751\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 170.2390 - val_loss: 952.5081\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 236.3406 - val_loss: 148.6094\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.6487 - val_loss: 150.6280\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 163.1429 - val_loss: 201.3830\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 152.0877 - val_loss: 145.1178\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.3234 - val_loss: 158.9427\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 181.9048 - val_loss: 336.7155\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.5622 - val_loss: 169.5910\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 428.8127 - val_loss: 675.7237\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 319.0110 - val_loss: 220.6293\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 273.2584 - val_loss: 187.8453\n",
      "Epoch 512/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 196.1257 - val_loss: 160.7237\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 166.5210 - val_loss: 180.2157\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 168.3978 - val_loss: 167.5517\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 193.2633 - val_loss: 230.9079\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 199.8426 - val_loss: 480.6429\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 178.7264 - val_loss: 158.3207\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 168.1665 - val_loss: 154.9163\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 170.3273 - val_loss: 174.2055\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 162.9292 - val_loss: 154.7843\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 166.0356 - val_loss: 211.6082\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 218.4975 - val_loss: 192.5656\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 185.6510 - val_loss: 153.7752\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 165.9026 - val_loss: 160.4818\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 196.8323 - val_loss: 265.1884\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 161.7090 - val_loss: 152.6336\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 165.3074 - val_loss: 246.2521\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.5680 - val_loss: 151.6983\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.2352 - val_loss: 173.5970\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.4507 - val_loss: 170.1425\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 198.6676 - val_loss: 180.8857\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 174.2773 - val_loss: 161.1972\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 167.0240 - val_loss: 151.8840\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.8469 - val_loss: 172.6385\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 194.3879 - val_loss: 278.6737\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 171.4712 - val_loss: 319.3161\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 162.0503 - val_loss: 156.6306\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.7406 - val_loss: 156.4233\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 164.9507 - val_loss: 147.9256\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 154.0859 - val_loss: 145.3282\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 184.7992 - val_loss: 182.0361\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 167.2470 - val_loss: 155.5771\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 168.3907 - val_loss: 152.0940\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 226.1862 - val_loss: 155.3583\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.3940 - val_loss: 149.7357\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.6648 - val_loss: 186.5188\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.4455 - val_loss: 174.4791\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 162.8848 - val_loss: 210.7849\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.3757 - val_loss: 166.6210\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 154.5967 - val_loss: 163.1483\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.9546 - val_loss: 183.4684\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 164.2142 - val_loss: 152.9751\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 160.5641 - val_loss: 152.6806\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 162.5440 - val_loss: 160.1028\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 172.1322 - val_loss: 182.4549\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 169.4899 - val_loss: 175.8124\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 181.1425 - val_loss: 152.6857\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 154.7435 - val_loss: 171.5394\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 211.7076 - val_loss: 160.5958\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.3544 - val_loss: 153.3889\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 165.1170 - val_loss: 224.9846\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.7680 - val_loss: 150.1237\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 140.8811 - val_loss: 153.9097\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 194.2233 - val_loss: 147.6740\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.9041 - val_loss: 145.6493\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 155.4209 - val_loss: 158.5445\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 148.1991 - val_loss: 190.3043\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 143.1971 - val_loss: 151.8067\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 161.2403 - val_loss: 155.0364\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.9215 - val_loss: 148.3657\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.5430 - val_loss: 154.3326\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.7962 - val_loss: 145.5876\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 169.5369 - val_loss: 271.5981\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 176.7676 - val_loss: 152.6589\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.4931 - val_loss: 153.4804\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 167.5421 - val_loss: 166.6330\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 139.6872 - val_loss: 176.7936\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.7980 - val_loss: 200.6599\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 170.6991 - val_loss: 144.6717\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.9750 - val_loss: 156.0823\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.7802 - val_loss: 147.7370\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 149.7744 - val_loss: 452.1852\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.4165 - val_loss: 183.8994\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 148.9454 - val_loss: 156.2209\n",
      "Epoch 585/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.3999 - val_loss: 152.5094\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 165.0102 - val_loss: 1047.3710\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 377.7747 - val_loss: 174.7993\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 154.0442 - val_loss: 146.0418\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 150.3874 - val_loss: 194.9476\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 148.8402 - val_loss: 146.0324\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 160.8979 - val_loss: 140.2740\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 145.8217 - val_loss: 146.6886\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.8661 - val_loss: 151.4192\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 142.4953 - val_loss: 240.2580\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 198.6343 - val_loss: 146.3407\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 151.8064 - val_loss: 173.0623\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 152.8052 - val_loss: 197.7398\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 153.5313 - val_loss: 154.4880\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 146.3337 - val_loss: 139.7144\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 142.8723 - val_loss: 179.1655\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 163.6089 - val_loss: 164.0975\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.7152 - val_loss: 170.0274\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 154.5304 - val_loss: 276.5441\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 213.7128 - val_loss: 150.6357\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 152.3852 - val_loss: 144.1701\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 189.796 - 0s 51us/step - loss: 189.3547 - val_loss: 141.6622\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.7659 - val_loss: 154.1661\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.8689 - val_loss: 196.2095\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.9406 - val_loss: 141.0530\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 145.5164 - val_loss: 174.7653\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 150.6454 - val_loss: 183.1853\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 192.4092 - val_loss: 430.6768\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 215.9785 - val_loss: 244.8965\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.5027 - val_loss: 165.1233\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 205.2027 - val_loss: 374.7127\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 170.7080 - val_loss: 139.3312\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.6321 - val_loss: 154.5936\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 416.0716 - val_loss: 209.7168\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 154.9574 - val_loss: 145.7510\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 163.5952 - val_loss: 158.1729\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.8350 - val_loss: 173.0283\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 172.7235 - val_loss: 149.0373\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 173.5556 - val_loss: 143.1912\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.8015 - val_loss: 151.5347\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.8681 - val_loss: 222.8239\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.6872 - val_loss: 147.5917\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.1422 - val_loss: 219.8196\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 162.8610 - val_loss: 186.5089\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 175.6952 - val_loss: 145.2667\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 146.1404 - val_loss: 199.3246\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 158.1392 - val_loss: 148.1519\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.0965 - val_loss: 173.6300\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.1783 - val_loss: 160.7027\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 169.2017 - val_loss: 205.4727\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.6185 - val_loss: 140.2595\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 171.4840 - val_loss: 260.5176\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.7109 - val_loss: 151.4190\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 145.9572 - val_loss: 172.3172\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 150.9121 - val_loss: 151.3187\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.7741 - val_loss: 141.0739\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 278.7286 - val_loss: 191.4203\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.5122 - val_loss: 174.4518\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.3881 - val_loss: 170.0359\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.4998 - val_loss: 184.1046\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 176.7683 - val_loss: 215.0838\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 186.6491 - val_loss: 217.7189\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.3792 - val_loss: 141.9387\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.5281 - val_loss: 149.4906\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 152.9995 - val_loss: 142.2424\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.6735 - val_loss: 167.0214\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 155.1958 - val_loss: 245.1356\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 172.6025 - val_loss: 219.0530\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 175.5395 - val_loss: 140.5143\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 151.0017 - val_loss: 265.2652\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 147.2431 - val_loss: 145.8525\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.3260 - val_loss: 165.1894\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 154.8224 - val_loss: 172.5458\n",
      "Epoch 658/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.1791 - val_loss: 162.6031\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.4518 - val_loss: 159.5170\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 160.3914 - val_loss: 150.6339\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 146.3444 - val_loss: 158.0685\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 237.0486 - val_loss: 146.7817\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.8562 - val_loss: 163.7527\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 154.7265 - val_loss: 157.2468\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 151.2780 - val_loss: 173.0168\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 147.7707 - val_loss: 152.5220\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.5181 - val_loss: 172.0179\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 143.9040 - val_loss: 140.6554\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 152.8515 - val_loss: 141.0280\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.6738 - val_loss: 156.7936\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 144.6074 - val_loss: 154.4425\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.6555 - val_loss: 142.7300\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.8784 - val_loss: 139.3982\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 178.8736 - val_loss: 141.8312\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 165.2056 - val_loss: 148.2789\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.3223 - val_loss: 148.5893\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.9883 - val_loss: 141.3692\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 155.8777 - val_loss: 206.1555\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 137.0415 - val_loss: 143.0223\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 140.4211 - val_loss: 186.1849\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 233.7104 - val_loss: 143.5722\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 135.5087 - val_loss: 143.4350\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 189.7335 - val_loss: 141.7599\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 158.1379 - val_loss: 142.7132\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 150.8989 - val_loss: 146.1102\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.5246 - val_loss: 140.5132\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.3324 - val_loss: 150.7353\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 155.5103 - val_loss: 173.0190\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.4965 - val_loss: 150.9362\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.6650 - val_loss: 182.2806\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 149.9198 - val_loss: 163.3880\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.3682 - val_loss: 145.1529\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.5927 - val_loss: 192.8934\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 151.0637 - val_loss: 149.7319\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 142.7492 - val_loss: 144.6082\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 168.9982 - val_loss: 143.4630\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.3570 - val_loss: 139.0299\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.4064 - val_loss: 153.2867\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.5437 - val_loss: 194.4785\n",
      "Epoch 700/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 184.2829 - val_loss: 307.6055\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 152.0520 - val_loss: 168.6071\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.2049 - val_loss: 143.3102\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 138.1745 - val_loss: 153.9618\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 153.9406 - val_loss: 149.2168\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.2538 - val_loss: 153.4945\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 160.3078 - val_loss: 193.0039\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.1100 - val_loss: 143.2725\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 145.2499 - val_loss: 145.1497\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.7565 - val_loss: 148.9862\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.4548 - val_loss: 149.7165\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 184.8485 - val_loss: 144.4384\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 150.5249 - val_loss: 160.3776\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 182.2407 - val_loss: 211.8315\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.5247 - val_loss: 146.9938\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.3893 - val_loss: 269.8179\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.2788 - val_loss: 217.1900\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.9302 - val_loss: 153.7709\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.0329 - val_loss: 144.9854\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 153.3325 - val_loss: 229.5469\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.8961 - val_loss: 139.7390\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 203.4742 - val_loss: 159.9722\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.3263 - val_loss: 136.6831\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.7004 - val_loss: 144.4281\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.6949 - val_loss: 143.3534\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.9250 - val_loss: 148.6236\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.7329 - val_loss: 155.0216\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 178.6969 - val_loss: 139.0701\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.2620 - val_loss: 139.3990\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.4796 - val_loss: 141.5748\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 146.7297 - val_loss: 155.6453\n",
      "Epoch 731/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.4164 - val_loss: 158.5436\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 193.2756 - val_loss: 435.9396\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 165.3554 - val_loss: 171.9212\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.5731 - val_loss: 144.7380\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.2922 - val_loss: 148.5905\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 141.6594 - val_loss: 192.0791\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.7392 - val_loss: 235.8696\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.8750 - val_loss: 219.0713\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 167.3798 - val_loss: 237.7531\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 303.4159 - val_loss: 145.3648\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.4677 - val_loss: 176.1608\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 1s 145us/step - loss: 139.6973 - val_loss: 158.5643\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 136.6657 - val_loss: 146.8423\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 145.6174 - val_loss: 141.1506\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.9539 - val_loss: 235.7076\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.9520 - val_loss: 143.2525\n",
      "Epoch 747/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 139.5536 - val_loss: 156.3293\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 136.6988 - val_loss: 143.7467\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 146.4440 - val_loss: 142.7634\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 138.6680 - val_loss: 140.2694\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 139.4761 - val_loss: 184.2003\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.3384 - val_loss: 153.6664\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.9852 - val_loss: 145.8663\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 164.8948 - val_loss: 157.7756\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.8984 - val_loss: 188.6133\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 137.0367 - val_loss: 151.2182\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 155.0009 - val_loss: 140.5520\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 1s 144us/step - loss: 150.8616 - val_loss: 234.6789\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 140.3109 - val_loss: 145.3918\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 136.2153 - val_loss: 150.7470\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 140.9788 - val_loss: 146.3522\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 144.7074 - val_loss: 137.6749\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 169.6435 - val_loss: 153.6670\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 141.1887 - val_loss: 156.3088\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 153.6651 - val_loss: 158.3019\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 297.4417 - val_loss: 157.0318\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 133.3819 - val_loss: 154.0491\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 135.5723 - val_loss: 169.3123\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 133.6092 - val_loss: 143.8123\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 143.661 - 1s 98us/step - loss: 141.5602 - val_loss: 145.4985\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 151.6752 - val_loss: 136.9879\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 154.2320 - val_loss: 206.2853\n",
      "Epoch 773/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 141.0422 - val_loss: 143.5994\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.8005 - val_loss: 145.2850\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.7236 - val_loss: 163.3628\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 139.1151 - val_loss: 154.3145\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 138.0205 - val_loss: 205.7011\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 144.4129 - val_loss: 164.9541\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.4532 - val_loss: 160.7394\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 153.7728 - val_loss: 174.5593\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 413.8781 - val_loss: 324.2963\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 1s 141us/step - loss: 263.0338 - val_loss: 203.2597\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 209.4747 - val_loss: 172.4668\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 186.8458 - val_loss: 179.0258\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 185.5797 - val_loss: 176.0419\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 181.9035 - val_loss: 172.7737\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 194.6830 - val_loss: 193.3642\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 197.2215 - val_loss: 237.3899\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 170.8202 - val_loss: 156.2603\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 191.7577 - val_loss: 239.7995\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 1s 124us/step - loss: 177.9029 - val_loss: 277.1548\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 165.2730 - val_loss: 228.2550\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 174.2834 - val_loss: 155.4966\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 163.9065 - val_loss: 152.1089\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 166.3010 - val_loss: 168.1436\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 166.5330 - val_loss: 309.9690\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 170.6643 - val_loss: 169.0943\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 157.8135 - val_loss: 164.4615\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 157.2231 - val_loss: 159.4090\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 175.4085 - val_loss: 151.4615\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 163.3205 - val_loss: 191.1565\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 176.2634 - val_loss: 151.4799\n",
      "Epoch 803/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 150.6415 - val_loss: 168.3141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 804/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 152.2448 - val_loss: 233.9766\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.7059 - val_loss: 161.7496\n",
      "Epoch 806/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 165.2148 - val_loss: 205.3306\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.8091 - val_loss: 209.7706\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 166.3138 - val_loss: 150.1080\n",
      "Epoch 809/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 192.0148 - val_loss: 244.3949\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.4316 - val_loss: 226.6142\n",
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 245.2078 - val_loss: 193.0356\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 150.3576 - val_loss: 145.4979\n",
      "Epoch 813/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 149.5439 - val_loss: 179.7797\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 147.8897 - val_loss: 288.1077\n",
      "Epoch 815/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 147.2259 - val_loss: 157.5833\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.5220 - val_loss: 215.8964\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 191.1199 - val_loss: 216.3585\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 149.5117 - val_loss: 149.6211\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 143.5167 - val_loss: 185.0256\n",
      "Epoch 820/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 149.0334 - val_loss: 145.2962\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 152.9522 - val_loss: 284.5372\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.0747 - val_loss: 163.9854\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 157.3516 - val_loss: 183.3482\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.5958 - val_loss: 148.0232\n",
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.4399 - val_loss: 160.1449\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 147.7728 - val_loss: 196.5576\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.9724 - val_loss: 174.0038\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.8349 - val_loss: 263.5454\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 147.1667 - val_loss: 141.0304\n",
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 143.0258 - val_loss: 175.5040\n",
      "Epoch 831/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 157.6802 - val_loss: 248.7261\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 150.9331 - val_loss: 146.7547\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 187.3588 - val_loss: 155.7435\n",
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 152.4984 - val_loss: 152.9663\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 193.2317 - val_loss: 203.5818\n",
      "Epoch 836/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.4656 - val_loss: 181.4442\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 153.0970 - val_loss: 140.8342\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 171.0498 - val_loss: 182.8372\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 179.9490 - val_loss: 142.2401\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.3682 - val_loss: 158.6843\n",
      "Epoch 841/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.5201 - val_loss: 143.4388\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 142.7278 - val_loss: 150.6868\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.4965 - val_loss: 169.8299\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.4621 - val_loss: 168.1386\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 169.9579 - val_loss: 155.4554\n",
      "Epoch 846/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 146.9338 - val_loss: 154.3691\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 137.9928 - val_loss: 139.4575\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 167.6695 - val_loss: 182.9987\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 154.2416 - val_loss: 143.0902\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 175.7647 - val_loss: 208.9066\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.4165 - val_loss: 162.3319\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 150.1831 - val_loss: 176.2472\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 157.1017 - val_loss: 146.7095\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 149.6669 - val_loss: 254.5468\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.5186 - val_loss: 165.0880\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 149.5665 - val_loss: 187.4157\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 177.0726 - val_loss: 144.4728\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.0983 - val_loss: 157.5841\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 148.8902 - val_loss: 147.2212\n",
      "Epoch 860/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 157.6439 - val_loss: 214.5934\n",
      "Epoch 861/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.9067 - val_loss: 147.1246\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.1214 - val_loss: 162.7534\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.5725 - val_loss: 146.2273\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 167.0087 - val_loss: 154.9184\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.5887 - val_loss: 150.2262\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 151.0836 - val_loss: 186.9682\n",
      "Epoch 867/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.1650 - val_loss: 140.1387\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.8589 - val_loss: 149.4480\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 163.4376 - val_loss: 154.0873\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 160.9693 - val_loss: 136.4849\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 199.2926 - val_loss: 310.9505\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 261.0592 - val_loss: 140.9572\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 134.1712 - val_loss: 143.0900\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.8682 - val_loss: 143.2314\n",
      "Epoch 875/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.4343 - val_loss: 138.2009\n",
      "Epoch 876/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.9030 - val_loss: 153.4984\n",
      "Epoch 877/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 133.0366 - val_loss: 139.8938\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 140.5009 - val_loss: 136.4173\n",
      "Epoch 879/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.5521 - val_loss: 141.9411\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 172.6756 - val_loss: 168.7276\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.4233 - val_loss: 136.8179\n",
      "Epoch 882/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 138.9784 - val_loss: 144.5194\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 147.3169 - val_loss: 143.5545\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 148.1284 - val_loss: 144.9717\n",
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 157.8576 - val_loss: 172.4646\n",
      "Epoch 886/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 155.3510 - val_loss: 144.7894\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 148.8026 - val_loss: 165.3998\n",
      "Epoch 888/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 172.8335 - val_loss: 155.2633\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 150.6806 - val_loss: 146.1318\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 153.2867 - val_loss: 184.0333\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 151.5637 - val_loss: 164.8532\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 156.4670 - val_loss: 164.0215\n",
      "Epoch 893/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 136.7898 - val_loss: 146.7051\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.5088 - val_loss: 168.6000\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.9682 - val_loss: 186.0649\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.6433 - val_loss: 136.1455\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 155.3799 - val_loss: 143.5245\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.3130 - val_loss: 180.9481\n",
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 166.1950 - val_loss: 166.6449\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 156.9460 - val_loss: 141.9266\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 145.2970 - val_loss: 162.0009\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 134.0481 - val_loss: 143.8040\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.5328 - val_loss: 147.9545\n",
      "Epoch 904/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 224.6930 - val_loss: 137.9998\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.9073 - val_loss: 165.0808\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 138.4886 - val_loss: 172.8549\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 134.4350 - val_loss: 153.3955\n",
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.0694 - val_loss: 169.6077\n",
      "Epoch 909/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 153.5241 - val_loss: 206.7082\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 219.2720 - val_loss: 275.0538\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.4938 - val_loss: 136.3029\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.8605 - val_loss: 209.3499\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 159.6577 - val_loss: 151.5936\n",
      "Epoch 914/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.4774 - val_loss: 143.1603\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.3490 - val_loss: 162.6640\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 148.7956 - val_loss: 139.1488\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 143.2458 - val_loss: 159.4160\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 152.088 - 1s 89us/step - loss: 151.1021 - val_loss: 140.4120\n",
      "Epoch 919/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 140.2121 - val_loss: 152.2880\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 137.8737 - val_loss: 234.9959\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 163.5699 - val_loss: 184.1443\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 138.5815 - val_loss: 135.3772\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 138.5462 - val_loss: 154.9302\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 137.0756 - val_loss: 165.7090\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 152.6344 - val_loss: 173.6391\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 142.0901 - val_loss: 175.2181\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 142.1645 - val_loss: 170.1472\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 157.0729 - val_loss: 165.9786\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 138.6455 - val_loss: 177.4761\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 151.0365 - val_loss: 194.6710\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 170.3354 - val_loss: 140.1573\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 145.9844 - val_loss: 139.6697\n",
      "Epoch 933/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 140.1225 - val_loss: 138.8659\n",
      "Epoch 934/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 145.5232 - val_loss: 152.6485\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 140.5070 - val_loss: 148.9040\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 159.8729 - val_loss: 136.1014\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 137.0960 - val_loss: 183.8104\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 145.8033 - val_loss: 155.2826\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 143.0307 - val_loss: 150.9209\n",
      "Epoch 940/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.6930 - val_loss: 139.9183\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 149.3922 - val_loss: 140.9164\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.9187 - val_loss: 146.5643\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 145.3810 - val_loss: 160.3350\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 151.9326 - val_loss: 156.2618\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 142.3789 - val_loss: 197.8752\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.4427 - val_loss: 147.3509\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.6298 - val_loss: 148.0312\n",
      "Epoch 948/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.0925 - val_loss: 158.7413\n",
      "Epoch 949/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.9134 - val_loss: 154.5383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 950/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 152.5802 - val_loss: 179.2478\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 143.6007 - val_loss: 156.3426\n",
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 166.5400 - val_loss: 144.6683\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.2662 - val_loss: 137.9833\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 142.8746 - val_loss: 140.3128\n",
      "Epoch 955/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 138.1204 - val_loss: 148.6215\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.6009 - val_loss: 151.0985\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 152.5832 - val_loss: 153.0651\n",
      "Epoch 958/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 142.5767 - val_loss: 220.6202\n",
      "Epoch 959/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 146.4707 - val_loss: 187.5558\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 184.5020 - val_loss: 147.2976\n",
      "Epoch 961/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 137.3122 - val_loss: 144.8501\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 139.0205 - val_loss: 146.1334\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.3995 - val_loss: 169.0077\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.1908 - val_loss: 150.8579\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.7898 - val_loss: 149.2074\n",
      "Epoch 966/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.9795 - val_loss: 144.4377\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.7472 - val_loss: 157.6054\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.6137 - val_loss: 140.4143\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.7862 - val_loss: 141.4540\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 158.1011 - val_loss: 139.1418\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 134.6471 - val_loss: 142.3722\n",
      "Epoch 972/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.1600 - val_loss: 153.4289\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 153.2639 - val_loss: 147.0220\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 140.5190 - val_loss: 145.6414\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.7507 - val_loss: 326.6835\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.9060 - val_loss: 143.3147\n",
      "Epoch 977/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.3723 - val_loss: 137.9929\n",
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 186.6976 - val_loss: 162.5550\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.5356 - val_loss: 137.0686\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.9687 - val_loss: 163.4474\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.1681 - val_loss: 143.4952\n",
      "Epoch 982/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.2691 - val_loss: 141.5892\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.6874 - val_loss: 170.7561\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 147.0809 - val_loss: 140.8151\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 146.4758 - val_loss: 177.4843\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 180.2075 - val_loss: 180.3409\n",
      "Epoch 987/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.7100 - val_loss: 142.9008\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.9754 - val_loss: 138.7562\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.1002 - val_loss: 140.0546\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.5461 - val_loss: 138.5150\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.0597 - val_loss: 169.3334\n",
      "Epoch 992/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.4904 - val_loss: 164.8082\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.9972 - val_loss: 187.2795\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.0688 - val_loss: 155.6679\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.6102 - val_loss: 175.5985\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.5116 - val_loss: 142.4203\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 139.5980 - val_loss: 156.3384\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 157.6897 - val_loss: 147.4470\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 152.5244 - val_loss: 171.7376\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.9664 - val_loss: 163.2321\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 157.9640 - val_loss: 162.6234\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.4052 - val_loss: 228.7780\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.0692 - val_loss: 148.2262\n",
      "Epoch 1004/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.8308 - val_loss: 141.0758\n",
      "Epoch 1005/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.0829 - val_loss: 143.0910\n",
      "Epoch 1006/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.7507 - val_loss: 172.1717\n",
      "Epoch 1007/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 156.3130 - val_loss: 146.1782\n",
      "Epoch 1008/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.0509 - val_loss: 140.6286\n",
      "Epoch 1009/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.8192 - val_loss: 175.7658\n",
      "Epoch 1010/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.9694 - val_loss: 148.2861\n",
      "Epoch 1011/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.1280 - val_loss: 158.0236\n",
      "Epoch 1012/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 184.5243 - val_loss: 187.0920\n",
      "Epoch 1013/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.4041 - val_loss: 183.9231\n",
      "Epoch 1014/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.6010 - val_loss: 146.8303\n",
      "Epoch 1015/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.0211 - val_loss: 140.1574\n",
      "Epoch 1016/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.4800 - val_loss: 143.7184\n",
      "Epoch 1017/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.5759 - val_loss: 145.1075\n",
      "Epoch 1018/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.0150 - val_loss: 137.6623\n",
      "Epoch 1019/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.7297 - val_loss: 136.7366\n",
      "Epoch 1020/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 332.3457 - val_loss: 146.4428\n",
      "Epoch 1021/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 128.4524 - val_loss: 153.3050\n",
      "Epoch 1022/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.4736 - val_loss: 167.1295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1023/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.2162 - val_loss: 181.0599\n",
      "Epoch 1024/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 142.7119 - val_loss: 142.2321\n",
      "Epoch 1025/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.3675 - val_loss: 152.9897\n",
      "Epoch 1026/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.9233 - val_loss: 138.3237\n",
      "Epoch 1027/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.3234 - val_loss: 137.3770\n",
      "Epoch 1028/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.6673 - val_loss: 148.7112\n",
      "Epoch 1029/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.9220 - val_loss: 228.8074\n",
      "Epoch 1030/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 146.9817 - val_loss: 152.4184\n",
      "Epoch 1031/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 143.9797 - val_loss: 143.9121\n",
      "Epoch 1032/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 130.9688 - val_loss: 144.8086\n",
      "Epoch 1033/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 138.4862 - val_loss: 136.2276\n",
      "Epoch 1034/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.1789 - val_loss: 146.7731\n",
      "Epoch 1035/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.9016 - val_loss: 137.0600\n",
      "Epoch 1036/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.4882 - val_loss: 139.9447\n",
      "Epoch 1037/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 200.9209 - val_loss: 140.8540\n",
      "Epoch 1038/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.0422 - val_loss: 150.0815\n",
      "Epoch 1039/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.1311 - val_loss: 152.2641\n",
      "Epoch 1040/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 126.2639 - val_loss: 153.1118\n",
      "Epoch 1041/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.9275 - val_loss: 150.1036\n",
      "Epoch 1042/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 143.0615 - val_loss: 143.4122\n",
      "Epoch 1043/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.0651 - val_loss: 133.9523\n",
      "Epoch 1044/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.7141 - val_loss: 179.3932\n",
      "Epoch 1045/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 136.9170 - val_loss: 136.8483\n",
      "Epoch 1046/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.6971 - val_loss: 160.5459\n",
      "Epoch 1047/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.6592 - val_loss: 169.3524\n",
      "Epoch 1048/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.8969 - val_loss: 140.3145\n",
      "Epoch 1049/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 161.8582 - val_loss: 156.0221\n",
      "Epoch 1050/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 142.6390 - val_loss: 186.1255\n",
      "Epoch 1051/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.4805 - val_loss: 300.0688\n",
      "Epoch 1052/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.2767 - val_loss: 192.9613\n",
      "Epoch 1053/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 171.6310 - val_loss: 153.9621\n",
      "Epoch 1054/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.2552 - val_loss: 147.4026\n",
      "Epoch 1055/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.7777 - val_loss: 155.7339\n",
      "Epoch 1056/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.5503 - val_loss: 202.0472\n",
      "Epoch 1057/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 147.6695 - val_loss: 138.8614\n",
      "Epoch 1058/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.7374 - val_loss: 186.6660\n",
      "Epoch 1059/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.4595 - val_loss: 144.2799\n",
      "Epoch 1060/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 149.7752 - val_loss: 160.6793\n",
      "Epoch 1061/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.3021 - val_loss: 141.7111\n",
      "Epoch 1062/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.0777 - val_loss: 237.4024\n",
      "Epoch 1063/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 133.2924 - val_loss: 161.5562\n",
      "Epoch 1064/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.8870 - val_loss: 146.4113\n",
      "Epoch 1065/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.0416 - val_loss: 138.9583\n",
      "Epoch 1066/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 144.6080 - val_loss: 157.0363\n",
      "Epoch 1067/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.9492 - val_loss: 221.6233\n",
      "Epoch 1068/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.8191 - val_loss: 138.4773\n",
      "Epoch 1069/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.0260 - val_loss: 133.3555\n",
      "Epoch 1070/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.0076 - val_loss: 167.0778\n",
      "Epoch 1071/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.4529 - val_loss: 138.7137\n",
      "Epoch 1072/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 140.0978 - val_loss: 228.2503\n",
      "Epoch 1073/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.3515 - val_loss: 143.0885\n",
      "Epoch 1074/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 200.5367 - val_loss: 160.3739\n",
      "Epoch 1075/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 145.4624 - val_loss: 146.0615\n",
      "Epoch 1076/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 150.4277 - val_loss: 239.2210\n",
      "Epoch 1077/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.7375 - val_loss: 145.7965\n",
      "Epoch 1078/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.9551 - val_loss: 268.3972\n",
      "Epoch 1079/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.1536 - val_loss: 161.8515\n",
      "Epoch 1080/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.9461 - val_loss: 195.5761\n",
      "Epoch 1081/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.3390 - val_loss: 164.0391\n",
      "Epoch 1082/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.5444 - val_loss: 137.9524\n",
      "Epoch 1083/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 212.8265 - val_loss: 221.1969\n",
      "Epoch 1084/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.0957 - val_loss: 170.7337\n",
      "Epoch 1085/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.2104 - val_loss: 147.5087\n",
      "Epoch 1086/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.3894 - val_loss: 141.3295\n",
      "Epoch 1087/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.4104 - val_loss: 145.8931\n",
      "Epoch 1088/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.3280 - val_loss: 146.5897\n",
      "Epoch 1089/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.5895 - val_loss: 154.8309\n",
      "Epoch 1090/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.8447 - val_loss: 136.5756\n",
      "Epoch 1091/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.6179 - val_loss: 142.4118\n",
      "Epoch 1092/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.1160 - val_loss: 192.9155\n",
      "Epoch 1093/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 164.1802 - val_loss: 143.3235\n",
      "Epoch 1094/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.8374 - val_loss: 157.9730\n",
      "Epoch 1095/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.8559 - val_loss: 151.6734\n",
      "Epoch 1096/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.9916 - val_loss: 136.5782\n",
      "Epoch 1097/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.0946 - val_loss: 210.6612\n",
      "Epoch 1098/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.4913 - val_loss: 135.9842\n",
      "Epoch 1099/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.9294 - val_loss: 148.8276\n",
      "Epoch 1100/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 146.9090 - val_loss: 144.6759\n",
      "Epoch 1101/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.4500 - val_loss: 280.1955\n",
      "Epoch 1102/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 155.2344 - val_loss: 162.1579\n",
      "Epoch 1103/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.1207 - val_loss: 139.6785\n",
      "Epoch 1104/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 132.0460 - val_loss: 141.6716\n",
      "Epoch 1105/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.2346 - val_loss: 153.0893\n",
      "Epoch 1106/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 163.1431 - val_loss: 161.3951\n",
      "Epoch 1107/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.5419 - val_loss: 140.9914\n",
      "Epoch 1108/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.8853 - val_loss: 171.8821\n",
      "Epoch 1109/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.7413 - val_loss: 144.7408\n",
      "Epoch 1110/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.7839 - val_loss: 142.0754\n",
      "Epoch 1111/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 140.2557 - val_loss: 151.5211\n",
      "Epoch 1112/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.0518 - val_loss: 145.0595\n",
      "Epoch 1113/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.3631 - val_loss: 136.8420\n",
      "Epoch 1114/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.2216 - val_loss: 290.0428\n",
      "Epoch 1115/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 146.3899 - val_loss: 147.1192\n",
      "Epoch 1116/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 142.9373 - val_loss: 157.9199\n",
      "Epoch 1117/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.4286 - val_loss: 197.8814\n",
      "Epoch 1118/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 171.3926 - val_loss: 156.5392\n",
      "Epoch 1119/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.5418 - val_loss: 147.7820\n",
      "Epoch 1120/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.7236 - val_loss: 275.7762\n",
      "Epoch 1121/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.5196 - val_loss: 139.3699\n",
      "Epoch 1122/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 145.8382 - val_loss: 150.0823\n",
      "Epoch 1123/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.6313 - val_loss: 154.8593\n",
      "Epoch 1124/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.6799 - val_loss: 138.4988\n",
      "Epoch 1125/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 147.7919 - val_loss: 220.2995\n",
      "Epoch 1126/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 149.4004 - val_loss: 138.9933\n",
      "Epoch 1127/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.0133 - val_loss: 190.8324\n",
      "Epoch 1128/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.3842 - val_loss: 174.8708\n",
      "Epoch 1129/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 258.7021 - val_loss: 140.2663\n",
      "Epoch 1130/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.3499 - val_loss: 143.3322\n",
      "Epoch 1131/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.4590 - val_loss: 176.6059\n",
      "Epoch 1132/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.0382 - val_loss: 139.1324\n",
      "Epoch 1133/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.2895 - val_loss: 135.1790\n",
      "Epoch 1134/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.9224 - val_loss: 140.3644\n",
      "Epoch 1135/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.0685 - val_loss: 152.6811\n",
      "Epoch 1136/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.5523 - val_loss: 198.9374\n",
      "Epoch 1137/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 127.3369 - val_loss: 181.9961\n",
      "Epoch 1138/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.9842 - val_loss: 155.7782\n",
      "Epoch 1139/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 156.4542 - val_loss: 373.8329\n",
      "Epoch 1140/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 150.5523 - val_loss: 140.8615\n",
      "Epoch 1141/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 134.6684 - val_loss: 152.9983\n",
      "Epoch 1142/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.3069 - val_loss: 175.1427\n",
      "Epoch 1143/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.9368 - val_loss: 141.3098\n",
      "Epoch 1144/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 192.4605 - val_loss: 147.9483\n",
      "Epoch 1145/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.7826 - val_loss: 139.1649\n",
      "Epoch 1146/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.3430 - val_loss: 137.9566\n",
      "Epoch 1147/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.9769 - val_loss: 141.2321\n",
      "Epoch 1148/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.0333 - val_loss: 138.7882\n",
      "Epoch 1149/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.1641 - val_loss: 143.9237\n",
      "Epoch 1150/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 152.5506 - val_loss: 150.3374\n",
      "Epoch 1151/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.7096 - val_loss: 150.3715\n",
      "Epoch 1152/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.8084 - val_loss: 152.5504\n",
      "Epoch 1153/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.8953 - val_loss: 175.0923\n",
      "Epoch 1154/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 159.8456 - val_loss: 164.6341\n",
      "Epoch 1155/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 135.8587 - val_loss: 136.6654\n",
      "Epoch 1156/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.4312 - val_loss: 164.6957\n",
      "Epoch 1157/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.1683 - val_loss: 141.2286\n",
      "Epoch 1158/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 180.3568 - val_loss: 359.1801\n",
      "Epoch 1159/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.0701 - val_loss: 140.4846\n",
      "Epoch 1160/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.7601 - val_loss: 146.8937\n",
      "Epoch 1161/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.5533 - val_loss: 194.9187\n",
      "Epoch 1162/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.2992 - val_loss: 143.4091\n",
      "Epoch 1163/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 140.8689 - val_loss: 132.9511\n",
      "Epoch 1164/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.4518 - val_loss: 155.1949\n",
      "Epoch 1165/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.7356 - val_loss: 176.9134\n",
      "Epoch 1166/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.5412 - val_loss: 164.4335\n",
      "Epoch 1167/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step - loss: 143.4668 - val_loss: 144.4523\n",
      "Epoch 1168/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.5833 - val_loss: 155.8493\n",
      "Epoch 1169/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.0992 - val_loss: 172.6933\n",
      "Epoch 1170/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 142.4725 - val_loss: 188.8383\n",
      "Epoch 1171/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 193.5673 - val_loss: 154.6676\n",
      "Epoch 1172/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.3226 - val_loss: 143.7839\n",
      "Epoch 1173/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.6698 - val_loss: 135.0443\n",
      "Epoch 1174/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.6454 - val_loss: 135.7876\n",
      "Epoch 1175/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 139.2668 - val_loss: 321.0959\n",
      "Epoch 1176/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 134.7204 - val_loss: 146.5294\n",
      "Epoch 1177/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.7160 - val_loss: 151.5057\n",
      "Epoch 1178/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 143.0934 - val_loss: 146.1536\n",
      "Epoch 1179/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 140.5080 - val_loss: 162.6846\n",
      "Epoch 1180/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.7508 - val_loss: 148.2689\n",
      "Epoch 1181/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.6743 - val_loss: 134.7222\n",
      "Epoch 1182/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.4006 - val_loss: 151.9758\n",
      "Epoch 1183/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 136.2583 - val_loss: 141.3522\n",
      "Epoch 1184/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 179.6555 - val_loss: 142.0307\n",
      "Epoch 1185/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.2330 - val_loss: 146.8740\n",
      "Epoch 1186/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.1691 - val_loss: 150.6468\n",
      "Epoch 1187/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.9215 - val_loss: 138.3846\n",
      "Epoch 1188/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.7022 - val_loss: 155.3530\n",
      "Epoch 1189/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 153.0396 - val_loss: 150.3109\n",
      "Epoch 1190/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.7952 - val_loss: 133.9331\n",
      "Epoch 1191/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.0992 - val_loss: 139.0095\n",
      "Epoch 1192/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.8522 - val_loss: 151.1878\n",
      "Epoch 1193/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 139.8172 - val_loss: 150.4672\n",
      "Epoch 1194/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.1058 - val_loss: 193.8449\n",
      "Epoch 1195/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 150.7368 - val_loss: 192.3668\n",
      "Epoch 1196/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 139.3773 - val_loss: 135.9791\n",
      "Epoch 1197/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.3602 - val_loss: 153.0209\n",
      "Epoch 1198/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.8216 - val_loss: 136.9707\n",
      "Epoch 1199/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 147.8517 - val_loss: 183.3625\n",
      "Epoch 1200/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.0223 - val_loss: 133.5401\n",
      "Epoch 1201/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.4061 - val_loss: 160.0592\n",
      "Epoch 1202/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.5215 - val_loss: 149.4167\n",
      "Epoch 1203/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.3143 - val_loss: 144.8321\n",
      "Epoch 1204/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.2568 - val_loss: 141.5502\n",
      "Epoch 1205/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 136.5941 - val_loss: 180.7083\n",
      "Epoch 1206/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 146.1800 - val_loss: 185.2150\n",
      "Epoch 1207/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.0380 - val_loss: 139.0636\n",
      "Epoch 1208/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.5475 - val_loss: 138.8813\n",
      "Epoch 1209/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.8676 - val_loss: 134.9562\n",
      "Epoch 1210/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.6029 - val_loss: 155.5272\n",
      "Epoch 1211/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.9688 - val_loss: 211.4968\n",
      "Epoch 1212/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 159.6725 - val_loss: 134.7586\n",
      "Epoch 1213/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.7167 - val_loss: 149.8240\n",
      "Epoch 1214/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.6966 - val_loss: 158.5268\n",
      "Epoch 1215/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.9117 - val_loss: 182.4323\n",
      "Epoch 1216/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.5636 - val_loss: 146.8325\n",
      "Epoch 1217/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.2752 - val_loss: 148.7055\n",
      "Epoch 1218/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.0782 - val_loss: 145.4893\n",
      "Epoch 1219/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.9294 - val_loss: 148.4173\n",
      "Epoch 1220/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.9493 - val_loss: 142.7827\n",
      "Epoch 1221/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.6422 - val_loss: 139.7921\n",
      "Epoch 1222/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.6906 - val_loss: 163.4012\n",
      "Epoch 1223/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.7414 - val_loss: 253.2362\n",
      "Epoch 1224/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.1735 - val_loss: 138.6552\n",
      "Epoch 1225/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.6565 - val_loss: 139.8063\n",
      "Epoch 1226/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 139.7521 - val_loss: 235.1010\n",
      "Epoch 1227/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 144.6395 - val_loss: 180.7372\n",
      "Epoch 1228/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 194.0558 - val_loss: 160.2710\n",
      "Epoch 1229/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.5079 - val_loss: 138.0623\n",
      "Epoch 1230/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.1746 - val_loss: 150.9162\n",
      "Epoch 1231/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.1066 - val_loss: 163.4802\n",
      "Epoch 1232/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.1529 - val_loss: 157.6303\n",
      "Epoch 1233/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.5763 - val_loss: 151.3830\n",
      "Epoch 1234/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.5554 - val_loss: 136.9323\n",
      "Epoch 1235/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.4427 - val_loss: 141.4694\n",
      "Epoch 1236/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 133.3178 - val_loss: 199.0207\n",
      "Epoch 1237/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.2604 - val_loss: 157.7799\n",
      "Epoch 1238/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.6086 - val_loss: 202.3576\n",
      "Epoch 1239/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.6170 - val_loss: 140.8500\n",
      "Epoch 1240/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.2125 - val_loss: 186.0560\n",
      "Epoch 1241/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 144.0409 - val_loss: 150.0220\n",
      "Epoch 1242/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 157.8921 - val_loss: 134.6951\n",
      "Epoch 1243/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 138.0818 - val_loss: 145.3401\n",
      "Epoch 1244/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.3715 - val_loss: 138.3309\n",
      "Epoch 1245/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.7041 - val_loss: 168.9576\n",
      "Epoch 1246/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.0509 - val_loss: 144.1819\n",
      "Epoch 1247/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 134.9220 - val_loss: 139.1263\n",
      "Epoch 1248/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.5480 - val_loss: 145.8335\n",
      "Epoch 1249/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 136.6037 - val_loss: 179.1837\n",
      "Epoch 1250/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 127.9915 - val_loss: 144.4670\n",
      "Epoch 1251/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.3426 - val_loss: 149.5531\n",
      "Epoch 1252/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.0325 - val_loss: 136.2023\n",
      "Epoch 1253/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.5484 - val_loss: 153.3678\n",
      "Epoch 1254/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.7121 - val_loss: 139.5754\n",
      "Epoch 1255/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.0097 - val_loss: 133.5591\n",
      "Epoch 1256/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 170.8430 - val_loss: 172.5631\n",
      "Epoch 1257/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.9796 - val_loss: 170.1943\n",
      "Epoch 1258/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 133.2198 - val_loss: 138.5137\n",
      "Epoch 1259/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.2201 - val_loss: 149.0842\n",
      "Epoch 1260/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.4341 - val_loss: 139.9055\n",
      "Epoch 1261/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 261.4195 - val_loss: 778.3480\n",
      "Epoch 1262/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 173.9798 - val_loss: 139.0540\n",
      "Epoch 1263/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 129.5221 - val_loss: 146.9500\n",
      "Epoch 1264/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.1013 - val_loss: 139.7376\n",
      "Epoch 1265/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.1541 - val_loss: 155.6519\n",
      "Epoch 1266/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.9341 - val_loss: 149.9172\n",
      "Epoch 1267/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 128.5830 - val_loss: 136.2724\n",
      "Epoch 1268/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.9929 - val_loss: 164.9467\n",
      "Epoch 1269/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.1328 - val_loss: 137.0106\n",
      "Epoch 1270/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.7968 - val_loss: 143.4770\n",
      "Epoch 1271/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.9207 - val_loss: 136.3660\n",
      "Epoch 1272/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.8309 - val_loss: 146.6375\n",
      "Epoch 1273/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.7640 - val_loss: 134.2749\n",
      "Epoch 1274/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.8356 - val_loss: 133.7116\n",
      "Epoch 1275/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.1176 - val_loss: 141.1026\n",
      "Epoch 1276/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.3424 - val_loss: 144.7281\n",
      "Epoch 1277/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.6796 - val_loss: 169.8474\n",
      "Epoch 1278/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.0346 - val_loss: 145.7556\n",
      "Epoch 1279/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.0125 - val_loss: 152.0259\n",
      "Epoch 1280/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 152.7870 - val_loss: 5059.7839\n",
      "Epoch 1281/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 205.1896 - val_loss: 139.9139\n",
      "Epoch 1282/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 127.5993 - val_loss: 133.2458\n",
      "Epoch 1283/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.2263 - val_loss: 149.0688\n",
      "Epoch 1284/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.6754 - val_loss: 162.8438\n",
      "Epoch 1285/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.7297 - val_loss: 138.0958\n",
      "Epoch 1286/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 130.3188 - val_loss: 132.6951\n",
      "Epoch 1287/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.4057 - val_loss: 138.0411\n",
      "Epoch 1288/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.5962 - val_loss: 149.9813\n",
      "Epoch 1289/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.1190 - val_loss: 137.1738\n",
      "Epoch 1290/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 171.9426 - val_loss: 201.3666\n",
      "Epoch 1291/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.4705 - val_loss: 144.0928\n",
      "Epoch 1292/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.1983 - val_loss: 134.7238\n",
      "Epoch 1293/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.2375 - val_loss: 143.8435\n",
      "Epoch 1294/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.6978 - val_loss: 194.9779\n",
      "Epoch 1295/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.6354 - val_loss: 142.4618\n",
      "Epoch 1296/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.6770 - val_loss: 155.5666\n",
      "Epoch 1297/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.0812 - val_loss: 166.2802\n",
      "Epoch 1298/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.6216 - val_loss: 190.8865\n",
      "Epoch 1299/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.1426 - val_loss: 150.5663\n",
      "Epoch 1300/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.3382 - val_loss: 171.8317\n",
      "Epoch 1301/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.8764 - val_loss: 144.0839\n",
      "Epoch 1302/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.4094 - val_loss: 157.0533\n",
      "Epoch 1303/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.2323 - val_loss: 451.7938\n",
      "Epoch 1304/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.6540 - val_loss: 136.4922\n",
      "Epoch 1305/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.0328 - val_loss: 134.3138\n",
      "Epoch 1306/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.2712 - val_loss: 151.0176\n",
      "Epoch 1307/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 125.1552 - val_loss: 137.6309\n",
      "Epoch 1308/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 149.7911 - val_loss: 137.2642\n",
      "Epoch 1309/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 142.8640 - val_loss: 148.6918\n",
      "Epoch 1310/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.3145 - val_loss: 200.8122\n",
      "Epoch 1311/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.6291 - val_loss: 148.2177\n",
      "Epoch 1312/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.4698 - val_loss: 207.3092\n",
      "Epoch 1313/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 154.5860 - val_loss: 145.6658\n",
      "Epoch 1314/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 140.5825 - val_loss: 138.1936\n",
      "Epoch 1315/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.0738 - val_loss: 147.3442\n",
      "Epoch 1316/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.0614 - val_loss: 138.7693\n",
      "Epoch 1317/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.1559 - val_loss: 144.1958\n",
      "Epoch 1318/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.9034 - val_loss: 188.4024\n",
      "Epoch 1319/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 141.5561 - val_loss: 225.2661\n",
      "Epoch 1320/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 201.4979 - val_loss: 136.8568\n",
      "Epoch 1321/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 130.0200 - val_loss: 141.4538\n",
      "Epoch 1322/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 135.1674 - val_loss: 150.6818\n",
      "Epoch 1323/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.9060 - val_loss: 166.3229\n",
      "Epoch 1324/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.6697 - val_loss: 145.9697\n",
      "Epoch 1325/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.7308 - val_loss: 136.0938\n",
      "Epoch 1326/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.6564 - val_loss: 199.6068\n",
      "Epoch 1327/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 173.4360 - val_loss: 204.3427\n",
      "Epoch 1328/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.4522 - val_loss: 147.4462\n",
      "Epoch 1329/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 133.0715 - val_loss: 143.6054\n",
      "Epoch 1330/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.3151 - val_loss: 163.3976\n",
      "Epoch 1331/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.5976 - val_loss: 135.4434\n",
      "Epoch 1332/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.9181 - val_loss: 158.6005\n",
      "Epoch 1333/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.8120 - val_loss: 141.9415\n",
      "Epoch 1334/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.8894 - val_loss: 160.9785\n",
      "Epoch 1335/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.8510 - val_loss: 139.8137\n",
      "Epoch 1336/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.2918 - val_loss: 157.2600\n",
      "Epoch 1337/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.7542 - val_loss: 140.4527\n",
      "Epoch 1338/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.5455 - val_loss: 167.9789\n",
      "Epoch 1339/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.5502 - val_loss: 151.6702\n",
      "Epoch 1340/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.2490 - val_loss: 141.7310\n",
      "Epoch 1341/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.9841 - val_loss: 142.0905\n",
      "Epoch 1342/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.8201 - val_loss: 145.9173\n",
      "Epoch 1343/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.9754 - val_loss: 150.1277\n",
      "Epoch 1344/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.2199 - val_loss: 148.3764\n",
      "Epoch 1345/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.6207 - val_loss: 183.6995\n",
      "Epoch 1346/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 141.2397 - val_loss: 152.3821\n",
      "Epoch 1347/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 168.6165 - val_loss: 134.2302\n",
      "Epoch 1348/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.0526 - val_loss: 152.3354\n",
      "Epoch 1349/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 137.7847 - val_loss: 150.3597\n",
      "Epoch 1350/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.1678 - val_loss: 144.7932\n",
      "Epoch 1351/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.3831 - val_loss: 140.6766\n",
      "Epoch 1352/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 135.9401 - val_loss: 210.0206\n",
      "Epoch 1353/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.5180 - val_loss: 136.9389\n",
      "Epoch 1354/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.6745 - val_loss: 161.4295\n",
      "Epoch 1355/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 133.8840 - val_loss: 140.8848\n",
      "Epoch 1356/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 133.7948 - val_loss: 134.0531\n",
      "Epoch 1357/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 150.1190 - val_loss: 149.2594\n",
      "Epoch 1358/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 126.3222 - val_loss: 280.3573\n",
      "Epoch 1359/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 126.4753 - val_loss: 145.6007\n",
      "Epoch 1360/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.6878 - val_loss: 151.3583\n",
      "Epoch 1361/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.5559 - val_loss: 144.8394\n",
      "Epoch 1362/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.9346 - val_loss: 135.5903\n",
      "Epoch 1363/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 126.8196 - val_loss: 132.6134\n",
      "Epoch 1364/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 127.7714 - val_loss: 135.1561\n",
      "Epoch 1365/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.1802 - val_loss: 165.4706\n",
      "Epoch 1366/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.1382 - val_loss: 144.7432\n",
      "Epoch 1367/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.6415 - val_loss: 143.3051\n",
      "Epoch 1368/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.6990 - val_loss: 156.1627\n",
      "Epoch 1369/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.2771 - val_loss: 206.4894\n",
      "Epoch 1370/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 136.2190 - val_loss: 148.7885\n",
      "Epoch 1371/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.0311 - val_loss: 149.2635\n",
      "Epoch 1372/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.9191 - val_loss: 166.7028\n",
      "Epoch 1373/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 200.0064 - val_loss: 140.1239\n",
      "Epoch 1374/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.8155 - val_loss: 164.0821\n",
      "Epoch 1375/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.2553 - val_loss: 134.3587\n",
      "Epoch 1376/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.5640 - val_loss: 149.5144\n",
      "Epoch 1377/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.8036 - val_loss: 137.5928\n",
      "Epoch 1378/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.3888 - val_loss: 132.6007\n",
      "Epoch 1379/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.5701 - val_loss: 136.9396\n",
      "Epoch 1380/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.9496 - val_loss: 143.4228\n",
      "Epoch 1381/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 178.9742 - val_loss: 168.0831\n",
      "Epoch 1382/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.7982 - val_loss: 141.1146\n",
      "Epoch 1383/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.2215 - val_loss: 185.2390\n",
      "Epoch 1384/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.6213 - val_loss: 134.3060\n",
      "Epoch 1385/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.8405 - val_loss: 142.8166\n",
      "Epoch 1386/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.6457 - val_loss: 147.9097\n",
      "Epoch 1387/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.2811 - val_loss: 158.1994\n",
      "Epoch 1388/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.5694 - val_loss: 142.5753\n",
      "Epoch 1389/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.9225 - val_loss: 154.5009\n",
      "Epoch 1390/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 207.5259 - val_loss: 135.2869\n",
      "Epoch 1391/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 130.3690 - val_loss: 146.6597\n",
      "Epoch 1392/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 121.8292 - val_loss: 135.0039\n",
      "Epoch 1393/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.1497 - val_loss: 197.2290\n",
      "Epoch 1394/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.2386 - val_loss: 135.0551\n",
      "Epoch 1395/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 174.3583 - val_loss: 137.2440\n",
      "Epoch 1396/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 127.0171 - val_loss: 143.9950\n",
      "Epoch 1397/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.9204 - val_loss: 139.2522\n",
      "Epoch 1398/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.1607 - val_loss: 207.9888\n",
      "Epoch 1399/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.6253 - val_loss: 225.3474\n",
      "Epoch 1400/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.3147 - val_loss: 180.7079\n",
      "Epoch 1401/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 145.3920 - val_loss: 181.2660\n",
      "Epoch 1402/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.7578 - val_loss: 194.5755\n",
      "Epoch 1403/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.1729 - val_loss: 137.3180\n",
      "Epoch 1404/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.6800 - val_loss: 147.1937\n",
      "Epoch 1405/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.6528 - val_loss: 146.5567\n",
      "Epoch 1406/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.5459 - val_loss: 133.9490\n",
      "Epoch 1407/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.6157 - val_loss: 135.4981\n",
      "Epoch 1408/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.5139 - val_loss: 165.2717\n",
      "Epoch 1409/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.6343 - val_loss: 142.3816\n",
      "Epoch 1410/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 143.0786 - val_loss: 149.0922\n",
      "Epoch 1411/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.5529 - val_loss: 133.3985\n",
      "Epoch 1412/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.5638 - val_loss: 148.3743\n",
      "Epoch 1413/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 239.5256 - val_loss: 151.3273\n",
      "Epoch 1414/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.7772 - val_loss: 155.4700\n",
      "Epoch 1415/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.3911 - val_loss: 155.8692\n",
      "Epoch 1416/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.7947 - val_loss: 150.9184\n",
      "Epoch 1417/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.8396 - val_loss: 138.3481\n",
      "Epoch 1418/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 133.0883 - val_loss: 138.2382\n",
      "Epoch 1419/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.1891 - val_loss: 150.6376\n",
      "Epoch 1420/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.2512 - val_loss: 142.1613\n",
      "Epoch 1421/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.4448 - val_loss: 136.5287\n",
      "Epoch 1422/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.4806 - val_loss: 195.7607\n",
      "Epoch 1423/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.2241 - val_loss: 136.6161\n",
      "Epoch 1424/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.0336 - val_loss: 139.1369\n",
      "Epoch 1425/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.6228 - val_loss: 147.7229\n",
      "Epoch 1426/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.1411 - val_loss: 150.6739\n",
      "Epoch 1427/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.3055 - val_loss: 153.1071\n",
      "Epoch 1428/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.0351 - val_loss: 154.2915\n",
      "Epoch 1429/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.6581 - val_loss: 237.9069\n",
      "Epoch 1430/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.2132 - val_loss: 134.4771\n",
      "Epoch 1431/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.1241 - val_loss: 178.0126\n",
      "Epoch 1432/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.6033 - val_loss: 141.0798\n",
      "Epoch 1433/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.9590 - val_loss: 133.2738\n",
      "Epoch 1434/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.6491 - val_loss: 135.7680\n",
      "Epoch 1435/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.6679 - val_loss: 151.8143\n",
      "Epoch 1436/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 160.6976 - val_loss: 158.0950\n",
      "Epoch 1437/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.8378 - val_loss: 134.7826\n",
      "Epoch 1438/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.8666 - val_loss: 143.3579\n",
      "Epoch 1439/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.7635 - val_loss: 139.4325\n",
      "Epoch 1440/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.1933 - val_loss: 162.9933\n",
      "Epoch 1441/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.8073 - val_loss: 158.2878\n",
      "Epoch 1442/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.7799 - val_loss: 231.6346\n",
      "Epoch 1443/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.1952 - val_loss: 159.2395\n",
      "Epoch 1444/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.2566 - val_loss: 146.0969\n",
      "Epoch 1445/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.5802 - val_loss: 177.0337\n",
      "Epoch 1446/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.3727 - val_loss: 172.3947\n",
      "Epoch 1447/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 142.1488 - val_loss: 201.0645\n",
      "Epoch 1448/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.8088 - val_loss: 139.0781\n",
      "Epoch 1449/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.8897 - val_loss: 134.2115\n",
      "Epoch 1450/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.8405 - val_loss: 141.8219\n",
      "Epoch 1451/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.6497 - val_loss: 138.7367\n",
      "Epoch 1452/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.3346 - val_loss: 142.4202\n",
      "Epoch 1453/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 127.7274 - val_loss: 144.5333\n",
      "Epoch 1454/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.4808 - val_loss: 164.3454\n",
      "Epoch 1455/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 287.0952 - val_loss: 136.1523\n",
      "Epoch 1456/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.2506 - val_loss: 137.9912\n",
      "Epoch 1457/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.1633 - val_loss: 139.2192\n",
      "Epoch 1458/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.4185 - val_loss: 145.8754\n",
      "Epoch 1459/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.6242 - val_loss: 138.4709\n",
      "Epoch 1460/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.2537 - val_loss: 160.4692\n",
      "Epoch 1461/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.0032 - val_loss: 136.5237\n",
      "Epoch 1462/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.4878 - val_loss: 134.2954\n",
      "Epoch 1463/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.5606 - val_loss: 152.5647\n",
      "Epoch 1464/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 131.0034 - val_loss: 138.2695\n",
      "Epoch 1465/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 127.4199 - val_loss: 134.0753\n",
      "Epoch 1466/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.7519 - val_loss: 134.0577\n",
      "Epoch 1467/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.7439 - val_loss: 136.9892\n",
      "Epoch 1468/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.6689 - val_loss: 170.8408\n",
      "Epoch 1469/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.5073 - val_loss: 150.2042\n",
      "Epoch 1470/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.5572 - val_loss: 154.9533\n",
      "Epoch 1471/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.0298 - val_loss: 177.7513\n",
      "Epoch 1472/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.9724 - val_loss: 135.3732\n",
      "Epoch 1473/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.3796 - val_loss: 132.3590\n",
      "Epoch 1474/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.8651 - val_loss: 142.3919\n",
      "Epoch 1475/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.7672 - val_loss: 145.7272\n",
      "Epoch 1476/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 180.4119 - val_loss: 142.4009\n",
      "Epoch 1477/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.4182 - val_loss: 141.6070\n",
      "Epoch 1478/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.7332 - val_loss: 132.8618\n",
      "Epoch 1479/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.8251 - val_loss: 143.6091\n",
      "Epoch 1480/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.0683 - val_loss: 149.3388\n",
      "Epoch 1481/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.2859 - val_loss: 147.5524\n",
      "Epoch 1482/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.7237 - val_loss: 149.5121\n",
      "Epoch 1483/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.3683 - val_loss: 134.9220\n",
      "Epoch 1484/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.5685 - val_loss: 133.0753\n",
      "Epoch 1485/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.1971 - val_loss: 145.5285\n",
      "Epoch 1486/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.6339 - val_loss: 134.2016\n",
      "Epoch 1487/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.5814 - val_loss: 140.4172\n",
      "Epoch 1488/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.2226 - val_loss: 142.2150\n",
      "Epoch 1489/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.7264 - val_loss: 135.7085\n",
      "Epoch 1490/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.1487 - val_loss: 139.1084\n",
      "Epoch 1491/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.0606 - val_loss: 151.5698\n",
      "Epoch 1492/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 139.6155 - val_loss: 138.6024\n",
      "Epoch 1493/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.1121 - val_loss: 147.9299\n",
      "Epoch 1494/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.0032 - val_loss: 160.0306\n",
      "Epoch 1495/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.5297 - val_loss: 135.5102\n",
      "Epoch 1496/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 164.0129 - val_loss: 139.0146\n",
      "Epoch 1497/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.5353 - val_loss: 136.6899\n",
      "Epoch 1498/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.8511 - val_loss: 138.9903\n",
      "Epoch 1499/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.6085 - val_loss: 144.0508\n",
      "Epoch 1500/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.4784 - val_loss: 146.1804\n",
      "Epoch 1501/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.1257 - val_loss: 136.3762\n",
      "Epoch 1502/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.2996 - val_loss: 157.2770\n",
      "Epoch 1503/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.8945 - val_loss: 175.2475\n",
      "Epoch 1504/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.8375 - val_loss: 150.9254\n",
      "Epoch 1505/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.1714 - val_loss: 152.2320\n",
      "Epoch 1506/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.4212 - val_loss: 158.4091\n",
      "Epoch 1507/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.0334 - val_loss: 138.8035\n",
      "Epoch 1508/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.5668 - val_loss: 143.9411\n",
      "Epoch 1509/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 129.4479 - val_loss: 156.6499\n",
      "Epoch 1510/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 142.9937 - val_loss: 132.9592\n",
      "Epoch 1511/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.8509 - val_loss: 135.1393\n",
      "Epoch 1512/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 128.2350 - val_loss: 132.1725\n",
      "Epoch 1513/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 132.0175 - val_loss: 176.1252\n",
      "Epoch 1514/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.3280 - val_loss: 133.9198\n",
      "Epoch 1515/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.9955 - val_loss: 141.7688\n",
      "Epoch 1516/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 302.8272 - val_loss: 152.1086\n",
      "Epoch 1517/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 133.6105 - val_loss: 144.2494\n",
      "Epoch 1518/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.8414 - val_loss: 137.0327\n",
      "Epoch 1519/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.1504 - val_loss: 141.4749\n",
      "Epoch 1520/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.8180 - val_loss: 216.8861\n",
      "Epoch 1521/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.7472 - val_loss: 134.1496\n",
      "Epoch 1522/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.3801 - val_loss: 159.6649\n",
      "Epoch 1523/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.0018 - val_loss: 174.7877\n",
      "Epoch 1524/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 130.7389 - val_loss: 153.3044\n",
      "Epoch 1525/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.1413 - val_loss: 157.3577\n",
      "Epoch 1526/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.6845 - val_loss: 139.0896\n",
      "Epoch 1527/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.6174 - val_loss: 148.2637\n",
      "Epoch 1528/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.3989 - val_loss: 161.9445\n",
      "Epoch 1529/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.8989 - val_loss: 160.0754\n",
      "Epoch 1530/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.5087 - val_loss: 138.9909\n",
      "Epoch 1531/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.2392 - val_loss: 160.3688\n",
      "Epoch 1532/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 161.0128 - val_loss: 149.5334\n",
      "Epoch 1533/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.2254 - val_loss: 138.1235\n",
      "Epoch 1534/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.2230 - val_loss: 138.2229\n",
      "Epoch 1535/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.9394 - val_loss: 137.5682\n",
      "Epoch 1536/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 128.9432 - val_loss: 158.2844\n",
      "Epoch 1537/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.7954 - val_loss: 200.0905\n",
      "Epoch 1538/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 138.3694 - val_loss: 143.8585\n",
      "Epoch 1539/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 125.4751 - val_loss: 137.6842\n",
      "Epoch 1540/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 127.7229 - val_loss: 151.4513\n",
      "Epoch 1541/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 127.7164 - val_loss: 203.0509\n",
      "Epoch 1542/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 134.3701 - val_loss: 159.5737\n",
      "Epoch 1543/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.8188 - val_loss: 181.4795\n",
      "Epoch 1544/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.7561 - val_loss: 182.6076\n",
      "Epoch 1545/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 380.4610 - val_loss: 230.1312\n",
      "Epoch 1546/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 164.2366 - val_loss: 162.6328\n",
      "Epoch 1547/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.8795 - val_loss: 147.0591\n",
      "Epoch 1548/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.0723 - val_loss: 135.5390\n",
      "Epoch 1549/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.0214 - val_loss: 179.2301\n",
      "Epoch 1550/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.0806 - val_loss: 136.8738\n",
      "Epoch 1551/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.6262 - val_loss: 154.1021\n",
      "Epoch 1552/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.9836 - val_loss: 189.7196\n",
      "Epoch 1553/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.5452 - val_loss: 150.7334\n",
      "Epoch 1554/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.7724 - val_loss: 135.8545\n",
      "Epoch 1555/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.2301 - val_loss: 146.3105\n",
      "Epoch 1556/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.8370 - val_loss: 144.7516\n",
      "Epoch 1557/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.3336 - val_loss: 135.2480\n",
      "Epoch 1558/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 133.5761 - val_loss: 134.6474\n",
      "Epoch 1559/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.4996 - val_loss: 132.8987\n",
      "Epoch 1560/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.0690 - val_loss: 195.8969\n",
      "Epoch 1561/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.6041 - val_loss: 136.8693\n",
      "Epoch 1562/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 138.1359 - val_loss: 205.3218\n",
      "Epoch 1563/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.6001 - val_loss: 158.2773\n",
      "Epoch 1564/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.1677 - val_loss: 153.5153\n",
      "Epoch 1565/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.3347 - val_loss: 136.2818\n",
      "Epoch 1566/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.6814 - val_loss: 154.7569\n",
      "Epoch 1567/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.8484 - val_loss: 135.1956\n",
      "Epoch 1568/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.5313 - val_loss: 135.9626\n",
      "Epoch 1569/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.6013 - val_loss: 187.5817\n",
      "Epoch 1570/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.0997 - val_loss: 145.9598\n",
      "Epoch 1571/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 154.1164 - val_loss: 143.8846\n",
      "Epoch 1572/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 132.2949 - val_loss: 151.3349\n",
      "Epoch 1573/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 138.1300 - val_loss: 141.9896\n",
      "Epoch 1574/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.8372 - val_loss: 140.3938\n",
      "Epoch 1575/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.2321 - val_loss: 146.0058\n",
      "Epoch 1576/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.5331 - val_loss: 152.3094\n",
      "Epoch 1577/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 173.9899 - val_loss: 360.2696\n",
      "Epoch 1578/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.0654 - val_loss: 150.1220\n",
      "Epoch 1579/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.2553 - val_loss: 132.1132\n",
      "Epoch 1580/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.8718 - val_loss: 138.8470\n",
      "Epoch 1581/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.1542 - val_loss: 187.6816\n",
      "Epoch 1582/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.8313 - val_loss: 136.7159\n",
      "Epoch 1583/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.7455 - val_loss: 137.7183\n",
      "Epoch 1584/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 142.1719 - val_loss: 135.6574\n",
      "Epoch 1585/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 147.3533 - val_loss: 142.7181\n",
      "Epoch 1586/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.6556 - val_loss: 134.0835\n",
      "Epoch 1587/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.8095 - val_loss: 146.2734\n",
      "Epoch 1588/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.5706 - val_loss: 184.8943\n",
      "Epoch 1589/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 134.5717 - val_loss: 132.2285\n",
      "Epoch 1590/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 128.3765 - val_loss: 140.4408\n",
      "Epoch 1591/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 205.9496 - val_loss: 1562.3708\n",
      "Epoch 1592/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 202.0091 - val_loss: 153.2559\n",
      "Epoch 1593/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.6758 - val_loss: 141.6336\n",
      "Epoch 1594/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.3521 - val_loss: 137.1392\n",
      "Epoch 1595/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.7115 - val_loss: 156.1137\n",
      "Epoch 1596/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.7178 - val_loss: 137.1629\n",
      "Epoch 1597/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.2570 - val_loss: 137.9507\n",
      "Epoch 1598/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.0055 - val_loss: 157.6035\n",
      "Epoch 1599/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 56us/step - loss: 131.1500 - val_loss: 139.5138\n",
      "Epoch 1600/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.9746 - val_loss: 178.3241\n",
      "Epoch 1601/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.2876 - val_loss: 249.6989\n",
      "Epoch 1602/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.4337 - val_loss: 149.7386\n",
      "Epoch 1603/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.4513 - val_loss: 134.5290\n",
      "Epoch 1604/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.2425 - val_loss: 183.9893\n",
      "Epoch 1605/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.8535 - val_loss: 141.4674\n",
      "Epoch 1606/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 127.8547 - val_loss: 199.0329\n",
      "Epoch 1607/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 133.4964 - val_loss: 141.0237\n",
      "Epoch 1608/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.8593 - val_loss: 138.0181\n",
      "Epoch 1609/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.6287 - val_loss: 141.2977\n",
      "Epoch 1610/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.8035 - val_loss: 234.6455\n",
      "Epoch 1611/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.4708 - val_loss: 142.5117\n",
      "Epoch 1612/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.0717 - val_loss: 145.1009\n",
      "Epoch 1613/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.4131 - val_loss: 140.2213\n",
      "Epoch 1614/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.0712 - val_loss: 177.2125\n",
      "Epoch 1615/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.2089 - val_loss: 140.2620\n",
      "Epoch 1616/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.2593 - val_loss: 136.7281\n",
      "Epoch 1617/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 136.4778 - val_loss: 153.0702\n",
      "Epoch 1618/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 132.4456 - val_loss: 140.6321\n",
      "Epoch 1619/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 188.1072 - val_loss: 147.5391\n",
      "Epoch 1620/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.6143 - val_loss: 140.0391\n",
      "Epoch 1621/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.0777 - val_loss: 153.4027\n",
      "Epoch 1622/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.8056 - val_loss: 180.0788\n",
      "Epoch 1623/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.1546 - val_loss: 136.4525\n",
      "Epoch 1624/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.9803 - val_loss: 138.3828\n",
      "Epoch 1625/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.6062 - val_loss: 165.6407\n",
      "Epoch 1626/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 129.5301 - val_loss: 148.9743\n",
      "Epoch 1627/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.1167 - val_loss: 136.3592\n",
      "Epoch 1628/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.2316 - val_loss: 144.5562\n",
      "Epoch 1629/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.2816 - val_loss: 147.1999\n",
      "Epoch 1630/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.0419 - val_loss: 196.6330\n",
      "Epoch 1631/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.4679 - val_loss: 155.3152\n",
      "Epoch 1632/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.9391 - val_loss: 155.1436\n",
      "Epoch 1633/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.7778 - val_loss: 137.9840\n",
      "Epoch 1634/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 124.7179 - val_loss: 149.3899\n",
      "Epoch 1635/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.8266 - val_loss: 187.8253\n",
      "Epoch 1636/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.7602 - val_loss: 140.5291\n",
      "Epoch 1637/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.4912 - val_loss: 167.5310\n",
      "Epoch 1638/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.7716 - val_loss: 176.7677\n",
      "Epoch 1639/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.7358 - val_loss: 164.4871\n",
      "Epoch 1640/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.6894 - val_loss: 168.7433\n",
      "Epoch 1641/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.4573 - val_loss: 162.2989\n",
      "Epoch 1642/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.6258 - val_loss: 163.4597\n",
      "Epoch 1643/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.8365 - val_loss: 146.5873\n",
      "Epoch 1644/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 127.9297 - val_loss: 185.2651\n",
      "Epoch 1645/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 128.0056 - val_loss: 137.5183\n",
      "Epoch 1646/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 133.2779 - val_loss: 136.9898\n",
      "Epoch 1647/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.8040 - val_loss: 227.6662\n",
      "Epoch 1648/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.7187 - val_loss: 139.9561\n",
      "Epoch 1649/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 133.1473 - val_loss: 137.9761\n",
      "Epoch 1650/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 131.2981 - val_loss: 163.4746\n",
      "Epoch 1651/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.4128 - val_loss: 140.2657\n",
      "Epoch 1652/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.7183 - val_loss: 166.7853\n",
      "Epoch 1653/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.3748 - val_loss: 163.6858\n",
      "Epoch 1654/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 124.7092 - val_loss: 138.0495\n",
      "Epoch 1655/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 126.7853 - val_loss: 138.4434\n",
      "Epoch 1656/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.8593 - val_loss: 146.5908\n",
      "Epoch 1657/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 124.0148 - val_loss: 132.9173\n",
      "Epoch 1658/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 138.2808 - val_loss: 134.4263\n",
      "Epoch 1659/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 135.0643 - val_loss: 209.8858\n",
      "Epoch 1660/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 146.3645 - val_loss: 155.0705\n",
      "Epoch 1661/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.6992 - val_loss: 141.1915\n",
      "Epoch 1662/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.8679 - val_loss: 153.4575\n",
      "Epoch 1663/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.7368 - val_loss: 143.1370\n",
      "Epoch 1664/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.0212 - val_loss: 159.8575\n",
      "Epoch 1665/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.2724 - val_loss: 140.1485\n",
      "Epoch 1666/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.4477 - val_loss: 148.3576\n",
      "Epoch 1667/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 211.6528 - val_loss: 144.1738\n",
      "Epoch 1668/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.3599 - val_loss: 140.9166\n",
      "Epoch 1669/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.8663 - val_loss: 133.0179\n",
      "Epoch 1670/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.4639 - val_loss: 162.4003\n",
      "Epoch 1671/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.1727 - val_loss: 149.9104\n",
      "Epoch 1672/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 128.3547 - val_loss: 149.8528\n",
      "Epoch 1673/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.2787 - val_loss: 147.3359\n",
      "Epoch 1674/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.8141 - val_loss: 136.7745\n",
      "Epoch 1675/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.4797 - val_loss: 143.6077\n",
      "Epoch 1676/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.4406 - val_loss: 147.7479\n",
      "Epoch 1677/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.9168 - val_loss: 201.9826\n",
      "Epoch 1678/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.6660 - val_loss: 141.9263\n",
      "Epoch 1679/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.9073 - val_loss: 145.2083\n",
      "Epoch 1680/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 153.6439 - val_loss: 151.6469\n",
      "Epoch 1681/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.9968 - val_loss: 134.4060\n",
      "Epoch 1682/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 127.2665 - val_loss: 157.7020\n",
      "Epoch 1683/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.9442 - val_loss: 137.0419\n",
      "Epoch 1684/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.6491 - val_loss: 231.1829\n",
      "Epoch 1685/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.3615 - val_loss: 146.4373\n",
      "Epoch 1686/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.0240 - val_loss: 137.5832\n",
      "Epoch 1687/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.4060 - val_loss: 142.1196\n",
      "Epoch 1688/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.1895 - val_loss: 136.4617\n",
      "Epoch 1689/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.5320 - val_loss: 134.9998\n",
      "Epoch 1690/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.2639 - val_loss: 188.8059\n",
      "Epoch 1691/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.6596 - val_loss: 145.1946\n",
      "Epoch 1692/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 126.8769 - val_loss: 158.4109\n",
      "Epoch 1693/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.5513 - val_loss: 134.9735\n",
      "Epoch 1694/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.5011 - val_loss: 134.1018\n",
      "Epoch 1695/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.4090 - val_loss: 378.1863\n",
      "Epoch 1696/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.9268 - val_loss: 139.3542\n",
      "Epoch 1697/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.6567 - val_loss: 156.5843\n",
      "Epoch 1698/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.4560 - val_loss: 296.2074\n",
      "Epoch 1699/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.9809 - val_loss: 165.0321\n",
      "Epoch 1700/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.1434 - val_loss: 140.1519\n",
      "Epoch 1701/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 135.5305 - val_loss: 146.6565\n",
      "Epoch 1702/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.6298 - val_loss: 165.0519\n",
      "Epoch 1703/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.9776 - val_loss: 147.8380\n",
      "Epoch 1704/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 196.0709 - val_loss: 259.8027\n",
      "Epoch 1705/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.8893 - val_loss: 145.5431\n",
      "Epoch 1706/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.7365 - val_loss: 187.8097\n",
      "Epoch 1707/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.1991 - val_loss: 151.1299\n",
      "Epoch 1708/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.3286 - val_loss: 159.0137\n",
      "Epoch 1709/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 127.0554 - val_loss: 137.9771\n",
      "Epoch 1710/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 127.7431 - val_loss: 132.8678\n",
      "Epoch 1711/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.6137 - val_loss: 133.2599\n",
      "Epoch 1712/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.2944 - val_loss: 134.9418\n",
      "Epoch 1713/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.4708 - val_loss: 140.6286\n",
      "Epoch 1714/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.4644 - val_loss: 205.7728\n",
      "Epoch 1715/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 125.3294 - val_loss: 137.8787\n",
      "Epoch 1716/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.7120 - val_loss: 152.8740\n",
      "Epoch 1717/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.8565 - val_loss: 133.6872\n",
      "Epoch 1718/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.2147 - val_loss: 140.0866\n",
      "Epoch 1719/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.8268 - val_loss: 134.8053\n",
      "Epoch 1720/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.7104 - val_loss: 141.3893\n",
      "Epoch 1721/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.3984 - val_loss: 190.4804\n",
      "Epoch 1722/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.4922 - val_loss: 137.2671\n",
      "Epoch 1723/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 148.9954 - val_loss: 144.9120\n",
      "Epoch 1724/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 148.1331 - val_loss: 135.1541\n",
      "Epoch 1725/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.7285 - val_loss: 154.5928\n",
      "Epoch 1726/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.8983 - val_loss: 162.7465\n",
      "Epoch 1727/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.8220 - val_loss: 136.5463\n",
      "Epoch 1728/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.9362 - val_loss: 185.9824\n",
      "Epoch 1729/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.5983 - val_loss: 133.8436\n",
      "Epoch 1730/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.4152 - val_loss: 178.7524\n",
      "Epoch 1731/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.1330 - val_loss: 145.8938\n",
      "Epoch 1732/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.7251 - val_loss: 136.8446\n",
      "Epoch 1733/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.8432 - val_loss: 138.0197\n",
      "Epoch 1734/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.7748 - val_loss: 133.4069\n",
      "Epoch 1735/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.1626 - val_loss: 140.0434\n",
      "Epoch 1736/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.2287 - val_loss: 161.3044\n",
      "Epoch 1737/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 140.8649 - val_loss: 134.6174\n",
      "Epoch 1738/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.8992 - val_loss: 224.8506\n",
      "Epoch 1739/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.8273 - val_loss: 167.1683\n",
      "Epoch 1740/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.2927 - val_loss: 153.1046\n",
      "Epoch 1741/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.8231 - val_loss: 147.1734\n",
      "Epoch 1742/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.9853 - val_loss: 179.7464\n",
      "Epoch 1743/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 173.6870 - val_loss: 183.3746\n",
      "Epoch 1744/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.9587 - val_loss: 138.1237\n",
      "Epoch 1745/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.9288 - val_loss: 134.4847\n",
      "Epoch 1746/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.9781 - val_loss: 169.3844\n",
      "Epoch 1747/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.8283 - val_loss: 261.8246\n",
      "Epoch 1748/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.0175 - val_loss: 144.5860\n",
      "Epoch 1749/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.3517 - val_loss: 134.8939\n",
      "Epoch 1750/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.4150 - val_loss: 134.8346\n",
      "Epoch 1751/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.2013 - val_loss: 146.5113\n",
      "Epoch 1752/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 150.5513 - val_loss: 177.8271\n",
      "Epoch 1753/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.5394 - val_loss: 135.7187\n",
      "Epoch 1754/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 121.0512 - val_loss: 132.4639\n",
      "Epoch 1755/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 125.0700 - val_loss: 142.6003\n",
      "Epoch 1756/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 148.2336 - val_loss: 165.8692\n",
      "Epoch 1757/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 125.4747 - val_loss: 138.0913\n",
      "Epoch 1758/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 127.1785 - val_loss: 133.7753\n",
      "Epoch 1759/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.5331 - val_loss: 140.2374\n",
      "Epoch 1760/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 132.9532 - val_loss: 131.4770\n",
      "Epoch 1761/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.8397 - val_loss: 158.8636\n",
      "Epoch 1762/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.8294 - val_loss: 138.3450\n",
      "Epoch 1763/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.9643 - val_loss: 158.8155\n",
      "Epoch 1764/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.6520 - val_loss: 223.8294\n",
      "Epoch 1765/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.1450 - val_loss: 155.6451\n",
      "Epoch 1766/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.5579 - val_loss: 138.7393\n",
      "Epoch 1767/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.1578 - val_loss: 158.3931\n",
      "Epoch 1768/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.4825 - val_loss: 152.8400\n",
      "Epoch 1769/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.9295 - val_loss: 160.7572\n",
      "Epoch 1770/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.1962 - val_loss: 157.8419\n",
      "Epoch 1771/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.6809 - val_loss: 147.5592\n",
      "Epoch 1772/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.7736 - val_loss: 187.9962\n",
      "Epoch 1773/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.0898 - val_loss: 139.5119\n",
      "Epoch 1774/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.9360 - val_loss: 132.6098\n",
      "Epoch 1775/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.9295 - val_loss: 222.7774\n",
      "Epoch 1776/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.7776 - val_loss: 133.2821\n",
      "Epoch 1777/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.0707 - val_loss: 153.1548\n",
      "Epoch 1778/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.7242 - val_loss: 209.9421\n",
      "Epoch 1779/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.9387 - val_loss: 147.9762\n",
      "Epoch 1780/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.5180 - val_loss: 139.2897\n",
      "Epoch 1781/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.3058 - val_loss: 186.3608\n",
      "Epoch 1782/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.9980 - val_loss: 136.1385\n",
      "Epoch 1783/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.2572 - val_loss: 144.3282\n",
      "Epoch 1784/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.4902 - val_loss: 136.9794\n",
      "Epoch 1785/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.3215 - val_loss: 143.2315\n",
      "Epoch 1786/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.3603 - val_loss: 153.6481\n",
      "Epoch 1787/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.1505 - val_loss: 131.9282\n",
      "Epoch 1788/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.1515 - val_loss: 133.0341\n",
      "Epoch 1789/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.2018 - val_loss: 142.3579\n",
      "Epoch 1790/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.6441 - val_loss: 145.5129\n",
      "Epoch 1791/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.6966 - val_loss: 143.6536\n",
      "Epoch 1792/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.0438 - val_loss: 147.1729\n",
      "Epoch 1793/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 143.7862 - val_loss: 131.7929\n",
      "Epoch 1794/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 149.2501 - val_loss: 136.0498\n",
      "Epoch 1795/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.6549 - val_loss: 138.3637\n",
      "Epoch 1796/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.7914 - val_loss: 207.7703\n",
      "Epoch 1797/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.1098 - val_loss: 164.9982\n",
      "Epoch 1798/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.8943 - val_loss: 135.5924\n",
      "Epoch 1799/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.5859 - val_loss: 160.6197\n",
      "Epoch 1800/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.9457 - val_loss: 167.7846\n",
      "Epoch 1801/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.5188 - val_loss: 142.3224\n",
      "Epoch 1802/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.3475 - val_loss: 154.3093\n",
      "Epoch 1803/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.3934 - val_loss: 137.3986\n",
      "Epoch 1804/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.6642 - val_loss: 143.2660\n",
      "Epoch 1805/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.4107 - val_loss: 177.0899\n",
      "Epoch 1806/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 152.4597 - val_loss: 133.4085\n",
      "Epoch 1807/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.4271 - val_loss: 166.9705\n",
      "Epoch 1808/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.8256 - val_loss: 150.2900\n",
      "Epoch 1809/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.0841 - val_loss: 155.0528\n",
      "Epoch 1810/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 228.3942 - val_loss: 217.9793\n",
      "Epoch 1811/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.3114 - val_loss: 144.2336\n",
      "Epoch 1812/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.0280 - val_loss: 137.1182\n",
      "Epoch 1813/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.5709 - val_loss: 141.0553\n",
      "Epoch 1814/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.2981 - val_loss: 161.4752\n",
      "Epoch 1815/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 143.8281 - val_loss: 142.2427\n",
      "Epoch 1816/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.4629 - val_loss: 144.4728\n",
      "Epoch 1817/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.6426 - val_loss: 137.2285\n",
      "Epoch 1818/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.3195 - val_loss: 135.3261\n",
      "Epoch 1819/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.7864 - val_loss: 159.3973\n",
      "Epoch 1820/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.3476 - val_loss: 148.0366\n",
      "Epoch 1821/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.0063 - val_loss: 171.1986\n",
      "Epoch 1822/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.0298 - val_loss: 186.8996\n",
      "Epoch 1823/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.5955 - val_loss: 157.9248\n",
      "Epoch 1824/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.8018 - val_loss: 147.5656\n",
      "Epoch 1825/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.8595 - val_loss: 172.4113\n",
      "Epoch 1826/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 127.2129 - val_loss: 142.3251\n",
      "Epoch 1827/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.0866 - val_loss: 150.1128\n",
      "Epoch 1828/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 135.2171 - val_loss: 138.5694\n",
      "Epoch 1829/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 129.0035 - val_loss: 148.3139\n",
      "Epoch 1830/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.7235 - val_loss: 145.2195\n",
      "Epoch 1831/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.4351 - val_loss: 145.8354\n",
      "Epoch 1832/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.4354 - val_loss: 157.5192\n",
      "Epoch 1833/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.7645 - val_loss: 179.3951\n",
      "Epoch 1834/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 217.6024 - val_loss: 178.3572\n",
      "Epoch 1835/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.0135 - val_loss: 139.6165\n",
      "Epoch 1836/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.3225 - val_loss: 143.2190\n",
      "Epoch 1837/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.9418 - val_loss: 135.5204\n",
      "Epoch 1838/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.7490 - val_loss: 183.8745\n",
      "Epoch 1839/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.2494 - val_loss: 205.0462\n",
      "Epoch 1840/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.1998 - val_loss: 138.1685\n",
      "Epoch 1841/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.8271 - val_loss: 140.5196\n",
      "Epoch 1842/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.7179 - val_loss: 147.2950\n",
      "Epoch 1843/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.5717 - val_loss: 142.8565\n",
      "Epoch 1844/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.7588 - val_loss: 148.6465\n",
      "Epoch 1845/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.1326 - val_loss: 141.8173\n",
      "Epoch 1846/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.1793 - val_loss: 141.5713\n",
      "Epoch 1847/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.8016 - val_loss: 141.0570\n",
      "Epoch 1848/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.5585 - val_loss: 165.0455\n",
      "Epoch 1849/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.4399 - val_loss: 136.7091\n",
      "Epoch 1850/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.2020 - val_loss: 142.3107\n",
      "Epoch 1851/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.1004 - val_loss: 135.7793\n",
      "Epoch 1852/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.5521 - val_loss: 134.4467\n",
      "Epoch 1853/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.8359 - val_loss: 138.9036\n",
      "Epoch 1854/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.0245 - val_loss: 147.2963\n",
      "Epoch 1855/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.4879 - val_loss: 138.7165\n",
      "Epoch 1856/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.9927 - val_loss: 141.9716\n",
      "Epoch 1857/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.8730 - val_loss: 210.8193\n",
      "Epoch 1858/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.4866 - val_loss: 152.7254\n",
      "Epoch 1859/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 122.5770 - val_loss: 196.2242\n",
      "Epoch 1860/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.6314 - val_loss: 131.5344\n",
      "Epoch 1861/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.2288 - val_loss: 132.3295\n",
      "Epoch 1862/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.8383 - val_loss: 134.8043\n",
      "Epoch 1863/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.8267 - val_loss: 206.2095\n",
      "Epoch 1864/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.8471 - val_loss: 140.9926\n",
      "Epoch 1865/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.0567 - val_loss: 154.6369\n",
      "Epoch 1866/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 166.6512 - val_loss: 139.2054\n",
      "Epoch 1867/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.1417 - val_loss: 132.4331\n",
      "Epoch 1868/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.2461 - val_loss: 135.9864\n",
      "Epoch 1869/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.5904 - val_loss: 149.9632\n",
      "Epoch 1870/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.9792 - val_loss: 133.2756\n",
      "Epoch 1871/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.7018 - val_loss: 144.3746\n",
      "Epoch 1872/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.9285 - val_loss: 135.2335\n",
      "Epoch 1873/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.7781 - val_loss: 140.6246\n",
      "Epoch 1874/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.1222 - val_loss: 142.7788\n",
      "Epoch 1875/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.6440 - val_loss: 135.7297\n",
      "Epoch 1876/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 124.7060 - val_loss: 135.2249\n",
      "Epoch 1877/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.1113 - val_loss: 160.0207\n",
      "Epoch 1878/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 134.0005 - val_loss: 143.9697\n",
      "Epoch 1879/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.1247 - val_loss: 134.1210\n",
      "Epoch 1880/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.3728 - val_loss: 141.3407\n",
      "Epoch 1881/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.8315 - val_loss: 171.5428\n",
      "Epoch 1882/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.0639 - val_loss: 157.5042\n",
      "Epoch 1883/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 142.9328 - val_loss: 134.8560\n",
      "Epoch 1884/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.9850 - val_loss: 159.2000\n",
      "Epoch 1885/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.8297 - val_loss: 139.0126\n",
      "Epoch 1886/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.7042 - val_loss: 138.2758\n",
      "Epoch 1887/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 133.0729 - val_loss: 132.2549\n",
      "Epoch 1888/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.1732 - val_loss: 135.6713\n",
      "Epoch 1889/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.7090 - val_loss: 137.4983\n",
      "Epoch 1890/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.7033 - val_loss: 138.5725\n",
      "Epoch 1891/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.9989 - val_loss: 144.6254\n",
      "Epoch 1892/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 126.2524 - val_loss: 158.7329\n",
      "Epoch 1893/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.8649 - val_loss: 135.7690\n",
      "Epoch 1894/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.3005 - val_loss: 138.9481\n",
      "Epoch 1895/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.3151 - val_loss: 134.5559\n",
      "Epoch 1896/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.7313 - val_loss: 260.6722\n",
      "Epoch 1897/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.1553 - val_loss: 147.9292\n",
      "Epoch 1898/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 208.6079 - val_loss: 168.4254\n",
      "Epoch 1899/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.2365 - val_loss: 152.1211\n",
      "Epoch 1900/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.5343 - val_loss: 159.5471\n",
      "Epoch 1901/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 120.5650 - val_loss: 141.8070\n",
      "Epoch 1902/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 120.3489 - val_loss: 178.1163\n",
      "Epoch 1903/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 124.8772 - val_loss: 136.3826\n",
      "Epoch 1904/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 126.8011 - val_loss: 158.3997\n",
      "Epoch 1905/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 123.4734 - val_loss: 155.8593\n",
      "Epoch 1906/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.5371 - val_loss: 137.4253\n",
      "Epoch 1907/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.1011 - val_loss: 147.1080\n",
      "Epoch 1908/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.7533 - val_loss: 208.1870\n",
      "Epoch 1909/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.0457 - val_loss: 132.4763\n",
      "Epoch 1910/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.1682 - val_loss: 164.0302\n",
      "Epoch 1911/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.9328 - val_loss: 134.5430\n",
      "Epoch 1912/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 160.2345 - val_loss: 173.9526\n",
      "Epoch 1913/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.0787 - val_loss: 142.1269\n",
      "Epoch 1914/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.2421 - val_loss: 139.2059\n",
      "Epoch 1915/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.4320 - val_loss: 147.5812\n",
      "Epoch 1916/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 121.0764 - val_loss: 147.6212\n",
      "Epoch 1917/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.1368 - val_loss: 172.3818\n",
      "Epoch 1918/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.3560 - val_loss: 162.8510\n",
      "Epoch 1919/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.2202 - val_loss: 134.5314\n",
      "Epoch 1920/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 145.8418 - val_loss: 165.2554\n",
      "Epoch 1921/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 136.3428 - val_loss: 132.8488\n",
      "Epoch 1922/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 140.232 - 0s 51us/step - loss: 137.6292 - val_loss: 135.0342\n",
      "Epoch 1923/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.5458 - val_loss: 151.5292\n",
      "Epoch 1924/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.8469 - val_loss: 213.3740\n",
      "Epoch 1925/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.5775 - val_loss: 136.8725\n",
      "Epoch 1926/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 124.7923 - val_loss: 147.0165\n",
      "Epoch 1927/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.9070 - val_loss: 136.3164\n",
      "Epoch 1928/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.3871 - val_loss: 211.4033\n",
      "Epoch 1929/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.2665 - val_loss: 138.9549\n",
      "Epoch 1930/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.8059 - val_loss: 159.9391\n",
      "Epoch 1931/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.4923 - val_loss: 162.4537\n",
      "Epoch 1932/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.6074 - val_loss: 218.6988\n",
      "Epoch 1933/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.3679 - val_loss: 136.5119\n",
      "Epoch 1934/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.0274 - val_loss: 148.9489\n",
      "Epoch 1935/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.3390 - val_loss: 139.6057\n",
      "Epoch 1936/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.8580 - val_loss: 148.2724\n",
      "Epoch 1937/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 119.9489 - val_loss: 151.1942\n",
      "Epoch 1938/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.4100 - val_loss: 165.1137\n",
      "Epoch 1939/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.1705 - val_loss: 180.1112\n",
      "Epoch 1940/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 137.0186 - val_loss: 167.2257\n",
      "Epoch 1941/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.1984 - val_loss: 149.1829\n",
      "Epoch 1942/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.3585 - val_loss: 134.7303\n",
      "Epoch 1943/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.9963 - val_loss: 141.4112\n",
      "Epoch 1944/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.0233 - val_loss: 146.5484\n",
      "Epoch 1945/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.5469 - val_loss: 185.9999\n",
      "Epoch 1946/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.6510 - val_loss: 140.6468\n",
      "Epoch 1947/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.3624 - val_loss: 134.9576\n",
      "Epoch 1948/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 225.0750 - val_loss: 214.8541\n",
      "Epoch 1949/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.5338 - val_loss: 132.9178\n",
      "Epoch 1950/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 125.1297 - val_loss: 138.5694\n",
      "Epoch 1951/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.0032 - val_loss: 148.7955\n",
      "Epoch 1952/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.5063 - val_loss: 143.7450\n",
      "Epoch 1953/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.6177 - val_loss: 132.8952\n",
      "Epoch 1954/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.7311 - val_loss: 158.9095\n",
      "Epoch 1955/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.9275 - val_loss: 168.1754\n",
      "Epoch 1956/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 124.8146 - val_loss: 205.7058\n",
      "Epoch 1957/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.0282 - val_loss: 171.8719\n",
      "Epoch 1958/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.1706 - val_loss: 143.9068\n",
      "Epoch 1959/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.5185 - val_loss: 164.4161\n",
      "Epoch 1960/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.9370 - val_loss: 130.3107\n",
      "Epoch 1961/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.2703 - val_loss: 164.2486\n",
      "Epoch 1962/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.6051 - val_loss: 139.3670\n",
      "Epoch 1963/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 175.6769 - val_loss: 133.3000\n",
      "Epoch 1964/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.2498 - val_loss: 142.3054\n",
      "Epoch 1965/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.3766 - val_loss: 146.1015\n",
      "Epoch 1966/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.6548 - val_loss: 142.4869\n",
      "Epoch 1967/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.6904 - val_loss: 133.6707\n",
      "Epoch 1968/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.0469 - val_loss: 134.0752\n",
      "Epoch 1969/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.8219 - val_loss: 147.1603\n",
      "Epoch 1970/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.1250 - val_loss: 158.3275\n",
      "Epoch 1971/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.7525 - val_loss: 136.5733\n",
      "Epoch 1972/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.9190 - val_loss: 162.0339\n",
      "Epoch 1973/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.0158 - val_loss: 145.8847\n",
      "Epoch 1974/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.2334 - val_loss: 142.0947\n",
      "Epoch 1975/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.7641 - val_loss: 128.9160\n",
      "Epoch 1976/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.0070 - val_loss: 144.6360\n",
      "Epoch 1977/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.6379 - val_loss: 136.0765\n",
      "Epoch 1978/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 132.4936 - val_loss: 146.1970\n",
      "Epoch 1979/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 143.2372 - val_loss: 238.0675\n",
      "Epoch 1980/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 123.4745 - val_loss: 130.2342\n",
      "Epoch 1981/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.9479 - val_loss: 184.9821\n",
      "Epoch 1982/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.2621 - val_loss: 169.7772\n",
      "Epoch 1983/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.4770 - val_loss: 169.9981\n",
      "Epoch 1984/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.8690 - val_loss: 201.0725\n",
      "Epoch 1985/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.4325 - val_loss: 138.1101\n",
      "Epoch 1986/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.9468 - val_loss: 132.3027\n",
      "Epoch 1987/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.5837 - val_loss: 139.1767\n",
      "Epoch 1988/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.3285 - val_loss: 139.6171\n",
      "Epoch 1989/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.6498 - val_loss: 154.4839\n",
      "Epoch 1990/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.5078 - val_loss: 155.0726\n",
      "Epoch 1991/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.7291 - val_loss: 142.0022\n",
      "Epoch 1992/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.0295 - val_loss: 146.5981\n",
      "Epoch 1993/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.8970 - val_loss: 158.5150\n",
      "Epoch 1994/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.8998 - val_loss: 156.4890\n",
      "Epoch 1995/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.8298 - val_loss: 162.3986\n",
      "Epoch 1996/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.9250 - val_loss: 159.3218\n",
      "Epoch 1997/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.4058 - val_loss: 156.0272\n",
      "Epoch 1998/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.0992 - val_loss: 137.8529\n",
      "Epoch 1999/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 140.7030 - val_loss: 166.9298\n",
      "Epoch 2000/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 127.2645 - val_loss: 139.8357\n",
      "Epoch 2001/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.0189 - val_loss: 134.2690\n",
      "Epoch 2002/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.9374 - val_loss: 132.6479\n",
      "Epoch 2003/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.0465 - val_loss: 157.6691\n",
      "Epoch 2004/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.5700 - val_loss: 134.7323\n",
      "Epoch 2005/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.1133 - val_loss: 148.8449\n",
      "Epoch 2006/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.0150 - val_loss: 158.3912\n",
      "Epoch 2007/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.1596 - val_loss: 201.6101\n",
      "Epoch 2008/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.8425 - val_loss: 132.5424\n",
      "Epoch 2009/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.8770 - val_loss: 157.2865\n",
      "Epoch 2010/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.6862 - val_loss: 166.8600\n",
      "Epoch 2011/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.4130 - val_loss: 136.1030\n",
      "Epoch 2012/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.6364 - val_loss: 150.2661\n",
      "Epoch 2013/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.9785 - val_loss: 140.1057\n",
      "Epoch 2014/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.2007 - val_loss: 139.2900\n",
      "Epoch 2015/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.6447 - val_loss: 168.3830\n",
      "Epoch 2016/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.6582 - val_loss: 151.8902\n",
      "Epoch 2017/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.7011 - val_loss: 144.9281\n",
      "Epoch 2018/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.3444 - val_loss: 136.7137\n",
      "Epoch 2019/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.5812 - val_loss: 151.8999\n",
      "Epoch 2020/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.5200 - val_loss: 141.8798\n",
      "Epoch 2021/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.7322 - val_loss: 137.3261\n",
      "Epoch 2022/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.1540 - val_loss: 145.1507\n",
      "Epoch 2023/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.2376 - val_loss: 158.2208\n",
      "Epoch 2024/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 129.4266 - val_loss: 129.8965\n",
      "Epoch 2025/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.7159 - val_loss: 137.8480\n",
      "Epoch 2026/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 136.8569 - val_loss: 133.6630\n",
      "Epoch 2027/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.2776 - val_loss: 173.1111\n",
      "Epoch 2028/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.4608 - val_loss: 173.3130\n",
      "Epoch 2029/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.0937 - val_loss: 129.3028\n",
      "Epoch 2030/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.6492 - val_loss: 130.9926\n",
      "Epoch 2031/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.0780 - val_loss: 168.7211\n",
      "Epoch 2032/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.7505 - val_loss: 145.7485\n",
      "Epoch 2033/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 127.0834 - val_loss: 155.5430\n",
      "Epoch 2034/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 121.8395 - val_loss: 159.9081\n",
      "Epoch 2035/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 180.8494 - val_loss: 144.0576\n",
      "Epoch 2036/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 123.3789 - val_loss: 135.3421\n",
      "Epoch 2037/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.7951 - val_loss: 175.9962\n",
      "Epoch 2038/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 123.9952 - val_loss: 134.6518\n",
      "Epoch 2039/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.5580 - val_loss: 146.6766\n",
      "Epoch 2040/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 122.6002 - val_loss: 136.8331\n",
      "Epoch 2041/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.1189 - val_loss: 163.3330\n",
      "Epoch 2042/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.4242 - val_loss: 133.5422\n",
      "Epoch 2043/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.8626 - val_loss: 132.6159\n",
      "Epoch 2044/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.6978 - val_loss: 145.7585\n",
      "Epoch 2045/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.9162 - val_loss: 152.8418\n",
      "Epoch 2046/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 127.1009 - val_loss: 133.4768\n",
      "Epoch 2047/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 126.9064 - val_loss: 164.7270\n",
      "Epoch 2048/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 127.9009 - val_loss: 140.7708\n",
      "Epoch 2049/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.4836 - val_loss: 133.2785\n",
      "Epoch 2050/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.8620 - val_loss: 144.6760\n",
      "Epoch 2051/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.1674 - val_loss: 131.9255\n",
      "Epoch 2052/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.7631 - val_loss: 275.8912\n",
      "Epoch 2053/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.4974 - val_loss: 140.6130\n",
      "Epoch 2054/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.7710 - val_loss: 154.6430\n",
      "Epoch 2055/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 167.4311 - val_loss: 177.6265\n",
      "Epoch 2056/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 121.6155 - val_loss: 138.2188\n",
      "Epoch 2057/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 120.9513 - val_loss: 133.5091\n",
      "Epoch 2058/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 127.0174 - val_loss: 169.8769\n",
      "Epoch 2059/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.8821 - val_loss: 136.9078\n",
      "Epoch 2060/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.3694 - val_loss: 136.6721\n",
      "Epoch 2061/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.6200 - val_loss: 134.2097\n",
      "Epoch 2062/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.3971 - val_loss: 155.3028\n",
      "Epoch 2063/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.1048 - val_loss: 134.8288\n",
      "Epoch 2064/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.1327 - val_loss: 147.8292\n",
      "Epoch 2065/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 128.6997 - val_loss: 158.5869\n",
      "Epoch 2066/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.5621 - val_loss: 146.4762\n",
      "Epoch 2067/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.6408 - val_loss: 141.0925\n",
      "Epoch 2068/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.4746 - val_loss: 150.8178\n",
      "Epoch 2069/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.5953 - val_loss: 138.6691\n",
      "Epoch 2070/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.4779 - val_loss: 150.7347\n",
      "Epoch 2071/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.3771 - val_loss: 147.3579\n",
      "Epoch 2072/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.0821 - val_loss: 135.1260\n",
      "Epoch 2073/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.8710 - val_loss: 129.3290\n",
      "Epoch 2074/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.3513 - val_loss: 159.1258\n",
      "Epoch 2075/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.9685 - val_loss: 168.8433\n",
      "Epoch 2076/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.7581 - val_loss: 139.0884\n",
      "Epoch 2077/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.2669 - val_loss: 164.8314\n",
      "Epoch 2078/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.4685 - val_loss: 157.4624\n",
      "Epoch 2079/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.0770 - val_loss: 142.1550\n",
      "Epoch 2080/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.8962 - val_loss: 130.8537\n",
      "Epoch 2081/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.7120 - val_loss: 251.2245\n",
      "Epoch 2082/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.3317 - val_loss: 195.0769\n",
      "Epoch 2083/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.1643 - val_loss: 163.7728\n",
      "Epoch 2084/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.8298 - val_loss: 133.1085\n",
      "Epoch 2085/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.3434 - val_loss: 146.1620\n",
      "Epoch 2086/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.6060 - val_loss: 145.3410\n",
      "Epoch 2087/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.4484 - val_loss: 145.9627\n",
      "Epoch 2088/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.0470 - val_loss: 129.7938\n",
      "Epoch 2089/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.4847 - val_loss: 158.1522\n",
      "Epoch 2090/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.6576 - val_loss: 188.5129\n",
      "Epoch 2091/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.2037 - val_loss: 130.4683\n",
      "Epoch 2092/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.3794 - val_loss: 129.7289\n",
      "Epoch 2093/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.5610 - val_loss: 134.5020\n",
      "Epoch 2094/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.6573 - val_loss: 194.6248\n",
      "Epoch 2095/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.8703 - val_loss: 134.0025\n",
      "Epoch 2096/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.4900 - val_loss: 181.0754\n",
      "Epoch 2097/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.0434 - val_loss: 138.5246\n",
      "Epoch 2098/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.8923 - val_loss: 142.3498\n",
      "Epoch 2099/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.0083 - val_loss: 137.3353\n",
      "Epoch 2100/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.0851 - val_loss: 142.2358\n",
      "Epoch 2101/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 128.5854 - val_loss: 130.6194\n",
      "Epoch 2102/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.2208 - val_loss: 166.3203\n",
      "Epoch 2103/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 231.9256 - val_loss: 142.2939\n",
      "Epoch 2104/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.7833 - val_loss: 136.9315\n",
      "Epoch 2105/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.2629 - val_loss: 135.9655\n",
      "Epoch 2106/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 128.3384 - val_loss: 136.3723\n",
      "Epoch 2107/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.6940 - val_loss: 159.6241\n",
      "Epoch 2108/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.4373 - val_loss: 135.3776\n",
      "Epoch 2109/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 128.6029 - val_loss: 137.1626\n",
      "Epoch 2110/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.7347 - val_loss: 154.0111\n",
      "Epoch 2111/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.3830 - val_loss: 145.2534\n",
      "Epoch 2112/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.9656 - val_loss: 130.8869\n",
      "Epoch 2113/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.4935 - val_loss: 133.5415\n",
      "Epoch 2114/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.4114 - val_loss: 133.4363\n",
      "Epoch 2115/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 159.2538 - val_loss: 139.5141\n",
      "Epoch 2116/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.6702 - val_loss: 135.3243\n",
      "Epoch 2117/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.1977 - val_loss: 146.1450\n",
      "Epoch 2118/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.5700 - val_loss: 132.4093\n",
      "Epoch 2119/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.9553 - val_loss: 140.1946\n",
      "Epoch 2120/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 125.4061 - val_loss: 174.5676\n",
      "Epoch 2121/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 124.7627 - val_loss: 135.1244\n",
      "Epoch 2122/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.9998 - val_loss: 147.4950\n",
      "Epoch 2123/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 122.8137 - val_loss: 134.1630\n",
      "Epoch 2124/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 130.3361 - val_loss: 134.7422\n",
      "Epoch 2125/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.3852 - val_loss: 141.4200\n",
      "Epoch 2126/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.7472 - val_loss: 193.8055\n",
      "Epoch 2127/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.2042 - val_loss: 142.6876\n",
      "Epoch 2128/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.6236 - val_loss: 171.0071\n",
      "Epoch 2129/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.1248 - val_loss: 145.7224\n",
      "Epoch 2130/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.6806 - val_loss: 155.2710\n",
      "Epoch 2131/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.7812 - val_loss: 136.8223\n",
      "Epoch 2132/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.1002 - val_loss: 139.7180\n",
      "Epoch 2133/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.2403 - val_loss: 139.2053\n",
      "Epoch 2134/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.5036 - val_loss: 131.7299\n",
      "Epoch 2135/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.8167 - val_loss: 155.0431\n",
      "Epoch 2136/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.1170 - val_loss: 151.9834\n",
      "Epoch 2137/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.3530 - val_loss: 133.8090\n",
      "Epoch 2138/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.4315 - val_loss: 139.2759\n",
      "Epoch 2139/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.0919 - val_loss: 155.8635\n",
      "Epoch 2140/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.7726 - val_loss: 132.7963\n",
      "Epoch 2141/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 157.3191 - val_loss: 696.3488\n",
      "Epoch 2142/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 147.8139 - val_loss: 148.5829\n",
      "Epoch 2143/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.3004 - val_loss: 136.8172\n",
      "Epoch 2144/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.9688 - val_loss: 145.7369\n",
      "Epoch 2145/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.1309 - val_loss: 167.4925\n",
      "Epoch 2146/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.4777 - val_loss: 137.3471\n",
      "Epoch 2147/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.5081 - val_loss: 138.5074\n",
      "Epoch 2148/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.0613 - val_loss: 182.3106\n",
      "Epoch 2149/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.3927 - val_loss: 137.3223\n",
      "Epoch 2150/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.3966 - val_loss: 137.8547\n",
      "Epoch 2151/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 119.8983 - val_loss: 151.3154\n",
      "Epoch 2152/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.5244 - val_loss: 208.7650\n",
      "Epoch 2153/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.3435 - val_loss: 157.8159\n",
      "Epoch 2154/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.0915 - val_loss: 140.0692\n",
      "Epoch 2155/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 124.8105 - val_loss: 181.7567\n",
      "Epoch 2156/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.7338 - val_loss: 193.9629\n",
      "Epoch 2157/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.2378 - val_loss: 185.3342\n",
      "Epoch 2158/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 125.388 - 0s 51us/step - loss: 124.0311 - val_loss: 137.0454\n",
      "Epoch 2159/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.7540 - val_loss: 163.4913\n",
      "Epoch 2160/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.7130 - val_loss: 144.1294\n",
      "Epoch 2161/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.5914 - val_loss: 139.2567\n",
      "Epoch 2162/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.0853 - val_loss: 141.9725\n",
      "Epoch 2163/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 146.0340 - val_loss: 150.8035\n",
      "Epoch 2164/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.7143 - val_loss: 140.7489\n",
      "Epoch 2165/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.0392 - val_loss: 134.6785\n",
      "Epoch 2166/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.5187 - val_loss: 165.3238\n",
      "Epoch 2167/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.3687 - val_loss: 134.8022\n",
      "Epoch 2168/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.2820 - val_loss: 175.6992\n",
      "Epoch 2169/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.4545 - val_loss: 166.1207\n",
      "Epoch 2170/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.9000 - val_loss: 161.8755\n",
      "Epoch 2171/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.8010 - val_loss: 169.2955\n",
      "Epoch 2172/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 148.5386 - val_loss: 139.2183\n",
      "Epoch 2173/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 184.3794 - val_loss: 157.6025\n",
      "Epoch 2174/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 124.4535 - val_loss: 134.9304\n",
      "Epoch 2175/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 55us/step - loss: 126.2245 - val_loss: 149.1003\n",
      "Epoch 2176/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.5320 - val_loss: 162.5440\n",
      "Epoch 2177/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.0510 - val_loss: 155.3020\n",
      "Epoch 2178/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 126.8677 - val_loss: 131.0592\n",
      "Epoch 2179/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.9392 - val_loss: 134.3472\n",
      "Epoch 2180/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 129.8145 - val_loss: 143.0349\n",
      "Epoch 2181/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.9160 - val_loss: 139.7496\n",
      "Epoch 2182/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.6792 - val_loss: 164.0088\n",
      "Epoch 2183/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.7087 - val_loss: 162.7993\n",
      "Epoch 2184/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 124.3881 - val_loss: 129.3229\n",
      "Epoch 2185/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 123.8477 - val_loss: 137.5864\n",
      "Epoch 2186/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 122.7447 - val_loss: 136.2986\n",
      "Epoch 2187/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.0779 - val_loss: 158.1147\n",
      "Epoch 2188/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.6532 - val_loss: 144.4969\n",
      "Epoch 2189/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 130.1591 - val_loss: 170.6862\n",
      "Epoch 2190/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.4275 - val_loss: 204.6777\n",
      "Epoch 2191/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.4543 - val_loss: 138.4783\n",
      "Epoch 2192/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 132.4935 - val_loss: 144.5477\n",
      "Epoch 2193/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 162.3639 - val_loss: 146.2853\n",
      "Epoch 2194/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 127.6434 - val_loss: 133.7200\n",
      "Epoch 2195/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 123.2240 - val_loss: 152.3368\n",
      "Epoch 2196/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 128.3857 - val_loss: 146.5180\n",
      "Epoch 2197/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.8211 - val_loss: 141.1410\n",
      "Epoch 2198/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.6682 - val_loss: 150.5796\n",
      "Epoch 2199/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.9536 - val_loss: 143.3314\n",
      "Epoch 2200/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.2780 - val_loss: 130.2670\n",
      "Epoch 2201/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 126.3994 - val_loss: 139.4232\n",
      "Epoch 2202/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.3362 - val_loss: 138.6519\n",
      "Epoch 2203/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.2138 - val_loss: 133.8276\n",
      "Epoch 2204/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.2617 - val_loss: 142.9042\n",
      "Epoch 2205/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.4675 - val_loss: 149.0189\n",
      "Epoch 2206/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.9589 - val_loss: 144.3226\n",
      "Epoch 2207/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 157.4299 - val_loss: 142.5634\n",
      "Epoch 2208/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.5254 - val_loss: 134.8849\n",
      "Epoch 2209/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.4335 - val_loss: 142.9981\n",
      "Epoch 2210/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.7300 - val_loss: 169.5576\n",
      "Epoch 2211/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.8698 - val_loss: 141.2895\n",
      "Epoch 2212/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.2469 - val_loss: 144.6745\n",
      "Epoch 2213/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.9620 - val_loss: 168.0865\n",
      "Epoch 2214/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.7336 - val_loss: 152.3036\n",
      "Epoch 2215/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.6635 - val_loss: 224.6170\n",
      "Epoch 2216/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 160.9136 - val_loss: 150.1839\n",
      "Epoch 2217/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.4726 - val_loss: 133.5530\n",
      "Epoch 2218/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.1582 - val_loss: 130.4172\n",
      "Epoch 2219/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.5042 - val_loss: 174.4493\n",
      "Epoch 2220/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.3312 - val_loss: 137.5666\n",
      "Epoch 2221/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.6483 - val_loss: 129.7779\n",
      "Epoch 2222/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.2047 - val_loss: 177.4080\n",
      "Epoch 2223/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.6762 - val_loss: 171.5483\n",
      "Epoch 2224/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.2201 - val_loss: 137.5516\n",
      "Epoch 2225/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.6746 - val_loss: 139.2950\n",
      "Epoch 2226/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.8859 - val_loss: 137.8204\n",
      "Epoch 2227/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.1928 - val_loss: 134.4962\n",
      "Epoch 2228/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.7759 - val_loss: 133.2230\n",
      "Epoch 2229/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.5999 - val_loss: 137.3779\n",
      "Epoch 2230/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.0585 - val_loss: 162.9176\n",
      "Epoch 2231/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 166.4422 - val_loss: 145.9777\n",
      "Epoch 2232/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.1751 - val_loss: 137.5981\n",
      "Epoch 2233/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.6875 - val_loss: 137.7780\n",
      "Epoch 2234/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.3526 - val_loss: 141.9491\n",
      "Epoch 2235/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.2747 - val_loss: 138.7994\n",
      "Epoch 2236/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.3046 - val_loss: 132.2561\n",
      "Epoch 2237/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.8577 - val_loss: 128.8229\n",
      "Epoch 2238/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.2640 - val_loss: 159.7100\n",
      "Epoch 2239/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.8652 - val_loss: 152.2896\n",
      "Epoch 2240/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.9762 - val_loss: 149.5457\n",
      "Epoch 2241/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.9879 - val_loss: 129.2844\n",
      "Epoch 2242/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.1390 - val_loss: 135.1281\n",
      "Epoch 2243/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.7193 - val_loss: 157.2286\n",
      "Epoch 2244/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.8597 - val_loss: 145.6741\n",
      "Epoch 2245/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 127.1257 - val_loss: 141.2689\n",
      "Epoch 2246/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.7744 - val_loss: 135.4251\n",
      "Epoch 2247/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.2567 - val_loss: 137.7940\n",
      "Epoch 2248/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.0068 - val_loss: 140.8647\n",
      "Epoch 2249/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 134.9341 - val_loss: 135.7677\n",
      "Epoch 2250/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.5544 - val_loss: 183.2621\n",
      "Epoch 2251/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.4143 - val_loss: 133.1779\n",
      "Epoch 2252/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.7536 - val_loss: 151.5667\n",
      "Epoch 2253/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.7902 - val_loss: 139.3677\n",
      "Epoch 2254/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.6430 - val_loss: 132.7678\n",
      "Epoch 2255/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.0311 - val_loss: 172.6283\n",
      "Epoch 2256/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.6842 - val_loss: 166.5808\n",
      "Epoch 2257/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.0486 - val_loss: 140.6426\n",
      "Epoch 2258/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.8649 - val_loss: 130.1706\n",
      "Epoch 2259/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.0024 - val_loss: 133.1888\n",
      "Epoch 2260/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 129.041 - 0s 51us/step - loss: 128.7831 - val_loss: 131.5362\n",
      "Epoch 2261/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.7228 - val_loss: 155.7528\n",
      "Epoch 2262/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.4169 - val_loss: 150.2708\n",
      "Epoch 2263/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.4851 - val_loss: 135.4037\n",
      "Epoch 2264/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.5326 - val_loss: 138.7433\n",
      "Epoch 2265/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.0701 - val_loss: 131.4882\n",
      "Epoch 2266/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 128.9234 - val_loss: 131.3890\n",
      "Epoch 2267/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 232.6776 - val_loss: 238.3930\n",
      "Epoch 2268/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 154.7526 - val_loss: 146.7379\n",
      "Epoch 2269/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 125.9425 - val_loss: 136.2982\n",
      "Epoch 2270/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 122.8816 - val_loss: 175.8492\n",
      "Epoch 2271/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 124.9597 - val_loss: 138.6819\n",
      "Epoch 2272/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.1044 - val_loss: 164.1262\n",
      "Epoch 2273/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.0282 - val_loss: 138.0485\n",
      "Epoch 2274/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.0364 - val_loss: 142.9778\n",
      "Epoch 2275/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.6715 - val_loss: 133.8614\n",
      "Epoch 2276/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.6091 - val_loss: 185.5840\n",
      "Epoch 2277/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.6412 - val_loss: 131.5495\n",
      "Epoch 2278/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.4506 - val_loss: 138.4557\n",
      "Epoch 2279/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.8848 - val_loss: 163.3254\n",
      "Epoch 2280/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.7557 - val_loss: 145.6249\n",
      "Epoch 2281/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.6850 - val_loss: 212.3764\n",
      "Epoch 2282/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.7899 - val_loss: 168.3012\n",
      "Epoch 2283/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.9788 - val_loss: 148.6636\n",
      "Epoch 2284/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.4931 - val_loss: 135.9065\n",
      "Epoch 2285/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.6122 - val_loss: 131.5007\n",
      "Epoch 2286/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.3534 - val_loss: 177.4794\n",
      "Epoch 2287/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.8620 - val_loss: 144.1713\n",
      "Epoch 2288/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.3426 - val_loss: 161.8878\n",
      "Epoch 2289/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.2783 - val_loss: 151.0943\n",
      "Epoch 2290/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.1983 - val_loss: 141.6592\n",
      "Epoch 2291/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.0496 - val_loss: 128.8304\n",
      "Epoch 2292/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.6753 - val_loss: 133.4912\n",
      "Epoch 2293/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.1602 - val_loss: 141.2729\n",
      "Epoch 2294/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.3513 - val_loss: 160.5191\n",
      "Epoch 2295/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.3872 - val_loss: 186.2636\n",
      "Epoch 2296/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.7962 - val_loss: 129.4640\n",
      "Epoch 2297/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.3482 - val_loss: 131.7019\n",
      "Epoch 2298/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.0233 - val_loss: 147.6834\n",
      "Epoch 2299/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.1252 - val_loss: 155.2283\n",
      "Epoch 2300/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 124.2172 - val_loss: 134.6965\n",
      "Epoch 2301/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.4582 - val_loss: 147.4439\n",
      "Epoch 2302/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.0682 - val_loss: 130.5008\n",
      "Epoch 2303/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 120.2805 - val_loss: 144.2055\n",
      "Epoch 2304/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 135.6591 - val_loss: 141.1332\n",
      "Epoch 2305/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.6085 - val_loss: 137.0489\n",
      "Epoch 2306/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.6054 - val_loss: 152.2365\n",
      "Epoch 2307/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 120.9789 - val_loss: 134.2844\n",
      "Epoch 2308/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.9048 - val_loss: 138.8874\n",
      "Epoch 2309/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.9270 - val_loss: 138.7894\n",
      "Epoch 2310/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 132.6250 - val_loss: 139.7640\n",
      "Epoch 2311/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 130.6855 - val_loss: 138.6447\n",
      "Epoch 2312/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.4611 - val_loss: 150.9562\n",
      "Epoch 2313/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.2976 - val_loss: 136.5809\n",
      "Epoch 2314/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.1178 - val_loss: 132.7914\n",
      "Epoch 2315/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.9206 - val_loss: 140.6914\n",
      "Epoch 2316/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 158.4039 - val_loss: 215.2174\n",
      "Epoch 2317/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.9739 - val_loss: 152.3537\n",
      "Epoch 2318/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.1596 - val_loss: 133.3731\n",
      "Epoch 2319/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 49us/step - loss: 169.0017 - val_loss: 136.6516\n",
      "Epoch 2320/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.5451 - val_loss: 143.1127\n",
      "Epoch 2321/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.2490 - val_loss: 134.5117\n",
      "Epoch 2322/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.3156 - val_loss: 130.6247\n",
      "Epoch 2323/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.2158 - val_loss: 139.0067\n",
      "Epoch 2324/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.9184 - val_loss: 139.6656\n",
      "Epoch 2325/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 120.1853 - val_loss: 135.3646\n",
      "Epoch 2326/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.4747 - val_loss: 133.9288\n",
      "Epoch 2327/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.0331 - val_loss: 135.9113\n",
      "Epoch 2328/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.7128 - val_loss: 139.2840\n",
      "Epoch 2329/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 184.4283 - val_loss: 137.2998\n",
      "Epoch 2330/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.4687 - val_loss: 133.4457\n",
      "Epoch 2331/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.4487 - val_loss: 142.2571\n",
      "Epoch 2332/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.8291 - val_loss: 137.5246\n",
      "Epoch 2333/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.0605 - val_loss: 152.6131\n",
      "Epoch 2334/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.7049 - val_loss: 134.1719\n",
      "Epoch 2335/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.2717 - val_loss: 139.0088\n",
      "Epoch 2336/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.7010 - val_loss: 163.9222\n",
      "Epoch 2337/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 126.5391 - val_loss: 136.5687\n",
      "Epoch 2338/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.1345 - val_loss: 161.4320\n",
      "Epoch 2339/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.9844 - val_loss: 146.1691\n",
      "Epoch 2340/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 124.2217 - val_loss: 136.3675\n",
      "Epoch 2341/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.3488 - val_loss: 215.9993\n",
      "Epoch 2342/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 128.8710 - val_loss: 138.3567\n",
      "Epoch 2343/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.7034 - val_loss: 156.2800\n",
      "Epoch 2344/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 140.0106 - val_loss: 144.3026\n",
      "Epoch 2345/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.0440 - val_loss: 135.4533\n",
      "Epoch 2346/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 123.3401 - val_loss: 160.7062\n",
      "Epoch 2347/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.6547 - val_loss: 138.7010\n",
      "Epoch 2348/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.0628 - val_loss: 130.6508\n",
      "Epoch 2349/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.6747 - val_loss: 143.3990\n",
      "Epoch 2350/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.7851 - val_loss: 290.4682\n",
      "Epoch 2351/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.3146 - val_loss: 141.0342\n",
      "Epoch 2352/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 125.3044 - val_loss: 140.2765\n",
      "Epoch 2353/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 130.3478 - val_loss: 136.5113\n",
      "Epoch 2354/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.9481 - val_loss: 136.2988\n",
      "Epoch 2355/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.8406 - val_loss: 178.0531\n",
      "Epoch 2356/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.0520 - val_loss: 140.7499\n",
      "Epoch 2357/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.6897 - val_loss: 131.9099\n",
      "Epoch 2358/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.0150 - val_loss: 140.7361\n",
      "Epoch 2359/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.8167 - val_loss: 155.8282\n",
      "Epoch 2360/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 126.5606 - val_loss: 142.4044\n",
      "Epoch 2361/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.1906 - val_loss: 145.9555\n",
      "Epoch 2362/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.3686 - val_loss: 133.5092\n",
      "Epoch 2363/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.8012 - val_loss: 151.3102\n",
      "Epoch 2364/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.1015 - val_loss: 175.1700\n",
      "Epoch 2365/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 127.7140 - val_loss: 161.8106\n",
      "Epoch 2366/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 166.1159 - val_loss: 138.0623\n",
      "Epoch 2367/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 122.9183 - val_loss: 157.2548\n",
      "Epoch 2368/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 120.9342 - val_loss: 138.0201\n",
      "Epoch 2369/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 132.3975 - val_loss: 133.3142\n",
      "Epoch 2370/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.4488 - val_loss: 140.2139\n",
      "Epoch 2371/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.4884 - val_loss: 177.1918\n",
      "Epoch 2372/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 127.1904 - val_loss: 146.8426\n",
      "Epoch 2373/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.1302 - val_loss: 134.0219\n",
      "Epoch 2374/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.8763 - val_loss: 164.7944\n",
      "Epoch 2375/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.0312 - val_loss: 155.0206\n",
      "Epoch 2376/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.3995 - val_loss: 140.2001\n",
      "Epoch 2377/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.6225 - val_loss: 139.0047\n",
      "Epoch 2378/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 126.2437 - val_loss: 134.8264\n",
      "Epoch 2379/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.9480 - val_loss: 174.2385\n",
      "Epoch 2380/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.1710 - val_loss: 187.1132\n",
      "Epoch 2381/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.5392 - val_loss: 151.3931\n",
      "Epoch 2382/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.2659 - val_loss: 144.5570\n",
      "Epoch 2383/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.8201 - val_loss: 180.2473\n",
      "Epoch 2384/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.3799 - val_loss: 152.9057\n",
      "Epoch 2385/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.5651 - val_loss: 153.3187\n",
      "Epoch 2386/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.9975 - val_loss: 135.7171\n",
      "Epoch 2387/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.8057 - val_loss: 194.3162\n",
      "Epoch 2388/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.4519 - val_loss: 177.9505\n",
      "Epoch 2389/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.1122 - val_loss: 154.4975\n",
      "Epoch 2390/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.9959 - val_loss: 139.0393\n",
      "Epoch 2391/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.9851 - val_loss: 146.9282\n",
      "Epoch 2392/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.0721 - val_loss: 139.7419\n",
      "Epoch 2393/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.5806 - val_loss: 136.3055\n",
      "Epoch 2394/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 123.6695 - val_loss: 228.0570\n",
      "Epoch 2395/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.3856 - val_loss: 136.1019\n",
      "Epoch 2396/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 125.4796 - val_loss: 146.5138\n",
      "Epoch 2397/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.5844 - val_loss: 142.7522\n",
      "Epoch 2398/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.4730 - val_loss: 184.2864\n",
      "Epoch 2399/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 216.1650 - val_loss: 221.4891\n",
      "Epoch 2400/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.5641 - val_loss: 153.3978\n",
      "Epoch 2401/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.3334 - val_loss: 158.5295\n",
      "Epoch 2402/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.2493 - val_loss: 159.6606\n",
      "Epoch 2403/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.0305 - val_loss: 156.8425\n",
      "Epoch 2404/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.4960 - val_loss: 148.7222\n",
      "Epoch 2405/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.9877 - val_loss: 130.3008\n",
      "Epoch 2406/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.6603 - val_loss: 130.1758\n",
      "Epoch 2407/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.5327 - val_loss: 157.0791\n",
      "Epoch 2408/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.6950 - val_loss: 145.5888\n",
      "Epoch 2409/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.2961 - val_loss: 142.3074\n",
      "Epoch 2410/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.2623 - val_loss: 139.8468\n",
      "Epoch 2411/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.8008 - val_loss: 130.3747\n",
      "Epoch 2412/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 125.0580 - val_loss: 136.0293\n",
      "Epoch 2413/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.8363 - val_loss: 135.3507\n",
      "Epoch 2414/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 127.0414 - val_loss: 150.4456\n",
      "Epoch 2415/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 128.2572 - val_loss: 139.2947\n",
      "Epoch 2416/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 125.8133 - val_loss: 199.8789\n",
      "Epoch 2417/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.0733 - val_loss: 146.0223\n",
      "Epoch 2418/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.8824 - val_loss: 151.9625\n",
      "Epoch 2419/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.7305 - val_loss: 144.4904\n",
      "Epoch 2420/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.2321 - val_loss: 134.9125\n",
      "Epoch 2421/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.5440 - val_loss: 134.5645\n",
      "Epoch 2422/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.7020 - val_loss: 132.7629\n",
      "Epoch 2423/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.0211 - val_loss: 129.9377\n",
      "Epoch 2424/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.7788 - val_loss: 179.2288\n",
      "Epoch 2425/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.0919 - val_loss: 146.5875\n",
      "Epoch 2426/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.7433 - val_loss: 143.2558\n",
      "Epoch 2427/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.4047 - val_loss: 152.3728\n",
      "Epoch 2428/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.8599 - val_loss: 564.5630\n",
      "Epoch 2429/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 281.2383 - val_loss: 149.3796\n",
      "Epoch 2430/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.2829 - val_loss: 134.0493\n",
      "Epoch 2431/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 118.5127 - val_loss: 139.1544\n",
      "Epoch 2432/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.0681 - val_loss: 140.2135\n",
      "Epoch 2433/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.6606 - val_loss: 131.7760\n",
      "Epoch 2434/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 124.4810 - val_loss: 148.2351\n",
      "Epoch 2435/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.3424 - val_loss: 133.4481\n",
      "Epoch 2436/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.6755 - val_loss: 156.9537\n",
      "Epoch 2437/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.7720 - val_loss: 143.0374\n",
      "Epoch 2438/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.8011 - val_loss: 131.9995\n",
      "Epoch 2439/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.3403 - val_loss: 144.0524\n",
      "Epoch 2440/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.0917 - val_loss: 254.6950\n",
      "Epoch 2441/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.2769 - val_loss: 180.4206\n",
      "Epoch 2442/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.3274 - val_loss: 152.8699\n",
      "Epoch 2443/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.3607 - val_loss: 176.1816\n",
      "Epoch 2444/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.9717 - val_loss: 132.2017\n",
      "Epoch 2445/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 172.4314 - val_loss: 199.1145\n",
      "Epoch 2446/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.2479 - val_loss: 171.5525\n",
      "Epoch 2447/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.8638 - val_loss: 130.6730\n",
      "Epoch 2448/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.9155 - val_loss: 132.5616\n",
      "Epoch 2449/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.7692 - val_loss: 131.8522\n",
      "Epoch 2450/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.3268 - val_loss: 134.5613\n",
      "Epoch 2451/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 121.5813 - val_loss: 160.8839\n",
      "Epoch 2452/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.6689 - val_loss: 146.3617\n",
      "Epoch 2453/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.6075 - val_loss: 140.1322\n",
      "Epoch 2454/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.8083 - val_loss: 136.1612\n",
      "Epoch 2455/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.4469 - val_loss: 140.3197\n",
      "Epoch 2456/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.0839 - val_loss: 133.0559\n",
      "Epoch 2457/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.8399 - val_loss: 137.2216\n",
      "Epoch 2458/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.7667 - val_loss: 145.5843\n",
      "Epoch 2459/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.6031 - val_loss: 144.7458\n",
      "Epoch 2460/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 153.2632 - val_loss: 137.2022\n",
      "Epoch 2461/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 132.6601 - val_loss: 181.5663\n",
      "Epoch 2462/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.8286 - val_loss: 132.0660\n",
      "Epoch 2463/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.2882 - val_loss: 146.0065\n",
      "Epoch 2464/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.8318 - val_loss: 134.1175\n",
      "Epoch 2465/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.0846 - val_loss: 141.1169\n",
      "Epoch 2466/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.1286 - val_loss: 158.1314\n",
      "Epoch 2467/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.9022 - val_loss: 140.2750\n",
      "Epoch 2468/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.5946 - val_loss: 145.9546\n",
      "Epoch 2469/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.5056 - val_loss: 141.4449\n",
      "Epoch 2470/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.9024 - val_loss: 150.7619\n",
      "Epoch 2471/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.1343 - val_loss: 174.3219\n",
      "Epoch 2472/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.3240 - val_loss: 141.7103\n",
      "Epoch 2473/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 119.5207 - val_loss: 139.6289\n",
      "Epoch 2474/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.2874 - val_loss: 183.3409\n",
      "Epoch 2475/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.9331 - val_loss: 133.5398\n",
      "Epoch 2476/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.9860 - val_loss: 167.9869\n",
      "Epoch 2477/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.9230 - val_loss: 136.6872\n",
      "Epoch 2478/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.8735 - val_loss: 142.3141\n",
      "Epoch 2479/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.0942 - val_loss: 164.5021\n",
      "Epoch 2480/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 252.2208 - val_loss: 178.2544\n",
      "Epoch 2481/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.0066 - val_loss: 150.4328\n",
      "Epoch 2482/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.3902 - val_loss: 136.3028\n",
      "Epoch 2483/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 118.6113 - val_loss: 132.5760\n",
      "Epoch 2484/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.8554 - val_loss: 203.2539\n",
      "Epoch 2485/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.6681 - val_loss: 185.7741\n",
      "Epoch 2486/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.0561 - val_loss: 135.6973\n",
      "Epoch 2487/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 123.5230 - val_loss: 129.7106\n",
      "Epoch 2488/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 121.3038 - val_loss: 166.2760\n",
      "Epoch 2489/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 125.9561 - val_loss: 163.5154\n",
      "Epoch 2490/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.1704 - val_loss: 186.9114\n",
      "Epoch 2491/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.1349 - val_loss: 144.9089\n",
      "Epoch 2492/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.4156 - val_loss: 131.1422\n",
      "Epoch 2493/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 120.5836 - val_loss: 138.0195\n",
      "Epoch 2494/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.2573 - val_loss: 166.5151\n",
      "Epoch 2495/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.0926 - val_loss: 144.2026\n",
      "Epoch 2496/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.6591 - val_loss: 146.2443\n",
      "Epoch 2497/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.5190 - val_loss: 140.0375\n",
      "Epoch 2498/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.4984 - val_loss: 151.2324\n",
      "Epoch 2499/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.2479 - val_loss: 128.9769\n",
      "Epoch 2500/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.2522 - val_loss: 131.0949\n",
      "Epoch 2501/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.8497 - val_loss: 144.9414\n",
      "Epoch 2502/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.0055 - val_loss: 141.2781\n",
      "Epoch 2503/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.1823 - val_loss: 158.7008\n",
      "Epoch 2504/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.3657 - val_loss: 172.1598\n",
      "Epoch 2505/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.7628 - val_loss: 137.0835\n",
      "Epoch 2506/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.2420 - val_loss: 164.8042\n",
      "Epoch 2507/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.0881 - val_loss: 156.8214\n",
      "Epoch 2508/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 121.0399 - val_loss: 139.3023\n",
      "Epoch 2509/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.0455 - val_loss: 150.1391\n",
      "Epoch 2510/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 128.8057 - val_loss: 132.4999\n",
      "Epoch 2511/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 126.9426 - val_loss: 141.6265\n",
      "Epoch 2512/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.5972 - val_loss: 186.7640\n",
      "Epoch 2513/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.1074 - val_loss: 154.3210\n",
      "Epoch 2514/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.8084 - val_loss: 132.2632\n",
      "Epoch 2515/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.0369 - val_loss: 147.5230\n",
      "Epoch 2516/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.1936 - val_loss: 150.4500\n",
      "Epoch 2517/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.5835 - val_loss: 151.5920\n",
      "Epoch 2518/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 125.8974 - val_loss: 139.9155\n",
      "Epoch 2519/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.8886 - val_loss: 184.7510\n",
      "Epoch 2520/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.1216 - val_loss: 135.9666\n",
      "Epoch 2521/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.1271 - val_loss: 146.4212\n",
      "Epoch 2522/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 125.3803 - val_loss: 138.7536\n",
      "Epoch 2523/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.3577 - val_loss: 148.7752\n",
      "Epoch 2524/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.9592 - val_loss: 141.3566\n",
      "Epoch 2525/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 126.9164 - val_loss: 127.2387\n",
      "Epoch 2526/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 125.2964 - val_loss: 191.5799\n",
      "Epoch 2527/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 135.1944 - val_loss: 189.3723\n",
      "Epoch 2528/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.7080 - val_loss: 133.0348\n",
      "Epoch 2529/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.3778 - val_loss: 139.1768\n",
      "Epoch 2530/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.0723 - val_loss: 138.0077\n",
      "Epoch 2531/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.8629 - val_loss: 174.9978\n",
      "Epoch 2532/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.8340 - val_loss: 158.1685\n",
      "Epoch 2533/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.4744 - val_loss: 188.8973\n",
      "Epoch 2534/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 125.3539 - val_loss: 136.9744\n",
      "Epoch 2535/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.8867 - val_loss: 148.3147\n",
      "Epoch 2536/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 120.5528 - val_loss: 131.4178\n",
      "Epoch 2537/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.2647 - val_loss: 132.4908\n",
      "Epoch 2538/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.1107 - val_loss: 133.9241\n",
      "Epoch 2539/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 231.8717 - val_loss: 1069.6065\n",
      "Epoch 2540/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 138.2287 - val_loss: 137.8930\n",
      "Epoch 2541/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.0062 - val_loss: 133.0988\n",
      "Epoch 2542/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.8237 - val_loss: 142.3036\n",
      "Epoch 2543/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.1284 - val_loss: 162.0285\n",
      "Epoch 2544/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.2228 - val_loss: 131.5668\n",
      "Epoch 2545/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.5476 - val_loss: 142.8339\n",
      "Epoch 2546/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.0822 - val_loss: 142.6150\n",
      "Epoch 2547/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.8421 - val_loss: 135.0823\n",
      "Epoch 2548/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.8876 - val_loss: 133.9355\n",
      "Epoch 2549/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.8262 - val_loss: 140.0593\n",
      "Epoch 2550/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.3373 - val_loss: 132.6886\n",
      "Epoch 2551/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.9350 - val_loss: 144.0982\n",
      "Epoch 2552/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.1618 - val_loss: 133.9881\n",
      "Epoch 2553/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.8395 - val_loss: 131.0089\n",
      "Epoch 2554/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.8924 - val_loss: 153.1124\n",
      "Epoch 2555/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.2102 - val_loss: 144.3950\n",
      "Epoch 2556/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.7055 - val_loss: 150.0825\n",
      "Epoch 2557/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.8637 - val_loss: 178.0746\n",
      "Epoch 2558/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.4971 - val_loss: 138.7794\n",
      "Epoch 2559/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.6271 - val_loss: 158.1724\n",
      "Epoch 2560/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 119.5364 - val_loss: 138.7518\n",
      "Epoch 2561/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 121.9902 - val_loss: 129.4896\n",
      "Epoch 2562/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 125.9739 - val_loss: 168.8090\n",
      "Epoch 2563/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 160.5228 - val_loss: 158.8358\n",
      "Epoch 2564/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.9795 - val_loss: 133.0698\n",
      "Epoch 2565/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.8422 - val_loss: 138.5669\n",
      "Epoch 2566/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.2093 - val_loss: 134.9862\n",
      "Epoch 2567/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.8665 - val_loss: 178.4634\n",
      "Epoch 2568/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.1438 - val_loss: 174.4472\n",
      "Epoch 2569/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.6568 - val_loss: 155.2014\n",
      "Epoch 2570/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.3422 - val_loss: 153.2769\n",
      "Epoch 2571/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.5900 - val_loss: 148.0210\n",
      "Epoch 2572/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.3326 - val_loss: 135.7539\n",
      "Epoch 2573/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.9935 - val_loss: 154.5498\n",
      "Epoch 2574/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.7507 - val_loss: 128.6666\n",
      "Epoch 2575/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.7963 - val_loss: 141.1423\n",
      "Epoch 2576/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.6978 - val_loss: 145.0163\n",
      "Epoch 2577/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.0892 - val_loss: 169.1373\n",
      "Epoch 2578/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.0535 - val_loss: 139.4904\n",
      "Epoch 2579/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 158.0255 - val_loss: 169.9314\n",
      "Epoch 2580/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.2414 - val_loss: 132.0604\n",
      "Epoch 2581/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.1138 - val_loss: 134.8487\n",
      "Epoch 2582/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 122.8596 - val_loss: 138.5942\n",
      "Epoch 2583/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.2441 - val_loss: 173.4020\n",
      "Epoch 2584/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.2118 - val_loss: 133.8483\n",
      "Epoch 2585/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.8712 - val_loss: 136.7343\n",
      "Epoch 2586/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.0348 - val_loss: 150.9733\n",
      "Epoch 2587/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.2695 - val_loss: 149.7821\n",
      "Epoch 2588/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.8322 - val_loss: 134.1750\n",
      "Epoch 2589/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.4955 - val_loss: 259.9019\n",
      "Epoch 2590/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 239.0044 - val_loss: 201.9490\n",
      "Epoch 2591/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 151.6040 - val_loss: 141.2469\n",
      "Epoch 2592/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.5541 - val_loss: 177.3710\n",
      "Epoch 2593/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.8807 - val_loss: 136.7314\n",
      "Epoch 2594/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.9558 - val_loss: 158.0886\n",
      "Epoch 2595/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.1928 - val_loss: 137.4178\n",
      "Epoch 2596/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.1080 - val_loss: 134.5135\n",
      "Epoch 2597/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.5022 - val_loss: 154.9969\n",
      "Epoch 2598/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.3773 - val_loss: 139.1781\n",
      "Epoch 2599/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.9769 - val_loss: 140.1328\n",
      "Epoch 2600/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.9071 - val_loss: 139.9774\n",
      "Epoch 2601/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.9999 - val_loss: 155.3311\n",
      "Epoch 2602/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 208.3531 - val_loss: 137.7183\n",
      "Epoch 2603/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.8380 - val_loss: 134.0955\n",
      "Epoch 2604/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.3216 - val_loss: 139.1732\n",
      "Epoch 2605/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 120.6204 - val_loss: 129.6136\n",
      "Epoch 2606/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.6900 - val_loss: 275.7243\n",
      "Epoch 2607/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 65us/step - loss: 128.5132 - val_loss: 138.8870\n",
      "Epoch 2608/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 121.8579 - val_loss: 196.8660\n",
      "Epoch 2609/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.9272 - val_loss: 172.5638\n",
      "Epoch 2610/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.2328 - val_loss: 140.8070\n",
      "Epoch 2611/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 125.8076 - val_loss: 202.1352\n",
      "Epoch 2612/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 123.8542 - val_loss: 135.8613\n",
      "Epoch 2613/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 123.8484 - val_loss: 138.8612\n",
      "Epoch 2614/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.8993 - val_loss: 145.5631\n",
      "Epoch 2615/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 120.6378 - val_loss: 144.5239\n",
      "Epoch 2616/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 121.5613 - val_loss: 133.4373\n",
      "Epoch 2617/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 128.5703 - val_loss: 203.0523\n",
      "Epoch 2618/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 127.0697 - val_loss: 138.1773\n",
      "Epoch 2619/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.7036 - val_loss: 137.0901\n",
      "Epoch 2620/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.4715 - val_loss: 139.6710\n",
      "Epoch 2621/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 127.6092 - val_loss: 137.4357\n",
      "Epoch 2622/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 128.0798 - val_loss: 132.2303\n",
      "Epoch 2623/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.2308 - val_loss: 167.6494\n",
      "Epoch 2624/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.4516 - val_loss: 158.5661\n",
      "Epoch 2625/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.2158 - val_loss: 149.4812\n",
      "Epoch 2626/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.8782 - val_loss: 248.4069\n",
      "Epoch 2627/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.8526 - val_loss: 183.5805\n",
      "Epoch 2628/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.0221 - val_loss: 143.2821\n",
      "Epoch 2629/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.9997 - val_loss: 142.7086\n",
      "Epoch 2630/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.8310 - val_loss: 132.2491\n",
      "Epoch 2631/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.1480 - val_loss: 170.8608\n",
      "Epoch 2632/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 124.9017 - val_loss: 166.6699\n",
      "Epoch 2633/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 125.7349 - val_loss: 137.2773\n",
      "Epoch 2634/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.8616 - val_loss: 138.1143\n",
      "Epoch 2635/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.6497 - val_loss: 148.4015\n",
      "Epoch 2636/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.7161 - val_loss: 220.4768\n",
      "Epoch 2637/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.9672 - val_loss: 130.5581\n",
      "Epoch 2638/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.6605 - val_loss: 130.6993\n",
      "Epoch 2639/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 126.7104 - val_loss: 139.4928\n",
      "Epoch 2640/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.2453 - val_loss: 132.7177\n",
      "Epoch 2641/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.5557 - val_loss: 129.8323\n",
      "Epoch 2642/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 120.4983 - val_loss: 132.4843\n",
      "Epoch 2643/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 128.2636 - val_loss: 131.9138\n",
      "Epoch 2644/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 124.7202 - val_loss: 142.8915\n",
      "Epoch 2645/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.9149 - val_loss: 130.5588\n",
      "Epoch 2646/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.0910 - val_loss: 138.1284\n",
      "Epoch 2647/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.9967 - val_loss: 140.5244\n",
      "Epoch 2648/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 124.7407 - val_loss: 144.8553\n",
      "Epoch 2649/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 124.2828 - val_loss: 128.9478\n",
      "Epoch 2650/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.7049 - val_loss: 138.3389\n",
      "Epoch 2651/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.9353 - val_loss: 137.1013\n",
      "Epoch 2652/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 126.0459 - val_loss: 138.4456\n",
      "Epoch 2653/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 121.7462 - val_loss: 142.5387\n",
      "Epoch 2654/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 120.3067 - val_loss: 140.1961\n",
      "Epoch 2655/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 127.3999 - val_loss: 141.0959\n",
      "Epoch 2656/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.3286 - val_loss: 141.3518\n",
      "Epoch 2657/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.6306 - val_loss: 161.2646\n",
      "Epoch 2658/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 123.8502 - val_loss: 136.4527\n",
      "Epoch 2659/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.8084 - val_loss: 144.4199\n",
      "Epoch 2660/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.3522 - val_loss: 136.9922\n",
      "Epoch 2661/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.5378 - val_loss: 150.8727\n",
      "Epoch 2662/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.8081 - val_loss: 174.2836\n",
      "Epoch 2663/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 125.6451 - val_loss: 138.2145\n",
      "Epoch 2664/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.3327 - val_loss: 144.5702\n",
      "Epoch 2665/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.1459 - val_loss: 129.5570\n",
      "Epoch 2666/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.6318 - val_loss: 135.6494\n",
      "Epoch 2667/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.9709 - val_loss: 139.2187\n",
      "Epoch 2668/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 121.9233 - val_loss: 149.1852\n",
      "Epoch 2669/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.8357 - val_loss: 132.6198\n",
      "Epoch 2670/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.7883 - val_loss: 132.8993\n",
      "Epoch 2671/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.0517 - val_loss: 136.3197\n",
      "Epoch 2672/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.0512 - val_loss: 175.1777\n",
      "Epoch 2673/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.5950 - val_loss: 142.4830\n",
      "Epoch 2674/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.3919 - val_loss: 131.4719\n",
      "Epoch 2675/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.0919 - val_loss: 129.7369\n",
      "Epoch 2676/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.4435 - val_loss: 955.1074\n",
      "Epoch 2677/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 163.3641 - val_loss: 133.3956\n",
      "Epoch 2678/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.4845 - val_loss: 134.1177\n",
      "Epoch 2679/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.3866 - val_loss: 134.6036\n",
      "Epoch 2680/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.9040 - val_loss: 139.6191\n",
      "Epoch 2681/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 119.1369 - val_loss: 140.3457\n",
      "Epoch 2682/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 118.0299 - val_loss: 152.0714\n",
      "Epoch 2683/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.5408 - val_loss: 148.1990\n",
      "Epoch 2684/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.8209 - val_loss: 145.6749\n",
      "Epoch 2685/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.1849 - val_loss: 128.7723\n",
      "Epoch 2686/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.3945 - val_loss: 155.4988\n",
      "Epoch 2687/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 121.0355 - val_loss: 142.8971\n",
      "Epoch 2688/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 219.2307 - val_loss: 191.3890\n",
      "Epoch 2689/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.6815 - val_loss: 134.3565\n",
      "Epoch 2690/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.3534 - val_loss: 151.5822\n",
      "Epoch 2691/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 118.6383 - val_loss: 133.7201\n",
      "Epoch 2692/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.2373 - val_loss: 149.4050\n",
      "Epoch 2693/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.0567 - val_loss: 148.5022\n",
      "Epoch 2694/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.6375 - val_loss: 147.4527\n",
      "Epoch 2695/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.2863 - val_loss: 142.4758\n",
      "Epoch 2696/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.6074 - val_loss: 138.5059\n",
      "Epoch 2697/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.6439 - val_loss: 142.5172\n",
      "Epoch 2698/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 128.8601 - val_loss: 182.0801\n",
      "Epoch 2699/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.1779 - val_loss: 220.0158\n",
      "Epoch 2700/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.3718 - val_loss: 137.3924\n",
      "Epoch 2701/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.5994 - val_loss: 138.3577\n",
      "Epoch 2702/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.5393 - val_loss: 141.9163\n",
      "Epoch 2703/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.7301 - val_loss: 156.2404\n",
      "Epoch 2704/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 118.9533 - val_loss: 130.7209\n",
      "Epoch 2705/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 135.5903 - val_loss: 139.3637\n",
      "Epoch 2706/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 122.2435 - val_loss: 165.1199\n",
      "Epoch 2707/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.2566 - val_loss: 148.1909\n",
      "Epoch 2708/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.2129 - val_loss: 128.7587\n",
      "Epoch 2709/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.1926 - val_loss: 132.1049\n",
      "Epoch 2710/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 120.3926 - val_loss: 135.6887\n",
      "Epoch 2711/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.3541 - val_loss: 136.2660\n",
      "Epoch 2712/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.8715 - val_loss: 135.1330\n",
      "Epoch 2713/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 125.634 - 0s 51us/step - loss: 125.9494 - val_loss: 132.2782\n",
      "Epoch 2714/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.9998 - val_loss: 132.4034\n",
      "Epoch 2715/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.5245 - val_loss: 147.1879\n",
      "Epoch 2716/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.0786 - val_loss: 133.1260\n",
      "Epoch 2717/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.3877 - val_loss: 159.7037\n",
      "Epoch 2718/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.0461 - val_loss: 133.5290\n",
      "Epoch 2719/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.5464 - val_loss: 137.3204\n",
      "Epoch 2720/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.3172 - val_loss: 162.2967\n",
      "Epoch 2721/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.6305 - val_loss: 149.9393\n",
      "Epoch 2722/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 121.8109 - val_loss: 152.5206\n",
      "Epoch 2723/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.9040 - val_loss: 145.3499\n",
      "Epoch 2724/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.0478 - val_loss: 155.3533\n",
      "Epoch 2725/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.7514 - val_loss: 170.3972\n",
      "Epoch 2726/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.5055 - val_loss: 143.9639\n",
      "Epoch 2727/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.3896 - val_loss: 172.2882\n",
      "Epoch 2728/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.2622 - val_loss: 147.4825\n",
      "Epoch 2729/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.4441 - val_loss: 137.2879\n",
      "Epoch 2730/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 127.8324 - val_loss: 131.3326\n",
      "Epoch 2731/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.1788 - val_loss: 138.1338\n",
      "Epoch 2732/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.7980 - val_loss: 147.6566\n",
      "Epoch 2733/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.8886 - val_loss: 134.3867\n",
      "Epoch 2734/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.2527 - val_loss: 132.4013\n",
      "Epoch 2735/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.6277 - val_loss: 148.1463\n",
      "Epoch 2736/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.0094 - val_loss: 133.3753\n",
      "Epoch 2737/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.0938 - val_loss: 147.9047\n",
      "Epoch 2738/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.2337 - val_loss: 140.1057\n",
      "Epoch 2739/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.5197 - val_loss: 138.0875\n",
      "Epoch 2740/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.0210 - val_loss: 132.5920\n",
      "Epoch 2741/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.2605 - val_loss: 128.5115\n",
      "Epoch 2742/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.8778 - val_loss: 144.7268\n",
      "Epoch 2743/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.1321 - val_loss: 135.6968\n",
      "Epoch 2744/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.7329 - val_loss: 170.4593\n",
      "Epoch 2745/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.3439 - val_loss: 152.7980\n",
      "Epoch 2746/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 153.1326 - val_loss: 127.8281\n",
      "Epoch 2747/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 118.8640 - val_loss: 133.8564\n",
      "Epoch 2748/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.8903 - val_loss: 152.5774\n",
      "Epoch 2749/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.7540 - val_loss: 137.1587\n",
      "Epoch 2750/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 118.3436 - val_loss: 138.9453\n",
      "Epoch 2751/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.9993 - val_loss: 138.1870\n",
      "Epoch 2752/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.8451 - val_loss: 163.9004\n",
      "Epoch 2753/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.2906 - val_loss: 136.6778\n",
      "Epoch 2754/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.1403 - val_loss: 149.2109\n",
      "Epoch 2755/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.1128 - val_loss: 138.2256\n",
      "Epoch 2756/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.9112 - val_loss: 166.5629\n",
      "Epoch 2757/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.8126 - val_loss: 193.3170\n",
      "Epoch 2758/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.1608 - val_loss: 136.6560\n",
      "Epoch 2759/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.1974 - val_loss: 157.8774\n",
      "Epoch 2760/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.8956 - val_loss: 144.0541\n",
      "Epoch 2761/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.7860 - val_loss: 131.9150\n",
      "Epoch 2762/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.0448 - val_loss: 135.9373\n",
      "Epoch 2763/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.4246 - val_loss: 133.4034\n",
      "Epoch 2764/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.8813 - val_loss: 145.9870\n",
      "Epoch 2765/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 120.7829 - val_loss: 133.5066\n",
      "Epoch 2766/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 129.9523 - val_loss: 130.6064\n",
      "Epoch 2767/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 123.8330 - val_loss: 132.0958\n",
      "Epoch 2768/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 121.7814 - val_loss: 134.0165\n",
      "Epoch 2769/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.7824 - val_loss: 129.8619\n",
      "Epoch 2770/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.8685 - val_loss: 135.5319\n",
      "Epoch 2771/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.7967 - val_loss: 134.4187\n",
      "Epoch 2772/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 167.1957 - val_loss: 221.2495\n",
      "Epoch 2773/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.7980 - val_loss: 131.9197\n",
      "Epoch 2774/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.0743 - val_loss: 130.1194\n",
      "Epoch 2775/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.7157 - val_loss: 141.0325\n",
      "Epoch 2776/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.4150 - val_loss: 138.3901\n",
      "Epoch 2777/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.5014 - val_loss: 157.1390\n",
      "Epoch 2778/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 130.8277 - val_loss: 142.7292\n",
      "Epoch 2779/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 130.0674 - val_loss: 147.9556\n",
      "Epoch 2780/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 122.8515 - val_loss: 144.1665\n",
      "Epoch 2781/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 122.2393 - val_loss: 130.6831\n",
      "Epoch 2782/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.9873 - val_loss: 138.3687\n",
      "Epoch 2783/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 120.6531 - val_loss: 135.1307\n",
      "Epoch 2784/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 124.9821 - val_loss: 134.0029\n",
      "Epoch 2785/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.9727 - val_loss: 150.3576\n",
      "Epoch 2786/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 123.6083 - val_loss: 135.1930\n",
      "Epoch 2787/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.4704 - val_loss: 141.5767\n",
      "Epoch 2788/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.0112 - val_loss: 145.7019\n",
      "Epoch 2789/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.9510 - val_loss: 132.4099\n",
      "Epoch 2790/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.5631 - val_loss: 143.9640\n",
      "Epoch 2791/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.3292 - val_loss: 149.4007\n",
      "Epoch 2792/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.8856 - val_loss: 142.0133\n",
      "Epoch 2793/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.7320 - val_loss: 154.2592\n",
      "Epoch 2794/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 124.7440 - val_loss: 146.5483\n",
      "Epoch 2795/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 121.4519 - val_loss: 139.6030\n",
      "Epoch 2796/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.6236 - val_loss: 144.0986\n",
      "Epoch 2797/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 120.3282 - val_loss: 145.0522\n",
      "Epoch 2798/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.2936 - val_loss: 148.0354\n",
      "Epoch 2799/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 180.5525 - val_loss: 583.0817\n",
      "Epoch 2800/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 152.9980 - val_loss: 134.1594\n",
      "Epoch 2801/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.4764 - val_loss: 201.5141\n",
      "Epoch 2802/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 129.6825 - val_loss: 139.8496\n",
      "Epoch 2803/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.8935 - val_loss: 133.9130\n",
      "Epoch 2804/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.5992 - val_loss: 206.9427\n",
      "Epoch 2805/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.7201 - val_loss: 137.6441\n",
      "Epoch 2806/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.1684 - val_loss: 131.4551\n",
      "Epoch 2807/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.9093 - val_loss: 165.2884\n",
      "Epoch 2808/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.6409 - val_loss: 134.9289\n",
      "Epoch 2809/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.6870 - val_loss: 152.2344\n",
      "Epoch 2810/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.4644 - val_loss: 175.9772\n",
      "Epoch 2811/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.6683 - val_loss: 152.4258\n",
      "Epoch 2812/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.9744 - val_loss: 128.5163\n",
      "Epoch 2813/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.4576 - val_loss: 155.8369\n",
      "Epoch 2814/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.2881 - val_loss: 130.6387\n",
      "Epoch 2815/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.3502 - val_loss: 130.0224\n",
      "Epoch 2816/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.9980 - val_loss: 233.4376\n",
      "Epoch 2817/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 120.7513 - val_loss: 147.8351\n",
      "Epoch 2818/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.8340 - val_loss: 135.3182\n",
      "Epoch 2819/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 119.2085 - val_loss: 138.1383\n",
      "Epoch 2820/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 119.8543 - val_loss: 140.6442\n",
      "Epoch 2821/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.2212 - val_loss: 150.9441\n",
      "Epoch 2822/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.9342 - val_loss: 145.4139\n",
      "Epoch 2823/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 120.1835 - val_loss: 143.8991\n",
      "Epoch 2824/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.1102 - val_loss: 160.9531\n",
      "Epoch 2825/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 121.1046 - val_loss: 133.4370\n",
      "Epoch 2826/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.2105 - val_loss: 131.4976\n",
      "Epoch 2827/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.8819 - val_loss: 141.5133\n",
      "Epoch 2828/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.6874 - val_loss: 139.0820\n",
      "Epoch 2829/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 121.9204 - val_loss: 146.3576\n",
      "Epoch 2830/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 125.1628 - val_loss: 144.0930\n",
      "Epoch 2831/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 123.8260 - val_loss: 235.6694\n",
      "Epoch 2832/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 157.4704 - val_loss: 318.8748\n",
      "Epoch 2833/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.6389 - val_loss: 145.7625\n",
      "Epoch 2834/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.0138 - val_loss: 158.5222\n",
      "Epoch 2835/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.2075 - val_loss: 131.2589\n",
      "Epoch 2836/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 121.0224 - val_loss: 143.4213\n",
      "Epoch 2837/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.1689 - val_loss: 135.6198\n",
      "Epoch 2838/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.7361 - val_loss: 135.4998\n",
      "Epoch 2839/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.2012 - val_loss: 132.2965\n",
      "Epoch 2840/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.6118 - val_loss: 136.5652\n",
      "Epoch 2841/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 125.9364 - val_loss: 131.1163\n",
      "Epoch 2842/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 127.4450 - val_loss: 140.2747\n",
      "Epoch 2843/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.7092 - val_loss: 144.2172\n",
      "Epoch 2844/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.4276 - val_loss: 147.3753\n",
      "Epoch 2845/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.0981 - val_loss: 136.1661\n",
      "Epoch 2846/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 124.5570 - val_loss: 136.7629\n",
      "Epoch 2847/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 125.5337 - val_loss: 132.3891\n",
      "Epoch 2848/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.2413 - val_loss: 150.5671\n",
      "Epoch 2849/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 125.9854 - val_loss: 138.7638\n",
      "Epoch 2850/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 173.5988 - val_loss: 163.4871\n",
      "Epoch 2851/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 122.2208 - val_loss: 156.2846\n",
      "Epoch 2852/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 121.4662 - val_loss: 136.2107\n",
      "Epoch 2853/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 121.7514 - val_loss: 132.7213\n",
      "Epoch 2854/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.0999 - val_loss: 148.9284\n",
      "Epoch 2855/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.0152 - val_loss: 138.1744\n",
      "Epoch 2856/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.0478 - val_loss: 181.2850\n",
      "Epoch 2857/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.1709 - val_loss: 135.7822\n",
      "Epoch 2858/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 119.8880 - val_loss: 179.5926\n",
      "Epoch 2859/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 128.2010 - val_loss: 139.3581\n",
      "Epoch 2860/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.9179 - val_loss: 132.0704\n",
      "Epoch 2861/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 128.2558 - val_loss: 157.8595\n",
      "Epoch 2862/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 119.1047 - val_loss: 140.8205\n",
      "Epoch 2863/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.8220 - val_loss: 137.0643\n",
      "Epoch 2864/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 120.8945 - val_loss: 153.2191\n",
      "Epoch 2865/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.8991 - val_loss: 132.0990\n",
      "Epoch 2866/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 219.7860 - val_loss: 145.1069\n",
      "Epoch 2867/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 120.4194 - val_loss: 133.9520\n",
      "Epoch 2868/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 118.3200 - val_loss: 152.0139\n",
      "Epoch 2869/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 120.3670 - val_loss: 134.1379\n",
      "Epoch 2870/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 124.0784 - val_loss: 128.0490\n",
      "Epoch 2871/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 119.7383 - val_loss: 144.8919\n",
      "Epoch 2872/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 121.9197 - val_loss: 152.0715\n",
      "Epoch 2873/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.7010 - val_loss: 133.7282\n",
      "Epoch 2874/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.2928 - val_loss: 130.6758\n",
      "Epoch 2875/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.5826 - val_loss: 142.3159\n",
      "Epoch 2876/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 120.3802 - val_loss: 137.9064\n",
      "Epoch 2877/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 121.4651 - val_loss: 136.1249\n",
      "Epoch 2878/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 120.5481 - val_loss: 136.8070\n",
      "Epoch 2879/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.2649 - val_loss: 145.6589\n",
      "Epoch 2880/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.9278 - val_loss: 129.3225\n",
      "Epoch 2881/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 119.4742 - val_loss: 148.9784\n",
      "Epoch 2882/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.7045 - val_loss: 142.9307\n",
      "Epoch 2883/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.7884 - val_loss: 145.3783\n",
      "Epoch 2884/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.6054 - val_loss: 157.7918\n",
      "Epoch 2885/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.3489 - val_loss: 141.6129\n",
      "Epoch 2886/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.7568 - val_loss: 126.3261\n",
      "Epoch 2887/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.8500 - val_loss: 153.0508\n",
      "Epoch 2888/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.7193 - val_loss: 139.9443\n",
      "Epoch 2889/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.0610 - val_loss: 156.6341\n",
      "Epoch 2890/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.9911 - val_loss: 145.8079\n",
      "Epoch 2891/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.6697 - val_loss: 134.9026\n",
      "Epoch 2892/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 125.5532 - val_loss: 139.3365\n",
      "Epoch 2893/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 119.0614 - val_loss: 148.2637\n",
      "Epoch 2894/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 120.2037 - val_loss: 147.2577\n",
      "Epoch 2895/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.1667 - val_loss: 132.2576\n",
      "Epoch 2896/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 122.9485 - val_loss: 172.2712\n",
      "Epoch 2897/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 127.7826 - val_loss: 161.0081\n",
      "Epoch 2898/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 118.6726 - val_loss: 154.4677\n",
      "Epoch 2899/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.7450 - val_loss: 133.2257\n",
      "Epoch 2900/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 152.8265 - val_loss: 135.8270\n",
      "Epoch 2901/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.3117 - val_loss: 133.7357\n",
      "Epoch 2902/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.9351 - val_loss: 168.2919\n",
      "Epoch 2903/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.2949 - val_loss: 131.2210\n",
      "Epoch 2904/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.8230 - val_loss: 137.3157\n",
      "Epoch 2905/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.0700 - val_loss: 138.7606\n",
      "Epoch 2906/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 122.3114 - val_loss: 135.1816\n",
      "Epoch 2907/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 156.1016 - val_loss: 173.8584\n",
      "Epoch 2908/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 126.9027 - val_loss: 150.6247\n",
      "Epoch 2909/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 121.5663 - val_loss: 143.7157\n",
      "Epoch 2910/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 118.8034 - val_loss: 148.3139\n",
      "Epoch 2911/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 124.6735 - val_loss: 165.5546\n",
      "Epoch 2912/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.5487 - val_loss: 167.3635\n",
      "Epoch 2913/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.6348 - val_loss: 156.0534\n",
      "Epoch 2914/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.4246 - val_loss: 139.0300\n",
      "Epoch 2915/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.0736 - val_loss: 130.8646\n",
      "Epoch 2916/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.9898 - val_loss: 166.9152\n",
      "Epoch 2917/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.9138 - val_loss: 149.9848\n",
      "Epoch 2918/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.3682 - val_loss: 184.6156\n",
      "Epoch 2919/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 129.4455 - val_loss: 136.8546\n",
      "Epoch 2920/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 124.5115 - val_loss: 148.8368\n",
      "Epoch 2921/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 128.6448 - val_loss: 135.5851\n",
      "Epoch 2922/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 123.0856 - val_loss: 152.4585\n",
      "Epoch 2923/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.2404 - val_loss: 162.2442\n",
      "Epoch 2924/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.0723 - val_loss: 163.4391\n",
      "Epoch 2925/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 119.9673 - val_loss: 183.3487\n",
      "Epoch 2926/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 208.7563 - val_loss: 132.7504\n",
      "Epoch 2927/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 119.6997 - val_loss: 177.9161\n",
      "Epoch 2928/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.6303 - val_loss: 142.9287\n",
      "Epoch 2929/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 117.1259 - val_loss: 128.2721\n",
      "Epoch 2930/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.7826 - val_loss: 140.7473\n",
      "Epoch 2931/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.8038 - val_loss: 142.9979\n",
      "Epoch 2932/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 127.9220 - val_loss: 135.7283\n",
      "Epoch 2933/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 122.2987 - val_loss: 138.3385\n",
      "Epoch 2934/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.5808 - val_loss: 134.4183\n",
      "Epoch 2935/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.1031 - val_loss: 135.1069\n",
      "Epoch 2936/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 120.2442 - val_loss: 137.1780\n",
      "Epoch 2937/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.6482 - val_loss: 185.9378\n",
      "Epoch 2938/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 162.2806 - val_loss: 128.7101\n",
      "Epoch 2939/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.1824 - val_loss: 138.7400\n",
      "Epoch 2940/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 120.9571 - val_loss: 132.3948\n",
      "Epoch 2941/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.1791 - val_loss: 134.3897\n",
      "Epoch 2942/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.1445 - val_loss: 148.8812\n",
      "Epoch 2943/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.1434 - val_loss: 179.2880\n",
      "Epoch 2944/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.3533 - val_loss: 143.0526\n",
      "Epoch 2945/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 121.7991 - val_loss: 129.6786\n",
      "Epoch 2946/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 119.1323 - val_loss: 130.2843\n",
      "Epoch 2947/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 127.3915 - val_loss: 146.0011\n",
      "Epoch 2948/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 120.9191 - val_loss: 151.3805\n",
      "Epoch 2949/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 123.1649 - val_loss: 131.3240\n",
      "Epoch 2950/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 125.3399 - val_loss: 155.7466\n",
      "Epoch 2951/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 126.3895 - val_loss: 146.9925\n",
      "Epoch 2952/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 120.1099 - val_loss: 147.2016\n",
      "Epoch 2953/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 125.1091 - val_loss: 136.4397\n",
      "Epoch 2954/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 127.2357 - val_loss: 139.7132\n",
      "Epoch 2955/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 119.038 - 1s 76us/step - loss: 120.5354 - val_loss: 190.1968\n",
      "Epoch 2956/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 158.4944 - val_loss: 137.4958\n",
      "Epoch 2957/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 119.4000 - val_loss: 151.1723\n",
      "Epoch 2958/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 124.6879 - val_loss: 133.9461\n",
      "Epoch 2959/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 120.8247 - val_loss: 148.3998\n",
      "Epoch 2960/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 123.2035 - val_loss: 159.5834\n",
      "Epoch 2961/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 123.4085 - val_loss: 150.9697\n",
      "Epoch 2962/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 121.3773 - val_loss: 167.2100\n",
      "Epoch 2963/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 129.1161 - val_loss: 227.6915\n",
      "Epoch 2964/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 149.1321 - val_loss: 133.6699\n",
      "Epoch 2965/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 120.1973 - val_loss: 141.9108\n",
      "Epoch 2966/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 122.9503 - val_loss: 137.8472\n",
      "Epoch 2967/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 123.2778 - val_loss: 142.2599\n",
      "Epoch 2968/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 128.3415 - val_loss: 140.5613\n",
      "Epoch 2969/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 121.6741 - val_loss: 130.9582\n",
      "Epoch 2970/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 118.5871 - val_loss: 134.2189\n",
      "Epoch 2971/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 123.8465 - val_loss: 132.7871\n",
      "Epoch 2972/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 123.9936 - val_loss: 159.4855\n",
      "Epoch 2973/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 130.3236 - val_loss: 140.0158\n",
      "Epoch 2974/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 119.6662 - val_loss: 132.5099\n",
      "Epoch 2975/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 132.0505 - val_loss: 136.9952\n",
      "Epoch 2976/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 117.4029 - val_loss: 131.1123\n",
      "Epoch 2977/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 123.2732 - val_loss: 142.2174\n",
      "Epoch 2978/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 127.8167 - val_loss: 147.8334\n",
      "Epoch 2979/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 119.7624 - val_loss: 139.0461\n",
      "Epoch 2980/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 120.5729 - val_loss: 140.5001\n",
      "Epoch 2981/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 125.3872 - val_loss: 136.8707\n",
      "Epoch 2982/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 160.0838 - val_loss: 136.8951\n",
      "Epoch 2983/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 125.8279 - val_loss: 134.5360\n",
      "Epoch 2984/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 118.3872 - val_loss: 145.0839\n",
      "Epoch 2985/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 119.7854 - val_loss: 140.9431\n",
      "Epoch 2986/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 119.8427 - val_loss: 140.0713\n",
      "Epoch 2987/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 121.006 - 0s 57us/step - loss: 121.9421 - val_loss: 144.6957\n",
      "Epoch 2988/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.1697 - val_loss: 146.7711\n",
      "Epoch 2989/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 127.4652 - val_loss: 130.6781\n",
      "Epoch 2990/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 123.9510 - val_loss: 131.3990\n",
      "Epoch 2991/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 122.3103 - val_loss: 137.4160\n",
      "Epoch 2992/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 124.8373 - val_loss: 145.5779\n",
      "Epoch 2993/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 132.2663 - val_loss: 158.5642\n",
      "Epoch 2994/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.2063 - val_loss: 146.5456\n",
      "Epoch 2995/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.1410 - val_loss: 146.2475\n",
      "Epoch 2996/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.7210 - val_loss: 138.5263\n",
      "Epoch 2997/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.9348 - val_loss: 138.0803\n",
      "Epoch 2998/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.5931 - val_loss: 140.0096\n",
      "Epoch 2999/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.3348 - val_loss: 142.3511\n",
      "Epoch 3000/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.1368 - val_loss: 164.1536\n",
      "Epoch 3001/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.2694 - val_loss: 144.5188\n",
      "Epoch 3002/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 119.3305 - val_loss: 129.5954\n",
      "Epoch 3003/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.5898 - val_loss: 135.4634\n",
      "Epoch 3004/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.9962 - val_loss: 145.2013\n",
      "Epoch 3005/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.8299 - val_loss: 143.9686\n",
      "Epoch 3006/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.3524 - val_loss: 133.3973\n",
      "Epoch 3007/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.5346 - val_loss: 211.2029\n",
      "Epoch 3008/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.2442 - val_loss: 169.2155\n",
      "Epoch 3009/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.0910 - val_loss: 147.7366\n",
      "Epoch 3010/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.2852 - val_loss: 140.6220\n",
      "Epoch 3011/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.8399 - val_loss: 189.8290\n",
      "Epoch 3012/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.0546 - val_loss: 162.7749\n",
      "Epoch 3013/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.6057 - val_loss: 137.7762\n",
      "Epoch 3014/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.8226 - val_loss: 134.7624\n",
      "Epoch 3015/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 121.5405 - val_loss: 137.7035\n",
      "Epoch 3016/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.2528 - val_loss: 138.6600\n",
      "Epoch 3017/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.2374 - val_loss: 212.2854\n",
      "Epoch 3018/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.3255 - val_loss: 134.2861\n",
      "Epoch 3019/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.7279 - val_loss: 170.9662\n",
      "Epoch 3020/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.2068 - val_loss: 148.4260\n",
      "Epoch 3021/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.1747 - val_loss: 149.7552\n",
      "Epoch 3022/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.8831 - val_loss: 133.8590\n",
      "Epoch 3023/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.7509 - val_loss: 146.3955\n",
      "Epoch 3024/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.1369 - val_loss: 131.5256\n",
      "Epoch 3025/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.5204 - val_loss: 196.3016\n",
      "Epoch 3026/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.9365 - val_loss: 181.8377\n",
      "Epoch 3027/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.6949 - val_loss: 139.8701\n",
      "Epoch 3028/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.4701 - val_loss: 137.8900\n",
      "Epoch 3029/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.3869 - val_loss: 132.6241\n",
      "Epoch 3030/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.2553 - val_loss: 148.6457\n",
      "Epoch 3031/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.2196 - val_loss: 141.6450\n",
      "Epoch 3032/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.2263 - val_loss: 132.9402\n",
      "Epoch 3033/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 162.9488 - val_loss: 137.6201\n",
      "Epoch 3034/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 119.8952 - val_loss: 132.8683\n",
      "Epoch 3035/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 122.4193 - val_loss: 133.0227\n",
      "Epoch 3036/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.6483 - val_loss: 131.8450\n",
      "Epoch 3037/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.2905 - val_loss: 139.1634\n",
      "Epoch 3038/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 124.1666 - val_loss: 135.5849\n",
      "Epoch 3039/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 84us/step - loss: 122.3107 - val_loss: 133.7447\n",
      "Epoch 3040/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 125.4093 - val_loss: 153.0168\n",
      "Epoch 3041/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.6648 - val_loss: 131.0436\n",
      "Epoch 3042/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 122.8499 - val_loss: 142.6748\n",
      "Epoch 3043/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.6192 - val_loss: 150.4479\n",
      "Epoch 3044/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 124.2195 - val_loss: 132.4279\n",
      "Epoch 3045/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 122.9051 - val_loss: 142.0909\n",
      "Epoch 3046/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 121.4678 - val_loss: 175.7351\n",
      "Epoch 3047/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 126.4692 - val_loss: 152.3553\n",
      "Epoch 3048/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 119.7605 - val_loss: 154.9493\n",
      "Epoch 3049/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 123.0330 - val_loss: 141.2189\n",
      "Epoch 3050/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 125.2647 - val_loss: 281.1820\n",
      "Epoch 3051/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 121.2524 - val_loss: 148.6753\n",
      "Epoch 3052/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.7269 - val_loss: 175.3980\n",
      "Epoch 3053/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 126.5275 - val_loss: 131.3186\n",
      "Epoch 3054/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 130.7356 - val_loss: 182.9530\n",
      "Epoch 3055/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 124.3877 - val_loss: 160.9864\n",
      "Epoch 3056/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 123.0051 - val_loss: 143.2260\n",
      "Epoch 3057/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 122.2191 - val_loss: 136.1844\n",
      "Epoch 3058/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 121.3659 - val_loss: 132.2354\n",
      "Epoch 3059/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.6239 - val_loss: 166.0220\n",
      "Epoch 3060/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.0350 - val_loss: 148.9421\n",
      "Epoch 3061/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 119.2182 - val_loss: 255.2968\n",
      "Epoch 3062/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 126.1940 - val_loss: 148.5337\n",
      "Epoch 3063/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 121.0537 - val_loss: 135.8252\n",
      "Epoch 3064/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.2468 - val_loss: 139.6046\n",
      "Epoch 3065/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.1744 - val_loss: 134.4072\n",
      "Epoch 3066/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 118.6398 - val_loss: 130.2020\n",
      "Epoch 3067/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.6104 - val_loss: 134.6425\n",
      "Epoch 3068/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 119.3307 - val_loss: 139.8382\n",
      "Epoch 3069/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 123.0487 - val_loss: 147.3302\n",
      "Epoch 3070/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 124.4273 - val_loss: 145.8888\n",
      "Epoch 3071/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 122.9909 - val_loss: 132.2819\n",
      "Epoch 3072/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 122.6996 - val_loss: 134.0343\n",
      "Epoch 3073/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 122.6910 - val_loss: 140.1555\n",
      "Epoch 3074/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 129.2339 - val_loss: 133.1149\n",
      "Epoch 3075/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 120.0756 - val_loss: 137.3900\n",
      "Epoch 3076/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 168.1944 - val_loss: 220.0192\n",
      "Epoch 3077/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 132.3454 - val_loss: 141.9283\n",
      "Epoch 3078/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 119.5162 - val_loss: 152.3824\n",
      "Epoch 3079/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.2414 - val_loss: 141.4260\n",
      "Epoch 3080/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.0542 - val_loss: 155.2119\n",
      "Epoch 3081/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 127.1422 - val_loss: 150.5440\n",
      "Epoch 3082/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 124.1133 - val_loss: 140.5864\n",
      "Epoch 3083/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 122.0117 - val_loss: 147.3732\n",
      "Epoch 3084/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 124.8782 - val_loss: 131.1912\n",
      "Epoch 3085/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 128.5471 - val_loss: 143.5996\n",
      "Epoch 3086/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.2335 - val_loss: 152.4199\n",
      "Epoch 3087/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.1483 - val_loss: 149.4491\n",
      "Epoch 3088/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 126.2807 - val_loss: 169.8559\n",
      "Epoch 3089/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.3254 - val_loss: 139.9039\n",
      "Epoch 3090/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.8351 - val_loss: 148.5285\n",
      "Epoch 3091/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.1881 - val_loss: 138.8822\n",
      "Epoch 3092/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 123.9455 - val_loss: 199.1567\n",
      "Epoch 3093/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.7507 - val_loss: 164.3800\n",
      "Epoch 3094/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 145.6870 - val_loss: 169.7977\n",
      "Epoch 3095/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.8733 - val_loss: 157.5099\n",
      "Epoch 3096/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.8524 - val_loss: 132.6118\n",
      "Epoch 3097/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 119.6880 - val_loss: 134.9446\n",
      "Epoch 3098/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.4908 - val_loss: 136.6593\n",
      "Epoch 3099/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.0883 - val_loss: 137.5401\n",
      "Epoch 3100/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.6285 - val_loss: 186.7303\n",
      "Epoch 3101/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 122.5890 - val_loss: 135.8152\n",
      "Epoch 3102/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 118.2688 - val_loss: 138.8673\n",
      "Epoch 3103/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.8576 - val_loss: 140.8877\n",
      "Epoch 3104/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.7364 - val_loss: 134.7610\n",
      "Epoch 3105/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.0786 - val_loss: 132.2334\n",
      "Epoch 3106/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 153.2826 - val_loss: 187.8785\n",
      "Epoch 3107/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 120.6767 - val_loss: 144.5481\n",
      "Epoch 3108/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.9281 - val_loss: 138.2657\n",
      "Epoch 3109/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 116.6090 - val_loss: 163.0701\n",
      "Epoch 3110/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.5553 - val_loss: 134.9744\n",
      "Epoch 3111/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.2399 - val_loss: 140.9279\n",
      "Epoch 3112/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.3311 - val_loss: 134.0067\n",
      "Epoch 3113/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.5342 - val_loss: 159.2881\n",
      "Epoch 3114/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.1181 - val_loss: 213.9790\n",
      "Epoch 3115/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 127.8777 - val_loss: 127.5775\n",
      "Epoch 3116/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 119.9153 - val_loss: 138.1361\n",
      "Epoch 3117/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 120.6921 - val_loss: 143.9447\n",
      "Epoch 3118/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.1323 - val_loss: 138.0410\n",
      "Epoch 3119/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.7819 - val_loss: 139.9638\n",
      "Epoch 3120/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 162.2454 - val_loss: 152.7724\n",
      "Epoch 3121/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.1721 - val_loss: 154.9835\n",
      "Epoch 3122/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 119.2704 - val_loss: 131.0344\n",
      "Epoch 3123/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.3561 - val_loss: 169.6088\n",
      "Epoch 3124/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.4888 - val_loss: 172.6220\n",
      "Epoch 3125/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.8448 - val_loss: 139.2137\n",
      "Epoch 3126/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.4911 - val_loss: 128.1800\n",
      "Epoch 3127/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.6762 - val_loss: 131.0577\n",
      "Epoch 3128/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.2479 - val_loss: 139.0097\n",
      "Epoch 3129/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.1141 - val_loss: 149.2787\n",
      "Epoch 3130/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.9279 - val_loss: 133.7337\n",
      "Epoch 3131/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.3451 - val_loss: 136.6341\n",
      "Epoch 3132/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.0663 - val_loss: 131.6057\n",
      "Epoch 3133/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.4548 - val_loss: 164.7883\n",
      "Epoch 3134/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.6263 - val_loss: 161.2589\n",
      "Epoch 3135/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 122.8945 - val_loss: 135.9237\n",
      "Epoch 3136/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 123.4085 - val_loss: 157.4089\n",
      "Epoch 3137/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 122.2383 - val_loss: 143.9190\n",
      "Epoch 3138/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 120.4089 - val_loss: 138.5420\n",
      "Epoch 3139/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 125.1037 - val_loss: 147.6390\n",
      "Epoch 3140/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 119.5876 - val_loss: 143.6384\n",
      "Epoch 3141/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 122.8348 - val_loss: 134.1493\n",
      "Epoch 3142/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.7515 - val_loss: 136.0206\n",
      "Epoch 3143/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 174.5113 - val_loss: 134.3019\n",
      "Epoch 3144/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 123.7958 - val_loss: 132.2358\n",
      "Epoch 3145/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 124.0914 - val_loss: 182.2315\n",
      "Epoch 3146/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 121.1120 - val_loss: 130.2039\n",
      "Epoch 3147/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 122.6670 - val_loss: 147.1547\n",
      "Epoch 3148/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 127.3634 - val_loss: 156.7774\n",
      "Epoch 3149/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 123.3381 - val_loss: 138.9334\n",
      "Epoch 3150/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 127.9428 - val_loss: 129.4912\n",
      "Epoch 3151/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.7174 - val_loss: 142.1339\n",
      "Epoch 3152/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.3457 - val_loss: 152.6291\n",
      "Epoch 3153/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.9532 - val_loss: 146.5506\n",
      "Epoch 3154/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.2120 - val_loss: 194.5292\n",
      "Epoch 3155/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.7888 - val_loss: 244.2883\n",
      "Epoch 3156/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.9982 - val_loss: 144.8594\n",
      "Epoch 3157/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.7742 - val_loss: 131.0213\n",
      "Epoch 3158/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.4922 - val_loss: 130.8000\n",
      "Epoch 3159/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.8780 - val_loss: 140.6174\n",
      "Epoch 3160/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.0794 - val_loss: 135.5261\n",
      "Epoch 3161/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.8129 - val_loss: 130.6299\n",
      "Epoch 3162/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.2190 - val_loss: 136.1127\n",
      "Epoch 3163/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.0757 - val_loss: 135.4393\n",
      "Epoch 3164/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 121.8603 - val_loss: 156.9686\n",
      "Epoch 3165/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.5192 - val_loss: 148.1644\n",
      "Epoch 3166/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.7648 - val_loss: 148.0345\n",
      "Epoch 3167/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 119.4093 - val_loss: 132.7203\n",
      "Epoch 3168/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 118.5432 - val_loss: 156.6078\n",
      "Epoch 3169/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.5232 - val_loss: 140.7333\n",
      "Epoch 3170/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 124.9184 - val_loss: 169.6178\n",
      "Epoch 3171/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 119.7266 - val_loss: 157.7376\n",
      "Epoch 3172/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 159.3068 - val_loss: 142.9311\n",
      "Epoch 3173/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 115.9796 - val_loss: 180.9634\n",
      "Epoch 3174/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 118.5375 - val_loss: 146.3964\n",
      "Epoch 3175/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.2352 - val_loss: 169.4684\n",
      "Epoch 3176/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.2047 - val_loss: 178.6324\n",
      "Epoch 3177/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 121.9823 - val_loss: 133.8880\n",
      "Epoch 3178/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 119.3699 - val_loss: 133.9560\n",
      "Epoch 3179/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 125.0276 - val_loss: 133.5586\n",
      "Epoch 3180/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 121.2416 - val_loss: 133.0829\n",
      "Epoch 3181/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 119.2928 - val_loss: 150.6624\n",
      "Epoch 3182/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.4779 - val_loss: 137.5135\n",
      "Epoch 3183/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 116us/step - loss: 122.1923 - val_loss: 136.5729\n",
      "Epoch 3184/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 120.9566 - val_loss: 141.4202\n",
      "Epoch 3185/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 120.4596 - val_loss: 140.6251\n",
      "Epoch 3186/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.2932 - val_loss: 134.1276\n",
      "Epoch 3187/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.0792 - val_loss: 171.4358\n",
      "Epoch 3188/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.9048 - val_loss: 131.6245\n",
      "Epoch 3189/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 121.3644 - val_loss: 172.4311\n",
      "Epoch 3190/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 130.5400 - val_loss: 142.9940\n",
      "Epoch 3191/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 123.5221 - val_loss: 139.5344\n",
      "Epoch 3192/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 242.3271 - val_loss: 141.1095\n",
      "Epoch 3193/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 122.1301 - val_loss: 145.7619\n",
      "Epoch 3194/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 118.1571 - val_loss: 150.3551\n",
      "Epoch 3195/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 120.8476 - val_loss: 168.9900\n",
      "Epoch 3196/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 118.9277 - val_loss: 137.5775\n",
      "Epoch 3197/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 121.2031 - val_loss: 156.5473\n",
      "Epoch 3198/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.7111 - val_loss: 204.2135\n",
      "Epoch 3199/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.5195 - val_loss: 129.4941\n",
      "Epoch 3200/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.8488 - val_loss: 146.3480\n",
      "Epoch 3201/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 126.0318 - val_loss: 148.7573\n",
      "Epoch 3202/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 121.7762 - val_loss: 159.4059\n",
      "Epoch 3203/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.6915 - val_loss: 140.4086\n",
      "Epoch 3204/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 120.7668 - val_loss: 146.0016\n",
      "Epoch 3205/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.1353 - val_loss: 136.8101\n",
      "Epoch 3206/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.4006 - val_loss: 134.5471\n",
      "Epoch 3207/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 124.5984 - val_loss: 153.0134\n",
      "Epoch 3208/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.0148 - val_loss: 139.0968\n",
      "Epoch 3209/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 120.0949 - val_loss: 145.4977\n",
      "Epoch 3210/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.6609 - val_loss: 143.8297\n",
      "Epoch 3211/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.2026 - val_loss: 143.8321\n",
      "Epoch 3212/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 120.6394 - val_loss: 135.2542\n",
      "Epoch 3213/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 122.3326 - val_loss: 140.0042\n",
      "Epoch 3214/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 121.4498 - val_loss: 148.4027\n",
      "Epoch 3215/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 122.2899 - val_loss: 135.7235\n",
      "Epoch 3216/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 122.8575 - val_loss: 172.9635\n",
      "Epoch 3217/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 141.9301 - val_loss: 138.1060\n",
      "Epoch 3218/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 119.3642 - val_loss: 149.3286\n",
      "Epoch 3219/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 124.6890 - val_loss: 138.6924\n",
      "Epoch 3220/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.3317 - val_loss: 136.5723\n",
      "Epoch 3221/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 122.8120 - val_loss: 137.7645\n",
      "Epoch 3222/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 119.9557 - val_loss: 138.4964\n",
      "Epoch 3223/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 122.7442 - val_loss: 148.0052\n",
      "Epoch 3224/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 119.5649 - val_loss: 143.9733\n",
      "Epoch 3225/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.4775 - val_loss: 146.7987\n",
      "Epoch 3226/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.1737 - val_loss: 145.7731\n",
      "Epoch 3227/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 119.2486 - val_loss: 165.7569\n",
      "Epoch 3228/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 122.3765 - val_loss: 155.1379\n",
      "Epoch 3229/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.5237 - val_loss: 179.8191\n",
      "Epoch 3230/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.7379 - val_loss: 169.7237\n",
      "Epoch 3231/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.3898 - val_loss: 136.3219\n",
      "Epoch 3232/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 119.7116 - val_loss: 140.7446\n",
      "Epoch 3233/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.3394 - val_loss: 129.3536\n",
      "Epoch 3234/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.4897 - val_loss: 147.4276\n",
      "Epoch 3235/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 121.4002 - val_loss: 150.0060\n",
      "Epoch 3236/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 127.9865 - val_loss: 131.5153\n",
      "Epoch 3237/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 121.7355 - val_loss: 163.2389\n",
      "Epoch 3238/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 123.9565 - val_loss: 129.2876\n",
      "Epoch 3239/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 147.0487 - val_loss: 225.9044\n",
      "Epoch 3240/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.1490 - val_loss: 136.0120\n",
      "Epoch 3241/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 119.0395 - val_loss: 149.9508\n",
      "Epoch 3242/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 123.3671 - val_loss: 135.8753\n",
      "Epoch 3243/10000\n",
      "8000/8000 [==============================] - 1s 146us/step - loss: 123.5621 - val_loss: 158.7465\n",
      "Epoch 3244/10000\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 121.9288 - val_loss: 136.9670\n",
      "Epoch 3245/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 123.7866 - val_loss: 136.4925\n",
      "Epoch 3246/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 123.8399 - val_loss: 155.1747\n",
      "Epoch 3247/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 122.2737 - val_loss: 175.0565\n",
      "Epoch 3248/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 126.7346 - val_loss: 133.4304\n",
      "Epoch 3249/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.9806 - val_loss: 146.8837\n",
      "Epoch 3250/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.1264 - val_loss: 164.3155\n",
      "Epoch 3251/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 120.1397 - val_loss: 147.3678\n",
      "Epoch 3252/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.0927 - val_loss: 146.1455\n",
      "Epoch 3253/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 122.5019 - val_loss: 165.9899\n",
      "Epoch 3254/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 119.6197 - val_loss: 161.0031\n",
      "Epoch 3255/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 65us/step - loss: 123.0918 - val_loss: 132.8602\n",
      "Epoch 3256/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 123.2085 - val_loss: 133.8129\n",
      "Epoch 3257/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 163.3287 - val_loss: 302.3838\n",
      "Epoch 3258/10000\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 131.9942 - val_loss: 175.8968\n",
      "Epoch 3259/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 123.2588 - val_loss: 143.8636\n",
      "Epoch 3260/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 119.8352 - val_loss: 183.7749\n",
      "Epoch 3261/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 123.0229 - val_loss: 153.0255\n",
      "Epoch 3262/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.6391 - val_loss: 131.1770\n",
      "Epoch 3263/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 121.2246 - val_loss: 137.2789\n",
      "Epoch 3264/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.4803 - val_loss: 165.5170\n",
      "Epoch 3265/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.8190 - val_loss: 151.7311\n",
      "Epoch 3266/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 121.8283 - val_loss: 140.1012\n",
      "Epoch 3267/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 121.8261 - val_loss: 139.5411\n",
      "Epoch 3268/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.2196 - val_loss: 140.4296\n",
      "Epoch 3269/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 118.1663 - val_loss: 134.9867\n",
      "Epoch 3270/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.6577 - val_loss: 128.6971\n",
      "Epoch 3271/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.0641 - val_loss: 132.8905\n",
      "Epoch 3272/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.6630 - val_loss: 136.6468\n",
      "Epoch 3273/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.6824 - val_loss: 135.7305\n",
      "Epoch 3274/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.9995 - val_loss: 167.2077\n",
      "Epoch 3275/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.7132 - val_loss: 203.9765\n",
      "Epoch 3276/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.8892 - val_loss: 142.1464\n",
      "Epoch 3277/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.3412 - val_loss: 134.5777\n",
      "Epoch 3278/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.0346 - val_loss: 131.8182\n",
      "Epoch 3279/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.2482 - val_loss: 139.1547\n",
      "Epoch 3280/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 120.7047 - val_loss: 131.2341\n",
      "Epoch 3281/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.8774 - val_loss: 143.0667\n",
      "Epoch 3282/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.1909 - val_loss: 162.4540\n",
      "Epoch 3283/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.8321 - val_loss: 129.6329\n",
      "Epoch 3284/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.2739 - val_loss: 155.2668\n",
      "Epoch 3285/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.7459 - val_loss: 137.2241\n",
      "Epoch 3286/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 134.3929 - val_loss: 190.3940\n",
      "Epoch 3287/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.5602 - val_loss: 181.8261\n",
      "Epoch 3288/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 131.0670 - val_loss: 142.4960\n",
      "Epoch 3289/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.8205 - val_loss: 132.3357\n",
      "Epoch 3290/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 118.3971 - val_loss: 142.1298\n",
      "Epoch 3291/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 118.4614 - val_loss: 148.7841\n",
      "Epoch 3292/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.7813 - val_loss: 136.4762\n",
      "Epoch 3293/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.9946 - val_loss: 138.4027\n",
      "Epoch 3294/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 119.9938 - val_loss: 140.7838\n",
      "Epoch 3295/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 122.2028 - val_loss: 173.0454\n",
      "Epoch 3296/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 127.4237 - val_loss: 129.9140\n",
      "Epoch 3297/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 117.8087 - val_loss: 133.1515\n",
      "Epoch 3298/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.9274 - val_loss: 131.7574\n",
      "Epoch 3299/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.6804 - val_loss: 136.2716\n",
      "Epoch 3300/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 128.5490 - val_loss: 154.4676\n",
      "Epoch 3301/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 125.8564 - val_loss: 136.5697\n",
      "Epoch 3302/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 127.5558 - val_loss: 144.6977\n",
      "Epoch 3303/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.6485 - val_loss: 152.8953\n",
      "Epoch 3304/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 656.5100 - val_loss: 282.5700\n",
      "Epoch 3305/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 270.9293 - val_loss: 274.4096\n",
      "Epoch 3306/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 235.3153 - val_loss: 202.5289\n",
      "Epoch 3307/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 227.2916 - val_loss: 202.9947\n",
      "Epoch 3308/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 211.8077 - val_loss: 177.9608\n",
      "Epoch 3309/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 199.6851 - val_loss: 179.6141\n",
      "Epoch 3310/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 165.1141 - val_loss: 170.1021\n",
      "Epoch 3311/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 185.6411 - val_loss: 225.5658\n",
      "Epoch 3312/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 163.2962 - val_loss: 165.6535\n",
      "Epoch 3313/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 181.1252 - val_loss: 309.5952\n",
      "Epoch 3314/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 171.1877 - val_loss: 157.9489\n",
      "Epoch 3315/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 152.3956 - val_loss: 152.5861\n",
      "Epoch 3316/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 169.6628 - val_loss: 162.4041\n",
      "Epoch 3317/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 174.3011 - val_loss: 153.9973\n",
      "Epoch 3318/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 153.2925 - val_loss: 262.5181\n",
      "Epoch 3319/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 162.5662 - val_loss: 147.4250\n",
      "Epoch 3320/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 155.7992 - val_loss: 181.9146\n",
      "Epoch 3321/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.6977 - val_loss: 179.0709\n",
      "Epoch 3322/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.6911 - val_loss: 172.0508\n",
      "Epoch 3323/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.6276 - val_loss: 160.4869\n",
      "Epoch 3324/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 148.0780 - val_loss: 171.1810\n",
      "Epoch 3325/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 156.8579 - val_loss: 168.4580\n",
      "Epoch 3326/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 143.4540 - val_loss: 167.1115\n",
      "Epoch 3327/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 55us/step - loss: 148.4380 - val_loss: 155.8512\n",
      "Epoch 3328/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.5776 - val_loss: 145.3706\n",
      "Epoch 3329/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 136.6161 - val_loss: 185.3796\n",
      "Epoch 3330/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 134.6025 - val_loss: 138.1291\n",
      "Epoch 3331/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 140.8998 - val_loss: 156.5037\n",
      "Epoch 3332/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.4629 - val_loss: 149.8078\n",
      "Epoch 3333/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 133.6594 - val_loss: 168.9362\n",
      "Epoch 3334/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 164.6973 - val_loss: 160.5648\n",
      "Epoch 3335/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 132.3981 - val_loss: 163.5090\n",
      "Epoch 3336/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 134.5736 - val_loss: 155.9450\n",
      "Epoch 3337/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 166.2014 - val_loss: 151.7991\n",
      "Epoch 3338/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 133.9572 - val_loss: 151.2615\n",
      "Epoch 3339/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 130.7201 - val_loss: 147.1178\n",
      "Epoch 3340/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 173.2313 - val_loss: 143.8045\n",
      "Epoch 3341/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.2013 - val_loss: 165.6084\n",
      "Epoch 3342/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 131.9414 - val_loss: 169.3231\n",
      "Epoch 3343/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 139.5899 - val_loss: 197.4603\n",
      "Epoch 3344/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 130.9472 - val_loss: 135.0738\n",
      "Epoch 3345/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 125.6622 - val_loss: 147.5214\n",
      "Epoch 3346/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 143.7117 - val_loss: 139.6183\n",
      "Epoch 3347/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.2408 - val_loss: 146.5441\n",
      "Epoch 3348/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 136.0677 - val_loss: 168.4126\n",
      "Epoch 3349/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 145.2857 - val_loss: 231.9120\n",
      "Epoch 3350/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.5763 - val_loss: 146.3534\n",
      "Epoch 3351/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 135.9689 - val_loss: 162.3876\n",
      "Epoch 3352/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.1167 - val_loss: 157.3644\n",
      "Epoch 3353/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 127.8398 - val_loss: 141.2089\n",
      "Epoch 3354/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 142.9446 - val_loss: 159.2947\n",
      "Epoch 3355/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.3138 - val_loss: 149.2386\n",
      "Epoch 3356/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 127.6314 - val_loss: 147.4293\n",
      "Epoch 3357/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 128.3969 - val_loss: 146.2608\n",
      "Epoch 3358/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 128.2593 - val_loss: 143.0811\n",
      "Epoch 3359/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 176.1839 - val_loss: 133.2779\n",
      "Epoch 3360/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 122.0622 - val_loss: 141.0988\n",
      "Epoch 3361/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.5206 - val_loss: 159.8446\n",
      "Epoch 3362/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 130.5726 - val_loss: 176.2533\n",
      "Epoch 3363/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 130.2492 - val_loss: 152.4055\n",
      "Epoch 3364/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.3501 - val_loss: 176.3891\n",
      "Epoch 3365/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.0257 - val_loss: 148.7728\n",
      "Epoch 3366/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.3031 - val_loss: 144.2699\n",
      "Epoch 3367/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.9982 - val_loss: 147.4780\n",
      "Epoch 3368/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.7425 - val_loss: 155.4427\n",
      "Epoch 3369/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 143.7186 - val_loss: 146.6949\n",
      "Epoch 3370/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 123.5159 - val_loss: 142.9331\n",
      "Epoch 3371/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 129.1121 - val_loss: 171.1933\n",
      "Epoch 3372/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.2035 - val_loss: 164.4754\n",
      "Epoch 3373/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.9678 - val_loss: 143.1838\n",
      "Epoch 3374/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.4622 - val_loss: 140.5523\n",
      "Epoch 3375/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.2634 - val_loss: 190.3318\n",
      "Epoch 3376/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.4923 - val_loss: 179.4896\n",
      "Epoch 3377/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 136.1627 - val_loss: 141.4270\n",
      "Epoch 3378/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 130.1665 - val_loss: 153.6611\n",
      "Epoch 3379/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 137.4126 - val_loss: 154.2587\n",
      "Epoch 3380/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 132.5988 - val_loss: 130.4535\n",
      "Epoch 3381/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 124.6867 - val_loss: 130.4942\n",
      "Epoch 3382/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.3421 - val_loss: 133.7290\n",
      "Epoch 3383/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.2602 - val_loss: 145.2919\n",
      "Epoch 3384/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 137.2125 - val_loss: 135.6325\n",
      "Epoch 3385/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.0402 - val_loss: 133.5181\n",
      "Epoch 3386/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.3668 - val_loss: 154.0321 ETA: 0s - loss: 135\n",
      "Epoch 3387/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 151.2518 - val_loss: 143.6976\n",
      "Epoch 3388/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.5886 - val_loss: 148.6277\n",
      "Epoch 3389/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.0113 - val_loss: 196.7167\n",
      "Epoch 3390/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.6423 - val_loss: 147.2011\n",
      "Epoch 3391/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.4397 - val_loss: 154.3998\n",
      "Epoch 3392/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.0030 - val_loss: 154.2812\n",
      "Epoch 3393/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.7123 - val_loss: 150.9961\n",
      "Epoch 3394/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.0554 - val_loss: 199.9599\n",
      "Epoch 3395/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.4330 - val_loss: 160.3364\n",
      "Epoch 3396/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 126.0909 - val_loss: 134.0361\n",
      "Epoch 3397/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.2291 - val_loss: 205.2082\n",
      "Epoch 3398/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 161.0660 - val_loss: 136.9547\n",
      "Epoch 3399/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.5123 - val_loss: 152.6890\n",
      "Epoch 3400/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.8476 - val_loss: 137.9467\n",
      "Epoch 3401/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.7084 - val_loss: 131.5498\n",
      "Epoch 3402/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.9406 - val_loss: 129.2825\n",
      "Epoch 3403/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.8157 - val_loss: 132.0907\n",
      "Epoch 3404/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.1609 - val_loss: 133.9665\n",
      "Epoch 3405/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.5551 - val_loss: 170.5607\n",
      "Epoch 3406/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.6584 - val_loss: 180.2145\n",
      "Epoch 3407/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.2179 - val_loss: 167.3212\n",
      "Epoch 3408/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 166.3717 - val_loss: 206.7179\n",
      "Epoch 3409/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.0101 - val_loss: 153.5676\n",
      "Epoch 3410/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.3377 - val_loss: 151.0774\n",
      "Epoch 3411/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.6993 - val_loss: 131.2359\n",
      "Epoch 3412/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.7114 - val_loss: 185.3671\n",
      "Epoch 3413/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.3627 - val_loss: 163.4778\n",
      "Epoch 3414/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 170.0404 - val_loss: 139.3562\n",
      "Epoch 3415/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.2180 - val_loss: 132.1560\n",
      "Epoch 3416/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.6134 - val_loss: 137.8413\n",
      "Epoch 3417/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.6958 - val_loss: 133.7055\n",
      "Epoch 3418/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 120.7252 - val_loss: 132.1013\n",
      "Epoch 3419/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.6271 - val_loss: 144.3769\n",
      "Epoch 3420/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.1596 - val_loss: 130.3209\n",
      "Epoch 3421/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.1511 - val_loss: 131.5529\n",
      "Epoch 3422/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.4864 - val_loss: 179.3117\n",
      "Epoch 3423/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.3409 - val_loss: 142.3918\n",
      "Epoch 3424/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.1548 - val_loss: 158.3137\n",
      "Epoch 3425/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.4618 - val_loss: 128.8884\n",
      "Epoch 3426/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 123.5131 - val_loss: 128.9566\n",
      "Epoch 3427/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.0824 - val_loss: 206.8232\n",
      "Epoch 3428/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 130.4380 - val_loss: 130.3118\n",
      "Epoch 3429/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 184.2147 - val_loss: 2664.0489\n",
      "Epoch 3430/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 317.4678 - val_loss: 136.3452\n",
      "Epoch 3431/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 119.7205 - val_loss: 151.8212\n",
      "Epoch 3432/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 121.1422 - val_loss: 131.6701\n",
      "Epoch 3433/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 122.1570 - val_loss: 140.8065\n",
      "Epoch 3434/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 117.2152 - val_loss: 131.2241\n",
      "Epoch 3435/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 124.8337 - val_loss: 141.4275\n",
      "Epoch 3436/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.4719 - val_loss: 165.9008\n",
      "Epoch 3437/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 125.9020 - val_loss: 133.5065\n",
      "Epoch 3438/10000\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 124.5594 - val_loss: 135.5248\n",
      "Epoch 3439/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 122.1454 - val_loss: 149.1573\n",
      "Epoch 3440/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 133.0119 - val_loss: 139.4102\n",
      "Epoch 3441/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 129.4000 - val_loss: 133.3196\n",
      "Epoch 3442/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 126.3577 - val_loss: 154.4469\n",
      "Epoch 3443/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 123.4896 - val_loss: 130.6596\n",
      "Epoch 3444/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 124.8105 - val_loss: 156.6565\n",
      "Epoch 3445/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 123.8187 - val_loss: 141.4534\n",
      "Epoch 3446/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 141.3717 - val_loss: 136.1145\n",
      "Epoch 3447/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 122.7486 - val_loss: 170.3555\n",
      "Epoch 3448/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.4054 - val_loss: 153.8539\n",
      "Epoch 3449/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.3131 - val_loss: 138.9830\n",
      "Epoch 3450/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.0244 - val_loss: 140.6546\n",
      "Epoch 3451/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.2964 - val_loss: 253.6523\n",
      "Epoch 3452/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 122.8419 - val_loss: 133.7138\n",
      "Epoch 3453/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 125.4150 - val_loss: 137.4737\n",
      "Epoch 3454/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 124.1331 - val_loss: 137.9391\n",
      "Epoch 3455/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 132.5556 - val_loss: 140.8828\n",
      "Epoch 3456/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 123.4801 - val_loss: 165.6369\n",
      "Epoch 3457/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 124.2072 - val_loss: 135.3191\n",
      "Epoch 3458/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 124.3536 - val_loss: 141.8118\n",
      "Epoch 3459/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.2302 - val_loss: 144.6426\n",
      "Epoch 3460/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 129.9312 - val_loss: 161.6299\n",
      "Epoch 3461/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 189.1557 - val_loss: 130.1001\n",
      "Epoch 3462/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 120.1639 - val_loss: 132.6422\n",
      "Epoch 3463/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 121.7491 - val_loss: 134.9746\n",
      "Epoch 3464/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 120.1169 - val_loss: 135.8827\n",
      "Epoch 3465/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 119.6179 - val_loss: 150.0809\n",
      "Epoch 3466/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 126.1792 - val_loss: 168.1995\n",
      "Epoch 3467/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 123.2384 - val_loss: 152.8647\n",
      "Epoch 3468/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 133.8951 - val_loss: 148.0332\n",
      "Epoch 3469/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 127.8249 - val_loss: 156.1942\n",
      "Epoch 3470/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 128.9955 - val_loss: 149.5950\n",
      "Epoch 3471/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 71us/step - loss: 133.0863 - val_loss: 173.7844\n",
      "Epoch 3472/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 128.8550 - val_loss: 137.7768\n",
      "Epoch 3473/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.5144 - val_loss: 130.6585\n",
      "Epoch 3474/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 127.2236 - val_loss: 134.8344\n",
      "Epoch 3475/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.4335 - val_loss: 169.8473\n",
      "Epoch 3476/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.8266 - val_loss: 212.3504\n",
      "Epoch 3477/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.7829 - val_loss: 140.6194\n",
      "Epoch 3478/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 121.1784 - val_loss: 133.8143\n",
      "Epoch 3479/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 128.3071 - val_loss: 131.0219\n",
      "Epoch 3480/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 124.3587 - val_loss: 156.1035\n",
      "Epoch 3481/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 126.5699 - val_loss: 154.3807\n",
      "Epoch 3482/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 123.2353 - val_loss: 140.2048\n",
      "Epoch 3483/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 129.7116 - val_loss: 142.8720\n",
      "Epoch 3484/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 130.1761 - val_loss: 146.0497\n",
      "Epoch 3485/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 128.2323 - val_loss: 137.8518\n",
      "Epoch 3486/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.4837 - val_loss: 141.9062\n",
      "Epoch 3487/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 126.2480 - val_loss: 140.1592\n",
      "Epoch 3488/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 133.0049 - val_loss: 175.2694\n",
      "Epoch 3489/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.0536 - val_loss: 143.2159\n",
      "Epoch 3490/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.4456 - val_loss: 181.0439\n",
      "Epoch 3491/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 123.9564 - val_loss: 131.8401\n",
      "Epoch 3492/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.1038 - val_loss: 145.6057\n",
      "Epoch 3493/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.3208 - val_loss: 150.0452\n",
      "Epoch 3494/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 140.7755 - val_loss: 133.6462\n",
      "Epoch 3495/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 122.9096 - val_loss: 143.0810\n",
      "Epoch 3496/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 127.0956 - val_loss: 133.8820\n",
      "Epoch 3497/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 121.6700 - val_loss: 131.4483\n",
      "Epoch 3498/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 122.8950 - val_loss: 164.4741\n",
      "Epoch 3499/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 127.4389 - val_loss: 153.5166\n",
      "Epoch 3500/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 122.0521 - val_loss: 133.3255\n",
      "Epoch 3501/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 123.2510 - val_loss: 161.4574\n",
      "Epoch 3502/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 159.7989 - val_loss: 175.5798\n",
      "Epoch 3503/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.0647 - val_loss: 132.6150\n",
      "Epoch 3504/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 119.1385 - val_loss: 146.1794\n",
      "Epoch 3505/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 119.4097 - val_loss: 130.0711\n",
      "Epoch 3506/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.1495 - val_loss: 171.3816\n",
      "Epoch 3507/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 145.1097 - val_loss: 138.5219\n",
      "Epoch 3508/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 129.8960 - val_loss: 140.9900\n",
      "Epoch 3509/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 121.6543 - val_loss: 131.9978\n",
      "Epoch 3510/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 122.3086 - val_loss: 155.8113\n",
      "Epoch 3511/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 121.4318 - val_loss: 161.0436\n",
      "Epoch 3512/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 124.6802 - val_loss: 153.6378\n",
      "Epoch 3513/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 133.7037 - val_loss: 142.5051\n",
      "Epoch 3514/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.1886 - val_loss: 133.6467\n",
      "Epoch 3515/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 124.6418 - val_loss: 152.8118\n",
      "Epoch 3516/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 123.6711 - val_loss: 156.8558\n",
      "Epoch 3517/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 121.9460 - val_loss: 136.0858\n",
      "Epoch 3518/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 134.0736 - val_loss: 174.9654\n",
      "Epoch 3519/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 126.5080 - val_loss: 145.4440\n",
      "Epoch 3520/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 123.9629 - val_loss: 137.7731\n",
      "Epoch 3521/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.9957 - val_loss: 128.1298\n",
      "Epoch 3522/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.7659 - val_loss: 145.2864\n",
      "Epoch 3523/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 123.6430 - val_loss: 138.9652\n",
      "Epoch 3524/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.4728 - val_loss: 142.2981\n",
      "Epoch 3525/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.3691 - val_loss: 142.5575\n",
      "Epoch 3526/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 141.7831 - val_loss: 140.7238\n",
      "Epoch 3527/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 122.5466 - val_loss: 140.1069\n",
      "Epoch 3528/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 124.9819 - val_loss: 131.8545\n",
      "Epoch 3529/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 118.5580 - val_loss: 134.8397\n",
      "Epoch 3530/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 125.6522 - val_loss: 147.0203\n",
      "Epoch 3531/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 128.0845 - val_loss: 176.3075\n",
      "Epoch 3532/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 178.1055 - val_loss: 132.6301\n",
      "Epoch 3533/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 122.607 - 0s 57us/step - loss: 122.0637 - val_loss: 133.8380\n",
      "Epoch 3534/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 121.8823 - val_loss: 128.3079\n",
      "Epoch 3535/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 118.1484 - val_loss: 132.4667\n",
      "Epoch 3536/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 121.2615 - val_loss: 131.2876\n",
      "Epoch 3537/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 122.2902 - val_loss: 147.8766\n",
      "Epoch 3538/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 125.5785 - val_loss: 149.3256\n",
      "Epoch 3539/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.2653 - val_loss: 142.4390\n",
      "Epoch 3540/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 121.3966 - val_loss: 140.8078\n",
      "Epoch 3541/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 126.8861 - val_loss: 144.8954\n",
      "Epoch 3542/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.2684 - val_loss: 147.0542\n",
      "Epoch 3543/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.7048 - val_loss: 138.9801\n",
      "Epoch 3544/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 121.1517 - val_loss: 146.4220\n",
      "Epoch 3545/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 128.4549 - val_loss: 131.7933\n",
      "Epoch 3546/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.7242 - val_loss: 170.7872\n",
      "Epoch 3547/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.1454 - val_loss: 225.5554\n",
      "Epoch 3548/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.4426 - val_loss: 152.7900\n",
      "Epoch 3549/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.7192 - val_loss: 171.0359\n",
      "Epoch 3550/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 128.4107 - val_loss: 139.3326\n",
      "Epoch 3551/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 121.9078 - val_loss: 144.1753\n",
      "Epoch 3552/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 126.4349 - val_loss: 170.2942\n",
      "Epoch 3553/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.4677 - val_loss: 155.5965\n",
      "Epoch 3554/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 121.0865 - val_loss: 140.0891\n",
      "Epoch 3555/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 128.2618 - val_loss: 154.7709\n",
      "Epoch 3556/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.1924 - val_loss: 137.0655\n",
      "Epoch 3557/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 126.0223 - val_loss: 130.7755\n",
      "Epoch 3558/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.3042 - val_loss: 176.8226\n",
      "Epoch 3559/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.5266 - val_loss: 185.9679\n",
      "Epoch 3560/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.1853 - val_loss: 140.9830\n",
      "Epoch 3561/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.0290 - val_loss: 129.6612\n",
      "Epoch 3562/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 125.2105 - val_loss: 135.7208\n",
      "Epoch 3563/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 128.6829 - val_loss: 139.0295\n",
      "Epoch 3564/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 123.2977 - val_loss: 261.7548\n",
      "Epoch 3565/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 124.0200 - val_loss: 141.8784\n",
      "Epoch 3566/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.1697 - val_loss: 166.0496\n",
      "Epoch 3567/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 151.4610 - val_loss: 151.0027\n",
      "Epoch 3568/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 118.6332 - val_loss: 143.4540\n",
      "Epoch 3569/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 119.7723 - val_loss: 158.0699\n",
      "Epoch 3570/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 125.4467 - val_loss: 148.0497\n",
      "Epoch 3571/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 125.8631 - val_loss: 146.5976\n",
      "Epoch 3572/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 122.1949 - val_loss: 229.8916\n",
      "Epoch 3573/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.5262 - val_loss: 152.7562\n",
      "Epoch 3574/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.5618 - val_loss: 144.2727\n",
      "Epoch 3575/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.3947 - val_loss: 140.9283\n",
      "Epoch 3576/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.1070 - val_loss: 152.8683\n",
      "Epoch 3577/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.9124 - val_loss: 157.1001\n",
      "Epoch 3578/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 120.5842 - val_loss: 130.0350\n",
      "Epoch 3579/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.5431 - val_loss: 144.6149\n",
      "Epoch 3580/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.0782 - val_loss: 185.3318\n",
      "Epoch 3581/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 164.9039 - val_loss: 153.0721\n",
      "Epoch 3582/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.9205 - val_loss: 129.0830\n",
      "Epoch 3583/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 118.1840 - val_loss: 131.6804\n",
      "Epoch 3584/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.9428 - val_loss: 135.8076\n",
      "Epoch 3585/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.0980 - val_loss: 129.0519\n",
      "Epoch 3586/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.7184 - val_loss: 132.2107\n",
      "Epoch 3587/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 126.4134 - val_loss: 141.3248\n",
      "Epoch 3588/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 122.7995 - val_loss: 140.3468\n",
      "Epoch 3589/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 117.9995 - val_loss: 131.4168\n",
      "Epoch 3590/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 129.5222 - val_loss: 154.0893\n",
      "Epoch 3591/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.0744 - val_loss: 150.6477\n",
      "Epoch 3592/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 123.1897 - val_loss: 148.6526\n",
      "Epoch 3593/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.7679 - val_loss: 138.8521\n",
      "Epoch 3594/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.6616 - val_loss: 133.9538\n",
      "Epoch 3595/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.7604 - val_loss: 143.7259\n",
      "Epoch 3596/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 124.5906 - val_loss: 133.5316\n",
      "Epoch 3597/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 125.9720 - val_loss: 140.2389\n",
      "Epoch 3598/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 123.1689 - val_loss: 129.8763\n",
      "Epoch 3599/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 125.2304 - val_loss: 148.9321\n",
      "Epoch 3600/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 134.9722 - val_loss: 161.1354\n",
      "Epoch 3601/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 125.9206 - val_loss: 139.2006\n",
      "Epoch 3602/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 121.4533 - val_loss: 132.7497\n",
      "Epoch 3603/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.0168 - val_loss: 167.6130\n",
      "Epoch 3604/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 165.0357 - val_loss: 141.7763\n",
      "Epoch 3605/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 120.8213 - val_loss: 140.7577\n",
      "Epoch 3606/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 118.2236 - val_loss: 150.8642\n",
      "Epoch 3607/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 118.2259 - val_loss: 160.0716\n",
      "Epoch 3608/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 119.5810 - val_loss: 172.6727\n",
      "Epoch 3609/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.5293 - val_loss: 150.2506\n",
      "Epoch 3610/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 122.6324 - val_loss: 145.9906\n",
      "Epoch 3611/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 126.4219 - val_loss: 130.4446\n",
      "Epoch 3612/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 125.2445 - val_loss: 160.8225\n",
      "Epoch 3613/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 127.7705 - val_loss: 143.9326\n",
      "Epoch 3614/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 120.1913 - val_loss: 137.1549\n",
      "Epoch 3615/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 73us/step - loss: 123.9736 - val_loss: 138.8357\n",
      "Epoch 3616/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 128.8446 - val_loss: 131.0824\n",
      "Epoch 3617/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 119.5279 - val_loss: 134.2252\n",
      "Epoch 3618/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.5958 - val_loss: 140.5329\n",
      "Epoch 3619/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.8988 - val_loss: 149.5974\n",
      "Epoch 3620/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 119.8789 - val_loss: 162.4999\n",
      "Epoch 3621/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.5785 - val_loss: 134.6222\n",
      "Epoch 3622/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 121.2266 - val_loss: 142.4900\n",
      "Epoch 3623/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 126.8859 - val_loss: 146.7368\n",
      "Epoch 3624/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.8964 - val_loss: 144.4613\n",
      "Epoch 3625/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 128.0368 - val_loss: 132.4309\n",
      "Epoch 3626/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 120.0038 - val_loss: 138.5711\n",
      "Epoch 3627/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 123.0308 - val_loss: 152.7771\n",
      "Epoch 3628/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 123.5896 - val_loss: 137.0296\n",
      "Epoch 3629/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 129.8018 - val_loss: 206.1245\n",
      "Epoch 3630/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 128.9444 - val_loss: 131.2468\n",
      "Epoch 3631/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 125.7983 - val_loss: 130.2959\n",
      "Epoch 3632/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 118.1338 - val_loss: 130.4797\n",
      "Epoch 3633/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 195.5757 - val_loss: 2157.1369\n",
      "Epoch 3634/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 155.5783 - val_loss: 132.5620\n",
      "Epoch 3635/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 118.6873 - val_loss: 137.5094\n",
      "Epoch 3636/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 118.2581 - val_loss: 137.9726\n",
      "Epoch 3637/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 117.9251 - val_loss: 135.3556\n",
      "Epoch 3638/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 122.1742 - val_loss: 140.7498\n",
      "Epoch 3639/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 120.0955 - val_loss: 153.0040\n",
      "Epoch 3640/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 120.8168 - val_loss: 142.0203\n",
      "Epoch 3641/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 118.8028 - val_loss: 138.5436\n",
      "Epoch 3642/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 129.8853 - val_loss: 138.7748\n",
      "Epoch 3643/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 123.9260 - val_loss: 131.9076\n",
      "Epoch 3644/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 121.2295 - val_loss: 135.2955\n",
      "Epoch 3645/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 149.3824 - val_loss: 144.7149\n",
      "Epoch 3646/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 122.5928 - val_loss: 131.4681\n",
      "Epoch 3647/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 119.7607 - val_loss: 142.7146\n",
      "Epoch 3648/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 119.3242 - val_loss: 129.2082\n",
      "Epoch 3649/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 127.2834 - val_loss: 136.6415\n",
      "Epoch 3650/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 131.0433 - val_loss: 141.3036\n",
      "Epoch 3651/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 122.3274 - val_loss: 157.8122\n",
      "Epoch 3652/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 124.0981 - val_loss: 182.2886\n",
      "Epoch 3653/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 123.7158 - val_loss: 139.9855\n",
      "Epoch 3654/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 121.7198 - val_loss: 131.9603\n",
      "Epoch 3655/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.5154 - val_loss: 136.5163\n",
      "Epoch 3656/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.6215 - val_loss: 128.3869\n",
      "Epoch 3657/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 119.7231 - val_loss: 199.4460\n",
      "Epoch 3658/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 182.9313 - val_loss: 131.8353\n",
      "Epoch 3659/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 119.6834 - val_loss: 132.7673\n",
      "Epoch 3660/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 117.4829 - val_loss: 135.6309\n",
      "Epoch 3661/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 119.9345 - val_loss: 170.5301\n",
      "Epoch 3662/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 118.8259 - val_loss: 155.6028\n",
      "Epoch 3663/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 118.9489 - val_loss: 129.9920\n",
      "Epoch 3664/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 120.3365 - val_loss: 149.1357\n",
      "Epoch 3665/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 120.2750 - val_loss: 137.7185\n",
      "Epoch 3666/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.4748 - val_loss: 132.3634\n",
      "Epoch 3667/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.4587 - val_loss: 138.7145\n",
      "Epoch 3668/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 121.6417 - val_loss: 140.8954\n",
      "Epoch 3669/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.1211 - val_loss: 132.4045\n",
      "Epoch 3670/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.2799 - val_loss: 135.1151\n",
      "Epoch 3671/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.1550 - val_loss: 142.0473\n",
      "Epoch 3672/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.9183 - val_loss: 142.6011\n",
      "Epoch 3673/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.7281 - val_loss: 136.4553\n",
      "Epoch 3674/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.3703 - val_loss: 163.5930\n",
      "Epoch 3675/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.8911 - val_loss: 154.8951\n",
      "Epoch 3676/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 123.5193 - val_loss: 194.0652\n",
      "Epoch 3677/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 296.7002 - val_loss: 133.6134\n",
      "Epoch 3678/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 115.3016 - val_loss: 140.4732\n",
      "Epoch 3679/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 115.1613 - val_loss: 137.9043\n",
      "Epoch 3680/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.9236 - val_loss: 135.6118\n",
      "Epoch 3681/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 116.6749 - val_loss: 139.5978\n",
      "Epoch 3682/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 121.4761 - val_loss: 133.1314\n",
      "Epoch 3683/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 120.2219 - val_loss: 154.3040\n",
      "Epoch 3684/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 120.2417 - val_loss: 139.7623\n",
      "Epoch 3685/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 116.8836 - val_loss: 131.1148\n",
      "Epoch 3686/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 122.1818 - val_loss: 128.7784\n",
      "Epoch 3687/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 66us/step - loss: 122.6609 - val_loss: 132.9979\n",
      "Epoch 3688/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 126.1929 - val_loss: 140.2455\n",
      "Epoch 3689/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.1887 - val_loss: 156.0892\n",
      "Epoch 3690/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.7337 - val_loss: 141.9570\n",
      "Epoch 3691/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.3135 - val_loss: 144.9412\n",
      "Epoch 3692/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 118.1262 - val_loss: 129.0023\n",
      "Epoch 3693/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.3725 - val_loss: 147.4984\n",
      "Epoch 3694/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.7358 - val_loss: 136.4522\n",
      "Epoch 3695/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.8222 - val_loss: 134.8688\n",
      "Epoch 3696/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.6657 - val_loss: 150.8037\n",
      "Epoch 3697/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.7497 - val_loss: 136.2215\n",
      "Epoch 3698/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.4249 - val_loss: 139.1419\n",
      "Epoch 3699/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 121.4003 - val_loss: 132.5966\n",
      "Epoch 3700/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 119.7937 - val_loss: 162.8331\n",
      "Epoch 3701/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 129.0088 - val_loss: 129.7242\n",
      "Epoch 3702/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 123.3996 - val_loss: 131.7946\n",
      "Epoch 3703/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.9680 - val_loss: 129.2666\n",
      "Epoch 3704/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 121.8453 - val_loss: 138.6865\n",
      "Epoch 3705/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.9652 - val_loss: 186.1091\n",
      "Epoch 3706/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.3503 - val_loss: 138.1993\n",
      "Epoch 3707/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.5722 - val_loss: 169.5928\n",
      "Epoch 3708/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.1105 - val_loss: 127.4318\n",
      "Epoch 3709/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.7435 - val_loss: 131.9011\n",
      "Epoch 3710/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.8628 - val_loss: 148.4099\n",
      "Epoch 3711/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.1002 - val_loss: 140.6096\n",
      "Epoch 3712/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 128.8091 - val_loss: 149.5020\n",
      "Epoch 3713/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 124.8882 - val_loss: 148.0503\n",
      "Epoch 3714/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.5380 - val_loss: 153.4209\n",
      "Epoch 3715/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 122.0983 - val_loss: 138.5517\n",
      "Epoch 3716/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 124.0283 - val_loss: 186.4429\n",
      "Epoch 3717/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 126.4421 - val_loss: 193.5534\n",
      "Epoch 3718/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.2051 - val_loss: 141.7627\n",
      "Epoch 3719/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.2074 - val_loss: 167.0369\n",
      "Epoch 3720/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 124.4127 - val_loss: 142.2565\n",
      "Epoch 3721/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 121.730 - 0s 57us/step - loss: 121.5298 - val_loss: 153.7283\n",
      "Epoch 3722/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.5467 - val_loss: 147.5448\n",
      "Epoch 3723/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 121.3124 - val_loss: 131.7746\n",
      "Epoch 3724/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.8414 - val_loss: 136.9909\n",
      "Epoch 3725/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.5309 - val_loss: 135.2367\n",
      "Epoch 3726/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.6310 - val_loss: 201.1433\n",
      "Epoch 3727/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 124.4210 - val_loss: 132.4342\n",
      "Epoch 3728/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 123.1952 - val_loss: 176.7131\n",
      "Epoch 3729/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 125.5668 - val_loss: 132.5082\n",
      "Epoch 3730/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 119.6061 - val_loss: 203.5684\n",
      "Epoch 3731/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.0120 - val_loss: 140.2400\n",
      "Epoch 3732/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 124.0695 - val_loss: 147.2338\n",
      "Epoch 3733/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 121.9842 - val_loss: 144.0943\n",
      "Epoch 3734/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 125.9421 - val_loss: 142.1321\n",
      "Epoch 3735/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.6487 - val_loss: 130.4820\n",
      "Epoch 3736/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.4184 - val_loss: 140.4889\n",
      "Epoch 3737/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.7110 - val_loss: 142.7162\n",
      "Epoch 3738/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 133.8162 - val_loss: 130.9646\n",
      "Epoch 3739/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.7913 - val_loss: 138.7431\n",
      "Epoch 3740/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.9772 - val_loss: 142.5599\n",
      "Epoch 3741/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.5226 - val_loss: 130.2700\n",
      "Epoch 3742/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 125.5370 - val_loss: 133.9179\n",
      "Epoch 3743/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 125.0193 - val_loss: 141.4582\n",
      "Epoch 3744/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 123.2432 - val_loss: 137.3375\n",
      "Epoch 3745/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 169.5671 - val_loss: 144.4269\n",
      "Epoch 3746/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 123.9602 - val_loss: 146.4798\n",
      "Epoch 3747/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 118.7927 - val_loss: 138.6996\n",
      "Epoch 3748/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 120.7519 - val_loss: 133.0146\n",
      "Epoch 3749/10000\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 121.8739 - val_loss: 133.2669\n",
      "Epoch 3750/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 117.8378 - val_loss: 162.6099\n",
      "Epoch 3751/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 123.7362 - val_loss: 141.3418\n",
      "Epoch 3752/10000\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 124.8430 - val_loss: 129.7640\n",
      "Epoch 3753/10000\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 126.9545 - val_loss: 144.9904\n",
      "Epoch 3754/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 124.0642 - val_loss: 141.0232\n",
      "Epoch 3755/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 124.4296 - val_loss: 166.0690\n",
      "Epoch 3756/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 125.4704 - val_loss: 129.7668\n",
      "Epoch 3757/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 120.4909 - val_loss: 155.8314\n",
      "Epoch 3758/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 132.5811 - val_loss: 129.3573\n",
      "Epoch 3759/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 125.0846 - val_loss: 144.7262\n",
      "Epoch 3760/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 124.8490 - val_loss: 131.5543\n",
      "Epoch 3761/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 131.9115 - val_loss: 137.5139\n",
      "Epoch 3762/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 122.8410 - val_loss: 132.8133\n",
      "Epoch 3763/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 118.0548 - val_loss: 139.0861\n",
      "Epoch 3764/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 124.0824 - val_loss: 132.7464\n",
      "Epoch 3765/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 119.6539 - val_loss: 135.3192\n",
      "Epoch 3766/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 127.6894 - val_loss: 186.6791\n",
      "Epoch 3767/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 122.8787 - val_loss: 143.4060\n",
      "Epoch 3768/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 122.7447 - val_loss: 138.1690\n",
      "Epoch 3769/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 120.9671 - val_loss: 131.3064\n",
      "Epoch 3770/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 123.3699 - val_loss: 135.3979\n",
      "Epoch 3771/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 125.0532 - val_loss: 142.0754\n",
      "Epoch 3772/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.4793 - val_loss: 136.2941\n",
      "Epoch 3773/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 119.7636 - val_loss: 136.7929\n",
      "Epoch 3774/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 122.9759 - val_loss: 134.9808\n",
      "Epoch 3775/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.5002 - val_loss: 160.0173\n",
      "Epoch 3776/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 139.1260 - val_loss: 152.1777\n",
      "Epoch 3777/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 126.0898 - val_loss: 144.3891\n",
      "Epoch 3778/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 120.3896 - val_loss: 168.3385\n",
      "Epoch 3779/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 121.6398 - val_loss: 144.6730\n",
      "Epoch 3780/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 120.2808 - val_loss: 154.6802\n",
      "Epoch 3781/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 120.4783 - val_loss: 134.1256\n",
      "Epoch 3782/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.6545 - val_loss: 131.8502\n",
      "Epoch 3783/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 124.7716 - val_loss: 160.7243\n",
      "Epoch 3784/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.1983 - val_loss: 150.3476\n",
      "Epoch 3785/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.1353 - val_loss: 147.4974\n",
      "Epoch 3786/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 191.1081 - val_loss: 135.3272\n",
      "Epoch 3787/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 116.0942 - val_loss: 142.4537\n",
      "Epoch 3788/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 117.6285 - val_loss: 176.5248\n",
      "Epoch 3789/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 121.0556 - val_loss: 147.5279\n",
      "Epoch 3790/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 117.0957 - val_loss: 137.1711\n",
      "Epoch 3791/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.0156 - val_loss: 139.9882\n",
      "Epoch 3792/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.6984 - val_loss: 160.8475\n",
      "Epoch 3793/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.6956 - val_loss: 131.4700\n",
      "Epoch 3794/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 118.7268 - val_loss: 141.5342\n",
      "Epoch 3795/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.7963 - val_loss: 154.0192\n",
      "Epoch 3796/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.6477 - val_loss: 153.6728\n",
      "Epoch 3797/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.9506 - val_loss: 156.4722\n",
      "Epoch 3798/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 125.5138 - val_loss: 150.2135\n",
      "Epoch 3799/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.1540 - val_loss: 164.5689\n",
      "Epoch 3800/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 129.2175 - val_loss: 132.9922\n",
      "Epoch 3801/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.0567 - val_loss: 141.2041\n",
      "Epoch 3802/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 115.7903 - val_loss: 132.7782\n",
      "Epoch 3803/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 121.4526 - val_loss: 138.3401\n",
      "Epoch 3804/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 148.8794 - val_loss: 166.4503\n",
      "Epoch 3805/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 118.8592 - val_loss: 135.6578\n",
      "Epoch 3806/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.4012 - val_loss: 1977.1259\n",
      "Epoch 3807/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 147.1897 - val_loss: 175.2686\n",
      "Epoch 3808/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 119.7931 - val_loss: 145.2374\n",
      "Epoch 3809/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 120.8456 - val_loss: 134.0978\n",
      "Epoch 3810/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 121.1080 - val_loss: 126.7667\n",
      "Epoch 3811/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 117.8328 - val_loss: 151.3885\n",
      "Epoch 3812/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 121.2157 - val_loss: 134.0249\n",
      "Epoch 3813/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 115.9512 - val_loss: 161.9342\n",
      "Epoch 3814/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 124.3841 - val_loss: 141.5205\n",
      "Epoch 3815/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 126.1355 - val_loss: 135.3734\n",
      "Epoch 3816/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 121.4840 - val_loss: 130.2607\n",
      "Epoch 3817/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 124.9266 - val_loss: 133.5686\n",
      "Epoch 3818/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 125.6889 - val_loss: 140.2951\n",
      "Epoch 3819/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 123.5821 - val_loss: 140.5493\n",
      "Epoch 3820/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 122.5529 - val_loss: 130.0621\n",
      "Epoch 3821/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.9694 - val_loss: 131.1530\n",
      "Epoch 3822/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 125.7847 - val_loss: 136.3998\n",
      "Epoch 3823/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.9256 - val_loss: 135.4562\n",
      "Epoch 3824/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 121.3399 - val_loss: 141.4107\n",
      "Epoch 3825/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 119.9613 - val_loss: 135.0787\n",
      "Epoch 3826/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.3693 - val_loss: 183.1086\n",
      "Epoch 3827/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.6272 - val_loss: 135.8707\n",
      "Epoch 3828/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 121.2550 - val_loss: 133.4728\n",
      "Epoch 3829/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 117.8600 - val_loss: 149.4287\n",
      "Epoch 3830/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.3572 - val_loss: 155.8743\n",
      "Epoch 3831/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 120.3381 - val_loss: 134.5204\n",
      "Epoch 3832/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.1131 - val_loss: 167.1655\n",
      "Epoch 3833/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 119.8776 - val_loss: 147.2176\n",
      "Epoch 3834/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.8223 - val_loss: 152.3308\n",
      "Epoch 3835/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.0146 - val_loss: 151.2013\n",
      "Epoch 3836/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.5389 - val_loss: 148.3303\n",
      "Epoch 3837/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.7158 - val_loss: 134.1569\n",
      "Epoch 3838/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.8318 - val_loss: 142.6873\n",
      "Epoch 3839/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.7925 - val_loss: 149.1176\n",
      "Epoch 3840/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.2387 - val_loss: 131.6825\n",
      "Epoch 3841/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 132.8217 - val_loss: 127.9437\n",
      "Epoch 3842/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 120.6259 - val_loss: 128.0894\n",
      "Epoch 3843/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 121.1618 - val_loss: 137.8957\n",
      "Epoch 3844/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.2675 - val_loss: 133.1827\n",
      "Epoch 3845/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 120.2954 - val_loss: 134.4451\n",
      "Epoch 3846/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.8349 - val_loss: 138.7326\n",
      "Epoch 3847/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 121.9257 - val_loss: 166.2852\n",
      "Epoch 3848/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 185.3127 - val_loss: 153.4365\n",
      "Epoch 3849/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 122.1026 - val_loss: 130.3748\n",
      "Epoch 3850/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 121.3098 - val_loss: 133.2800\n",
      "Epoch 3851/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 121.0164 - val_loss: 132.8075\n",
      "Epoch 3852/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 117.7264 - val_loss: 135.9581\n",
      "Epoch 3853/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 122.3587 - val_loss: 131.9757\n",
      "Epoch 3854/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 116.6787 - val_loss: 201.2158\n",
      "Epoch 3855/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 118.7872 - val_loss: 133.3189\n",
      "Epoch 3856/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 117.2949 - val_loss: 154.8409\n",
      "Epoch 3857/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 119.1811 - val_loss: 160.0094\n",
      "Epoch 3858/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.5698 - val_loss: 136.3889\n",
      "Epoch 3859/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.1110 - val_loss: 138.9394\n",
      "Epoch 3860/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.4883 - val_loss: 134.1593\n",
      "Epoch 3861/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.4435 - val_loss: 164.1022\n",
      "Epoch 3862/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.6915 - val_loss: 160.9636\n",
      "Epoch 3863/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.3749 - val_loss: 142.5972\n",
      "Epoch 3864/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 121.5398 - val_loss: 137.3694\n",
      "Epoch 3865/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 123.2336 - val_loss: 153.1594\n",
      "Epoch 3866/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 119.9554 - val_loss: 157.1283\n",
      "Epoch 3867/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 123.5285 - val_loss: 133.5449\n",
      "Epoch 3868/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.5012 - val_loss: 129.0697\n",
      "Epoch 3869/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 123.5933 - val_loss: 137.5827\n",
      "Epoch 3870/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 119.0218 - val_loss: 134.0987\n",
      "Epoch 3871/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.0196 - val_loss: 144.0745\n",
      "Epoch 3872/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 119.5837 - val_loss: 136.9338\n",
      "Epoch 3873/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 120.1963 - val_loss: 135.8796\n",
      "Epoch 3874/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 118.9825 - val_loss: 145.8311\n",
      "Epoch 3875/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 118.0714 - val_loss: 186.3412\n",
      "Epoch 3876/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.2108 - val_loss: 137.1838\n",
      "Epoch 3877/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 121.0391 - val_loss: 142.8892\n",
      "Epoch 3878/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 121.5631 - val_loss: 137.8115\n",
      "Epoch 3879/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.0523 - val_loss: 2202.0274\n",
      "Epoch 3880/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 265.4714 - val_loss: 167.3491\n",
      "Epoch 3881/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 120.5667 - val_loss: 140.9249\n",
      "Epoch 3882/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 125.0380 - val_loss: 159.2428\n",
      "Epoch 3883/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 121.2160 - val_loss: 143.0984\n",
      "Epoch 3884/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 120.9085 - val_loss: 138.2806\n",
      "Epoch 3885/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 123.3799 - val_loss: 144.6067\n",
      "Epoch 3886/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 135.7361 - val_loss: 128.5071\n",
      "Epoch 03886: early stopping\n",
      "Fold score (RMSE): 11.131796836853027\n",
      "Fold #2\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 4935.8781 - val_loss: 4711.6119\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 4509.1262 - val_loss: 4604.8456\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 4095.6110 - val_loss: 4712.1640\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 3992.7726 - val_loss: 4377.5677\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 3953.8927 - val_loss: 4292.5788\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 3965.5034 - val_loss: 4430.0930\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 3817.6435 - val_loss: 4073.0065\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 3779.4201 - val_loss: 4134.8751\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 3490.3544 - val_loss: 3670.2343\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 3585.7861 - val_loss: 3522.3161\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 3228.3811 - val_loss: 3691.3738\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 2875.7685 - val_loss: 3228.4180\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 2574.0584 - val_loss: 2285.2001\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 2077.9076 - val_loss: 2263.3322\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 1795.8005 - val_loss: 2677.3148\n",
      "Epoch 16/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 1266.8734 - val_loss: 900.1804\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 1039.0353 - val_loss: 929.4529\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 1012.2202 - val_loss: 695.1793\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 828.9213 - val_loss: 573.3917\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 614.3627 - val_loss: 542.9640\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 742.7885 - val_loss: 522.1774\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 746.7913 - val_loss: 1422.6935\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 827.1248 - val_loss: 502.0278\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 607.2851 - val_loss: 441.1708\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 611.8957 - val_loss: 1460.5807\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 646.8110 - val_loss: 482.5734\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 578.2893 - val_loss: 3099.6920\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 632.7765 - val_loss: 497.2221\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 523.7318 - val_loss: 425.0560\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 550.8867 - val_loss: 403.2355\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 622.3214 - val_loss: 724.0778\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 455.5354 - val_loss: 386.5307\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 1326.8486 - val_loss: 535.8667\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 528.8003 - val_loss: 366.7726\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 499.2733 - val_loss: 424.6825\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 511.9193 - val_loss: 1241.6570\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 496.1227 - val_loss: 352.3397\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 495.3004 - val_loss: 622.7691\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 632.3818 - val_loss: 784.0043\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 445.5286 - val_loss: 406.1764\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 396.0264 - val_loss: 321.1081\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 470.8098 - val_loss: 563.2873\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 405.2381 - val_loss: 1101.3124\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 491.4057 - val_loss: 348.5910\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 407.2046 - val_loss: 288.4090\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 374.3534 - val_loss: 400.1872\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 619.5241 - val_loss: 328.6294\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 495.8134 - val_loss: 506.6706\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 438.5370 - val_loss: 293.9533\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 468.6862 - val_loss: 374.5010\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 381.2766 - val_loss: 270.2405\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 462.4397 - val_loss: 451.3797\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 391.9239 - val_loss: 388.8344\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 454.6654 - val_loss: 320.4626\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 344.0952 - val_loss: 333.9689\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 344.6912 - val_loss: 301.1117\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 416.3287 - val_loss: 270.2301\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 534.0217 - val_loss: 392.5545\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 307.2000 - val_loss: 290.4875\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 502.0723 - val_loss: 368.5735\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 311.8067 - val_loss: 269.2268\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 413.8493 - val_loss: 287.4084\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 368.7769 - val_loss: 482.2557\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 733.1321 - val_loss: 464.1464\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 439.5304 - val_loss: 339.8751\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 309.8505 - val_loss: 350.4466\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 401.4363 - val_loss: 620.5801\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 347.7183 - val_loss: 486.4448\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 299.3455 - val_loss: 246.9984\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 349.2530 - val_loss: 244.7830\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 319.9099 - val_loss: 481.5676\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 341.5982 - val_loss: 326.4215\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 442.7431 - val_loss: 327.6682\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 446.5722 - val_loss: 496.6905\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 550.9946 - val_loss: 805.3484\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 311.4693 - val_loss: 267.3339\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 443.2163 - val_loss: 258.2547\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 322.9402 - val_loss: 547.2070\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 299.3137 - val_loss: 331.7310\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 290.0939 - val_loss: 275.1418\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 319.6650 - val_loss: 262.5105\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 346.5590 - val_loss: 260.2147\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 274.8176 - val_loss: 234.3495\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 278.3339 - val_loss: 229.5732\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 381.9089 - val_loss: 638.8460\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 454.0510 - val_loss: 270.7838\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 323.7278 - val_loss: 318.8769\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 395.5492 - val_loss: 932.9496\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 325.6682 - val_loss: 552.2130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 314.3367 - val_loss: 261.5448\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 335.7835 - val_loss: 494.8856\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 266.1813 - val_loss: 334.7089\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 324.0198 - val_loss: 536.4770\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 289.9255 - val_loss: 264.5023\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 258.1371 - val_loss: 289.8194\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 336.4551 - val_loss: 703.5591\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 465.2419 - val_loss: 483.2678\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 243.4643 - val_loss: 205.2874\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 320.5212 - val_loss: 292.0730\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 275.1542 - val_loss: 243.4904\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 291.4852 - val_loss: 478.6819\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 287.4173 - val_loss: 389.5270\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 271.2634 - val_loss: 263.5370\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 270.6848 - val_loss: 197.2673\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 390.1481 - val_loss: 240.8261\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 268.9063 - val_loss: 352.5392\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 363.5610 - val_loss: 322.5031\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 290.9514 - val_loss: 218.5309\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 275.4924 - val_loss: 228.4729\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 262.5285 - val_loss: 311.5566\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 248.4719 - val_loss: 236.5448\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 357.6806 - val_loss: 208.5243\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 263.2409 - val_loss: 213.3663\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 357.3426 - val_loss: 223.4856\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 271.8185 - val_loss: 281.3902\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 213.5041 - val_loss: 198.8964\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 357.8279 - val_loss: 424.0314\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 289.2225 - val_loss: 228.0581\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 299.0973 - val_loss: 247.2861\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 253.0574 - val_loss: 300.6520\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 236.4255 - val_loss: 567.8041\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 269.0352 - val_loss: 217.3167\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 282.2308 - val_loss: 217.7718\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 211.5883 - val_loss: 232.8473\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 291.9061 - val_loss: 299.8270\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 290.0581 - val_loss: 411.9793\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 218.9947 - val_loss: 234.4298\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 266.3690 - val_loss: 205.2091\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 267.4214 - val_loss: 286.6094\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 262.9141 - val_loss: 592.6103\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 245.8060 - val_loss: 219.1518\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 264.3535 - val_loss: 258.6043\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 275.9459 - val_loss: 270.2492\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 228.1378 - val_loss: 466.0791\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 234.5898 - val_loss: 481.1941\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 270.7758 - val_loss: 203.3521\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 235.1552 - val_loss: 206.8625\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 256.0699 - val_loss: 243.6854\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 292.0222 - val_loss: 282.4586\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 210.7989 - val_loss: 240.8728\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 198.6079 - val_loss: 334.8773\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 268.8591 - val_loss: 195.7038\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 255.0883 - val_loss: 222.5532\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 267.1583 - val_loss: 188.4463\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 254.3922 - val_loss: 197.0769\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 302.2937 - val_loss: 206.3680\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 186.4834 - val_loss: 260.0742\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 296.6754 - val_loss: 227.3369\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 259.7601 - val_loss: 410.5302\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 220.1001 - val_loss: 451.5933\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 253.5167 - val_loss: 192.4437\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 262.8573 - val_loss: 192.2116\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 261.5915 - val_loss: 446.1946\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 228.4104 - val_loss: 453.7910\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 221.6485 - val_loss: 244.4452\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 261.1708 - val_loss: 247.2634\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 196.2631 - val_loss: 195.2507\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 279.0989 - val_loss: 202.5657\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 261.7311 - val_loss: 544.8285\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 232.8164 - val_loss: 317.5975\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 209.6274 - val_loss: 226.5438\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 210.0940 - val_loss: 239.9521\n",
      "Epoch 163/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 240.4890 - val_loss: 244.1803\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 217.9634 - val_loss: 226.5238\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 233.2238 - val_loss: 383.3598\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 205.4898 - val_loss: 279.7155\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 233.2985 - val_loss: 185.3653\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 215.2205 - val_loss: 194.3087\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 251.4927 - val_loss: 281.6853\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 343.5936 - val_loss: 227.3159\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 233.3225 - val_loss: 200.4871\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 262.9131 - val_loss: 201.1834\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 211.1559 - val_loss: 193.7738\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 199.3794 - val_loss: 183.5121\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 259.9084 - val_loss: 215.9031\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 205.7936 - val_loss: 274.1731\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 297.7293 - val_loss: 203.4679\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 202.3323 - val_loss: 201.2392\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 231.3092 - val_loss: 172.3701\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 203.5764 - val_loss: 240.1077\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 198.0255 - val_loss: 212.3706\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 243.2768 - val_loss: 208.3154\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 209.9999 - val_loss: 189.9301\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 244.9059 - val_loss: 193.5357\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 230.9561 - val_loss: 206.4597\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 241.9497 - val_loss: 291.7773\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 245.7094 - val_loss: 180.5249\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 237.1685 - val_loss: 181.8449\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 207.8317 - val_loss: 276.0537\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 211.1356 - val_loss: 214.1050\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 201.3383 - val_loss: 289.8697\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 242.6385 - val_loss: 211.3721\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 301.3920 - val_loss: 240.4874\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 182.5718 - val_loss: 178.5187\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 179.1862 - val_loss: 641.6969\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 215.0298 - val_loss: 183.4813\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 185.6861 - val_loss: 304.9138\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 201.5183 - val_loss: 191.0710\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 181.0383 - val_loss: 172.2419\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 245.4329 - val_loss: 343.2528\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 229.8250 - val_loss: 844.0076\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 248.6186 - val_loss: 181.6908\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 207.5935 - val_loss: 266.5980\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 219.6248 - val_loss: 276.1131\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 201.1368 - val_loss: 177.7966\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 185.1831 - val_loss: 173.6101\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 184.6851 - val_loss: 258.1557\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 227.6214 - val_loss: 170.8099\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 250.1183 - val_loss: 202.4776\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 193.1058 - val_loss: 323.6773\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 236.3676 - val_loss: 341.9743\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 188.6593 - val_loss: 168.8029\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 204.8727 - val_loss: 182.4393\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 235.2248 - val_loss: 170.8281\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 311.9713 - val_loss: 739.4548\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 214.5092 - val_loss: 169.7592\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 176.2362 - val_loss: 180.1335\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 205.3405 - val_loss: 190.9437\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 202.5124 - val_loss: 185.9926\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 226.4486 - val_loss: 201.1026\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 180.4528 - val_loss: 255.6391\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 185.7348 - val_loss: 257.5402\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 197.6787 - val_loss: 169.2759\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 254.6595 - val_loss: 206.4175\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 204.8346 - val_loss: 173.8520\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 175.8689 - val_loss: 272.7435\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 200.0438 - val_loss: 184.2814\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 192.5795 - val_loss: 163.0009\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 172.2288 - val_loss: 169.6147\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 227.5356 - val_loss: 163.4334\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 235.0574 - val_loss: 184.7367\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 279.0827 - val_loss: 416.8656\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 218.0135 - val_loss: 285.6282\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 174.4174 - val_loss: 179.2892\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 177.6553 - val_loss: 203.6218\n",
      "Epoch 236/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 235.0582 - val_loss: 165.0668\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 214.8739 - val_loss: 313.3995\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 179.9982 - val_loss: 257.5328\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 175.1707 - val_loss: 277.6190\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 261.8931 - val_loss: 170.9140\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 178.9130 - val_loss: 175.9714\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 161.0249 - val_loss: 206.9523\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 202.1041 - val_loss: 421.7792\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 197.6908 - val_loss: 181.6158\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 201.1657 - val_loss: 562.0295\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 183.0623 - val_loss: 170.6489\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 184.5147 - val_loss: 180.2783\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 199.0998 - val_loss: 215.3667\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 236.1471 - val_loss: 208.9892\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 264.3229 - val_loss: 210.7231\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 190.5884 - val_loss: 173.1677\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 209.1470 - val_loss: 168.3058\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 183.9830 - val_loss: 197.0410\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 207.357 - 1s 63us/step - loss: 207.2531 - val_loss: 192.5923\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 173.6678 - val_loss: 760.7623\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 315.2721 - val_loss: 169.0528\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 178.3718 - val_loss: 216.7465\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 190.0393 - val_loss: 162.8678\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 153.1579 - val_loss: 221.5103\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 362.5661 - val_loss: 268.6546\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 178.0761 - val_loss: 169.5001\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 160.5004 - val_loss: 207.4388\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 168.0811 - val_loss: 163.5017\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 246.4052 - val_loss: 228.1786\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 168.0786 - val_loss: 158.6146\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 200.1753 - val_loss: 427.6094\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 179.2915 - val_loss: 154.4135\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 171.3146 - val_loss: 250.1448\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 270.9723 - val_loss: 225.6930\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 164.6919 - val_loss: 283.3368\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 232.1171 - val_loss: 296.6432\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 173.2979 - val_loss: 168.7495\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 236.8831 - val_loss: 178.0663\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 158.1748 - val_loss: 171.2030\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 180.6868 - val_loss: 164.8887\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 176.5360 - val_loss: 175.6713\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 176.5522 - val_loss: 188.4962\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 166.1932 - val_loss: 164.0298\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 324.5146 - val_loss: 168.7735\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.6305 - val_loss: 165.9283\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 174.5354 - val_loss: 175.3996\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.2054 - val_loss: 172.1265\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 192.6910 - val_loss: 174.2729\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 154.9808 - val_loss: 165.0362\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 251.0588 - val_loss: 191.1281\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 155.6225 - val_loss: 153.2775\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 308.8755 - val_loss: 264.3461\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 151.5524 - val_loss: 165.8334\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.9754 - val_loss: 192.4853\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 223.6246 - val_loss: 256.2177\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 165.6624 - val_loss: 192.3271\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 150.0466 - val_loss: 173.9898\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 165.2606 - val_loss: 165.7442\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 229.3402 - val_loss: 182.2012\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 249.3082 - val_loss: 158.9790\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 148.4478 - val_loss: 170.3797\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 159.1888 - val_loss: 171.2414\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 383.8652 - val_loss: 206.5185\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 217.4697 - val_loss: 218.4875\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 181.8487 - val_loss: 188.0506\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 160.5288 - val_loss: 193.7405\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 196.2433 - val_loss: 206.1971\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 186.3045 - val_loss: 172.9765\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 188.2136 - val_loss: 393.8767\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 178.1783 - val_loss: 165.3951\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 169.2264 - val_loss: 155.0958\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 175.5548 - val_loss: 258.8072\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 193.8710 - val_loss: 194.7979\n",
      "Epoch 309/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 113us/step - loss: 301.4446 - val_loss: 174.1176\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 273.1356 - val_loss: 212.4725\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 156.2313 - val_loss: 190.8728\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 197.0217 - val_loss: 248.3593\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 184.0251 - val_loss: 218.7075\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 181.6481 - val_loss: 179.0829\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 172.2220 - val_loss: 314.2150\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.7167 - val_loss: 152.6633\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 163.4673 - val_loss: 156.3817\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 230.9495 - val_loss: 166.4365\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 173.6004 - val_loss: 177.2510\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 181.4131 - val_loss: 212.7837\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 167.9547 - val_loss: 173.9300\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.0952 - val_loss: 253.9969\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 178.5495 - val_loss: 166.5653\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 274.1235 - val_loss: 240.6971\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 170.2097 - val_loss: 167.2458\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 155.6564 - val_loss: 161.8526\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 161.6882 - val_loss: 167.5397\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 181.2035 - val_loss: 229.8737\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 158.4772 - val_loss: 168.7671\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 174.5296 - val_loss: 162.5416\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 156.9319 - val_loss: 162.6853\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 167.8637 - val_loss: 211.7738\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 176.0024 - val_loss: 184.7433\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 193.5141 - val_loss: 165.6091\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 153.7315 - val_loss: 180.8544\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 187.7106 - val_loss: 300.2223\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 256.4459 - val_loss: 161.3359\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 174.6107 - val_loss: 171.4046\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.5673 - val_loss: 150.5717\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 165.3460 - val_loss: 157.4178\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.3831 - val_loss: 179.4435\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 163.8129 - val_loss: 241.7057\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 172.4555 - val_loss: 148.8603\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 215.3962 - val_loss: 156.3024\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 158.1271 - val_loss: 240.5230\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 170.2663 - val_loss: 156.5729\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 174.0254 - val_loss: 201.3635\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 194.0296 - val_loss: 160.9494\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 163.1605 - val_loss: 235.5439\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.2398 - val_loss: 150.7032\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 188.1259 - val_loss: 155.5846\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 165.8539 - val_loss: 159.0644\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 168.0878 - val_loss: 250.8525\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 160.6309 - val_loss: 168.6761\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 161.8850 - val_loss: 188.1773\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 159.8479 - val_loss: 178.2405\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 441.4748 - val_loss: 191.0969\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 194.2306 - val_loss: 175.8654\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 160.5535 - val_loss: 289.9392\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 182.5862 - val_loss: 153.6509\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 160.1803 - val_loss: 150.7676\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.0659 - val_loss: 166.7428\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 169.9764 - val_loss: 147.8890\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.6701 - val_loss: 161.8189\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 155.8914 - val_loss: 156.8833\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 156.5730 - val_loss: 151.7234\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.6176 - val_loss: 218.4434\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 173.2765 - val_loss: 192.0427\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 169.6372 - val_loss: 236.6513\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 177.8072 - val_loss: 150.0251\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 155.0031 - val_loss: 226.2390\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 175.6311 - val_loss: 256.5046\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 186.6658 - val_loss: 158.8060\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.9787 - val_loss: 158.6570\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 161.0848 - val_loss: 205.8990\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 191.8350 - val_loss: 171.8697\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 152.9071 - val_loss: 154.0050\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.5188 - val_loss: 174.4750\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.0645 - val_loss: 187.0112\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 188.0925 - val_loss: 159.9374\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.4067 - val_loss: 157.8955\n",
      "Epoch 382/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 161.1600 - val_loss: 281.1464\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 189.0838 - val_loss: 168.5035\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 152.4267 - val_loss: 297.7912\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 155.7000 - val_loss: 192.6905\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 398.3345 - val_loss: 217.5278\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 198.0706 - val_loss: 194.1535\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 184.1326 - val_loss: 149.0357\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.2173 - val_loss: 155.2715\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.4029 - val_loss: 149.8284\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 159.6776 - val_loss: 159.3061\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.9297 - val_loss: 158.3618\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.5940 - val_loss: 164.9175\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 162.5358 - val_loss: 178.6807\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 193.6961 - val_loss: 163.9328\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.3446 - val_loss: 155.3478\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 163.0177 - val_loss: 216.1903\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.9781 - val_loss: 159.9309\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 166.0695 - val_loss: 170.3857\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 213.0874 - val_loss: 201.0988\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.7196 - val_loss: 200.8186\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.8984 - val_loss: 150.5099\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 150.4150 - val_loss: 229.8079\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.9580 - val_loss: 229.2241\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 197.0715 - val_loss: 210.5458\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 158.5823 - val_loss: 167.8715\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.0152 - val_loss: 153.0699\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 157.0357 - val_loss: 145.4116\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.6840 - val_loss: 195.4113\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 174.6118 - val_loss: 193.0763\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.5745 - val_loss: 179.6958\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.9899 - val_loss: 156.3061\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 175.5740 - val_loss: 167.7511\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.7684 - val_loss: 156.7085\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 163.5351 - val_loss: 170.7658\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 166.1966 - val_loss: 856.4145\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 222.1793 - val_loss: 155.2372\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.7107 - val_loss: 217.6761\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.1303 - val_loss: 146.0885\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 151.0872 - val_loss: 147.3857\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.1302 - val_loss: 162.4642\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 181.1409 - val_loss: 166.7947\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.8360 - val_loss: 189.5252\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.5244 - val_loss: 152.3575\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.4637 - val_loss: 155.0809\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.8335 - val_loss: 146.2853\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 207.9532 - val_loss: 202.6559\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 167.3214 - val_loss: 156.8258\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.5833 - val_loss: 159.7414\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.4206 - val_loss: 148.4501\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 180.9124 - val_loss: 144.9717\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 166.6878 - val_loss: 165.5134\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.4567 - val_loss: 151.9984\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.7770 - val_loss: 148.5449\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.7419 - val_loss: 143.7655\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.8331 - val_loss: 149.2847\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.9484 - val_loss: 149.8367\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 207.7550 - val_loss: 166.4221\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 163.3336 - val_loss: 263.6019\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.2943 - val_loss: 185.5755\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.4985 - val_loss: 149.3770\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 145.5515 - val_loss: 149.6709\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.3666 - val_loss: 193.6887\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 165.0100 - val_loss: 199.8016\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.4826 - val_loss: 151.1569\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 247.1225 - val_loss: 306.9708\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 166.6825 - val_loss: 149.6687\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.1707 - val_loss: 190.0154\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.1639 - val_loss: 174.5064\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.9792 - val_loss: 203.0649\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.4716 - val_loss: 278.4730\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.4198 - val_loss: 155.9452\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 181.7797 - val_loss: 159.2468\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.2664 - val_loss: 142.8985\n",
      "Epoch 455/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.3701 - val_loss: 171.8470\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.4157 - val_loss: 163.9612\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.1819 - val_loss: 163.3738\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.7434 - val_loss: 168.3394\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 161.4498 - val_loss: 163.3092\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 177.7274 - val_loss: 157.1736\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 154.9552 - val_loss: 208.1927\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.1439 - val_loss: 171.3711\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.1035 - val_loss: 189.0934\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 201.1602 - val_loss: 149.9694\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 156.3819 - val_loss: 148.0432\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 152.5349 - val_loss: 144.8377\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.1195 - val_loss: 174.4278\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.4510 - val_loss: 143.4984\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.0632 - val_loss: 158.1043\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 224.5703 - val_loss: 147.7809\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 140.6153 - val_loss: 159.5227\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 162.7576 - val_loss: 157.5357\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.3825 - val_loss: 341.7657\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 153.3289 - val_loss: 153.2956\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 173.4020 - val_loss: 146.1081\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 145.0096 - val_loss: 172.2672\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 160.6025 - val_loss: 195.9039\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 571.2134 - val_loss: 211.3999\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 163.0132 - val_loss: 155.8506\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.6673 - val_loss: 219.4963\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.1916 - val_loss: 148.6994\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.4948 - val_loss: 459.7837\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 170.5602 - val_loss: 185.4035\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.6667 - val_loss: 148.3098\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.4449 - val_loss: 144.9213\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.9583 - val_loss: 166.0536\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 165.7014 - val_loss: 154.7762\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.3029 - val_loss: 151.9703\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.1302 - val_loss: 155.6788\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.9993 - val_loss: 171.3238\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 154.0817 - val_loss: 239.5362\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 153.0728 - val_loss: 169.3859\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 189.9483 - val_loss: 143.1814\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 153.0819 - val_loss: 158.3682\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.8291 - val_loss: 146.3480\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.9768 - val_loss: 297.8290\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 202.8716 - val_loss: 149.0658\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.0807 - val_loss: 164.6025\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.7857 - val_loss: 141.0518\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.7411 - val_loss: 150.3561\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 162.1287 - val_loss: 166.3135\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 172.7346 - val_loss: 223.0531\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.8045 - val_loss: 401.4219\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 178.0353 - val_loss: 147.3058\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 143.5174 - val_loss: 141.4324\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.3518 - val_loss: 157.9304\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 175.3632 - val_loss: 285.4414\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.2741 - val_loss: 150.3352\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 186.6335 - val_loss: 266.3005\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 151.9991 - val_loss: 149.9647\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.7341 - val_loss: 233.9539\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.1326 - val_loss: 162.7182\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.8425 - val_loss: 145.5178\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.7496 - val_loss: 170.2712\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.5605 - val_loss: 154.8123\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 196.1958 - val_loss: 381.0589\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.9761 - val_loss: 147.8318\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.5188 - val_loss: 161.7405\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.9666 - val_loss: 157.5647\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.8905 - val_loss: 203.4436\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 142.5202 - val_loss: 221.9906\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 206.2328 - val_loss: 144.5209\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 163.2430 - val_loss: 200.8592\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.1629 - val_loss: 180.5836\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 186.0946 - val_loss: 147.6670\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 147.7040 - val_loss: 171.4306\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.6358 - val_loss: 171.5766\n",
      "Epoch 528/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.6658 - val_loss: 351.0766\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 156.2199 - val_loss: 145.0141\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 138.6262 - val_loss: 167.9757\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 290.6341 - val_loss: 156.0932\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.3511 - val_loss: 295.2122\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.0662 - val_loss: 154.3970\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.1448 - val_loss: 140.3597\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.4691 - val_loss: 153.2276\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.6974 - val_loss: 151.4750\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.5916 - val_loss: 185.2229\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.1036 - val_loss: 155.5701\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.8228 - val_loss: 200.5004\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 181.1234 - val_loss: 151.4202\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.7102 - val_loss: 149.6705\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 154.4137 - val_loss: 217.2924\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.1505 - val_loss: 146.5372\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 419.7918 - val_loss: 458.3140\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 221.7342 - val_loss: 176.9382\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 241.2495 - val_loss: 216.1407\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 210.9899 - val_loss: 189.6893\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 175.6113 - val_loss: 164.6335\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 186.0375 - val_loss: 155.4064\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 154.4917 - val_loss: 174.4356\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 165.7797 - val_loss: 163.7504\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.1698 - val_loss: 157.8349\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 193.4691 - val_loss: 156.0923\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 162.3497 - val_loss: 160.3181\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 162.5506 - val_loss: 277.7586\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 180.3741 - val_loss: 242.0732\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 242.5849 - val_loss: 181.0043\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.0682 - val_loss: 191.4568\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 154.1937 - val_loss: 200.2934\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 164.3405 - val_loss: 196.2378\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 154.3701 - val_loss: 179.2464\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.2839 - val_loss: 212.0859\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.7158 - val_loss: 154.3204\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 165.1135 - val_loss: 162.9852\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 204.9131 - val_loss: 212.0000\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 201.1170 - val_loss: 165.9116\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.9759 - val_loss: 187.6743\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 165.7812 - val_loss: 190.9442\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 159.9452 - val_loss: 148.8227\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 151.4472 - val_loss: 169.5605\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 171.4321 - val_loss: 307.3895\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.3553 - val_loss: 191.9424\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 144.3989 - val_loss: 157.6284\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 164.3183 - val_loss: 148.1094\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 170.0474 - val_loss: 174.6435\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 149.4106 - val_loss: 154.3721\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.0506 - val_loss: 149.8704\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 153.7073 - val_loss: 159.1739\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 162.3528 - val_loss: 171.8024\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.1578 - val_loss: 169.0750\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 158.9206 - val_loss: 145.1917\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.4339 - val_loss: 186.0131\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 167.9156 - val_loss: 200.4189\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.8244 - val_loss: 141.0326\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.6914 - val_loss: 172.2456\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 155.1790 - val_loss: 147.1987\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.6160 - val_loss: 156.0735\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.7198 - val_loss: 160.8159\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.8763 - val_loss: 154.9116\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 163.3274 - val_loss: 142.3454\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.9328 - val_loss: 164.5729\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.7876 - val_loss: 160.9292\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 160.7947 - val_loss: 154.3271\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 145.6398 - val_loss: 171.0267\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 257.6984 - val_loss: 241.5444\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 161.1235 - val_loss: 163.7563\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.3557 - val_loss: 162.8937\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 157.9706 - val_loss: 177.7344\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.1580 - val_loss: 142.1704\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 138.4058 - val_loss: 181.8714\n",
      "Epoch 601/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.8282 - val_loss: 149.9111\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 146.9184 - val_loss: 143.1021\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.8267 - val_loss: 168.6486\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.3908 - val_loss: 223.0030\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 150.3189 - val_loss: 143.7067\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.6484 - val_loss: 140.5492\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.9819 - val_loss: 152.4436\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.5273 - val_loss: 214.1255\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.3415 - val_loss: 166.7249\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.3056 - val_loss: 181.1408\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 147.3082 - val_loss: 374.7522\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 168.0673 - val_loss: 221.4465\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.6387 - val_loss: 182.2021\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 160.6612 - val_loss: 160.3367\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.1953 - val_loss: 145.8883\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.9642 - val_loss: 155.2529\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.8862 - val_loss: 219.1555\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 203.0647 - val_loss: 176.4599\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.0837 - val_loss: 143.6766\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 152.1311 - val_loss: 148.6523\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 208.5747 - val_loss: 198.5033\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 228.4956 - val_loss: 217.9565\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.9993 - val_loss: 143.9907\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.4030 - val_loss: 209.8567\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.4429 - val_loss: 154.9747\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 142.1444 - val_loss: 150.9071\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.3154 - val_loss: 393.9262\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.6406 - val_loss: 143.3275\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.3295 - val_loss: 157.9270\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 143.9940 - val_loss: 150.0872\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.2796 - val_loss: 148.4265\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 149.9992 - val_loss: 158.6118\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.8301 - val_loss: 151.2456\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.0142 - val_loss: 171.3648\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 148.5546 - val_loss: 157.2130\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 143.0327 - val_loss: 256.6289\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 149.6464 - val_loss: 181.8993\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.6121 - val_loss: 182.5104\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 168.2917 - val_loss: 145.9633\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.6839 - val_loss: 146.7757\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.5340 - val_loss: 147.3456\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.9011 - val_loss: 155.9941\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 176.5163 - val_loss: 212.8884\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 164.0713 - val_loss: 184.1430\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 144.3604 - val_loss: 173.8495\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.8115 - val_loss: 212.9201\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.3915 - val_loss: 142.6436\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.1159 - val_loss: 150.4463\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.6265 - val_loss: 145.8799\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.9277 - val_loss: 145.4835\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.6589 - val_loss: 178.6053\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.4174 - val_loss: 178.1212\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.7403 - val_loss: 213.0336\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.5467 - val_loss: 167.3897\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 145.7460 - val_loss: 177.2317\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 169.0760 - val_loss: 152.1546\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.7931 - val_loss: 173.4553\n",
      "Epoch 658/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.9495 - val_loss: 145.1416\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.9780 - val_loss: 147.4709\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 139.7025 - val_loss: 192.8591\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.8040 - val_loss: 149.2902\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.7315 - val_loss: 181.9427\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 231.6118 - val_loss: 237.3774\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 151.6235 - val_loss: 154.4594\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.8163 - val_loss: 226.4503\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 151.9282 - val_loss: 139.7776\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.7127 - val_loss: 152.8021\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.0067 - val_loss: 264.2168\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 135.5648 - val_loss: 163.3790\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.2307 - val_loss: 168.1584\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 189.5947 - val_loss: 182.7633\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.4014 - val_loss: 148.5796\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.6940 - val_loss: 190.7800\n",
      "Epoch 674/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 138.3084 - val_loss: 149.3965\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.6362 - val_loss: 148.8554\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 136.8122 - val_loss: 156.3223\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.5968 - val_loss: 146.4863\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 351.8270 - val_loss: 141.2192\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.0576 - val_loss: 212.1876\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.4026 - val_loss: 170.1325\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.0465 - val_loss: 185.3359\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.8941 - val_loss: 143.5451\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.3369 - val_loss: 373.3911\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 266.5972 - val_loss: 153.3753\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.9831 - val_loss: 162.9053\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.4926 - val_loss: 144.7594\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 139.6840 - val_loss: 145.7626\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.2658 - val_loss: 169.5847\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.7981 - val_loss: 152.3365\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.1671 - val_loss: 223.1338\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.4626 - val_loss: 147.7900\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 170.3509 - val_loss: 155.3649\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.3335 - val_loss: 149.2053\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.8810 - val_loss: 163.9553\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.6333 - val_loss: 140.1671\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.1425 - val_loss: 142.6729\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 136.9511 - val_loss: 227.3458\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.6873 - val_loss: 186.0208\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.2970 - val_loss: 147.3132\n",
      "Epoch 700/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 172.9989 - val_loss: 141.4612\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.5687 - val_loss: 151.4411\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.7912 - val_loss: 141.4531\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 140.5974 - val_loss: 149.8625\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.8477 - val_loss: 145.7375\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.9156 - val_loss: 149.6160\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.9405 - val_loss: 146.8871\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.2770 - val_loss: 145.8108\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 188.8640 - val_loss: 159.8317\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.3838 - val_loss: 143.9386\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.9287 - val_loss: 143.5656\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.3684 - val_loss: 145.5514\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.4942 - val_loss: 143.4571\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.0065 - val_loss: 142.3269\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 155.9593 - val_loss: 186.9052\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.2852 - val_loss: 189.8664\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 147.3551 - val_loss: 168.5197\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 139.4559 - val_loss: 322.9134\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.4000 - val_loss: 150.4290\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.2823 - val_loss: 170.2712\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.4812 - val_loss: 157.9272\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.3294 - val_loss: 274.9571\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.6570 - val_loss: 154.5445\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 317.1146 - val_loss: 2174.1049\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 240.9631 - val_loss: 165.2427\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 158.5311 - val_loss: 363.1418\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.0549 - val_loss: 151.5260\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.7533 - val_loss: 150.3545\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.2369 - val_loss: 159.6433\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 181.3333 - val_loss: 215.3728\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.4879 - val_loss: 150.2893\n",
      "Epoch 731/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.5426 - val_loss: 174.6446\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.8969 - val_loss: 156.6650\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.2184 - val_loss: 168.9510\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.2389 - val_loss: 193.9837\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.4840 - val_loss: 181.7706\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.4626 - val_loss: 147.3196\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 156.2382 - val_loss: 141.1600\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 130.1537 - val_loss: 143.0302\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.2619 - val_loss: 144.8674\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.0566 - val_loss: 147.3321\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.7093 - val_loss: 143.0475\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 496.4552 - val_loss: 244.8386\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 175.1763 - val_loss: 192.7418\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.4613 - val_loss: 161.9701\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.9573 - val_loss: 165.2809\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.1873 - val_loss: 156.0716\n",
      "Epoch 747/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.2940 - val_loss: 173.4074\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.5781 - val_loss: 164.3929\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.4120 - val_loss: 147.8230\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 139.4216 - val_loss: 145.5061\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 152.0600 - val_loss: 178.0948\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.0327 - val_loss: 153.3567\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.0353 - val_loss: 196.7195\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.7675 - val_loss: 161.3643\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 161.2140 - val_loss: 165.4970\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 139.7094 - val_loss: 156.2198\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.4423 - val_loss: 166.7516\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.4234 - val_loss: 185.2323\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.8349 - val_loss: 152.3469\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.7754 - val_loss: 147.2078\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 166.4356 - val_loss: 160.5656\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.9975 - val_loss: 153.8839\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.2818 - val_loss: 251.8975\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.4741 - val_loss: 162.4169\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 139.4177 - val_loss: 145.8068\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.1754 - val_loss: 154.0528\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.0434 - val_loss: 152.6572\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.3918 - val_loss: 151.7010\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.8940 - val_loss: 178.9294\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 150.5253 - val_loss: 145.5114\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 202.0830 - val_loss: 202.7643\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.3783 - val_loss: 180.0727\n",
      "Epoch 773/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.3119 - val_loss: 143.4690\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 140.2713 - val_loss: 142.5782\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.3451 - val_loss: 181.0628\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.6344 - val_loss: 157.5234\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 131.4638 - val_loss: 148.9370\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.2860 - val_loss: 142.1318\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.9882 - val_loss: 140.1481\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 147.6426 - val_loss: 312.8347\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.5715 - val_loss: 144.2247\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.5911 - val_loss: 150.0860\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 151.2018 - val_loss: 147.4727\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 150.9969 - val_loss: 168.2552\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.4687 - val_loss: 153.1039\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 157.8630 - val_loss: 227.0719\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.4915 - val_loss: 163.3762\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.0744 - val_loss: 179.0477\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.0732 - val_loss: 180.3222\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.8791 - val_loss: 143.1227\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 185.9258 - val_loss: 154.7274\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.5006 - val_loss: 201.9293\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 140.3981 - val_loss: 277.7768\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 144.6479 - val_loss: 146.8465\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 127.4746 - val_loss: 157.3829\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 136.7406 - val_loss: 155.4467\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.9656 - val_loss: 145.5268\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 133.4159 - val_loss: 140.6892\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.2842 - val_loss: 147.3615\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.5827 - val_loss: 169.3120\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 150.1466 - val_loss: 143.6695\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.7093 - val_loss: 149.1770\n",
      "Epoch 803/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 146.5117 - val_loss: 173.3246\n",
      "Epoch 804/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.9164 - val_loss: 141.1187\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 138.3052 - val_loss: 142.7194\n",
      "Epoch 806/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 149.7160 - val_loss: 198.1815\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 229.6111 - val_loss: 154.3518\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.9153 - val_loss: 144.4376\n",
      "Epoch 809/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 128.0232 - val_loss: 153.6476\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.8597 - val_loss: 141.8728\n",
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.8734 - val_loss: 142.2724\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.9733 - val_loss: 146.5139\n",
      "Epoch 813/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.9959 - val_loss: 161.3479\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.3591 - val_loss: 144.1048\n",
      "Epoch 815/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.0836 - val_loss: 161.1657\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.6591 - val_loss: 153.6774\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.5465 - val_loss: 160.7710\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.9102 - val_loss: 149.2827\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 133.7414 - val_loss: 140.7136\n",
      "Epoch 820/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.1685 - val_loss: 144.0795\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 135.1800 - val_loss: 167.9841\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 182.4695 - val_loss: 149.9734\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.8896 - val_loss: 142.8955\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 135.0679 - val_loss: 143.9479\n",
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.7598 - val_loss: 172.7230\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.1902 - val_loss: 167.4777\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.1434 - val_loss: 153.6226\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 154.8870 - val_loss: 152.5496\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.3990 - val_loss: 142.0666\n",
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 347.3249 - val_loss: 153.7566\n",
      "Epoch 831/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 165.8353 - val_loss: 149.1558\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.0712 - val_loss: 146.4014\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.8348 - val_loss: 140.7143\n",
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.8118 - val_loss: 165.6922\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.8391 - val_loss: 188.5217\n",
      "Epoch 836/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.1520 - val_loss: 145.3664\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.8749 - val_loss: 187.8404\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.9725 - val_loss: 176.8134\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 266.2237 - val_loss: 151.0668\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.2799 - val_loss: 159.2110\n",
      "Epoch 841/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.0596 - val_loss: 188.8405\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.9410 - val_loss: 150.1418\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.9018 - val_loss: 141.2712\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.1951 - val_loss: 170.3650\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.0879 - val_loss: 259.0698\n",
      "Epoch 846/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.3794 - val_loss: 159.3780\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.6176 - val_loss: 149.5568\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.9328 - val_loss: 174.5475\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.3708 - val_loss: 193.3255\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 132.1607 - val_loss: 165.5211\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.0758 - val_loss: 143.4295\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.3777 - val_loss: 162.6103\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 159.5639 - val_loss: 241.2994\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 206.3950 - val_loss: 167.4000\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 128.4400 - val_loss: 143.2122\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.8447 - val_loss: 150.4448\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.5342 - val_loss: 161.9827\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.7543 - val_loss: 178.1605\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.0505 - val_loss: 174.5209\n",
      "Epoch 860/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.6992 - val_loss: 150.0233\n",
      "Epoch 861/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.3479 - val_loss: 144.8758\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.9196 - val_loss: 142.1371\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 335.8304 - val_loss: 526.5238\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 279.1575 - val_loss: 204.0821\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 220.0701 - val_loss: 214.3667\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 185.5818 - val_loss: 851.4404\n",
      "Epoch 867/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 186.5968 - val_loss: 196.0432\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 160.3060 - val_loss: 178.8151\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 171.5939 - val_loss: 156.8588\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 161.4589 - val_loss: 170.8466\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 166.1326 - val_loss: 151.8381\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 176.7060 - val_loss: 160.2157\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 171.8605 - val_loss: 156.5253\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.4228 - val_loss: 152.7954\n",
      "Epoch 875/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 151.1955 - val_loss: 160.1504\n",
      "Epoch 876/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 164.2431 - val_loss: 170.0596\n",
      "Epoch 877/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 154.6779 - val_loss: 325.6213\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.9282 - val_loss: 157.7650\n",
      "Epoch 879/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 160.0550 - val_loss: 229.8045\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.9507 - val_loss: 156.8095\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.8734 - val_loss: 197.0391\n",
      "Epoch 882/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.4868 - val_loss: 170.7372\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 155.9678 - val_loss: 224.9434\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 163.4540 - val_loss: 153.2445\n",
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 156.6919 - val_loss: 154.0547\n",
      "Epoch 886/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.1066 - val_loss: 144.2725\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.8417 - val_loss: 163.8865\n",
      "Epoch 888/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.4475 - val_loss: 148.6915\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.4217 - val_loss: 160.1657\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.7305 - val_loss: 191.9278\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.5868 - val_loss: 170.0979\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.1789 - val_loss: 161.3444\n",
      "Epoch 893/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.7855 - val_loss: 218.4796\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 154.8128 - val_loss: 163.9943\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.8594 - val_loss: 150.3173\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.4479 - val_loss: 140.8417\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.9908 - val_loss: 157.6628\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 184.0340 - val_loss: 175.6372\n",
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.0616 - val_loss: 147.7503\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.2101 - val_loss: 158.6384\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 135.4535 - val_loss: 188.8211\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.4804 - val_loss: 194.0440\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 156.0070 - val_loss: 187.6369\n",
      "Epoch 904/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.5426 - val_loss: 149.7203\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.8078 - val_loss: 176.8118\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.4444 - val_loss: 212.5268\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.8289 - val_loss: 153.9122\n",
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 157.4414 - val_loss: 164.7114\n",
      "Epoch 909/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.9040 - val_loss: 145.9288\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.2196 - val_loss: 162.6618\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.1633 - val_loss: 141.7145\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.0617 - val_loss: 156.4215\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.2030 - val_loss: 155.0019\n",
      "Epoch 914/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 161.6372 - val_loss: 148.4793\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.9719 - val_loss: 174.4577\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 157.2512 - val_loss: 142.9871\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.5163 - val_loss: 170.8009\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.5219 - val_loss: 155.4915\n",
      "Epoch 919/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.2499 - val_loss: 155.6003\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.8736 - val_loss: 177.1584\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 150.9944 - val_loss: 197.2516\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.4583 - val_loss: 164.0208\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 136.4298 - val_loss: 231.5446\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.4518 - val_loss: 143.3499\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.3511 - val_loss: 157.8965\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.9371 - val_loss: 151.7955\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.7980 - val_loss: 144.1641\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.3150 - val_loss: 172.9943\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 192.7918 - val_loss: 142.6807\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.1179 - val_loss: 140.9486\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.7310 - val_loss: 146.6868\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.2665 - val_loss: 163.9962\n",
      "Epoch 933/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 148.6698 - val_loss: 145.7623\n",
      "Epoch 934/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.4225 - val_loss: 156.0083\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.9297 - val_loss: 151.6063\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.2471 - val_loss: 141.5106\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 157.2154 - val_loss: 209.2015\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.9538 - val_loss: 148.5029\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.1781 - val_loss: 142.7686\n",
      "Epoch 940/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.4243 - val_loss: 155.2063\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.6732 - val_loss: 141.8708\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.8078 - val_loss: 339.4332\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.1971 - val_loss: 151.8561\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.2324 - val_loss: 148.1028\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.9146 - val_loss: 249.8610\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 164.0654 - val_loss: 205.1799\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.4271 - val_loss: 182.5335\n",
      "Epoch 948/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 202.8763 - val_loss: 169.2222\n",
      "Epoch 949/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.5970 - val_loss: 145.2476\n",
      "Epoch 950/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.0248 - val_loss: 275.7029\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.7714 - val_loss: 146.2337\n",
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 136.5900 - val_loss: 154.6728\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 143.8884 - val_loss: 164.4769\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.7332 - val_loss: 160.6647\n",
      "Epoch 955/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.5333 - val_loss: 254.7766\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 147.3693 - val_loss: 234.8813\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.0691 - val_loss: 170.9581\n",
      "Epoch 958/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.4854 - val_loss: 149.2657\n",
      "Epoch 959/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 168.2229 - val_loss: 161.3519\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.3197 - val_loss: 154.5831\n",
      "Epoch 961/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.4609 - val_loss: 143.8465\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.8056 - val_loss: 158.9889\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 154.4258 - val_loss: 171.4998\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 154.2056 - val_loss: 147.8517\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.4912 - val_loss: 145.9251\n",
      "Epoch 966/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.1037 - val_loss: 152.5754\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.1271 - val_loss: 162.7142\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 136.5575 - val_loss: 139.8328\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.7798 - val_loss: 147.2641\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.2962 - val_loss: 266.9657\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.1174 - val_loss: 241.4463\n",
      "Epoch 972/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 133.5878 - val_loss: 144.0496\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.2786 - val_loss: 181.1998\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 170.6229 - val_loss: 161.9720\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.1302 - val_loss: 160.8128\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.4193 - val_loss: 145.0015\n",
      "Epoch 977/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.8151 - val_loss: 146.7719\n",
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.0571 - val_loss: 139.6122\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 128.0998 - val_loss: 152.8416\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.1793 - val_loss: 190.5449\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.4957 - val_loss: 260.2509\n",
      "Epoch 982/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.1589 - val_loss: 208.6297\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.5261 - val_loss: 181.7286\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.2893 - val_loss: 181.6107\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.5736 - val_loss: 156.6816\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.9576 - val_loss: 157.1758\n",
      "Epoch 987/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.9063 - val_loss: 144.0637\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.1091 - val_loss: 144.1503\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 152.6687 - val_loss: 170.7264\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 147.7178 - val_loss: 138.0214\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.9578 - val_loss: 147.9424\n",
      "Epoch 992/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 176.8602 - val_loss: 351.6029\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 165.1712 - val_loss: 153.9695\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.6307 - val_loss: 146.5995\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 147.4540 - val_loss: 216.9806\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 130.9744 - val_loss: 160.8263\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.2978 - val_loss: 195.8229\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 139.4094 - val_loss: 149.8820\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.4778 - val_loss: 233.4184\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 242.3720 - val_loss: 153.6565\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.9974 - val_loss: 167.3001\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.7065 - val_loss: 145.8999\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 163.7433 - val_loss: 191.0737\n",
      "Epoch 1004/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 133.0299 - val_loss: 142.6701\n",
      "Epoch 1005/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.7561 - val_loss: 137.3447\n",
      "Epoch 1006/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.5286 - val_loss: 149.5723\n",
      "Epoch 1007/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.3830 - val_loss: 138.1879\n",
      "Epoch 1008/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.1597 - val_loss: 145.8300\n",
      "Epoch 1009/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.2115 - val_loss: 144.4352\n",
      "Epoch 1010/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.8188 - val_loss: 153.8950\n",
      "Epoch 1011/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 194.2227 - val_loss: 165.3090\n",
      "Epoch 1012/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 155.2389 - val_loss: 156.6349\n",
      "Epoch 1013/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.4796 - val_loss: 187.1242\n",
      "Epoch 1014/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.5348 - val_loss: 166.3296\n",
      "Epoch 1015/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.5901 - val_loss: 147.5710\n",
      "Epoch 1016/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 136.4690 - val_loss: 154.1039\n",
      "Epoch 1017/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 152.9844 - val_loss: 147.5916\n",
      "Epoch 1018/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.9027 - val_loss: 139.5370\n",
      "Epoch 1019/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.4107 - val_loss: 174.9025\n",
      "Epoch 1020/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.0049 - val_loss: 217.3455\n",
      "Epoch 1021/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.1547 - val_loss: 155.6954\n",
      "Epoch 1022/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.6901 - val_loss: 174.4166\n",
      "Epoch 1023/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.6317 - val_loss: 179.6874\n",
      "Epoch 1024/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.5014 - val_loss: 136.6151\n",
      "Epoch 1025/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 164.3483 - val_loss: 189.9552\n",
      "Epoch 1026/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.1483 - val_loss: 146.9887\n",
      "Epoch 1027/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.8056 - val_loss: 143.9537\n",
      "Epoch 1028/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.1846 - val_loss: 179.6945\n",
      "Epoch 1029/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 139.1210 - val_loss: 161.3664\n",
      "Epoch 1030/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 221.2621 - val_loss: 6093.1516\n",
      "Epoch 1031/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 720.8701 - val_loss: 225.1831\n",
      "Epoch 1032/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 222.2167 - val_loss: 311.6550\n",
      "Epoch 1033/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 207.5744 - val_loss: 241.0931\n",
      "Epoch 1034/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 189.4023 - val_loss: 241.6160\n",
      "Epoch 1035/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 162.3049 - val_loss: 170.1827\n",
      "Epoch 1036/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 165.6928 - val_loss: 242.3506\n",
      "Epoch 1037/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 157.4850 - val_loss: 184.5490\n",
      "Epoch 1038/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 182.3635 - val_loss: 178.5623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1039/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 161.8509 - val_loss: 181.1661\n",
      "Epoch 1040/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 161.3250 - val_loss: 160.5378\n",
      "Epoch 1041/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 158.9568 - val_loss: 159.3017\n",
      "Epoch 1042/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 159.1082 - val_loss: 154.2059\n",
      "Epoch 1043/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 192.5409 - val_loss: 213.6056\n",
      "Epoch 1044/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 159.9374 - val_loss: 256.0189\n",
      "Epoch 1045/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 166.2434 - val_loss: 224.2092\n",
      "Epoch 1046/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 150.9370 - val_loss: 193.3425\n",
      "Epoch 1047/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 173.2396 - val_loss: 149.9570\n",
      "Epoch 1048/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.4570 - val_loss: 157.9288\n",
      "Epoch 1049/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.8006 - val_loss: 429.3511\n",
      "Epoch 1050/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 156.7406 - val_loss: 150.6947\n",
      "Epoch 1051/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.2398 - val_loss: 156.4784\n",
      "Epoch 1052/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 459.9191 - val_loss: 234.7062\n",
      "Epoch 1053/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 180.7102 - val_loss: 176.5141\n",
      "Epoch 1054/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 163.6548 - val_loss: 235.2068\n",
      "Epoch 1055/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.3745 - val_loss: 201.1084\n",
      "Epoch 1056/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 160.2967 - val_loss: 169.5495\n",
      "Epoch 1057/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.3409 - val_loss: 160.4700\n",
      "Epoch 1058/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 174.7878 - val_loss: 187.0838\n",
      "Epoch 1059/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.4990 - val_loss: 213.7288\n",
      "Epoch 1060/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.5242 - val_loss: 151.7693\n",
      "Epoch 1061/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.5040 - val_loss: 148.0963\n",
      "Epoch 1062/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.9427 - val_loss: 156.3890\n",
      "Epoch 1063/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.1267 - val_loss: 169.0422\n",
      "Epoch 1064/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.8753 - val_loss: 152.9887\n",
      "Epoch 1065/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 151.6206 - val_loss: 378.0605\n",
      "Epoch 1066/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 154.2737 - val_loss: 187.9066\n",
      "Epoch 1067/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.9361 - val_loss: 158.4476\n",
      "Epoch 1068/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.9132 - val_loss: 155.0881\n",
      "Epoch 1069/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 164.4991 - val_loss: 156.8557\n",
      "Epoch 1070/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 137.3014 - val_loss: 150.2680\n",
      "Epoch 1071/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.5343 - val_loss: 155.2199\n",
      "Epoch 1072/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.0834 - val_loss: 231.9114\n",
      "Epoch 1073/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.4083 - val_loss: 173.0788\n",
      "Epoch 1074/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.2003 - val_loss: 159.4384\n",
      "Epoch 1075/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.9121 - val_loss: 162.6524\n",
      "Epoch 1076/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.0632 - val_loss: 189.8827\n",
      "Epoch 1077/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.0464 - val_loss: 159.0845\n",
      "Epoch 1078/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 149.7051 - val_loss: 142.9801\n",
      "Epoch 1079/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.1014 - val_loss: 143.6523\n",
      "Epoch 1080/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.0609 - val_loss: 143.5726\n",
      "Epoch 1081/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 158.5265 - val_loss: 169.8293\n",
      "Epoch 1082/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.4013 - val_loss: 140.4540\n",
      "Epoch 1083/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.5253 - val_loss: 154.1960\n",
      "Epoch 1084/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.2191 - val_loss: 145.3588\n",
      "Epoch 1085/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 134.7329 - val_loss: 185.3284\n",
      "Epoch 1086/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 170.2831 - val_loss: 167.5644\n",
      "Epoch 1087/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.7572 - val_loss: 156.4984\n",
      "Epoch 1088/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 147.2688 - val_loss: 142.6160\n",
      "Epoch 1089/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.0059 - val_loss: 143.6400\n",
      "Epoch 1090/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.2484 - val_loss: 142.4881\n",
      "Epoch 1091/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.0797 - val_loss: 150.8050\n",
      "Epoch 1092/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.3393 - val_loss: 143.8480\n",
      "Epoch 1093/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.3731 - val_loss: 148.2354\n",
      "Epoch 1094/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 137.7940 - val_loss: 149.9807\n",
      "Epoch 1095/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.7191 - val_loss: 141.8695\n",
      "Epoch 1096/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.7306 - val_loss: 172.7832\n",
      "Epoch 1097/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 159.1468 - val_loss: 240.6791\n",
      "Epoch 1098/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.1682 - val_loss: 156.4455\n",
      "Epoch 1099/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.9294 - val_loss: 217.2313\n",
      "Epoch 1100/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.4679 - val_loss: 151.1330\n",
      "Epoch 1101/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.6945 - val_loss: 188.7437\n",
      "Epoch 1102/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.1250 - val_loss: 149.2383\n",
      "Epoch 1103/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.6727 - val_loss: 162.2425\n",
      "Epoch 1104/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.7933 - val_loss: 151.1653\n",
      "Epoch 1105/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.8549 - val_loss: 151.1479\n",
      "Epoch 1106/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.1864 - val_loss: 161.7771\n",
      "Epoch 1107/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.8829 - val_loss: 142.5512\n",
      "Epoch 1108/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 143.5366 - val_loss: 159.4618- ETA: 0s - loss: 14\n",
      "Epoch 1109/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 140.0495 - val_loss: 158.3532\n",
      "Epoch 1110/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 138.6865 - val_loss: 141.7760\n",
      "Epoch 1111/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 55us/step - loss: 143.8214 - val_loss: 143.7196\n",
      "Epoch 1112/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 135.8368 - val_loss: 184.2214\n",
      "Epoch 1113/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 136.8019 - val_loss: 142.7031\n",
      "Epoch 1114/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 134.9879 - val_loss: 163.0920\n",
      "Epoch 1115/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.8697 - val_loss: 151.9029\n",
      "Epoch 1116/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.6988 - val_loss: 159.6825\n",
      "Epoch 1117/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.6124 - val_loss: 150.8949\n",
      "Epoch 1118/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.2445 - val_loss: 159.6202\n",
      "Epoch 1119/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.3458 - val_loss: 182.1222\n",
      "Epoch 1120/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.8299 - val_loss: 154.5653\n",
      "Epoch 1121/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.7247 - val_loss: 144.2085\n",
      "Epoch 1122/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.9434 - val_loss: 166.7969\n",
      "Epoch 1123/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.5331 - val_loss: 141.0798\n",
      "Epoch 1124/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.2803 - val_loss: 221.3971\n",
      "Epoch 1125/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.3382 - val_loss: 140.1642\n",
      "Epoch 1126/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.7646 - val_loss: 145.7896\n",
      "Epoch 1127/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.2329 - val_loss: 144.0593\n",
      "Epoch 1128/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.7889 - val_loss: 170.4779\n",
      "Epoch 1129/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.1128 - val_loss: 268.7964\n",
      "Epoch 1130/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.1018 - val_loss: 141.2127\n",
      "Epoch 1131/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.0626 - val_loss: 211.1218\n",
      "Epoch 1132/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.5287 - val_loss: 157.3663\n",
      "Epoch 1133/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.8703 - val_loss: 174.1864\n",
      "Epoch 1134/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.3599 - val_loss: 148.0423\n",
      "Epoch 1135/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.5928 - val_loss: 146.6867\n",
      "Epoch 1136/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.9469 - val_loss: 152.5500\n",
      "Epoch 1137/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 134.7175 - val_loss: 178.6277\n",
      "Epoch 1138/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.8679 - val_loss: 149.5381\n",
      "Epoch 1139/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 144.4734 - val_loss: 153.0906\n",
      "Epoch 1140/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 151.1379 - val_loss: 172.1374\n",
      "Epoch 1141/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.9052 - val_loss: 146.8464\n",
      "Epoch 1142/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.8086 - val_loss: 158.5184\n",
      "Epoch 1143/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.5315 - val_loss: 186.3944\n",
      "Epoch 1144/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.3567 - val_loss: 153.5457\n",
      "Epoch 1145/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.1727 - val_loss: 150.3161\n",
      "Epoch 1146/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.3069 - val_loss: 215.3808\n",
      "Epoch 1147/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.3662 - val_loss: 178.0303\n",
      "Epoch 1148/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 160.4455 - val_loss: 203.8454\n",
      "Epoch 1149/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.2902 - val_loss: 169.8968\n",
      "Epoch 1150/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.6253 - val_loss: 157.0338\n",
      "Epoch 1151/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.7761 - val_loss: 183.0626\n",
      "Epoch 1152/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.1443 - val_loss: 146.9287\n",
      "Epoch 1153/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 131.6806 - val_loss: 142.2001\n",
      "Epoch 1154/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.2272 - val_loss: 191.0448\n",
      "Epoch 1155/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.9235 - val_loss: 170.6726\n",
      "Epoch 1156/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.8549 - val_loss: 167.6476\n",
      "Epoch 1157/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.8967 - val_loss: 143.3597\n",
      "Epoch 1158/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.2511 - val_loss: 226.3329\n",
      "Epoch 1159/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.3203 - val_loss: 165.2851\n",
      "Epoch 1160/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.5244 - val_loss: 166.5716\n",
      "Epoch 1161/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.2423 - val_loss: 147.7008\n",
      "Epoch 1162/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.9540 - val_loss: 154.5392\n",
      "Epoch 1163/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.8947 - val_loss: 171.4902\n",
      "Epoch 1164/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.5609 - val_loss: 146.2900\n",
      "Epoch 1165/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.5408 - val_loss: 171.7351\n",
      "Epoch 1166/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 151.7653 - val_loss: 144.8306\n",
      "Epoch 1167/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.4275 - val_loss: 145.9864\n",
      "Epoch 1168/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.1467 - val_loss: 146.9270\n",
      "Epoch 1169/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.3626 - val_loss: 169.4022\n",
      "Epoch 1170/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 131.2116 - val_loss: 144.7423\n",
      "Epoch 1171/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.1106 - val_loss: 156.7785\n",
      "Epoch 1172/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.5901 - val_loss: 148.4414\n",
      "Epoch 1173/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.4426 - val_loss: 136.4245\n",
      "Epoch 1174/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.2265 - val_loss: 147.7747\n",
      "Epoch 1175/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 142.4879 - val_loss: 251.5859\n",
      "Epoch 1176/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.8243 - val_loss: 164.3756\n",
      "Epoch 1177/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.5960 - val_loss: 161.7848\n",
      "Epoch 1178/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.8074 - val_loss: 140.9044\n",
      "Epoch 1179/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.7107 - val_loss: 148.9052\n",
      "Epoch 1180/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 164.1848 - val_loss: 321.4185\n",
      "Epoch 1181/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.4548 - val_loss: 148.3225\n",
      "Epoch 1182/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 182.2245 - val_loss: 169.1338\n",
      "Epoch 1183/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.6533 - val_loss: 209.1618\n",
      "Epoch 1184/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.6372 - val_loss: 163.6789\n",
      "Epoch 1185/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 342.2839 - val_loss: 2108.7031\n",
      "Epoch 1186/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 438.2684 - val_loss: 246.7978\n",
      "Epoch 1187/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 220.8615 - val_loss: 183.6448\n",
      "Epoch 1188/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 181.0838 - val_loss: 176.6144\n",
      "Epoch 1189/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 178.2076 - val_loss: 199.7608\n",
      "Epoch 1190/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 162.7142 - val_loss: 163.5033\n",
      "Epoch 1191/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 154.0273 - val_loss: 170.9043\n",
      "Epoch 1192/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 149.6989 - val_loss: 154.4629\n",
      "Epoch 1193/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.7531 - val_loss: 160.5011\n",
      "Epoch 1194/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 156.7814 - val_loss: 181.8469\n",
      "Epoch 1195/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 157.1449 - val_loss: 160.0560\n",
      "Epoch 1196/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 160.9553 - val_loss: 165.2074\n",
      "Epoch 1197/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.3906 - val_loss: 200.3007\n",
      "Epoch 1198/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.7571 - val_loss: 176.4853\n",
      "Epoch 1199/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 171.9503 - val_loss: 160.8966\n",
      "Epoch 1200/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.5208 - val_loss: 153.4961\n",
      "Epoch 1201/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.2481 - val_loss: 150.6191\n",
      "Epoch 1202/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 156.3517 - val_loss: 148.6556\n",
      "Epoch 1203/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.2191 - val_loss: 161.8417\n",
      "Epoch 1204/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.1106 - val_loss: 174.2943\n",
      "Epoch 1205/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.0281 - val_loss: 255.1171\n",
      "Epoch 1206/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.4990 - val_loss: 154.6182\n",
      "Epoch 1207/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.8584 - val_loss: 155.5523\n",
      "Epoch 1208/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.0333 - val_loss: 145.5722\n",
      "Epoch 1209/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 212.7302 - val_loss: 359.2149\n",
      "Epoch 1210/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.4310 - val_loss: 203.6608\n",
      "Epoch 1211/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 148.8628 - val_loss: 151.7032\n",
      "Epoch 1212/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.4006 - val_loss: 148.6773\n",
      "Epoch 1213/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.4108 - val_loss: 165.6121\n",
      "Epoch 1214/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.7249 - val_loss: 163.2147\n",
      "Epoch 1215/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.4321 - val_loss: 156.9700\n",
      "Epoch 1216/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.5405 - val_loss: 165.1597\n",
      "Epoch 1217/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 162.0300 - val_loss: 149.1504\n",
      "Epoch 1218/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.9875 - val_loss: 148.4270\n",
      "Epoch 1219/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.3971 - val_loss: 145.0665\n",
      "Epoch 1220/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 168.1573 - val_loss: 151.1521\n",
      "Epoch 1221/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.6555 - val_loss: 151.3819\n",
      "Epoch 1222/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.6720 - val_loss: 151.2320\n",
      "Epoch 1223/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.3669 - val_loss: 142.6242\n",
      "Epoch 1224/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.8713 - val_loss: 254.8246\n",
      "Epoch 1225/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.3154 - val_loss: 146.2674\n",
      "Epoch 1226/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.7948 - val_loss: 171.8516\n",
      "Epoch 1227/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.1024 - val_loss: 177.6321\n",
      "Epoch 1228/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.7933 - val_loss: 203.9938\n",
      "Epoch 1229/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.5896 - val_loss: 149.5882\n",
      "Epoch 1230/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.7789 - val_loss: 167.1239\n",
      "Epoch 1231/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.6885 - val_loss: 157.6916\n",
      "Epoch 1232/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.4243 - val_loss: 193.9311\n",
      "Epoch 1233/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 164.6663 - val_loss: 173.7971\n",
      "Epoch 1234/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.8334 - val_loss: 184.8417\n",
      "Epoch 1235/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.3688 - val_loss: 155.5938\n",
      "Epoch 1236/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.7728 - val_loss: 150.2871\n",
      "Epoch 1237/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.1146 - val_loss: 142.3770\n",
      "Epoch 1238/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.4713 - val_loss: 147.9508\n",
      "Epoch 1239/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.2494 - val_loss: 188.3921\n",
      "Epoch 1240/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 150.1604 - val_loss: 170.3146\n",
      "Epoch 1241/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 270.5703 - val_loss: 149.5776\n",
      "Epoch 1242/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.0449 - val_loss: 150.3166\n",
      "Epoch 1243/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.5460 - val_loss: 143.3691\n",
      "Epoch 1244/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.2742 - val_loss: 145.6388\n",
      "Epoch 1245/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.4241 - val_loss: 200.9158\n",
      "Epoch 1246/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.6527 - val_loss: 150.4850\n",
      "Epoch 1247/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.0653 - val_loss: 147.0771\n",
      "Epoch 1248/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 214.0515 - val_loss: 146.6562\n",
      "Epoch 1249/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.2394 - val_loss: 158.4496\n",
      "Epoch 1250/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.3383 - val_loss: 149.0918\n",
      "Epoch 1251/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.2768 - val_loss: 146.1178\n",
      "Epoch 1252/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.6290 - val_loss: 149.1615\n",
      "Epoch 1253/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.0593 - val_loss: 142.6310\n",
      "Epoch 1254/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 151.4066 - val_loss: 170.4956\n",
      "Epoch 1255/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.4168 - val_loss: 143.3291\n",
      "Epoch 1256/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.2248 - val_loss: 151.8790\n",
      "Epoch 1257/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.9330 - val_loss: 151.2017\n",
      "Epoch 1258/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.7302 - val_loss: 168.2117\n",
      "Epoch 1259/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.4297 - val_loss: 208.0011\n",
      "Epoch 1260/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 203.2120 - val_loss: 479.4000\n",
      "Epoch 1261/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.8185 - val_loss: 178.1254\n",
      "Epoch 1262/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 135.4935 - val_loss: 145.1071\n",
      "Epoch 1263/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.0303 - val_loss: 154.9562\n",
      "Epoch 1264/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.7347 - val_loss: 165.1757\n",
      "Epoch 1265/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.7308 - val_loss: 163.1859\n",
      "Epoch 1266/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 167.3112 - val_loss: 146.8667\n",
      "Epoch 1267/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.2724 - val_loss: 168.2266\n",
      "Epoch 1268/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.5913 - val_loss: 144.0759\n",
      "Epoch 1269/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.5570 - val_loss: 154.4239\n",
      "Epoch 1270/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.4085 - val_loss: 151.0020\n",
      "Epoch 1271/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.6067 - val_loss: 156.8326\n",
      "Epoch 1272/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.6664 - val_loss: 147.9962\n",
      "Epoch 1273/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.2637 - val_loss: 172.8409\n",
      "Epoch 1274/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.4922 - val_loss: 161.1716\n",
      "Epoch 1275/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.6057 - val_loss: 155.6128\n",
      "Epoch 1276/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 134.9676 - val_loss: 174.7397\n",
      "Epoch 1277/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.7234 - val_loss: 171.2101\n",
      "Epoch 1278/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.4536 - val_loss: 211.1857\n",
      "Epoch 1279/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.8252 - val_loss: 152.8412\n",
      "Epoch 1280/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 131.7528 - val_loss: 164.9184\n",
      "Epoch 1281/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.7769 - val_loss: 145.5141\n",
      "Epoch 1282/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 262.4267 - val_loss: 180.6698\n",
      "Epoch 1283/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.0592 - val_loss: 154.1572\n",
      "Epoch 1284/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.4615 - val_loss: 144.2140\n",
      "Epoch 1285/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.2485 - val_loss: 163.0211\n",
      "Epoch 1286/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.8828 - val_loss: 179.8428\n",
      "Epoch 1287/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.6985 - val_loss: 150.1616\n",
      "Epoch 1288/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 135.0608 - val_loss: 139.5185\n",
      "Epoch 1289/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.4462 - val_loss: 148.1665\n",
      "Epoch 1290/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.2014 - val_loss: 147.2421\n",
      "Epoch 1291/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 237.7269 - val_loss: 223.8765\n",
      "Epoch 1292/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.4454 - val_loss: 144.4899\n",
      "Epoch 1293/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.6969 - val_loss: 151.5796\n",
      "Epoch 1294/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 135.8611 - val_loss: 193.7815\n",
      "Epoch 1295/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.7484 - val_loss: 156.7070\n",
      "Epoch 1296/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.1562 - val_loss: 141.5657\n",
      "Epoch 1297/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.8213 - val_loss: 187.7365\n",
      "Epoch 1298/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 135.4590 - val_loss: 145.7808\n",
      "Epoch 1299/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.4575 - val_loss: 144.8331\n",
      "Epoch 1300/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.2086 - val_loss: 184.1564\n",
      "Epoch 1301/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.6634 - val_loss: 169.5354\n",
      "Epoch 1302/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.5288 - val_loss: 153.4458\n",
      "Epoch 1303/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 175.1499 - val_loss: 801.9243\n",
      "Epoch 1304/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.9606 - val_loss: 154.2338\n",
      "Epoch 1305/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.7827 - val_loss: 149.3749\n",
      "Epoch 1306/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.4252 - val_loss: 145.5018\n",
      "Epoch 1307/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.7579 - val_loss: 152.1450\n",
      "Epoch 1308/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.0253 - val_loss: 152.2845\n",
      "Epoch 1309/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.7889 - val_loss: 150.1279\n",
      "Epoch 1310/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.1833 - val_loss: 145.9955\n",
      "Epoch 1311/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.7464 - val_loss: 151.6179\n",
      "Epoch 1312/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.3589 - val_loss: 142.5745\n",
      "Epoch 1313/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.4965 - val_loss: 150.7622\n",
      "Epoch 1314/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.1580 - val_loss: 213.1984\n",
      "Epoch 1315/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.3838 - val_loss: 163.6941\n",
      "Epoch 1316/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.9847 - val_loss: 177.7745\n",
      "Epoch 1317/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.3142 - val_loss: 143.0171\n",
      "Epoch 1318/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.9624 - val_loss: 218.0061\n",
      "Epoch 1319/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.7426 - val_loss: 150.4297\n",
      "Epoch 1320/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 207.7552 - val_loss: 413.6582\n",
      "Epoch 1321/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.9502 - val_loss: 169.8471\n",
      "Epoch 1322/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 128.1233 - val_loss: 163.4650\n",
      "Epoch 1323/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.4991 - val_loss: 202.7889\n",
      "Epoch 1324/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.9568 - val_loss: 140.4945\n",
      "Epoch 1325/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 134.5266 - val_loss: 149.9213\n",
      "Epoch 1326/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.7449 - val_loss: 154.7738\n",
      "Epoch 1327/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.1595 - val_loss: 142.9366\n",
      "Epoch 1328/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 129.9871 - val_loss: 173.1028\n",
      "Epoch 1329/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.1774 - val_loss: 145.8229\n",
      "Epoch 1330/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.2777 - val_loss: 147.2202\n",
      "Epoch 1331/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.0233 - val_loss: 163.2891\n",
      "Epoch 1332/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 219.9746 - val_loss: 513.3108\n",
      "Epoch 1333/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.4456 - val_loss: 155.6205\n",
      "Epoch 1334/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.5626 - val_loss: 152.9400\n",
      "Epoch 1335/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.7225 - val_loss: 167.6791\n",
      "Epoch 1336/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.5071 - val_loss: 166.0226\n",
      "Epoch 1337/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.0982 - val_loss: 142.3931\n",
      "Epoch 1338/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.0316 - val_loss: 141.5471\n",
      "Epoch 1339/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.5605 - val_loss: 142.8126\n",
      "Epoch 1340/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.1411 - val_loss: 139.6126\n",
      "Epoch 1341/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 150.5186 - val_loss: 177.7154\n",
      "Epoch 1342/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.5870 - val_loss: 145.2783\n",
      "Epoch 1343/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.0117 - val_loss: 139.5761\n",
      "Epoch 1344/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.2381 - val_loss: 146.2139\n",
      "Epoch 1345/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.4111 - val_loss: 177.3077\n",
      "Epoch 1346/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.8459 - val_loss: 196.6023\n",
      "Epoch 1347/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 133.3695 - val_loss: 148.9381\n",
      "Epoch 1348/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 134.1034 - val_loss: 206.3623\n",
      "Epoch 1349/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 161.2210 - val_loss: 184.0919\n",
      "Epoch 1350/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 132.4455 - val_loss: 147.1593\n",
      "Epoch 1351/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.8460 - val_loss: 161.6428\n",
      "Epoch 1352/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 156.0030 - val_loss: 191.8118\n",
      "Epoch 1353/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.9023 - val_loss: 141.3958\n",
      "Epoch 1354/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.3263 - val_loss: 154.8416\n",
      "Epoch 1355/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.4927 - val_loss: 143.8207\n",
      "Epoch 1356/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.1640 - val_loss: 145.2724\n",
      "Epoch 1357/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.5737 - val_loss: 159.2934\n",
      "Epoch 1358/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.7676 - val_loss: 154.2931\n",
      "Epoch 1359/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.4539 - val_loss: 142.7056\n",
      "Epoch 1360/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.7504 - val_loss: 151.6861\n",
      "Epoch 1361/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.1739 - val_loss: 157.2847\n",
      "Epoch 1362/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 135.0308 - val_loss: 145.2275\n",
      "Epoch 1363/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 393.9953 - val_loss: 190.3605\n",
      "Epoch 1364/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 190.2415 - val_loss: 166.1866\n",
      "Epoch 1365/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.0181 - val_loss: 168.9471\n",
      "Epoch 1366/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 166.6132 - val_loss: 152.1177\n",
      "Epoch 1367/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 177.9094 - val_loss: 185.6075\n",
      "Epoch 1368/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.6841 - val_loss: 163.8583\n",
      "Epoch 1369/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.1035 - val_loss: 154.4528\n",
      "Epoch 1370/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.4237 - val_loss: 163.4554\n",
      "Epoch 1371/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 156.8726 - val_loss: 196.4670\n",
      "Epoch 1372/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 160.4396 - val_loss: 251.0997\n",
      "Epoch 1373/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 154.2028 - val_loss: 169.2921\n",
      "Epoch 1374/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 140.4101 - val_loss: 178.4992\n",
      "Epoch 1375/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.9289 - val_loss: 168.9558\n",
      "Epoch 1376/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.8654 - val_loss: 155.8526\n",
      "Epoch 1377/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.2385 - val_loss: 151.8079\n",
      "Epoch 1378/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 152.1430 - val_loss: 153.1317\n",
      "Epoch 1379/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.3989 - val_loss: 147.3984\n",
      "Epoch 1380/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 146.6614 - val_loss: 154.0767\n",
      "Epoch 1381/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.9708 - val_loss: 142.4379\n",
      "Epoch 1382/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 144.8757 - val_loss: 163.7670\n",
      "Epoch 1383/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.9175 - val_loss: 337.3545\n",
      "Epoch 1384/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.0910 - val_loss: 237.9791\n",
      "Epoch 1385/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.3478 - val_loss: 142.5182\n",
      "Epoch 1386/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.2017 - val_loss: 156.8305\n",
      "Epoch 1387/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.0104 - val_loss: 174.8601\n",
      "Epoch 1388/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.0475 - val_loss: 151.0833\n",
      "Epoch 1389/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.4523 - val_loss: 151.1587\n",
      "Epoch 1390/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.2219 - val_loss: 148.9065\n",
      "Epoch 1391/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 140.9460 - val_loss: 164.7680\n",
      "Epoch 1392/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.8156 - val_loss: 491.8398\n",
      "Epoch 1393/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.2496 - val_loss: 141.0527\n",
      "Epoch 1394/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 153.9360 - val_loss: 151.8262\n",
      "Epoch 1395/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.7900 - val_loss: 171.0381\n",
      "Epoch 1396/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.1998 - val_loss: 145.8514\n",
      "Epoch 1397/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.9861 - val_loss: 150.1214\n",
      "Epoch 1398/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.3239 - val_loss: 148.1222\n",
      "Epoch 1399/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.3275 - val_loss: 168.5711\n",
      "Epoch 1400/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.8517 - val_loss: 149.1285\n",
      "Epoch 1401/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.9005 - val_loss: 182.5769\n",
      "Epoch 1402/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.3222 - val_loss: 150.8188\n",
      "Epoch 1403/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.0878 - val_loss: 142.5548\n",
      "Epoch 1404/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.6423 - val_loss: 160.5163\n",
      "Epoch 1405/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.4051 - val_loss: 139.4525\n",
      "Epoch 1406/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.6356 - val_loss: 163.0280\n",
      "Epoch 1407/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.8130 - val_loss: 151.1624\n",
      "Epoch 1408/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.4599 - val_loss: 149.4308\n",
      "Epoch 1409/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.8139 - val_loss: 149.8779\n",
      "Epoch 1410/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.1491 - val_loss: 190.7393\n",
      "Epoch 1411/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.8811 - val_loss: 193.3033\n",
      "Epoch 1412/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.0465 - val_loss: 144.7590\n",
      "Epoch 1413/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.4643 - val_loss: 143.8101\n",
      "Epoch 1414/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.5825 - val_loss: 142.4828\n",
      "Epoch 1415/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.9842 - val_loss: 196.8697\n",
      "Epoch 1416/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.4246 - val_loss: 155.7946\n",
      "Epoch 1417/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.5017 - val_loss: 165.2350\n",
      "Epoch 1418/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.1969 - val_loss: 157.9418\n",
      "Epoch 1419/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.4266 - val_loss: 140.0607\n",
      "Epoch 1420/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.8911 - val_loss: 162.5067\n",
      "Epoch 1421/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.7514 - val_loss: 144.1792\n",
      "Epoch 1422/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.3734 - val_loss: 150.0504\n",
      "Epoch 1423/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.6866 - val_loss: 269.5083\n",
      "Epoch 1424/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.8236 - val_loss: 209.5335\n",
      "Epoch 1425/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.5866 - val_loss: 138.3005\n",
      "Epoch 1426/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.1124 - val_loss: 142.6432\n",
      "Epoch 1427/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 128.8156 - val_loss: 144.4068\n",
      "Epoch 1428/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 136.5201 - val_loss: 138.9553\n",
      "Epoch 1429/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.7335 - val_loss: 144.4429\n",
      "Epoch 1430/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.9144 - val_loss: 159.3648\n",
      "Epoch 1431/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.5779 - val_loss: 143.7144\n",
      "Epoch 1432/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.8826 - val_loss: 157.0445\n",
      "Epoch 1433/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.7898 - val_loss: 145.3635\n",
      "Epoch 1434/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.9881 - val_loss: 146.8648\n",
      "Epoch 1435/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.9013 - val_loss: 156.8550\n",
      "Epoch 1436/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.2425 - val_loss: 155.5129\n",
      "Epoch 1437/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.9972 - val_loss: 149.5246\n",
      "Epoch 1438/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 134.3425 - val_loss: 143.9870\n",
      "Epoch 1439/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.2681 - val_loss: 154.3623\n",
      "Epoch 1440/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.5888 - val_loss: 140.9731\n",
      "Epoch 1441/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.5356 - val_loss: 152.8329\n",
      "Epoch 1442/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.9212 - val_loss: 171.1537\n",
      "Epoch 1443/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.9987 - val_loss: 147.2987\n",
      "Epoch 1444/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.3412 - val_loss: 267.2077\n",
      "Epoch 1445/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.1236 - val_loss: 184.1223\n",
      "Epoch 1446/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.1571 - val_loss: 169.4286\n",
      "Epoch 1447/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.1564 - val_loss: 162.9762\n",
      "Epoch 1448/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.5524 - val_loss: 144.9922\n",
      "Epoch 1449/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.0549 - val_loss: 136.7185\n",
      "Epoch 1450/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 589.5119 - val_loss: 750.7488\n",
      "Epoch 1451/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 291.5096 - val_loss: 379.4741\n",
      "Epoch 1452/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 220.2241 - val_loss: 196.2096\n",
      "Epoch 1453/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 192.5711 - val_loss: 182.2337\n",
      "Epoch 1454/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 178.0423 - val_loss: 213.4111\n",
      "Epoch 1455/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 199.4573 - val_loss: 174.0272\n",
      "Epoch 1456/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 192.7308 - val_loss: 206.1612\n",
      "Epoch 1457/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 172.2100 - val_loss: 187.6923\n",
      "Epoch 1458/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 182.8061 - val_loss: 171.9164\n",
      "Epoch 1459/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.2839 - val_loss: 202.9942\n",
      "Epoch 1460/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 170.6146 - val_loss: 200.3076\n",
      "Epoch 1461/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 159.8590 - val_loss: 165.6359\n",
      "Epoch 1462/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 161.8714 - val_loss: 253.5307\n",
      "Epoch 1463/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 170.5427 - val_loss: 203.9810\n",
      "Epoch 1464/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 162.7050 - val_loss: 158.9538\n",
      "Epoch 1465/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 162.2375 - val_loss: 169.7797\n",
      "Epoch 1466/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.8065 - val_loss: 160.6178\n",
      "Epoch 1467/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 162.6569 - val_loss: 183.0483\n",
      "Epoch 1468/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 163.0495 - val_loss: 162.5323\n",
      "Epoch 1469/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.2939 - val_loss: 162.6590\n",
      "Epoch 1470/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.7533 - val_loss: 165.7320\n",
      "Epoch 1471/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 182.3529 - val_loss: 170.4413\n",
      "Epoch 1472/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 152.8247 - val_loss: 153.9663\n",
      "Epoch 1473/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.8095 - val_loss: 159.1184\n",
      "Epoch 1474/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.0523 - val_loss: 151.2240\n",
      "Epoch 1475/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.1767 - val_loss: 171.5197\n",
      "Epoch 1476/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 154.5484 - val_loss: 163.3540\n",
      "Epoch 1477/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.5264 - val_loss: 154.6896\n",
      "Epoch 1478/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 155.9299 - val_loss: 174.8450\n",
      "Epoch 1479/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.3323 - val_loss: 156.9004\n",
      "Epoch 1480/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 172.3677 - val_loss: 180.5160\n",
      "Epoch 1481/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.3136 - val_loss: 151.6679\n",
      "Epoch 1482/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 157.5713 - val_loss: 230.8984\n",
      "Epoch 1483/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.7084 - val_loss: 158.8435\n",
      "Epoch 1484/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.2552 - val_loss: 303.3436\n",
      "Epoch 1485/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.6033 - val_loss: 158.3810\n",
      "Epoch 1486/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 142.1748 - val_loss: 156.9822\n",
      "Epoch 1487/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 154.0098 - val_loss: 180.7286\n",
      "Epoch 1488/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.5885 - val_loss: 182.1087\n",
      "Epoch 1489/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.6385 - val_loss: 163.2194\n",
      "Epoch 1490/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.1197 - val_loss: 161.5425\n",
      "Epoch 1491/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 169.9044 - val_loss: 165.8250\n",
      "Epoch 1492/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 150.8521 - val_loss: 161.6165\n",
      "Epoch 1493/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.8597 - val_loss: 160.1659\n",
      "Epoch 1494/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.8361 - val_loss: 147.6386\n",
      "Epoch 1495/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.2879 - val_loss: 167.3965\n",
      "Epoch 1496/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 155.8729 - val_loss: 147.1276\n",
      "Epoch 1497/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 154.4940 - val_loss: 158.4246\n",
      "Epoch 1498/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.1501 - val_loss: 159.4829\n",
      "Epoch 1499/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 152.3246 - val_loss: 156.7716\n",
      "Epoch 1500/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.1444 - val_loss: 156.2706\n",
      "Epoch 1501/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.0169 - val_loss: 151.4971\n",
      "Epoch 1502/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 155.4761 - val_loss: 151.6501\n",
      "Epoch 1503/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 142.2425 - val_loss: 173.1316\n",
      "Epoch 1504/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.3540 - val_loss: 213.9725\n",
      "Epoch 1505/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.5876 - val_loss: 151.6191\n",
      "Epoch 1506/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 150.4508 - val_loss: 166.1954\n",
      "Epoch 1507/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.1346 - val_loss: 163.7867\n",
      "Epoch 1508/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.2886 - val_loss: 152.8784\n",
      "Epoch 1509/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.0531 - val_loss: 189.9714\n",
      "Epoch 1510/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.6594 - val_loss: 148.7032\n",
      "Epoch 1511/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 154.6258 - val_loss: 947.4385\n",
      "Epoch 1512/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 166.0564 - val_loss: 161.6648\n",
      "Epoch 1513/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.8412 - val_loss: 199.8941\n",
      "Epoch 1514/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.5819 - val_loss: 164.1559\n",
      "Epoch 1515/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.4107 - val_loss: 175.6123\n",
      "Epoch 1516/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.2433 - val_loss: 163.5896\n",
      "Epoch 1517/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 139.9506 - val_loss: 199.1836\n",
      "Epoch 1518/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.6886 - val_loss: 152.4423\n",
      "Epoch 1519/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 136.9425 - val_loss: 153.5884\n",
      "Epoch 1520/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.7948 - val_loss: 160.0235\n",
      "Epoch 1521/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.6768 - val_loss: 184.3123\n",
      "Epoch 1522/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.1689 - val_loss: 152.4976\n",
      "Epoch 1523/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.5990 - val_loss: 156.2409\n",
      "Epoch 1524/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.7277 - val_loss: 152.8410\n",
      "Epoch 1525/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.5256 - val_loss: 171.7135\n",
      "Epoch 1526/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.6644 - val_loss: 160.6599\n",
      "Epoch 1527/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.6500 - val_loss: 157.9185\n",
      "Epoch 1528/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.1604 - val_loss: 147.3302\n",
      "Epoch 1529/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 156.7714 - val_loss: 163.0358\n",
      "Epoch 1530/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.4341 - val_loss: 144.6246\n",
      "Epoch 1531/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.4314 - val_loss: 207.8847\n",
      "Epoch 1532/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.4258 - val_loss: 180.5408\n",
      "Epoch 1533/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 142.8333 - val_loss: 150.7170\n",
      "Epoch 1534/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.9428 - val_loss: 166.2135\n",
      "Epoch 1535/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.5933 - val_loss: 145.8898\n",
      "Epoch 1536/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.1809 - val_loss: 145.7323\n",
      "Epoch 1537/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.0708 - val_loss: 147.7268\n",
      "Epoch 1538/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 146.1417 - val_loss: 143.0820\n",
      "Epoch 1539/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.6012 - val_loss: 192.0027\n",
      "Epoch 1540/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 139.5338 - val_loss: 146.4976\n",
      "Epoch 1541/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.6071 - val_loss: 150.9286\n",
      "Epoch 1542/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.4873 - val_loss: 146.8726\n",
      "Epoch 1543/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.8691 - val_loss: 143.0163\n",
      "Epoch 1544/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.0154 - val_loss: 215.8282\n",
      "Epoch 1545/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 189.7525 - val_loss: 144.4627\n",
      "Epoch 1546/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.2411 - val_loss: 153.1850\n",
      "Epoch 1547/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.8254 - val_loss: 172.8961\n",
      "Epoch 1548/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.4893 - val_loss: 192.9428\n",
      "Epoch 1549/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.5685 - val_loss: 142.8518\n",
      "Epoch 1550/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.4090 - val_loss: 146.6466\n",
      "Epoch 1551/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.7621 - val_loss: 151.3044\n",
      "Epoch 1552/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.0868 - val_loss: 150.1271\n",
      "Epoch 1553/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.2349 - val_loss: 144.5629\n",
      "Epoch 1554/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.7171 - val_loss: 197.2270\n",
      "Epoch 1555/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.9845 - val_loss: 165.1405\n",
      "Epoch 1556/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.7846 - val_loss: 152.8669\n",
      "Epoch 1557/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 134.6324 - val_loss: 144.4666\n",
      "Epoch 1558/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.2251 - val_loss: 146.3304\n",
      "Epoch 1559/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.0271 - val_loss: 144.9952\n",
      "Epoch 1560/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.3541 - val_loss: 169.3919\n",
      "Epoch 1561/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.8285 - val_loss: 152.3923\n",
      "Epoch 1562/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.7235 - val_loss: 152.0075\n",
      "Epoch 1563/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.1493 - val_loss: 153.7657\n",
      "Epoch 1564/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.3488 - val_loss: 212.9819\n",
      "Epoch 1565/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 135.3039 - val_loss: 193.9063\n",
      "Epoch 1566/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.7323 - val_loss: 146.3841\n",
      "Epoch 1567/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 137.6797 - val_loss: 149.7890\n",
      "Epoch 1568/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.4557 - val_loss: 151.0354\n",
      "Epoch 1569/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 138.7339 - val_loss: 152.2600\n",
      "Epoch 1570/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.0621 - val_loss: 250.6597\n",
      "Epoch 1571/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.2716 - val_loss: 152.5861\n",
      "Epoch 1572/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.4439 - val_loss: 141.7336\n",
      "Epoch 1573/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 131.1314 - val_loss: 150.5819\n",
      "Epoch 1574/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.5062 - val_loss: 149.5812\n",
      "Epoch 1575/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 148.2809 - val_loss: 171.8346\n",
      "Epoch 1576/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.8281 - val_loss: 164.8342\n",
      "Epoch 1577/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.4948 - val_loss: 183.9078\n",
      "Epoch 1578/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.7494 - val_loss: 159.8139\n",
      "Epoch 1579/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.1865 - val_loss: 149.7091\n",
      "Epoch 1580/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.3629 - val_loss: 142.1766\n",
      "Epoch 1581/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.6881 - val_loss: 152.4676\n",
      "Epoch 1582/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.7436 - val_loss: 201.6905\n",
      "Epoch 1583/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 161.9433 - val_loss: 149.2821\n",
      "Epoch 1584/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.7121 - val_loss: 143.8322\n",
      "Epoch 1585/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 137.8878 - val_loss: 168.9891\n",
      "Epoch 1586/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 136.1305 - val_loss: 143.4695\n",
      "Epoch 1587/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 133.8906 - val_loss: 159.3772\n",
      "Epoch 1588/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.2605 - val_loss: 169.3552\n",
      "Epoch 1589/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.5044 - val_loss: 172.6436\n",
      "Epoch 1590/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.9902 - val_loss: 144.9808\n",
      "Epoch 1591/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 142.5937 - val_loss: 163.8953\n",
      "Epoch 1592/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.1618 - val_loss: 149.9467\n",
      "Epoch 1593/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.9278 - val_loss: 179.6638\n",
      "Epoch 1594/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.2795 - val_loss: 147.1119\n",
      "Epoch 1595/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.1253 - val_loss: 139.5388\n",
      "Epoch 1596/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.2001 - val_loss: 185.0538\n",
      "Epoch 1597/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.0588 - val_loss: 160.3060\n",
      "Epoch 1598/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 314.6361 - val_loss: 171.6433\n",
      "Epoch 1599/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.5639 - val_loss: 148.7876\n",
      "Epoch 1600/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.2822 - val_loss: 142.6391\n",
      "Epoch 1601/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.2157 - val_loss: 143.3099\n",
      "Epoch 1602/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.2441 - val_loss: 170.8529\n",
      "Epoch 1603/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.6902 - val_loss: 154.4563\n",
      "Epoch 1604/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.4575 - val_loss: 142.4753\n",
      "Epoch 1605/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.6575 - val_loss: 142.2507\n",
      "Epoch 1606/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.2475 - val_loss: 143.7568\n",
      "Epoch 1607/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.9339 - val_loss: 147.1094\n",
      "Epoch 1608/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.6654 - val_loss: 148.8326\n",
      "Epoch 1609/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.4640 - val_loss: 145.3276\n",
      "Epoch 1610/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.7371 - val_loss: 187.4273\n",
      "Epoch 1611/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.3430 - val_loss: 170.2586\n",
      "Epoch 1612/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 129.8533 - val_loss: 250.5711\n",
      "Epoch 1613/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.5232 - val_loss: 146.2622\n",
      "Epoch 1614/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.4634 - val_loss: 158.1266\n",
      "Epoch 1615/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.9730 - val_loss: 165.4149\n",
      "Epoch 1616/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.8156 - val_loss: 195.7049\n",
      "Epoch 1617/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.1108 - val_loss: 161.0863\n",
      "Epoch 1618/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.3106 - val_loss: 149.3632\n",
      "Epoch 1619/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.4065 - val_loss: 194.7079\n",
      "Epoch 1620/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.4761 - val_loss: 148.3993\n",
      "Epoch 1621/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.8504 - val_loss: 141.6019\n",
      "Epoch 1622/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.1160 - val_loss: 185.9620\n",
      "Epoch 1623/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.3863 - val_loss: 156.0569\n",
      "Epoch 1624/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 148.3865 - val_loss: 147.4200\n",
      "Epoch 1625/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.1677 - val_loss: 157.9133\n",
      "Epoch 1626/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.7040 - val_loss: 163.2489\n",
      "Epoch 1627/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.3811 - val_loss: 143.6299\n",
      "Epoch 1628/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.6279 - val_loss: 163.3241\n",
      "Epoch 1629/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 140.9386 - val_loss: 141.5646\n",
      "Epoch 1630/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.2218 - val_loss: 160.3980\n",
      "Epoch 1631/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.3154 - val_loss: 179.4962\n",
      "Epoch 1632/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 171.1518 - val_loss: 266.9409\n",
      "Epoch 1633/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.9642 - val_loss: 147.8935\n",
      "Epoch 1634/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.5962 - val_loss: 151.6111\n",
      "Epoch 1635/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.9749 - val_loss: 140.5963\n",
      "Epoch 1636/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.3871 - val_loss: 146.3111\n",
      "Epoch 1637/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.5815 - val_loss: 170.2148\n",
      "Epoch 1638/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.5861 - val_loss: 177.7591\n",
      "Epoch 1639/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.1712 - val_loss: 146.0318\n",
      "Epoch 1640/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.8441 - val_loss: 146.0706\n",
      "Epoch 1641/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.0100 - val_loss: 179.8475\n",
      "Epoch 1642/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.6623 - val_loss: 176.5285\n",
      "Epoch 1643/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.1257 - val_loss: 160.8151\n",
      "Epoch 1644/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.7702 - val_loss: 205.8288\n",
      "Epoch 1645/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.8203 - val_loss: 168.0301\n",
      "Epoch 1646/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.9838 - val_loss: 140.1464\n",
      "Epoch 1647/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.1714 - val_loss: 169.3222\n",
      "Epoch 1648/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.9807 - val_loss: 158.8834\n",
      "Epoch 1649/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.3721 - val_loss: 167.1573\n",
      "Epoch 1650/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 182.1053 - val_loss: 140.9747\n",
      "Epoch 1651/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.8538 - val_loss: 170.8405\n",
      "Epoch 1652/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.9892 - val_loss: 159.1406\n",
      "Epoch 1653/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.1817 - val_loss: 186.5776\n",
      "Epoch 1654/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.3009 - val_loss: 140.8211\n",
      "Epoch 1655/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.6710 - val_loss: 150.3504\n",
      "Epoch 1656/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.9996 - val_loss: 149.1808\n",
      "Epoch 1657/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.5906 - val_loss: 149.6501\n",
      "Epoch 1658/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.9012 - val_loss: 149.3446\n",
      "Epoch 1659/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.6781 - val_loss: 144.2174\n",
      "Epoch 1660/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.3565 - val_loss: 143.8444\n",
      "Epoch 1661/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.1259 - val_loss: 140.6632\n",
      "Epoch 1662/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.0111 - val_loss: 146.5750\n",
      "Epoch 1663/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 131.1553 - val_loss: 142.6472\n",
      "Epoch 1664/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.2616 - val_loss: 191.0735\n",
      "Epoch 1665/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.0237 - val_loss: 150.4832\n",
      "Epoch 1666/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.7742 - val_loss: 147.7616\n",
      "Epoch 1667/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.0179 - val_loss: 141.2610\n",
      "Epoch 1668/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.1299 - val_loss: 233.4566\n",
      "Epoch 1669/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.8720 - val_loss: 139.8755\n",
      "Epoch 1670/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.3963 - val_loss: 160.0505\n",
      "Epoch 1671/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 135.8651 - val_loss: 145.2886\n",
      "Epoch 1672/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.9490 - val_loss: 149.9620\n",
      "Epoch 1673/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.0746 - val_loss: 141.5553\n",
      "Epoch 1674/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.1109 - val_loss: 156.2971\n",
      "Epoch 1675/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.7217 - val_loss: 170.1751\n",
      "Epoch 1676/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.4668 - val_loss: 154.9039\n",
      "Epoch 1677/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.3052 - val_loss: 151.6526\n",
      "Epoch 1678/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.8026 - val_loss: 163.4657\n",
      "Epoch 1679/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.3624 - val_loss: 144.7349\n",
      "Epoch 1680/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.3408 - val_loss: 182.8547\n",
      "Epoch 1681/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 219.4535 - val_loss: 310.9843\n",
      "Epoch 1682/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.3075 - val_loss: 154.2852\n",
      "Epoch 1683/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.9049 - val_loss: 146.4057\n",
      "Epoch 1684/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.8746 - val_loss: 147.8110\n",
      "Epoch 1685/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.2488 - val_loss: 162.8221\n",
      "Epoch 1686/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.5217 - val_loss: 163.0607\n",
      "Epoch 1687/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.0883 - val_loss: 148.3615\n",
      "Epoch 1688/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.9161 - val_loss: 189.4073\n",
      "Epoch 1689/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.6513 - val_loss: 141.2224\n",
      "Epoch 1690/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.2361 - val_loss: 143.1817\n",
      "Epoch 1691/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.0211 - val_loss: 139.1760\n",
      "Epoch 1692/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.2382 - val_loss: 155.1660\n",
      "Epoch 1693/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.2678 - val_loss: 150.6052\n",
      "Epoch 1694/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.9368 - val_loss: 174.5341\n",
      "Epoch 1695/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.8794 - val_loss: 169.3224\n",
      "Epoch 1696/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.1744 - val_loss: 156.8731\n",
      "Epoch 1697/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.0644 - val_loss: 204.6956\n",
      "Epoch 1698/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.5731 - val_loss: 140.7185\n",
      "Epoch 1699/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.6343 - val_loss: 148.1231\n",
      "Epoch 1700/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.9581 - val_loss: 146.7991\n",
      "Epoch 1701/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.1445 - val_loss: 143.7987\n",
      "Epoch 1702/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.3810 - val_loss: 152.9120\n",
      "Epoch 1703/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.8635 - val_loss: 148.2295\n",
      "Epoch 1704/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.5536 - val_loss: 140.4310\n",
      "Epoch 1705/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.4458 - val_loss: 154.3488\n",
      "Epoch 1706/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.5014 - val_loss: 141.6530\n",
      "Epoch 1707/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.4771 - val_loss: 141.0740\n",
      "Epoch 1708/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.5234 - val_loss: 139.2223\n",
      "Epoch 1709/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.4598 - val_loss: 140.7432\n",
      "Epoch 1710/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.8281 - val_loss: 173.0226\n",
      "Epoch 1711/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.0768 - val_loss: 208.2970\n",
      "Epoch 1712/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.8499 - val_loss: 148.8605\n",
      "Epoch 1713/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.9890 - val_loss: 295.8535\n",
      "Epoch 1714/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 182.0886 - val_loss: 143.0583\n",
      "Epoch 1715/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.1949 - val_loss: 139.7258\n",
      "Epoch 1716/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.7197 - val_loss: 153.7066\n",
      "Epoch 1717/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.8895 - val_loss: 145.2255\n",
      "Epoch 1718/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.8336 - val_loss: 159.0918\n",
      "Epoch 1719/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.2207 - val_loss: 158.9828\n",
      "Epoch 1720/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.6690 - val_loss: 148.1506\n",
      "Epoch 1721/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.7835 - val_loss: 176.5336\n",
      "Epoch 1722/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.4000 - val_loss: 172.6308\n",
      "Epoch 1723/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.9478 - val_loss: 139.2756\n",
      "Epoch 1724/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.1307 - val_loss: 144.3619\n",
      "Epoch 1725/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 132.9335 - val_loss: 146.8685\n",
      "Epoch 1726/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.1878 - val_loss: 148.0750\n",
      "Epoch 1727/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 134.3100 - val_loss: 158.3988\n",
      "Epoch 1728/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.9803 - val_loss: 146.0813\n",
      "Epoch 1729/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.5624 - val_loss: 213.7477\n",
      "Epoch 1730/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 134.3532 - val_loss: 146.0335\n",
      "Epoch 1731/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.1263 - val_loss: 146.5006\n",
      "Epoch 1732/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.9229 - val_loss: 156.9186\n",
      "Epoch 1733/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.4063 - val_loss: 158.6431\n",
      "Epoch 1734/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 131.0249 - val_loss: 149.3033\n",
      "Epoch 1735/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.5017 - val_loss: 141.2936\n",
      "Epoch 1736/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.0019 - val_loss: 158.3897\n",
      "Epoch 1737/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.5101 - val_loss: 143.4738\n",
      "Epoch 1738/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.4287 - val_loss: 200.7448\n",
      "Epoch 1739/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.0044 - val_loss: 149.8510\n",
      "Epoch 1740/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 129.5805 - val_loss: 147.1417\n",
      "Epoch 1741/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 134.1115 - val_loss: 149.8703\n",
      "Epoch 1742/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 129.5079 - val_loss: 164.2078\n",
      "Epoch 1743/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.6305 - val_loss: 149.7186\n",
      "Epoch 1744/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 132.2309 - val_loss: 259.8075\n",
      "Epoch 1745/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.3850 - val_loss: 139.4521\n",
      "Epoch 1746/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.5344 - val_loss: 140.8385\n",
      "Epoch 1747/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.4490 - val_loss: 183.9513\n",
      "Epoch 1748/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.0724 - val_loss: 350.9989\n",
      "Epoch 1749/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.1470 - val_loss: 327.3186\n",
      "Epoch 1750/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.8692 - val_loss: 143.4910\n",
      "Epoch 1751/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 133.4961 - val_loss: 178.2928\n",
      "Epoch 1752/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.8865 - val_loss: 145.0696\n",
      "Epoch 1753/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.4336 - val_loss: 143.8606\n",
      "Epoch 1754/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.5722 - val_loss: 174.0243\n",
      "Epoch 1755/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.9720 - val_loss: 142.1351\n",
      "Epoch 1756/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.6729 - val_loss: 149.9432\n",
      "Epoch 1757/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.9012 - val_loss: 147.1867\n",
      "Epoch 1758/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.8714 - val_loss: 148.2883\n",
      "Epoch 1759/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.5132 - val_loss: 139.8862\n",
      "Epoch 1760/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.8453 - val_loss: 163.4039\n",
      "Epoch 1761/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.9462 - val_loss: 147.9618\n",
      "Epoch 1762/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.9271 - val_loss: 170.5818\n",
      "Epoch 1763/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.4277 - val_loss: 198.9130\n",
      "Epoch 1764/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 171.8023 - val_loss: 880.6952\n",
      "Epoch 1765/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.9517 - val_loss: 152.3877\n",
      "Epoch 1766/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.4341 - val_loss: 159.5247\n",
      "Epoch 1767/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.9050 - val_loss: 147.0468\n",
      "Epoch 1768/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.9796 - val_loss: 142.9481\n",
      "Epoch 1769/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.9826 - val_loss: 144.0871\n",
      "Epoch 1770/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.0374 - val_loss: 144.7357\n",
      "Epoch 1771/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.8889 - val_loss: 179.7367\n",
      "Epoch 1772/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.2066 - val_loss: 156.4484\n",
      "Epoch 1773/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.4107 - val_loss: 157.7634\n",
      "Epoch 1774/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.9429 - val_loss: 159.7651\n",
      "Epoch 1775/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.4054 - val_loss: 155.9503\n",
      "Epoch 1776/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.8526 - val_loss: 190.3819\n",
      "Epoch 1777/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.3892 - val_loss: 144.4185\n",
      "Epoch 1778/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.7384 - val_loss: 138.7971\n",
      "Epoch 1779/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.4726 - val_loss: 163.9380\n",
      "Epoch 1780/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.7588 - val_loss: 155.5307\n",
      "Epoch 1781/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.1059 - val_loss: 161.2272\n",
      "Epoch 1782/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.0104 - val_loss: 143.4947\n",
      "Epoch 1783/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.2561 - val_loss: 139.1841\n",
      "Epoch 1784/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.4241 - val_loss: 168.4492\n",
      "Epoch 1785/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.5531 - val_loss: 155.8142\n",
      "Epoch 1786/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.7713 - val_loss: 175.7054\n",
      "Epoch 1787/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.8622 - val_loss: 141.7858\n",
      "Epoch 1788/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.0073 - val_loss: 142.2175\n",
      "Epoch 1789/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.8240 - val_loss: 153.3640\n",
      "Epoch 1790/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.4012 - val_loss: 157.4887\n",
      "Epoch 1791/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.0571 - val_loss: 146.3464\n",
      "Epoch 1792/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.0123 - val_loss: 210.5035\n",
      "Epoch 1793/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.6803 - val_loss: 154.3430\n",
      "Epoch 1794/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.2407 - val_loss: 146.2938\n",
      "Epoch 1795/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.6173 - val_loss: 138.9027\n",
      "Epoch 1796/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.3162 - val_loss: 183.6875\n",
      "Epoch 1797/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.1899 - val_loss: 150.2099\n",
      "Epoch 1798/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.8646 - val_loss: 200.2399\n",
      "Epoch 1799/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.7615 - val_loss: 179.0579\n",
      "Epoch 1800/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.8128 - val_loss: 155.2699\n",
      "Epoch 1801/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.5031 - val_loss: 157.1897\n",
      "Epoch 1802/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.4440 - val_loss: 147.6999\n",
      "Epoch 1803/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.0264 - val_loss: 157.0000\n",
      "Epoch 1804/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.5374 - val_loss: 149.5586\n",
      "Epoch 1805/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.7064 - val_loss: 142.5912\n",
      "Epoch 1806/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.2891 - val_loss: 150.5794\n",
      "Epoch 1807/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.0428 - val_loss: 145.1706\n",
      "Epoch 1808/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.2467 - val_loss: 207.3294\n",
      "Epoch 1809/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.9943 - val_loss: 172.7541\n",
      "Epoch 1810/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 232.0858 - val_loss: 167.3430\n",
      "Epoch 1811/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.7369 - val_loss: 153.2054\n",
      "Epoch 1812/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.7928 - val_loss: 144.8626\n",
      "Epoch 1813/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.3026 - val_loss: 157.2530\n",
      "Epoch 1814/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.9616 - val_loss: 142.1731\n",
      "Epoch 1815/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.6433 - val_loss: 146.1334\n",
      "Epoch 1816/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.1628 - val_loss: 150.9992\n",
      "Epoch 1817/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.0161 - val_loss: 144.4144\n",
      "Epoch 1818/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.1648 - val_loss: 221.2125\n",
      "Epoch 1819/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 152.8683 - val_loss: 141.0738\n",
      "Epoch 1820/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.0990 - val_loss: 138.7623\n",
      "Epoch 1821/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.0846 - val_loss: 152.1594\n",
      "Epoch 1822/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 123.3130 - val_loss: 147.8124\n",
      "Epoch 1823/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.6304 - val_loss: 144.3227\n",
      "Epoch 1824/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.0686 - val_loss: 172.0684\n",
      "Epoch 1825/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.7243 - val_loss: 145.7515\n",
      "Epoch 1826/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.5399 - val_loss: 149.8336\n",
      "Epoch 1827/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.9259 - val_loss: 139.4980\n",
      "Epoch 1828/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.7359 - val_loss: 144.7617\n",
      "Epoch 1829/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.6960 - val_loss: 155.7603\n",
      "Epoch 1830/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.0049 - val_loss: 152.2523\n",
      "Epoch 1831/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.5201 - val_loss: 182.5403\n",
      "Epoch 1832/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.8353 - val_loss: 196.5633\n",
      "Epoch 1833/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.6492 - val_loss: 145.4865\n",
      "Epoch 1834/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.6840 - val_loss: 138.8743\n",
      "Epoch 1835/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.9181 - val_loss: 142.8699\n",
      "Epoch 1836/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.4454 - val_loss: 138.3162\n",
      "Epoch 1837/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.6901 - val_loss: 158.6940\n",
      "Epoch 1838/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.7873 - val_loss: 199.8013\n",
      "Epoch 1839/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.3934 - val_loss: 148.1378\n",
      "Epoch 1840/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.0821 - val_loss: 140.1982\n",
      "Epoch 1841/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 151.0699 - val_loss: 280.1848\n",
      "Epoch 1842/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.4939 - val_loss: 141.4249\n",
      "Epoch 1843/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.0575 - val_loss: 162.4895\n",
      "Epoch 1844/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.5186 - val_loss: 147.3270\n",
      "Epoch 1845/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.4180 - val_loss: 143.8379\n",
      "Epoch 1846/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.3069 - val_loss: 150.2391\n",
      "Epoch 1847/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.9181 - val_loss: 168.5740\n",
      "Epoch 1848/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.0391 - val_loss: 161.4617\n",
      "Epoch 1849/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 132.2539 - val_loss: 161.2202\n",
      "Epoch 1850/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.4011 - val_loss: 162.5896\n",
      "Epoch 1851/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.9402 - val_loss: 152.9198\n",
      "Epoch 1852/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.9201 - val_loss: 149.4361\n",
      "Epoch 1853/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.9057 - val_loss: 166.5163\n",
      "Epoch 1854/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.1243 - val_loss: 144.9112\n",
      "Epoch 1855/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.7825 - val_loss: 140.0297\n",
      "Epoch 1856/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.5535 - val_loss: 176.2775\n",
      "Epoch 1857/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.4232 - val_loss: 141.4618\n",
      "Epoch 1858/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.7051 - val_loss: 142.7381\n",
      "Epoch 1859/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.2572 - val_loss: 162.8443\n",
      "Epoch 1860/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.5872 - val_loss: 145.9745\n",
      "Epoch 1861/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 133.9653 - val_loss: 150.2545\n",
      "Epoch 1862/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.6082 - val_loss: 150.0542\n",
      "Epoch 1863/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.7896 - val_loss: 170.0080\n",
      "Epoch 1864/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.6755 - val_loss: 141.7815\n",
      "Epoch 1865/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.3035 - val_loss: 141.6362\n",
      "Epoch 1866/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.3113 - val_loss: 142.7735\n",
      "Epoch 1867/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.4093 - val_loss: 146.6110\n",
      "Epoch 1868/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.7601 - val_loss: 145.6684\n",
      "Epoch 1869/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.3902 - val_loss: 191.8874\n",
      "Epoch 1870/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 129.7928 - val_loss: 158.3249\n",
      "Epoch 1871/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.3525 - val_loss: 148.0234\n",
      "Epoch 1872/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.9567 - val_loss: 141.8662\n",
      "Epoch 1873/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.9274 - val_loss: 149.2906\n",
      "Epoch 1874/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.9088 - val_loss: 175.2727\n",
      "Epoch 1875/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.4394 - val_loss: 167.3325\n",
      "Epoch 1876/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.4913 - val_loss: 154.4805\n",
      "Epoch 1877/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.0486 - val_loss: 145.1023\n",
      "Epoch 1878/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 165.7917 - val_loss: 146.8278\n",
      "Epoch 1879/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.3582 - val_loss: 159.8250\n",
      "Epoch 1880/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.5459 - val_loss: 141.5788\n",
      "Epoch 1881/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 131.7241 - val_loss: 148.5838\n",
      "Epoch 1882/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.4851 - val_loss: 138.5834\n",
      "Epoch 1883/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.8932 - val_loss: 162.9573\n",
      "Epoch 1884/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.7536 - val_loss: 210.8400\n",
      "Epoch 1885/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.5376 - val_loss: 146.6414\n",
      "Epoch 1886/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 224.2327 - val_loss: 150.1840\n",
      "Epoch 1887/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.5695 - val_loss: 147.8547\n",
      "Epoch 1888/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.5828 - val_loss: 141.1795\n",
      "Epoch 1889/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.4060 - val_loss: 153.4009\n",
      "Epoch 1890/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.8714 - val_loss: 168.0647\n",
      "Epoch 1891/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.9560 - val_loss: 144.7624\n",
      "Epoch 1892/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.5542 - val_loss: 143.5149\n",
      "Epoch 1893/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.9687 - val_loss: 139.8271\n",
      "Epoch 1894/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.8153 - val_loss: 203.5211\n",
      "Epoch 1895/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.6538 - val_loss: 156.5060\n",
      "Epoch 1896/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.2607 - val_loss: 139.9469\n",
      "Epoch 1897/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.2017 - val_loss: 145.4033\n",
      "Epoch 1898/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.6280 - val_loss: 165.2057\n",
      "Epoch 1899/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.7302 - val_loss: 186.3710\n",
      "Epoch 1900/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.1117 - val_loss: 150.6422\n",
      "Epoch 1901/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 123.6275 - val_loss: 149.5400\n",
      "Epoch 1902/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.1119 - val_loss: 142.6517\n",
      "Epoch 1903/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.0884 - val_loss: 172.6101\n",
      "Epoch 1904/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 307.9646 - val_loss: 152.6508\n",
      "Epoch 1905/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.5406 - val_loss: 141.6308\n",
      "Epoch 1906/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.1035 - val_loss: 162.6863\n",
      "Epoch 1907/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.5614 - val_loss: 180.4280\n",
      "Epoch 1908/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 120.3736 - val_loss: 141.0858\n",
      "Epoch 1909/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.1888 - val_loss: 153.8584\n",
      "Epoch 1910/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.1919 - val_loss: 154.1751\n",
      "Epoch 1911/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.0981 - val_loss: 142.2213\n",
      "Epoch 1912/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.4535 - val_loss: 146.0919\n",
      "Epoch 1913/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 132.4743 - val_loss: 153.5679\n",
      "Epoch 1914/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.0102 - val_loss: 147.7022\n",
      "Epoch 1915/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.3668 - val_loss: 152.0345\n",
      "Epoch 1916/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 127.0736 - val_loss: 140.2181\n",
      "Epoch 1917/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.7603 - val_loss: 140.8805\n",
      "Epoch 1918/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.8419 - val_loss: 154.1204\n",
      "Epoch 1919/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.5645 - val_loss: 145.5121\n",
      "Epoch 1920/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 144.5016 - val_loss: 141.7561\n",
      "Epoch 1921/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.5307 - val_loss: 148.9729\n",
      "Epoch 1922/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 127.9492 - val_loss: 156.2916\n",
      "Epoch 1923/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.7990 - val_loss: 140.3409\n",
      "Epoch 1924/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 128.1741 - val_loss: 153.3045\n",
      "Epoch 1925/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.5689 - val_loss: 150.7395\n",
      "Epoch 1926/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 136.1429 - val_loss: 142.3786\n",
      "Epoch 1927/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.7093 - val_loss: 183.2606\n",
      "Epoch 1928/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.9265 - val_loss: 147.5612\n",
      "Epoch 1929/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 128.1568 - val_loss: 140.8847\n",
      "Epoch 1930/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.6111 - val_loss: 142.0482\n",
      "Epoch 1931/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.1557 - val_loss: 142.6305\n",
      "Epoch 1932/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.9785 - val_loss: 149.5534\n",
      "Epoch 1933/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.3782 - val_loss: 148.2341\n",
      "Epoch 1934/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.2481 - val_loss: 142.6241\n",
      "Epoch 1935/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.4763 - val_loss: 148.3576\n",
      "Epoch 1936/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.9293 - val_loss: 171.6555\n",
      "Epoch 1937/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 709.8739 - val_loss: 667.9981\n",
      "Epoch 1938/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 364.9382 - val_loss: 271.9810\n",
      "Epoch 1939/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 204.2327 - val_loss: 217.6622\n",
      "Epoch 1940/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 173.2505 - val_loss: 215.5048\n",
      "Epoch 1941/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 177.1601 - val_loss: 203.4345\n",
      "Epoch 1942/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 180.6173 - val_loss: 180.6497\n",
      "Epoch 1943/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.8849 - val_loss: 179.1795\n",
      "Epoch 1944/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.9965 - val_loss: 184.3702\n",
      "Epoch 1945/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 160.1354 - val_loss: 169.8850\n",
      "Epoch 1946/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 145.8144 - val_loss: 171.5443\n",
      "Epoch 1947/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.0767 - val_loss: 156.8732\n",
      "Epoch 1948/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.1711 - val_loss: 190.5260\n",
      "Epoch 1949/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.3367 - val_loss: 155.0422\n",
      "Epoch 1950/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 168.5603 - val_loss: 176.3576\n",
      "Epoch 1951/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.6952 - val_loss: 170.6629\n",
      "Epoch 1952/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.4377 - val_loss: 154.4070\n",
      "Epoch 1953/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 165.4992 - val_loss: 154.2302\n",
      "Epoch 1954/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 136.0994 - val_loss: 182.1421\n",
      "Epoch 1955/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.0687 - val_loss: 162.2336\n",
      "Epoch 1956/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.9088 - val_loss: 197.0749\n",
      "Epoch 1957/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.5311 - val_loss: 167.9330\n",
      "Epoch 1958/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.3306 - val_loss: 152.3527\n",
      "Epoch 1959/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.7529 - val_loss: 151.5562\n",
      "Epoch 1960/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.0021 - val_loss: 151.3819\n",
      "Epoch 1961/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 150.9351 - val_loss: 185.2998\n",
      "Epoch 1962/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.1613 - val_loss: 151.0982\n",
      "Epoch 1963/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.4561 - val_loss: 153.0749\n",
      "Epoch 1964/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 132.1691 - val_loss: 150.2359\n",
      "Epoch 1965/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.9724 - val_loss: 147.2594\n",
      "Epoch 1966/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.9481 - val_loss: 151.3444\n",
      "Epoch 1967/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.9692 - val_loss: 150.2890\n",
      "Epoch 1968/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 146.1298 - val_loss: 149.2469\n",
      "Epoch 1969/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.2425 - val_loss: 146.0299\n",
      "Epoch 1970/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.9797 - val_loss: 144.9052\n",
      "Epoch 1971/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.6174 - val_loss: 156.2063\n",
      "Epoch 1972/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 177.1712 - val_loss: 147.6885\n",
      "Epoch 1973/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.4616 - val_loss: 158.0690\n",
      "Epoch 1974/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 140.8781 - val_loss: 148.4132\n",
      "Epoch 1975/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.9568 - val_loss: 154.2121\n",
      "Epoch 1976/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.3538 - val_loss: 197.1145\n",
      "Epoch 1977/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.3360 - val_loss: 183.1775\n",
      "Epoch 1978/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 137.1441 - val_loss: 201.9303\n",
      "Epoch 1979/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.1528 - val_loss: 158.7475\n",
      "Epoch 1980/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 131.8751 - val_loss: 233.5277\n",
      "Epoch 1981/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 136.9262 - val_loss: 349.6543\n",
      "Epoch 1982/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.3307 - val_loss: 146.0146\n",
      "Epoch 1983/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.0829 - val_loss: 220.8697\n",
      "Epoch 1984/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.1798 - val_loss: 201.6754\n",
      "Epoch 1985/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.2545 - val_loss: 140.0356\n",
      "Epoch 1986/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.8125 - val_loss: 144.4875\n",
      "Epoch 1987/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.0179 - val_loss: 142.1172\n",
      "Epoch 1988/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.2811 - val_loss: 157.5328\n",
      "Epoch 1989/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.7631 - val_loss: 155.3844\n",
      "Epoch 1990/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.6379 - val_loss: 144.5888\n",
      "Epoch 1991/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.2649 - val_loss: 146.4546\n",
      "Epoch 1992/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.4134 - val_loss: 144.8865\n",
      "Epoch 1993/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.4935 - val_loss: 141.4010\n",
      "Epoch 1994/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.7629 - val_loss: 142.1147\n",
      "Epoch 1995/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.1532 - val_loss: 152.6490\n",
      "Epoch 1996/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.2641 - val_loss: 157.1799\n",
      "Epoch 1997/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.0189 - val_loss: 182.9927\n",
      "Epoch 1998/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 136.0133 - val_loss: 150.7869\n",
      "Epoch 1999/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.5514 - val_loss: 148.6298\n",
      "Epoch 2000/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.4624 - val_loss: 179.0941\n",
      "Epoch 2001/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.4718 - val_loss: 155.6423\n",
      "Epoch 2002/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.0631 - val_loss: 152.3586\n",
      "Epoch 2003/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.6319 - val_loss: 145.5248\n",
      "Epoch 2004/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.5808 - val_loss: 153.7048\n",
      "Epoch 2005/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.1884 - val_loss: 184.9052\n",
      "Epoch 2006/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 144.1076 - val_loss: 170.0088\n",
      "Epoch 2007/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 168.0745 - val_loss: 162.9656\n",
      "Epoch 2008/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.7941 - val_loss: 143.3364\n",
      "Epoch 2009/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.9092 - val_loss: 164.5195\n",
      "Epoch 2010/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.2545 - val_loss: 144.0496\n",
      "Epoch 2011/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.3174 - val_loss: 177.5749\n",
      "Epoch 2012/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.6453 - val_loss: 165.5365\n",
      "Epoch 2013/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 136.9633 - val_loss: 153.8257\n",
      "Epoch 2014/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.2024 - val_loss: 143.2301\n",
      "Epoch 2015/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 126.3382 - val_loss: 146.3049\n",
      "Epoch 2016/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.7289 - val_loss: 145.1875\n",
      "Epoch 2017/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 281.2935 - val_loss: 644.4805\n",
      "Epoch 2018/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.1015 - val_loss: 164.0042\n",
      "Epoch 2019/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.8245 - val_loss: 172.1012\n",
      "Epoch 2020/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.8539 - val_loss: 148.0548\n",
      "Epoch 2021/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.3692 - val_loss: 157.6250\n",
      "Epoch 2022/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.3176 - val_loss: 187.5691\n",
      "Epoch 2023/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.1679 - val_loss: 144.3804\n",
      "Epoch 2024/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.4939 - val_loss: 145.0650\n",
      "Epoch 2025/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.7653 - val_loss: 145.8642\n",
      "Epoch 2026/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.8049 - val_loss: 151.3251\n",
      "Epoch 2027/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.7755 - val_loss: 150.3779\n",
      "Epoch 2028/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.1040 - val_loss: 152.2480\n",
      "Epoch 2029/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 198.3465 - val_loss: 149.4671\n",
      "Epoch 2030/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.3796 - val_loss: 150.0013\n",
      "Epoch 2031/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.4232 - val_loss: 141.8144\n",
      "Epoch 2032/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.5607 - val_loss: 154.8430\n",
      "Epoch 2033/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.4219 - val_loss: 148.6209\n",
      "Epoch 2034/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.4793 - val_loss: 142.0421\n",
      "Epoch 2035/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 182.7608 - val_loss: 214.4510\n",
      "Epoch 2036/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.3971 - val_loss: 161.9065\n",
      "Epoch 2037/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.9010 - val_loss: 163.0600\n",
      "Epoch 2038/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.0652 - val_loss: 149.6131\n",
      "Epoch 2039/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.8499 - val_loss: 161.7874\n",
      "Epoch 2040/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.6127 - val_loss: 144.4950\n",
      "Epoch 2041/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.9582 - val_loss: 163.5093\n",
      "Epoch 2042/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.2252 - val_loss: 167.0350\n",
      "Epoch 2043/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.2091 - val_loss: 156.5273\n",
      "Epoch 2044/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.5970 - val_loss: 170.5109\n",
      "Epoch 2045/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.9717 - val_loss: 194.7433\n",
      "Epoch 2046/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 152.2112 - val_loss: 179.2973\n",
      "Epoch 2047/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 131.9359 - val_loss: 155.2543\n",
      "Epoch 2048/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.9120 - val_loss: 141.1801\n",
      "Epoch 2049/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.6765 - val_loss: 142.5843\n",
      "Epoch 2050/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.2620 - val_loss: 143.0874\n",
      "Epoch 2051/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.6173 - val_loss: 220.0539\n",
      "Epoch 2052/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.4532 - val_loss: 227.2437\n",
      "Epoch 2053/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 149.5089 - val_loss: 143.9241\n",
      "Epoch 2054/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 128.1659 - val_loss: 143.4314\n",
      "Epoch 2055/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.4191 - val_loss: 252.3253\n",
      "Epoch 2056/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 179.1179 - val_loss: 841.7351\n",
      "Epoch 2057/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.6223 - val_loss: 152.0838\n",
      "Epoch 2058/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.4437 - val_loss: 185.1691\n",
      "Epoch 2059/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.1698 - val_loss: 138.2643\n",
      "Epoch 2060/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.2474 - val_loss: 149.5040\n",
      "Epoch 2061/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.4723 - val_loss: 148.4928\n",
      "Epoch 2062/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.3556 - val_loss: 145.0773\n",
      "Epoch 2063/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.7619 - val_loss: 160.0555\n",
      "Epoch 2064/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.5432 - val_loss: 141.5841\n",
      "Epoch 2065/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.7612 - val_loss: 138.6826\n",
      "Epoch 2066/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.8299 - val_loss: 142.4235\n",
      "Epoch 2067/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.7176 - val_loss: 158.2983\n",
      "Epoch 2068/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 178.2996 - val_loss: 160.8359\n",
      "Epoch 2069/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 126.9742 - val_loss: 140.8656\n",
      "Epoch 2070/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 127.7591 - val_loss: 151.0737\n",
      "Epoch 2071/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 125.9755 - val_loss: 139.8934\n",
      "Epoch 2072/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 156.7763 - val_loss: 150.8141\n",
      "Epoch 2073/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.2366 - val_loss: 140.7275\n",
      "Epoch 2074/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 135.5124 - val_loss: 140.9361\n",
      "Epoch 2075/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.8380 - val_loss: 156.8036\n",
      "Epoch 2076/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.5285 - val_loss: 152.7282\n",
      "Epoch 2077/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.7438 - val_loss: 170.6610\n",
      "Epoch 2078/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.8917 - val_loss: 144.4051\n",
      "Epoch 2079/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.6922 - val_loss: 143.6235\n",
      "Epoch 2080/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.1728 - val_loss: 143.7328\n",
      "Epoch 2081/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.3461 - val_loss: 140.6049\n",
      "Epoch 2082/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 129.5693 - val_loss: 144.0067\n",
      "Epoch 2083/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.4765 - val_loss: 239.8145\n",
      "Epoch 2084/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.0971 - val_loss: 149.3824\n",
      "Epoch 2085/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.1449 - val_loss: 144.7275\n",
      "Epoch 2086/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.9034 - val_loss: 144.7405\n",
      "Epoch 2087/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 126.1802 - val_loss: 167.5900\n",
      "Epoch 2088/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.9568 - val_loss: 156.0999\n",
      "Epoch 2089/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.5838 - val_loss: 145.0631\n",
      "Epoch 2090/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 125.5900 - val_loss: 160.0819\n",
      "Epoch 2091/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.4074 - val_loss: 142.2566\n",
      "Epoch 2092/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 155.3230 - val_loss: 155.1130\n",
      "Epoch 2093/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 127.8608 - val_loss: 156.6780\n",
      "Epoch 2094/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.8089 - val_loss: 152.1789\n",
      "Epoch 2095/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.4922 - val_loss: 186.4359\n",
      "Epoch 2096/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.3646 - val_loss: 155.5664\n",
      "Epoch 2097/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 130.9969 - val_loss: 165.8546\n",
      "Epoch 2098/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.2336 - val_loss: 147.3659\n",
      "Epoch 2099/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.7470 - val_loss: 223.3813\n",
      "Epoch 2100/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 129.1997 - val_loss: 141.1033\n",
      "Epoch 2101/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 133.5367 - val_loss: 140.6407\n",
      "Epoch 2102/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.8247 - val_loss: 170.6123\n",
      "Epoch 2103/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.7289 - val_loss: 136.9866\n",
      "Epoch 2104/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 130.4790 - val_loss: 151.1183\n",
      "Epoch 2105/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 127.5303 - val_loss: 166.7470\n",
      "Epoch 2106/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 154.1600 - val_loss: 230.6666\n",
      "Epoch 2107/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.0461 - val_loss: 150.2378\n",
      "Epoch 2108/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 130.5533 - val_loss: 140.9841\n",
      "Epoch 2109/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 125.4460 - val_loss: 144.2418\n",
      "Epoch 2110/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.8913 - val_loss: 140.6628\n",
      "Epoch 2111/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.2056 - val_loss: 166.6096\n",
      "Epoch 2112/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.0464 - val_loss: 147.9010\n",
      "Epoch 2113/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.3520 - val_loss: 175.2882\n",
      "Epoch 2114/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.0988 - val_loss: 157.6648\n",
      "Epoch 2115/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.6459 - val_loss: 136.4186\n",
      "Epoch 2116/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.1260 - val_loss: 141.3673\n",
      "Epoch 2117/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.9428 - val_loss: 144.5423\n",
      "Epoch 2118/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 124.4787 - val_loss: 158.6337\n",
      "Epoch 2119/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.3126 - val_loss: 137.4162\n",
      "Epoch 2120/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 129.5333 - val_loss: 147.6308\n",
      "Epoch 2121/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 127.8815 - val_loss: 140.0303\n",
      "Epoch 2122/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 124.5827 - val_loss: 164.4258\n",
      "Epoch 2123/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.8692 - val_loss: 147.5480\n",
      "Epoch 2124/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.5711 - val_loss: 149.1005\n",
      "Epoch 2125/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.1414 - val_loss: 147.5264\n",
      "Epoch 2126/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 126.6393 - val_loss: 138.3380\n",
      "Epoch 2127/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.3119 - val_loss: 138.8405\n",
      "Epoch 2128/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 150.4923 - val_loss: 153.6390\n",
      "Epoch 2129/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 124.6251 - val_loss: 142.1104\n",
      "Epoch 2130/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 123.0887 - val_loss: 150.0694\n",
      "Epoch 2131/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 127.8950 - val_loss: 164.0433\n",
      "Epoch 2132/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 138.4951 - val_loss: 142.8893\n",
      "Epoch 2133/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 122.2074 - val_loss: 145.2957\n",
      "Epoch 2134/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 125.4593 - val_loss: 141.4001\n",
      "Epoch 2135/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 127.9247 - val_loss: 147.1485\n",
      "Epoch 2136/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.7940 - val_loss: 139.4124\n",
      "Epoch 2137/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 138.6369 - val_loss: 154.2308\n",
      "Epoch 2138/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 121.8018 - val_loss: 144.7471\n",
      "Epoch 2139/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 129.5801 - val_loss: 153.8429\n",
      "Epoch 2140/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 123.7833 - val_loss: 215.8665\n",
      "Epoch 2141/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.7335 - val_loss: 139.4854\n",
      "Epoch 2142/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.0538 - val_loss: 143.8429\n",
      "Epoch 2143/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.0691 - val_loss: 159.1922\n",
      "Epoch 2144/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.0222 - val_loss: 144.6659\n",
      "Epoch 2145/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.2045 - val_loss: 150.0749\n",
      "Epoch 2146/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.9366 - val_loss: 153.5718\n",
      "Epoch 2147/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.4916 - val_loss: 227.4170\n",
      "Epoch 2148/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.2180 - val_loss: 230.2875\n",
      "Epoch 2149/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.0379 - val_loss: 157.1330\n",
      "Epoch 2150/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 124.8153 - val_loss: 145.4635\n",
      "Epoch 2151/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.5324 - val_loss: 161.8437\n",
      "Epoch 2152/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.6220 - val_loss: 179.1353\n",
      "Epoch 2153/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.3101 - val_loss: 154.6158\n",
      "Epoch 2154/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.3452 - val_loss: 152.1000\n",
      "Epoch 2155/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.1304 - val_loss: 146.0942\n",
      "Epoch 2156/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.9457 - val_loss: 146.7224\n",
      "Epoch 2157/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.5458 - val_loss: 137.1333\n",
      "Epoch 2158/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.6492 - val_loss: 140.8668\n",
      "Epoch 2159/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 132.8815 - val_loss: 141.3583\n",
      "Epoch 2160/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.3218 - val_loss: 137.1382\n",
      "Epoch 2161/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.2338 - val_loss: 143.9511\n",
      "Epoch 2162/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.1135 - val_loss: 137.7677\n",
      "Epoch 2163/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.5463 - val_loss: 149.9668\n",
      "Epoch 2164/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 128.4351 - val_loss: 222.1942\n",
      "Epoch 2165/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.2368 - val_loss: 144.0527\n",
      "Epoch 2166/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.6446 - val_loss: 144.7573\n",
      "Epoch 2167/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 131.4950 - val_loss: 163.8429\n",
      "Epoch 2168/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.7069 - val_loss: 162.3933\n",
      "Epoch 2169/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.3711 - val_loss: 155.9799\n",
      "Epoch 2170/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.5623 - val_loss: 151.7370\n",
      "Epoch 2171/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 129.8021 - val_loss: 138.3892\n",
      "Epoch 2172/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.3615 - val_loss: 144.3728\n",
      "Epoch 2173/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.7020 - val_loss: 201.9624\n",
      "Epoch 2174/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.7151 - val_loss: 149.2119\n",
      "Epoch 2175/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 128.8756 - val_loss: 174.8607\n",
      "Epoch 2176/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.7567 - val_loss: 137.7459\n",
      "Epoch 2177/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.5198 - val_loss: 216.8810\n",
      "Epoch 2178/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.4021 - val_loss: 165.1113\n",
      "Epoch 2179/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.0419 - val_loss: 240.0994\n",
      "Epoch 2180/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.3682 - val_loss: 143.7955\n",
      "Epoch 2181/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.0795 - val_loss: 156.1782\n",
      "Epoch 2182/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 184.5157 - val_loss: 245.3422\n",
      "Epoch 2183/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.7442 - val_loss: 135.3573\n",
      "Epoch 2184/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.3131 - val_loss: 153.6172\n",
      "Epoch 2185/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.5023 - val_loss: 140.9795\n",
      "Epoch 2186/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.0218 - val_loss: 144.9118\n",
      "Epoch 2187/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.2532 - val_loss: 159.1961\n",
      "Epoch 2188/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.0569 - val_loss: 152.2571\n",
      "Epoch 2189/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.0432 - val_loss: 137.0279\n",
      "Epoch 2190/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.1393 - val_loss: 158.2936\n",
      "Epoch 2191/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.9942 - val_loss: 144.3406\n",
      "Epoch 2192/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.7060 - val_loss: 134.4438\n",
      "Epoch 2193/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 195.5202 - val_loss: 141.3111\n",
      "Epoch 2194/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 122.0490 - val_loss: 151.9204\n",
      "Epoch 2195/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.7554 - val_loss: 164.2282\n",
      "Epoch 2196/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 132.3296 - val_loss: 139.0647\n",
      "Epoch 2197/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.3113 - val_loss: 154.3893\n",
      "Epoch 2198/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.5508 - val_loss: 140.2270\n",
      "Epoch 2199/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 127.1668 - val_loss: 153.2028\n",
      "Epoch 2200/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.9447 - val_loss: 160.7856\n",
      "Epoch 2201/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 162.7219 - val_loss: 405.3004\n",
      "Epoch 2202/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 134.3977 - val_loss: 147.7221\n",
      "Epoch 2203/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 124.2013 - val_loss: 145.9071\n",
      "Epoch 2204/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 129.7416 - val_loss: 140.4684\n",
      "Epoch 2205/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 128.9591 - val_loss: 149.7664\n",
      "Epoch 2206/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.7214 - val_loss: 162.7818\n",
      "Epoch 2207/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.4170 - val_loss: 140.9772\n",
      "Epoch 2208/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.7581 - val_loss: 180.7725\n",
      "Epoch 2209/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.5788 - val_loss: 142.1080\n",
      "Epoch 2210/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 121.2033 - val_loss: 142.3242\n",
      "Epoch 2211/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.5414 - val_loss: 153.5587\n",
      "Epoch 2212/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.2541 - val_loss: 157.1656\n",
      "Epoch 2213/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 160.4313 - val_loss: 144.9899\n",
      "Epoch 2214/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 128.5924 - val_loss: 138.9139\n",
      "Epoch 2215/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.3748 - val_loss: 136.9309\n",
      "Epoch 2216/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 129.6829 - val_loss: 172.3370\n",
      "Epoch 2217/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.7387 - val_loss: 165.7031\n",
      "Epoch 2218/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 130.1811 - val_loss: 144.0065\n",
      "Epoch 2219/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.1975 - val_loss: 148.1127\n",
      "Epoch 2220/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.4163 - val_loss: 136.9374\n",
      "Epoch 2221/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 172.0863 - val_loss: 140.4030\n",
      "Epoch 2222/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.3399 - val_loss: 152.7405\n",
      "Epoch 2223/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.0174 - val_loss: 153.5680\n",
      "Epoch 2224/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.8716 - val_loss: 137.1232\n",
      "Epoch 2225/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.9800 - val_loss: 153.1411\n",
      "Epoch 2226/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.3548 - val_loss: 149.0015\n",
      "Epoch 2227/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.7906 - val_loss: 196.7943\n",
      "Epoch 2228/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.0725 - val_loss: 154.6834\n",
      "Epoch 2229/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 129.1851 - val_loss: 159.4174\n",
      "Epoch 2230/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.8707 - val_loss: 142.4623\n",
      "Epoch 2231/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.6078 - val_loss: 141.4106\n",
      "Epoch 2232/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.1108 - val_loss: 144.6207\n",
      "Epoch 2233/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.4735 - val_loss: 168.3434\n",
      "Epoch 2234/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.9737 - val_loss: 147.3492\n",
      "Epoch 2235/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.9101 - val_loss: 174.7356\n",
      "Epoch 2236/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.7106 - val_loss: 145.5595\n",
      "Epoch 2237/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.8221 - val_loss: 143.0008\n",
      "Epoch 2238/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 133.7179 - val_loss: 138.8176\n",
      "Epoch 2239/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.7764 - val_loss: 144.3801\n",
      "Epoch 2240/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.3552 - val_loss: 154.3303\n",
      "Epoch 2241/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 132.3138 - val_loss: 138.5010\n",
      "Epoch 2242/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.5197 - val_loss: 155.9165\n",
      "Epoch 2243/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.5295 - val_loss: 143.8948\n",
      "Epoch 2244/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.0043 - val_loss: 142.7244\n",
      "Epoch 2245/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.5485 - val_loss: 199.4643\n",
      "Epoch 2246/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.8881 - val_loss: 138.5764\n",
      "Epoch 2247/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.5716 - val_loss: 155.8516\n",
      "Epoch 2248/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.8876 - val_loss: 185.1722\n",
      "Epoch 2249/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.9803 - val_loss: 139.7611\n",
      "Epoch 2250/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.7412 - val_loss: 185.7293\n",
      "Epoch 2251/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.3235 - val_loss: 142.1834\n",
      "Epoch 2252/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 126.6667 - val_loss: 155.4599\n",
      "Epoch 2253/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.2999 - val_loss: 139.6983\n",
      "Epoch 2254/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 129.1513 - val_loss: 144.7453\n",
      "Epoch 2255/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.2200 - val_loss: 139.1374\n",
      "Epoch 2256/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.2817 - val_loss: 148.1284\n",
      "Epoch 2257/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.5500 - val_loss: 169.9699\n",
      "Epoch 2258/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 127.1085 - val_loss: 138.1607\n",
      "Epoch 2259/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 166.7716 - val_loss: 137.2861\n",
      "Epoch 2260/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.5643 - val_loss: 162.4576\n",
      "Epoch 2261/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.1771 - val_loss: 140.9489\n",
      "Epoch 2262/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.0173 - val_loss: 152.1544\n",
      "Epoch 2263/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.4290 - val_loss: 162.2671\n",
      "Epoch 2264/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.0530 - val_loss: 170.2565\n",
      "Epoch 2265/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 128.8603 - val_loss: 167.1539\n",
      "Epoch 2266/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.0709 - val_loss: 152.0104\n",
      "Epoch 2267/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 123.7444 - val_loss: 142.3519\n",
      "Epoch 2268/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.2431 - val_loss: 154.5826\n",
      "Epoch 2269/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.6110 - val_loss: 141.7200\n",
      "Epoch 2270/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.9417 - val_loss: 157.2468\n",
      "Epoch 2271/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 130.2100 - val_loss: 138.2897\n",
      "Epoch 2272/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.3725 - val_loss: 149.0193\n",
      "Epoch 2273/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 137.8721 - val_loss: 152.7522\n",
      "Epoch 2274/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 133.2340 - val_loss: 156.1665\n",
      "Epoch 2275/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.9575 - val_loss: 158.6177\n",
      "Epoch 2276/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.6005 - val_loss: 144.3096\n",
      "Epoch 2277/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.7306 - val_loss: 137.8606\n",
      "Epoch 2278/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.5030 - val_loss: 152.2423\n",
      "Epoch 2279/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.9389 - val_loss: 144.6862\n",
      "Epoch 2280/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.3325 - val_loss: 140.9799\n",
      "Epoch 2281/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.7953 - val_loss: 175.1373\n",
      "Epoch 2282/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 140.8841 - val_loss: 149.5904\n",
      "Epoch 2283/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 124.7871 - val_loss: 143.1535\n",
      "Epoch 2284/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 130.6223 - val_loss: 162.9128\n",
      "Epoch 2285/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.2179 - val_loss: 151.0446\n",
      "Epoch 2286/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.5557 - val_loss: 151.6614\n",
      "Epoch 2287/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.9181 - val_loss: 157.4374\n",
      "Epoch 2288/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.7590 - val_loss: 139.7729\n",
      "Epoch 2289/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.3138 - val_loss: 138.1441\n",
      "Epoch 2290/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 122.5394 - val_loss: 160.8377\n",
      "Epoch 2291/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.6709 - val_loss: 234.0895\n",
      "Epoch 2292/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.6886 - val_loss: 178.5204\n",
      "Epoch 2293/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 127.2527 - val_loss: 143.2754\n",
      "Epoch 2294/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.8319 - val_loss: 161.2363\n",
      "Epoch 2295/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.1702 - val_loss: 154.0420\n",
      "Epoch 2296/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 134.6185 - val_loss: 241.6149\n",
      "Epoch 2297/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 142.7869 - val_loss: 157.6036\n",
      "Epoch 2298/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.3977 - val_loss: 154.7875\n",
      "Epoch 2299/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.6206 - val_loss: 142.7518\n",
      "Epoch 2300/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.1572 - val_loss: 155.6991\n",
      "Epoch 2301/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.6437 - val_loss: 141.8168\n",
      "Epoch 2302/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.8774 - val_loss: 138.9618\n",
      "Epoch 2303/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.2712 - val_loss: 146.1656\n",
      "Epoch 2304/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 133.3427 - val_loss: 146.3564\n",
      "Epoch 2305/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.6376 - val_loss: 168.7300\n",
      "Epoch 2306/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.6721 - val_loss: 155.5123\n",
      "Epoch 2307/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.8668 - val_loss: 261.1191\n",
      "Epoch 2308/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 177.7675 - val_loss: 141.7890\n",
      "Epoch 2309/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.7805 - val_loss: 195.3272\n",
      "Epoch 2310/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.1931 - val_loss: 212.7890\n",
      "Epoch 2311/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.3024 - val_loss: 153.9806\n",
      "Epoch 2312/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 123.9775 - val_loss: 181.7589\n",
      "Epoch 2313/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.8273 - val_loss: 142.8984\n",
      "Epoch 2314/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.0025 - val_loss: 143.1448\n",
      "Epoch 2315/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.4057 - val_loss: 196.2624\n",
      "Epoch 2316/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.6301 - val_loss: 138.4809\n",
      "Epoch 2317/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 126.5868 - val_loss: 148.5974\n",
      "Epoch 2318/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.0229 - val_loss: 164.7455\n",
      "Epoch 2319/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.6129 - val_loss: 164.3761\n",
      "Epoch 2320/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 123.5535 - val_loss: 175.1681\n",
      "Epoch 2321/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.3849 - val_loss: 230.1347\n",
      "Epoch 2322/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.4745 - val_loss: 135.3710\n",
      "Epoch 2323/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.9111 - val_loss: 142.1028\n",
      "Epoch 2324/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.6873 - val_loss: 137.6342\n",
      "Epoch 2325/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.7079 - val_loss: 141.1562\n",
      "Epoch 2326/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 122.7785 - val_loss: 143.7235\n",
      "Epoch 2327/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.1600 - val_loss: 140.6169\n",
      "Epoch 2328/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 127.8389 - val_loss: 140.9873\n",
      "Epoch 2329/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.7943 - val_loss: 166.1816\n",
      "Epoch 2330/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.5192 - val_loss: 145.7493\n",
      "Epoch 2331/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.6571 - val_loss: 175.1974\n",
      "Epoch 2332/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 137.8622 - val_loss: 142.7587\n",
      "Epoch 2333/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.6898 - val_loss: 275.2945\n",
      "Epoch 2334/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 133.8575 - val_loss: 137.7340\n",
      "Epoch 2335/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.4584 - val_loss: 186.2156\n",
      "Epoch 2336/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.8009 - val_loss: 142.6306\n",
      "Epoch 2337/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.8315 - val_loss: 147.6954\n",
      "Epoch 2338/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.1945 - val_loss: 159.6109\n",
      "Epoch 2339/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.0828 - val_loss: 139.7525\n",
      "Epoch 2340/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.0150 - val_loss: 180.1034\n",
      "Epoch 2341/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.3405 - val_loss: 140.8700\n",
      "Epoch 2342/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 131.1882 - val_loss: 136.4625\n",
      "Epoch 2343/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.9508 - val_loss: 172.9390\n",
      "Epoch 2344/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.9266 - val_loss: 138.2637\n",
      "Epoch 2345/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.1293 - val_loss: 139.6640\n",
      "Epoch 2346/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.8117 - val_loss: 147.8924\n",
      "Epoch 2347/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.3816 - val_loss: 264.7513\n",
      "Epoch 2348/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.7752 - val_loss: 163.3458\n",
      "Epoch 2349/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 189.8326 - val_loss: 141.8776\n",
      "Epoch 2350/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 122.3860 - val_loss: 141.2537\n",
      "Epoch 2351/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.7637 - val_loss: 175.3211\n",
      "Epoch 2352/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 130.1501 - val_loss: 142.0400\n",
      "Epoch 2353/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.9661 - val_loss: 145.6704\n",
      "Epoch 2354/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.2351 - val_loss: 143.2895\n",
      "Epoch 2355/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.4905 - val_loss: 140.9243\n",
      "Epoch 2356/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.4056 - val_loss: 168.7257\n",
      "Epoch 2357/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.2158 - val_loss: 179.5624\n",
      "Epoch 2358/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.7123 - val_loss: 139.2905\n",
      "Epoch 2359/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.0634 - val_loss: 134.8990\n",
      "Epoch 2360/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.0262 - val_loss: 142.7817\n",
      "Epoch 2361/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 160.9597 - val_loss: 234.3651\n",
      "Epoch 2362/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 123.3713 - val_loss: 136.8845\n",
      "Epoch 2363/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 119.1493 - val_loss: 140.1496\n",
      "Epoch 2364/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.4247 - val_loss: 135.6327\n",
      "Epoch 2365/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 122.2906 - val_loss: 170.2917\n",
      "Epoch 2366/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.8013 - val_loss: 139.0109\n",
      "Epoch 2367/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.3220 - val_loss: 158.1711\n",
      "Epoch 2368/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.1416 - val_loss: 158.3232\n",
      "Epoch 2369/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.9843 - val_loss: 144.2674\n",
      "Epoch 2370/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.9785 - val_loss: 148.4380\n",
      "Epoch 2371/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.4628 - val_loss: 151.6800\n",
      "Epoch 2372/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.5483 - val_loss: 168.7630\n",
      "Epoch 2373/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 129.4390 - val_loss: 137.6087\n",
      "Epoch 2374/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.7301 - val_loss: 145.4263\n",
      "Epoch 2375/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.4109 - val_loss: 145.1205\n",
      "Epoch 2376/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.0204 - val_loss: 136.1005\n",
      "Epoch 2377/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 126.0288 - val_loss: 140.4749\n",
      "Epoch 2378/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.6236 - val_loss: 138.3853\n",
      "Epoch 2379/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.3443 - val_loss: 157.3226\n",
      "Epoch 2380/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.1240 - val_loss: 142.8889\n",
      "Epoch 2381/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.1746 - val_loss: 148.5680\n",
      "Epoch 2382/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 132.1168 - val_loss: 154.4704\n",
      "Epoch 2383/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.1276 - val_loss: 151.2455\n",
      "Epoch 2384/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.3362 - val_loss: 142.2567\n",
      "Epoch 2385/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.1052 - val_loss: 148.7910\n",
      "Epoch 2386/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 136.4668 - val_loss: 145.6463\n",
      "Epoch 2387/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.1114 - val_loss: 159.3651\n",
      "Epoch 2388/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.3034 - val_loss: 152.5936\n",
      "Epoch 2389/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.6196 - val_loss: 146.2943\n",
      "Epoch 2390/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.1737 - val_loss: 146.0996\n",
      "Epoch 2391/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 118.4253 - val_loss: 146.0186\n",
      "Epoch 2392/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 129.3866 - val_loss: 157.5552\n",
      "Epoch 2393/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.6618 - val_loss: 151.5251\n",
      "Epoch 2394/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.5612 - val_loss: 197.1117\n",
      "Epoch 2395/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.3950 - val_loss: 144.7112\n",
      "Epoch 2396/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.3091 - val_loss: 163.1089\n",
      "Epoch 2397/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.2366 - val_loss: 147.0164\n",
      "Epoch 2398/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.5739 - val_loss: 172.4024\n",
      "Epoch 2399/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.3582 - val_loss: 165.6377\n",
      "Epoch 2400/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.5979 - val_loss: 169.7353\n",
      "Epoch 2401/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.8943 - val_loss: 150.3263\n",
      "Epoch 2402/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.6664 - val_loss: 146.0395\n",
      "Epoch 2403/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.1869 - val_loss: 150.2501\n",
      "Epoch 2404/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 128.7234 - val_loss: 164.7037\n",
      "Epoch 2405/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.2449 - val_loss: 156.5056\n",
      "Epoch 2406/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.6202 - val_loss: 140.8525\n",
      "Epoch 2407/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.0528 - val_loss: 145.8154\n",
      "Epoch 2408/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.0878 - val_loss: 147.7801\n",
      "Epoch 2409/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 198.1347 - val_loss: 150.5262\n",
      "Epoch 2410/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.8049 - val_loss: 158.3621\n",
      "Epoch 2411/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 119.2525 - val_loss: 138.8270\n",
      "Epoch 2412/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.9065 - val_loss: 184.2262\n",
      "Epoch 2413/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 120.3103 - val_loss: 141.1844\n",
      "Epoch 2414/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.1315 - val_loss: 159.1288\n",
      "Epoch 2415/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.4056 - val_loss: 140.2457\n",
      "Epoch 2416/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.5952 - val_loss: 168.9335\n",
      "Epoch 2417/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.8779 - val_loss: 135.6670\n",
      "Epoch 2418/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.3955 - val_loss: 167.5437\n",
      "Epoch 2419/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.4870 - val_loss: 142.2846\n",
      "Epoch 2420/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 118.6863 - val_loss: 165.8931\n",
      "Epoch 2421/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.3365 - val_loss: 150.0072\n",
      "Epoch 2422/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 123.1937 - val_loss: 152.9723\n",
      "Epoch 2423/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.3604 - val_loss: 160.5079\n",
      "Epoch 2424/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 126.8694 - val_loss: 153.8884\n",
      "Epoch 2425/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.1300 - val_loss: 161.0069\n",
      "Epoch 2426/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.2688 - val_loss: 143.0139\n",
      "Epoch 2427/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.0866 - val_loss: 138.2351\n",
      "Epoch 2428/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 121.7235 - val_loss: 145.8522\n",
      "Epoch 2429/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.5665 - val_loss: 138.8424\n",
      "Epoch 2430/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.9257 - val_loss: 187.8345\n",
      "Epoch 2431/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.5918 - val_loss: 175.4066\n",
      "Epoch 2432/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.7928 - val_loss: 155.1723\n",
      "Epoch 2433/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 122.5447 - val_loss: 148.7400\n",
      "Epoch 2434/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.9253 - val_loss: 205.9876\n",
      "Epoch 2435/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.1988 - val_loss: 141.4249\n",
      "Epoch 2436/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.5308 - val_loss: 143.0127\n",
      "Epoch 2437/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.5567 - val_loss: 140.6581\n",
      "Epoch 2438/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.8169 - val_loss: 140.7131\n",
      "Epoch 2439/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.6488 - val_loss: 136.6352\n",
      "Epoch 2440/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 120.3001 - val_loss: 147.2222\n",
      "Epoch 2441/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 125.3551 - val_loss: 137.9330\n",
      "Epoch 2442/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 128.0694 - val_loss: 178.2573\n",
      "Epoch 2443/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 129.0754 - val_loss: 143.0296\n",
      "Epoch 2444/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.7484 - val_loss: 142.3796\n",
      "Epoch 2445/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.5981 - val_loss: 141.0646\n",
      "Epoch 2446/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.4304 - val_loss: 163.4744\n",
      "Epoch 2447/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.0857 - val_loss: 141.2901\n",
      "Epoch 2448/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 120.2546 - val_loss: 141.8075\n",
      "Epoch 2449/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.2643 - val_loss: 137.0467\n",
      "Epoch 2450/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 121.1405 - val_loss: 137.4984\n",
      "Epoch 2451/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.4087 - val_loss: 141.3685\n",
      "Epoch 2452/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.4514 - val_loss: 179.7514\n",
      "Epoch 2453/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.1732 - val_loss: 142.7698\n",
      "Epoch 2454/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.1747 - val_loss: 139.8656\n",
      "Epoch 2455/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.3510 - val_loss: 177.2192\n",
      "Epoch 2456/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.5723 - val_loss: 139.2018\n",
      "Epoch 2457/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.5640 - val_loss: 137.0261\n",
      "Epoch 2458/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.8834 - val_loss: 144.3581\n",
      "Epoch 2459/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.8608 - val_loss: 140.7784\n",
      "Epoch 2460/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.6735 - val_loss: 158.9679\n",
      "Epoch 2461/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.5180 - val_loss: 136.3358\n",
      "Epoch 2462/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.3926 - val_loss: 138.3924\n",
      "Epoch 2463/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.9788 - val_loss: 153.9660\n",
      "Epoch 2464/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.4010 - val_loss: 138.9640\n",
      "Epoch 2465/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.2412 - val_loss: 159.9681\n",
      "Epoch 2466/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 120.8551 - val_loss: 162.4874\n",
      "Epoch 2467/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.6278 - val_loss: 141.1109\n",
      "Epoch 2468/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.2036 - val_loss: 147.1323\n",
      "Epoch 2469/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.5315 - val_loss: 153.2161\n",
      "Epoch 2470/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 128.4564 - val_loss: 151.0587\n",
      "Epoch 2471/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.6544 - val_loss: 138.5516\n",
      "Epoch 2472/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.6296 - val_loss: 141.3810\n",
      "Epoch 2473/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.2556 - val_loss: 154.3514\n",
      "Epoch 2474/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 122.9873 - val_loss: 146.4816\n",
      "Epoch 2475/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.0053 - val_loss: 155.1373\n",
      "Epoch 2476/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 126.6040 - val_loss: 138.5026\n",
      "Epoch 2477/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.9593 - val_loss: 139.2482\n",
      "Epoch 2478/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.1413 - val_loss: 136.7792\n",
      "Epoch 2479/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.2913 - val_loss: 152.4940\n",
      "Epoch 2480/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.4033 - val_loss: 168.6014\n",
      "Epoch 2481/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.0593 - val_loss: 145.1436\n",
      "Epoch 2482/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 126.3442 - val_loss: 146.8341\n",
      "Epoch 2483/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.8383 - val_loss: 193.9775\n",
      "Epoch 2484/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.6226 - val_loss: 146.1910\n",
      "Epoch 2485/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.0589 - val_loss: 169.7064\n",
      "Epoch 2486/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.6611 - val_loss: 143.8574\n",
      "Epoch 2487/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.9784 - val_loss: 160.2808\n",
      "Epoch 2488/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 147.2958 - val_loss: 148.0207\n",
      "Epoch 2489/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.4732 - val_loss: 143.0698\n",
      "Epoch 2490/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.7798 - val_loss: 145.5074\n",
      "Epoch 2491/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.2511 - val_loss: 164.4460\n",
      "Epoch 2492/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 149.1966 - val_loss: 150.1089\n",
      "Epoch 2493/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.6741 - val_loss: 149.1949\n",
      "Epoch 2494/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 119.5625 - val_loss: 136.0326\n",
      "Epoch 2495/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 119.5078 - val_loss: 148.8313\n",
      "Epoch 2496/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.9301 - val_loss: 135.2602\n",
      "Epoch 2497/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.6056 - val_loss: 152.2667\n",
      "Epoch 2498/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.2228 - val_loss: 136.5037\n",
      "Epoch 2499/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.7783 - val_loss: 136.7789\n",
      "Epoch 2500/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.3067 - val_loss: 327.3234\n",
      "Epoch 2501/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 162.1762 - val_loss: 149.6547\n",
      "Epoch 2502/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.2111 - val_loss: 138.6652\n",
      "Epoch 2503/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.6617 - val_loss: 140.3230\n",
      "Epoch 2504/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 120.1946 - val_loss: 154.6725\n",
      "Epoch 2505/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.1923 - val_loss: 139.6198\n",
      "Epoch 2506/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.2581 - val_loss: 138.0792\n",
      "Epoch 2507/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.4652 - val_loss: 147.7882\n",
      "Epoch 2508/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.8442 - val_loss: 152.1696\n",
      "Epoch 2509/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.7723 - val_loss: 164.4379\n",
      "Epoch 2510/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.7566 - val_loss: 203.8662\n",
      "Epoch 2511/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.0093 - val_loss: 143.1472\n",
      "Epoch 2512/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 120.5344 - val_loss: 212.4494\n",
      "Epoch 2513/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.6282 - val_loss: 139.4619\n",
      "Epoch 2514/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 121.0999 - val_loss: 139.6277\n",
      "Epoch 2515/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.6100 - val_loss: 200.7582\n",
      "Epoch 2516/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.2956 - val_loss: 146.0408\n",
      "Epoch 2517/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.9434 - val_loss: 147.9662\n",
      "Epoch 2518/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.8002 - val_loss: 172.6177\n",
      "Epoch 2519/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 144.6570 - val_loss: 154.4167\n",
      "Epoch 2520/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 124.2151 - val_loss: 186.6536\n",
      "Epoch 2521/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 124.8741 - val_loss: 148.8652\n",
      "Epoch 2522/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.1527 - val_loss: 137.7166\n",
      "Epoch 2523/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.6387 - val_loss: 177.5325\n",
      "Epoch 2524/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.1223 - val_loss: 258.0974\n",
      "Epoch 2525/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.6410 - val_loss: 138.7434\n",
      "Epoch 2526/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.4183 - val_loss: 144.7536\n",
      "Epoch 2527/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.0617 - val_loss: 139.3833\n",
      "Epoch 2528/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.5362 - val_loss: 181.5799\n",
      "Epoch 2529/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.1009 - val_loss: 142.1885\n",
      "Epoch 2530/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.2978 - val_loss: 144.1763\n",
      "Epoch 2531/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.1547 - val_loss: 146.0085\n",
      "Epoch 2532/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.6799 - val_loss: 163.3838\n",
      "Epoch 2533/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.5580 - val_loss: 152.0981\n",
      "Epoch 2534/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.4638 - val_loss: 185.2151\n",
      "Epoch 2535/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 133.1930 - val_loss: 196.5188\n",
      "Epoch 2536/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.9351 - val_loss: 143.5915\n",
      "Epoch 2537/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.8838 - val_loss: 141.3321\n",
      "Epoch 2538/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 176.6401 - val_loss: 134.4518\n",
      "Epoch 2539/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.2360 - val_loss: 149.9494\n",
      "Epoch 2540/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 139.4267 - val_loss: 143.4893\n",
      "Epoch 2541/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.3899 - val_loss: 143.5306\n",
      "Epoch 2542/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.6575 - val_loss: 142.2109\n",
      "Epoch 2543/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.3744 - val_loss: 153.4992\n",
      "Epoch 2544/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.1720 - val_loss: 141.4750\n",
      "Epoch 2545/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.1642 - val_loss: 146.3080\n",
      "Epoch 2546/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 131.3243 - val_loss: 141.7321\n",
      "Epoch 2547/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.0056 - val_loss: 159.9117\n",
      "Epoch 2548/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.6798 - val_loss: 152.5190\n",
      "Epoch 2549/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 136.7519 - val_loss: 152.0721\n",
      "Epoch 2550/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.6230 - val_loss: 136.6134\n",
      "Epoch 2551/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 119.7588 - val_loss: 175.4803\n",
      "Epoch 2552/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.2941 - val_loss: 144.0342\n",
      "Epoch 2553/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.0805 - val_loss: 154.2879\n",
      "Epoch 2554/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.5139 - val_loss: 146.2280\n",
      "Epoch 2555/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 126.1813 - val_loss: 145.7803\n",
      "Epoch 2556/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.4677 - val_loss: 143.5615\n",
      "Epoch 2557/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.2128 - val_loss: 227.1023\n",
      "Epoch 2558/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.7204 - val_loss: 190.2830\n",
      "Epoch 2559/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.0995 - val_loss: 212.3617\n",
      "Epoch 2560/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.2848 - val_loss: 143.5176\n",
      "Epoch 2561/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.6323 - val_loss: 229.2592\n",
      "Epoch 2562/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.5535 - val_loss: 165.2414\n",
      "Epoch 2563/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 129.4031 - val_loss: 154.4944\n",
      "Epoch 2564/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.0469 - val_loss: 160.3041\n",
      "Epoch 2565/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 122.3513 - val_loss: 168.4820\n",
      "Epoch 2566/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.8663 - val_loss: 146.6719\n",
      "Epoch 2567/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 122.8414 - val_loss: 169.7094\n",
      "Epoch 2568/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.7192 - val_loss: 143.4394\n",
      "Epoch 2569/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 126.3024 - val_loss: 171.5575\n",
      "Epoch 2570/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.7559 - val_loss: 150.0706\n",
      "Epoch 2571/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.1606 - val_loss: 140.8239\n",
      "Epoch 2572/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.2134 - val_loss: 145.7423\n",
      "Epoch 2573/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.3295 - val_loss: 138.3216\n",
      "Epoch 2574/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.5757 - val_loss: 138.6667\n",
      "Epoch 2575/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.5892 - val_loss: 158.1604\n",
      "Epoch 2576/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.2251 - val_loss: 168.9389\n",
      "Epoch 2577/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.4723 - val_loss: 148.9533\n",
      "Epoch 2578/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.3266 - val_loss: 154.1597\n",
      "Epoch 2579/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.8824 - val_loss: 196.0408\n",
      "Epoch 2580/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.2982 - val_loss: 163.6928\n",
      "Epoch 2581/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.4258 - val_loss: 145.1366\n",
      "Epoch 2582/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.1977 - val_loss: 196.3943\n",
      "Epoch 2583/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 121.6092 - val_loss: 202.3434\n",
      "Epoch 2584/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.2423 - val_loss: 184.0543\n",
      "Epoch 2585/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 128.7735 - val_loss: 143.1788\n",
      "Epoch 2586/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.3827 - val_loss: 164.6697\n",
      "Epoch 2587/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.4946 - val_loss: 284.5222\n",
      "Epoch 2588/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.2840 - val_loss: 150.9994\n",
      "Epoch 2589/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.1405 - val_loss: 142.1874\n",
      "Epoch 2590/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.0140 - val_loss: 166.2479\n",
      "Epoch 2591/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.6959 - val_loss: 148.1911\n",
      "Epoch 2592/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 183.0774 - val_loss: 140.2808\n",
      "Epoch 2593/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.0319 - val_loss: 140.9681\n",
      "Epoch 2594/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.8447 - val_loss: 144.3715\n",
      "Epoch 2595/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.7543 - val_loss: 141.7646\n",
      "Epoch 2596/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.0041 - val_loss: 139.8363\n",
      "Epoch 2597/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.4726 - val_loss: 135.1147\n",
      "Epoch 2598/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 117.1484 - val_loss: 135.1588\n",
      "Epoch 2599/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 122.9748 - val_loss: 143.9736\n",
      "Epoch 2600/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 120.2686 - val_loss: 141.5384\n",
      "Epoch 2601/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 136.9946 - val_loss: 154.5826\n",
      "Epoch 2602/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 121.9158 - val_loss: 157.4010\n",
      "Epoch 2603/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.5879 - val_loss: 169.3196\n",
      "Epoch 2604/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.8140 - val_loss: 138.2184\n",
      "Epoch 2605/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.7882 - val_loss: 173.3285\n",
      "Epoch 2606/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.3727 - val_loss: 174.9600\n",
      "Epoch 2607/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.3151 - val_loss: 139.6763\n",
      "Epoch 2608/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.7645 - val_loss: 147.0057\n",
      "Epoch 2609/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.7499 - val_loss: 147.4273\n",
      "Epoch 2610/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.4008 - val_loss: 140.4060\n",
      "Epoch 2611/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.1393 - val_loss: 145.7878\n",
      "Epoch 2612/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.9807 - val_loss: 147.8105\n",
      "Epoch 2613/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.5737 - val_loss: 215.5938\n",
      "Epoch 2614/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.8779 - val_loss: 149.9090\n",
      "Epoch 2615/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.3326 - val_loss: 149.5134\n",
      "Epoch 2616/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.6457 - val_loss: 182.1799\n",
      "Epoch 2617/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.2843 - val_loss: 144.7639\n",
      "Epoch 2618/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 129.4500 - val_loss: 155.4494\n",
      "Epoch 2619/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.9893 - val_loss: 172.1988\n",
      "Epoch 2620/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.6222 - val_loss: 142.1146\n",
      "Epoch 2621/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.1920 - val_loss: 151.9023\n",
      "Epoch 2622/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 128.6430 - val_loss: 140.5529\n",
      "Epoch 2623/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 186.3974 - val_loss: 142.6123\n",
      "Epoch 2624/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.4963 - val_loss: 155.1211\n",
      "Epoch 2625/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.6994 - val_loss: 157.0036\n",
      "Epoch 2626/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.7235 - val_loss: 144.7320\n",
      "Epoch 2627/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.3614 - val_loss: 186.0340\n",
      "Epoch 2628/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 126.3261 - val_loss: 183.7991\n",
      "Epoch 2629/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.4469 - val_loss: 137.3792\n",
      "Epoch 2630/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.5481 - val_loss: 146.1530\n",
      "Epoch 2631/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.3217 - val_loss: 173.5754\n",
      "Epoch 2632/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.0687 - val_loss: 151.5416\n",
      "Epoch 2633/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.5815 - val_loss: 138.7396\n",
      "Epoch 2634/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 126.8214 - val_loss: 182.6816\n",
      "Epoch 2635/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.9950 - val_loss: 187.6214\n",
      "Epoch 2636/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.9174 - val_loss: 154.1973\n",
      "Epoch 2637/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.3547 - val_loss: 149.5501\n",
      "Epoch 2638/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.3342 - val_loss: 207.5828\n",
      "Epoch 2639/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.2996 - val_loss: 179.0712\n",
      "Epoch 2640/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.4307 - val_loss: 142.4228\n",
      "Epoch 2641/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.5448 - val_loss: 149.6731\n",
      "Epoch 2642/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.0107 - val_loss: 159.9957\n",
      "Epoch 2643/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 137.0094 - val_loss: 138.3483\n",
      "Epoch 2644/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.7923 - val_loss: 146.2948\n",
      "Epoch 2645/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 123.7866 - val_loss: 144.4450\n",
      "Epoch 2646/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.1156 - val_loss: 141.0351\n",
      "Epoch 2647/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.7037 - val_loss: 148.9537\n",
      "Epoch 2648/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.0001 - val_loss: 145.4782\n",
      "Epoch 2649/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.3345 - val_loss: 170.1008\n",
      "Epoch 2650/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 134.8414 - val_loss: 205.7981\n",
      "Epoch 2651/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.1794 - val_loss: 179.3661\n",
      "Epoch 2652/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.3619 - val_loss: 139.5805\n",
      "Epoch 2653/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.2131 - val_loss: 160.4743\n",
      "Epoch 2654/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 134.5092 - val_loss: 136.1450\n",
      "Epoch 2655/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.2102 - val_loss: 142.5107\n",
      "Epoch 2656/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.3238 - val_loss: 205.1497\n",
      "Epoch 2657/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 122.6225 - val_loss: 163.8937\n",
      "Epoch 2658/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.9340 - val_loss: 148.8623\n",
      "Epoch 2659/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.9007 - val_loss: 152.6990\n",
      "Epoch 2660/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.1378 - val_loss: 152.5477\n",
      "Epoch 2661/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 157.5930 - val_loss: 163.9079\n",
      "Epoch 2662/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.3360 - val_loss: 140.3731\n",
      "Epoch 2663/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.2188 - val_loss: 176.2947\n",
      "Epoch 2664/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.6403 - val_loss: 155.6010\n",
      "Epoch 2665/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.7994 - val_loss: 162.3334\n",
      "Epoch 2666/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.6236 - val_loss: 136.7747\n",
      "Epoch 2667/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.4042 - val_loss: 142.9852\n",
      "Epoch 2668/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.5565 - val_loss: 147.3971\n",
      "Epoch 2669/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 177.2244 - val_loss: 148.5461\n",
      "Epoch 2670/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.7946 - val_loss: 142.5489\n",
      "Epoch 2671/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.0895 - val_loss: 135.2099\n",
      "Epoch 2672/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 119.7585 - val_loss: 146.9827\n",
      "Epoch 2673/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.8318 - val_loss: 141.2641\n",
      "Epoch 2674/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.0696 - val_loss: 143.7251\n",
      "Epoch 2675/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.5935 - val_loss: 149.3512\n",
      "Epoch 2676/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.4705 - val_loss: 150.0585\n",
      "Epoch 2677/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 120.3383 - val_loss: 187.9748\n",
      "Epoch 2678/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.1762 - val_loss: 166.0905\n",
      "Epoch 2679/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.5812 - val_loss: 138.5129\n",
      "Epoch 2680/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 124.6148 - val_loss: 141.0482\n",
      "Epoch 2681/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 153.4368 - val_loss: 139.1444\n",
      "Epoch 2682/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.5974 - val_loss: 142.6919\n",
      "Epoch 2683/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.5880 - val_loss: 254.9138\n",
      "Epoch 2684/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 126.5444 - val_loss: 146.5946\n",
      "Epoch 2685/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.1948 - val_loss: 162.7137\n",
      "Epoch 2686/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 123.7357 - val_loss: 143.2899\n",
      "Epoch 2687/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.2713 - val_loss: 160.2848\n",
      "Epoch 2688/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.7043 - val_loss: 265.6343\n",
      "Epoch 2689/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.1011 - val_loss: 148.9727\n",
      "Epoch 2690/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 126.2753 - val_loss: 139.9588\n",
      "Epoch 2691/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.1390 - val_loss: 181.5029\n",
      "Epoch 2692/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.0938 - val_loss: 147.8927\n",
      "Epoch 2693/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.9592 - val_loss: 137.8235\n",
      "Epoch 2694/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 120.3183 - val_loss: 161.6756\n",
      "Epoch 2695/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.6592 - val_loss: 153.2551\n",
      "Epoch 2696/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.4959 - val_loss: 181.0412\n",
      "Epoch 2697/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 123.9756 - val_loss: 142.1913\n",
      "Epoch 2698/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 121.9894 - val_loss: 148.6919\n",
      "Epoch 2699/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.4291 - val_loss: 155.0917\n",
      "Epoch 2700/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.4240 - val_loss: 146.6219\n",
      "Epoch 2701/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.1229 - val_loss: 138.6498\n",
      "Epoch 2702/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 130.6514 - val_loss: 139.6522\n",
      "Epoch 2703/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.0792 - val_loss: 150.5562\n",
      "Epoch 2704/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 127.0247 - val_loss: 140.6619\n",
      "Epoch 2705/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.3060 - val_loss: 207.0734\n",
      "Epoch 2706/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.7167 - val_loss: 147.9369\n",
      "Epoch 2707/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.9196 - val_loss: 150.0342\n",
      "Epoch 2708/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.5977 - val_loss: 142.6139\n",
      "Epoch 2709/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 133.0360 - val_loss: 162.2736\n",
      "Epoch 2710/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.6073 - val_loss: 172.1718\n",
      "Epoch 2711/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 149.4930 - val_loss: 156.2672\n",
      "Epoch 2712/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.3956 - val_loss: 141.0444\n",
      "Epoch 2713/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 120.7322 - val_loss: 152.9315\n",
      "Epoch 2714/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.6108 - val_loss: 141.4615\n",
      "Epoch 2715/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.8539 - val_loss: 189.7812\n",
      "Epoch 2716/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.4514 - val_loss: 161.0274\n",
      "Epoch 2717/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.0213 - val_loss: 150.6490\n",
      "Epoch 2718/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.7058 - val_loss: 141.7567\n",
      "Epoch 2719/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.9570 - val_loss: 169.7887\n",
      "Epoch 2720/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 123.6316 - val_loss: 142.6553\n",
      "Epoch 2721/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.1566 - val_loss: 154.7198\n",
      "Epoch 2722/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 133.9620 - val_loss: 139.3614\n",
      "Epoch 2723/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.6619 - val_loss: 154.5779\n",
      "Epoch 2724/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.3577 - val_loss: 189.6803\n",
      "Epoch 2725/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.3272 - val_loss: 145.1231\n",
      "Epoch 2726/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 135.5512 - val_loss: 164.2353\n",
      "Epoch 2727/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.5822 - val_loss: 150.2622\n",
      "Epoch 2728/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.0647 - val_loss: 208.0958\n",
      "Epoch 2729/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.0771 - val_loss: 181.8084\n",
      "Epoch 2730/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 123.2003 - val_loss: 147.9479\n",
      "Epoch 2731/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.7699 - val_loss: 142.6626\n",
      "Epoch 2732/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 122.7496 - val_loss: 149.5506\n",
      "Epoch 2733/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.0053 - val_loss: 152.8540\n",
      "Epoch 2734/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 123.2195 - val_loss: 188.9692\n",
      "Epoch 2735/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.2894 - val_loss: 137.4755\n",
      "Epoch 2736/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.0335 - val_loss: 149.6331\n",
      "Epoch 2737/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.7164 - val_loss: 154.5432\n",
      "Epoch 2738/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.8270 - val_loss: 142.0981\n",
      "Epoch 2739/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.3043 - val_loss: 149.6222\n",
      "Epoch 2740/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.3787 - val_loss: 149.2652\n",
      "Epoch 2741/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.7994 - val_loss: 140.8975\n",
      "Epoch 2742/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 121.5434 - val_loss: 149.6778\n",
      "Epoch 2743/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.9677 - val_loss: 607.8159\n",
      "Epoch 2744/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 137.8384 - val_loss: 148.5057\n",
      "Epoch 2745/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.0431 - val_loss: 134.7405\n",
      "Epoch 2746/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.5765 - val_loss: 157.6671\n",
      "Epoch 2747/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.6256 - val_loss: 162.5748\n",
      "Epoch 2748/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 128.6726 - val_loss: 153.8824\n",
      "Epoch 2749/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.0805 - val_loss: 139.1878\n",
      "Epoch 2750/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.5260 - val_loss: 140.2452\n",
      "Epoch 2751/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.8875 - val_loss: 148.4014\n",
      "Epoch 2752/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.4763 - val_loss: 169.9131\n",
      "Epoch 2753/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.7270 - val_loss: 184.5259\n",
      "Epoch 2754/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 134.9379 - val_loss: 139.8647\n",
      "Epoch 2755/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.6500 - val_loss: 138.3739\n",
      "Epoch 2756/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 114.2548 - val_loss: 144.0798\n",
      "Epoch 2757/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 118.8718 - val_loss: 167.5911\n",
      "Epoch 2758/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.8346 - val_loss: 147.3242\n",
      "Epoch 2759/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 129.9038 - val_loss: 156.3447\n",
      "Epoch 2760/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 128.3074 - val_loss: 149.4155\n",
      "Epoch 2761/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 123.3868 - val_loss: 151.9392\n",
      "Epoch 2762/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.7814 - val_loss: 162.4303\n",
      "Epoch 2763/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.8201 - val_loss: 204.3287\n",
      "Epoch 2764/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.5314 - val_loss: 156.5680\n",
      "Epoch 2765/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.0282 - val_loss: 184.1247\n",
      "Epoch 2766/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 122.8910 - val_loss: 157.6180\n",
      "Epoch 2767/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.2057 - val_loss: 195.9094\n",
      "Epoch 2768/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.7895 - val_loss: 281.3399\n",
      "Epoch 2769/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.0248 - val_loss: 159.4263\n",
      "Epoch 2770/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.3739 - val_loss: 147.6832\n",
      "Epoch 2771/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.5732 - val_loss: 141.1734\n",
      "Epoch 2772/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.1670 - val_loss: 152.9764\n",
      "Epoch 2773/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.4095 - val_loss: 148.9409\n",
      "Epoch 2774/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.8710 - val_loss: 149.3095\n",
      "Epoch 2775/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.8365 - val_loss: 138.1790\n",
      "Epoch 2776/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.3458 - val_loss: 156.3289\n",
      "Epoch 2777/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.2131 - val_loss: 300.2434\n",
      "Epoch 2778/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.1804 - val_loss: 160.7146\n",
      "Epoch 2779/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.8671 - val_loss: 180.7863\n",
      "Epoch 2780/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.4778 - val_loss: 161.3226\n",
      "Epoch 2781/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.1404 - val_loss: 199.1465\n",
      "Epoch 2782/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.2952 - val_loss: 146.2325\n",
      "Epoch 2783/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.0315 - val_loss: 159.8442\n",
      "Epoch 2784/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.5667 - val_loss: 169.7636\n",
      "Epoch 2785/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 144.3839 - val_loss: 160.0104\n",
      "Epoch 2786/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.9570 - val_loss: 148.9821\n",
      "Epoch 2787/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.1082 - val_loss: 152.2289\n",
      "Epoch 2788/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.8367 - val_loss: 161.8695\n",
      "Epoch 2789/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 120.9960 - val_loss: 144.1958\n",
      "Epoch 2790/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.5746 - val_loss: 147.6386\n",
      "Epoch 2791/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.4043 - val_loss: 151.0978\n",
      "Epoch 2792/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.3468 - val_loss: 155.9434\n",
      "Epoch 2793/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.5005 - val_loss: 140.5466\n",
      "Epoch 2794/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.1339 - val_loss: 145.4520\n",
      "Epoch 2795/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.5769 - val_loss: 147.4834\n",
      "Epoch 2796/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 118.7434 - val_loss: 143.9360\n",
      "Epoch 2797/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 122.5299 - val_loss: 145.7002\n",
      "Epoch 2798/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.7991 - val_loss: 145.4641\n",
      "Epoch 2799/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.5169 - val_loss: 143.9582\n",
      "Epoch 2800/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.6908 - val_loss: 144.3059\n",
      "Epoch 2801/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.7206 - val_loss: 144.5884\n",
      "Epoch 2802/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.6356 - val_loss: 182.7716\n",
      "Epoch 2803/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.0865 - val_loss: 179.9363\n",
      "Epoch 2804/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.2201 - val_loss: 139.7540\n",
      "Epoch 2805/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 123.6955 - val_loss: 161.6110\n",
      "Epoch 2806/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.2116 - val_loss: 142.0130\n",
      "Epoch 2807/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.1823 - val_loss: 259.6514\n",
      "Epoch 2808/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.7994 - val_loss: 162.2488\n",
      "Epoch 2809/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.3912 - val_loss: 150.9724\n",
      "Epoch 2810/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 127.5858 - val_loss: 142.3901\n",
      "Epoch 2811/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.7706 - val_loss: 140.6248\n",
      "Epoch 2812/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 120.0812 - val_loss: 139.7776\n",
      "Epoch 2813/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.6696 - val_loss: 141.1451\n",
      "Epoch 2814/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.5637 - val_loss: 195.2330\n",
      "Epoch 2815/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.1628 - val_loss: 191.6194\n",
      "Epoch 2816/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.2707 - val_loss: 185.2044\n",
      "Epoch 2817/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.3443 - val_loss: 176.2158\n",
      "Epoch 2818/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.9193 - val_loss: 153.5698\n",
      "Epoch 2819/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 118.6666 - val_loss: 140.6211\n",
      "Epoch 2820/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.5925 - val_loss: 145.0500\n",
      "Epoch 2821/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.1774 - val_loss: 144.2332\n",
      "Epoch 2822/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.3212 - val_loss: 188.6890\n",
      "Epoch 2823/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 126.2473 - val_loss: 147.9993\n",
      "Epoch 2824/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.7481 - val_loss: 159.5495\n",
      "Epoch 2825/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.0100 - val_loss: 176.5512\n",
      "Epoch 2826/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.2456 - val_loss: 141.0921\n",
      "Epoch 2827/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.7613 - val_loss: 137.2434\n",
      "Epoch 2828/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 127.1830 - val_loss: 177.1757\n",
      "Epoch 2829/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.1457 - val_loss: 146.8135\n",
      "Epoch 2830/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 122.4828 - val_loss: 156.8808\n",
      "Epoch 2831/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.7962 - val_loss: 195.0288\n",
      "Epoch 2832/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.9414 - val_loss: 137.2233\n",
      "Epoch 2833/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.3028 - val_loss: 152.7192\n",
      "Epoch 2834/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.1295 - val_loss: 139.5505\n",
      "Epoch 2835/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.9424 - val_loss: 142.4473\n",
      "Epoch 2836/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 122.9159 - val_loss: 147.3783\n",
      "Epoch 2837/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 122.0766 - val_loss: 140.3187\n",
      "Epoch 2838/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 121.1593 - val_loss: 146.5100\n",
      "Epoch 2839/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 79us/step - loss: 123.0518 - val_loss: 182.0283\n",
      "Epoch 2840/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 123.0458 - val_loss: 152.7623\n",
      "Epoch 2841/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 121.0330 - val_loss: 141.9867\n",
      "Epoch 2842/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.3335 - val_loss: 165.2597\n",
      "Epoch 2843/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 126.0466 - val_loss: 156.9476\n",
      "Epoch 2844/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 119.5711 - val_loss: 157.1322\n",
      "Epoch 2845/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 129.1600 - val_loss: 152.8347\n",
      "Epoch 2846/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 126.7907 - val_loss: 159.6190\n",
      "Epoch 2847/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 125.6886 - val_loss: 179.2089\n",
      "Epoch 2848/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 127.9768 - val_loss: 174.0151\n",
      "Epoch 2849/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 125.9036 - val_loss: 219.3071\n",
      "Epoch 2850/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 125.5686 - val_loss: 143.2833\n",
      "Epoch 2851/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 120.9902 - val_loss: 144.5626\n",
      "Epoch 2852/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 127.8477 - val_loss: 140.5432\n",
      "Epoch 2853/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.1524 - val_loss: 139.6349\n",
      "Epoch 2854/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 122.2809 - val_loss: 147.1579\n",
      "Epoch 2855/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 122.9811 - val_loss: 138.0932\n",
      "Epoch 2856/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.1164 - val_loss: 141.3221\n",
      "Epoch 2857/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.3997 - val_loss: 150.2869\n",
      "Epoch 2858/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 124.7606 - val_loss: 160.3951\n",
      "Epoch 2859/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.6672 - val_loss: 142.8899\n",
      "Epoch 2860/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.5164 - val_loss: 138.6778\n",
      "Epoch 2861/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 123.0629 - val_loss: 140.7844\n",
      "Epoch 2862/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 161.7643 - val_loss: 779.3976\n",
      "Epoch 2863/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 160.6017 - val_loss: 162.4469\n",
      "Epoch 2864/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 120.1759 - val_loss: 144.4020\n",
      "Epoch 2865/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 116.4584 - val_loss: 152.3674\n",
      "Epoch 2866/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.3883 - val_loss: 155.3695\n",
      "Epoch 2867/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.8352 - val_loss: 137.0364\n",
      "Epoch 2868/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.1712 - val_loss: 157.6595\n",
      "Epoch 2869/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 117.7118 - val_loss: 148.6795\n",
      "Epoch 2870/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 121.5188 - val_loss: 168.9701\n",
      "Epoch 2871/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 120.4750 - val_loss: 170.5329\n",
      "Epoch 2872/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.7831 - val_loss: 140.8686\n",
      "Epoch 2873/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.6639 - val_loss: 144.2692\n",
      "Epoch 2874/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.5935 - val_loss: 141.4505\n",
      "Epoch 2875/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.1696 - val_loss: 145.5000\n",
      "Epoch 2876/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 120.3702 - val_loss: 161.1237\n",
      "Epoch 2877/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 122.5455 - val_loss: 166.0234\n",
      "Epoch 2878/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 133.1661 - val_loss: 192.4385\n",
      "Epoch 2879/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.2061 - val_loss: 142.8846\n",
      "Epoch 2880/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 123.4153 - val_loss: 169.7504\n",
      "Epoch 2881/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 127.7511 - val_loss: 155.2104\n",
      "Epoch 2882/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 126.0204 - val_loss: 150.0989\n",
      "Epoch 2883/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 124.3033 - val_loss: 146.2082\n",
      "Epoch 2884/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 132.8895 - val_loss: 162.4039\n",
      "Epoch 2885/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 133.5217 - val_loss: 145.1954\n",
      "Epoch 2886/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 123.1870 - val_loss: 171.0897\n",
      "Epoch 2887/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 123.5278 - val_loss: 149.7429\n",
      "Epoch 2888/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.3874 - val_loss: 142.0811\n",
      "Epoch 2889/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 123.2107 - val_loss: 252.5771\n",
      "Epoch 2890/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 226.9778 - val_loss: 211.4662\n",
      "Epoch 2891/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 121.1244 - val_loss: 175.6440\n",
      "Epoch 2892/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 115.2542 - val_loss: 147.3449\n",
      "Epoch 2893/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 120.8716 - val_loss: 184.5451\n",
      "Epoch 2894/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.7902 - val_loss: 153.5711\n",
      "Epoch 2895/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 120.5745 - val_loss: 157.5164\n",
      "Epoch 2896/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.8262 - val_loss: 157.2923\n",
      "Epoch 2897/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 134.7528 - val_loss: 146.9211\n",
      "Epoch 2898/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.1855 - val_loss: 148.8945\n",
      "Epoch 2899/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.8936 - val_loss: 140.0493\n",
      "Epoch 2900/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.9206 - val_loss: 151.8210\n",
      "Epoch 2901/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.1400 - val_loss: 166.4502\n",
      "Epoch 2902/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.7813 - val_loss: 143.3952\n",
      "Epoch 2903/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.2841 - val_loss: 144.9724\n",
      "Epoch 2904/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.2197 - val_loss: 173.8718\n",
      "Epoch 2905/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.5182 - val_loss: 149.7263\n",
      "Epoch 2906/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 120.6952 - val_loss: 151.8506\n",
      "Epoch 2907/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 122.4565 - val_loss: 139.0953\n",
      "Epoch 2908/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 125.9094 - val_loss: 152.2246\n",
      "Epoch 2909/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.3132 - val_loss: 147.7482\n",
      "Epoch 2910/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.8046 - val_loss: 140.5991\n",
      "Epoch 2911/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.3768 - val_loss: 167.4667\n",
      "Epoch 2912/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.9196 - val_loss: 138.7667\n",
      "Epoch 2913/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 115.5926 - val_loss: 141.8524\n",
      "Epoch 2914/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 126.2563 - val_loss: 171.7344\n",
      "Epoch 2915/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.2987 - val_loss: 182.4233\n",
      "Epoch 2916/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 123.9011 - val_loss: 186.6856\n",
      "Epoch 2917/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.1206 - val_loss: 163.1055\n",
      "Epoch 2918/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.6722 - val_loss: 156.2137\n",
      "Epoch 2919/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.4933 - val_loss: 136.3882\n",
      "Epoch 2920/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.7421 - val_loss: 201.2864\n",
      "Epoch 2921/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.5217 - val_loss: 164.4717\n",
      "Epoch 2922/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.5670 - val_loss: 145.4450\n",
      "Epoch 2923/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.7128 - val_loss: 140.9439\n",
      "Epoch 2924/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 117.9114 - val_loss: 142.4232\n",
      "Epoch 2925/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.0418 - val_loss: 142.9754\n",
      "Epoch 2926/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.0592 - val_loss: 140.7270\n",
      "Epoch 2927/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.3506 - val_loss: 142.9749\n",
      "Epoch 2928/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.6940 - val_loss: 145.2483\n",
      "Epoch 2929/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.2857 - val_loss: 165.0508\n",
      "Epoch 2930/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.7561 - val_loss: 173.1633\n",
      "Epoch 2931/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.0784 - val_loss: 139.3497\n",
      "Epoch 2932/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.2392 - val_loss: 154.4068\n",
      "Epoch 2933/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.0423 - val_loss: 169.6870\n",
      "Epoch 2934/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.2545 - val_loss: 139.1100\n",
      "Epoch 2935/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 157.6522 - val_loss: 157.3568\n",
      "Epoch 2936/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.1114 - val_loss: 144.6017\n",
      "Epoch 2937/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.1758 - val_loss: 146.7021\n",
      "Epoch 2938/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.7919 - val_loss: 147.8499\n",
      "Epoch 2939/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.8139 - val_loss: 152.8670\n",
      "Epoch 2940/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.9194 - val_loss: 187.6494\n",
      "Epoch 2941/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.7424 - val_loss: 171.5119\n",
      "Epoch 2942/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.2136 - val_loss: 136.8201\n",
      "Epoch 2943/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.8686 - val_loss: 158.0057\n",
      "Epoch 2944/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.5235 - val_loss: 187.0132\n",
      "Epoch 2945/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.0201 - val_loss: 144.3245\n",
      "Epoch 2946/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.8564 - val_loss: 153.0079\n",
      "Epoch 2947/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.5633 - val_loss: 140.5061\n",
      "Epoch 2948/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 132.8958 - val_loss: 161.7342\n",
      "Epoch 2949/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.0577 - val_loss: 143.1707\n",
      "Epoch 2950/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.9216 - val_loss: 139.6288\n",
      "Epoch 2951/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.4770 - val_loss: 140.3637\n",
      "Epoch 2952/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 129.0220 - val_loss: 174.5962\n",
      "Epoch 2953/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.2832 - val_loss: 150.5530\n",
      "Epoch 2954/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.2816 - val_loss: 149.6504\n",
      "Epoch 2955/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.2669 - val_loss: 155.3405\n",
      "Epoch 2956/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.0781 - val_loss: 147.7671\n",
      "Epoch 2957/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.6452 - val_loss: 139.2027\n",
      "Epoch 2958/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.0454 - val_loss: 160.6154\n",
      "Epoch 2959/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 126.5027 - val_loss: 137.4112\n",
      "Epoch 2960/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.7538 - val_loss: 139.5826\n",
      "Epoch 2961/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.4810 - val_loss: 165.6349\n",
      "Epoch 2962/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.9864 - val_loss: 159.8779\n",
      "Epoch 2963/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.5362 - val_loss: 137.6290\n",
      "Epoch 2964/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.5982 - val_loss: 160.5199\n",
      "Epoch 2965/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.5854 - val_loss: 176.7845\n",
      "Epoch 2966/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.7567 - val_loss: 147.3743\n",
      "Epoch 2967/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.2148 - val_loss: 164.6134\n",
      "Epoch 2968/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 134.0393 - val_loss: 153.3727\n",
      "Epoch 2969/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.0994 - val_loss: 146.1307\n",
      "Epoch 2970/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 127.6488 - val_loss: 139.2758\n",
      "Epoch 2971/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.6624 - val_loss: 167.5277\n",
      "Epoch 2972/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.1568 - val_loss: 150.8832\n",
      "Epoch 2973/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.6097 - val_loss: 148.2356\n",
      "Epoch 2974/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.2616 - val_loss: 141.2469\n",
      "Epoch 2975/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.0858 - val_loss: 155.5617\n",
      "Epoch 2976/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 127.8247 - val_loss: 152.2251\n",
      "Epoch 2977/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.7355 - val_loss: 152.4534\n",
      "Epoch 2978/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.7701 - val_loss: 144.5064\n",
      "Epoch 2979/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.8043 - val_loss: 139.7502\n",
      "Epoch 2980/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.9507 - val_loss: 158.1145\n",
      "Epoch 2981/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.3255 - val_loss: 138.5559\n",
      "Epoch 2982/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.2404 - val_loss: 138.8623\n",
      "Epoch 2983/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 126.2361 - val_loss: 140.4604\n",
      "Epoch 2984/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.3026 - val_loss: 145.1792\n",
      "Epoch 2985/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 149.0552 - val_loss: 152.6080\n",
      "Epoch 2986/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 120.5414 - val_loss: 177.2769\n",
      "Epoch 2987/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.3586 - val_loss: 148.8887\n",
      "Epoch 2988/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.4166 - val_loss: 155.1809\n",
      "Epoch 2989/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 120.9053 - val_loss: 155.4797\n",
      "Epoch 2990/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.6993 - val_loss: 158.3274\n",
      "Epoch 2991/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.6539 - val_loss: 136.8337\n",
      "Epoch 2992/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.4532 - val_loss: 147.6298\n",
      "Epoch 2993/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 127.1171 - val_loss: 149.2221\n",
      "Epoch 2994/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.6536 - val_loss: 140.4969\n",
      "Epoch 2995/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.5631 - val_loss: 144.8773\n",
      "Epoch 2996/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.4073 - val_loss: 148.2700\n",
      "Epoch 2997/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.5770 - val_loss: 156.1440\n",
      "Epoch 2998/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.7224 - val_loss: 147.6303\n",
      "Epoch 2999/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.2055 - val_loss: 155.0258\n",
      "Epoch 3000/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.3023 - val_loss: 152.0683\n",
      "Epoch 3001/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.4300 - val_loss: 164.0478\n",
      "Epoch 3002/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 128.6058 - val_loss: 141.8332\n",
      "Epoch 3003/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.2416 - val_loss: 144.5999\n",
      "Epoch 3004/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.5461 - val_loss: 159.0910\n",
      "Epoch 3005/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.2449 - val_loss: 147.3485\n",
      "Epoch 3006/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 129.0060 - val_loss: 150.0283\n",
      "Epoch 3007/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.9757 - val_loss: 271.1643\n",
      "Epoch 3008/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.3926 - val_loss: 139.5007\n",
      "Epoch 3009/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.8194 - val_loss: 152.0681\n",
      "Epoch 3010/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 129.4995 - val_loss: 150.4703\n",
      "Epoch 3011/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.7239 - val_loss: 141.2907\n",
      "Epoch 3012/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.6658 - val_loss: 143.4308\n",
      "Epoch 3013/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.6666 - val_loss: 168.1892\n",
      "Epoch 3014/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.7241 - val_loss: 141.6774\n",
      "Epoch 3015/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.7122 - val_loss: 162.5800\n",
      "Epoch 3016/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.9519 - val_loss: 149.9050\n",
      "Epoch 3017/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.3890 - val_loss: 149.7390\n",
      "Epoch 3018/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.5477 - val_loss: 144.4097\n",
      "Epoch 3019/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 121.3575 - val_loss: 181.8072\n",
      "Epoch 3020/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.7655 - val_loss: 142.4752\n",
      "Epoch 3021/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.8073 - val_loss: 159.2785\n",
      "Epoch 3022/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.2733 - val_loss: 147.7176\n",
      "Epoch 3023/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.0443 - val_loss: 162.6056\n",
      "Epoch 3024/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.6985 - val_loss: 153.9396\n",
      "Epoch 3025/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.1354 - val_loss: 150.2206\n",
      "Epoch 3026/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.1484 - val_loss: 145.0983\n",
      "Epoch 3027/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.5070 - val_loss: 178.9362\n",
      "Epoch 3028/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.5671 - val_loss: 147.2973\n",
      "Epoch 3029/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.7820 - val_loss: 190.2839\n",
      "Epoch 3030/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.2606 - val_loss: 145.0850\n",
      "Epoch 3031/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 123.9366 - val_loss: 214.7170\n",
      "Epoch 3032/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.2259 - val_loss: 142.7868\n",
      "Epoch 3033/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 119.2466 - val_loss: 141.8856\n",
      "Epoch 3034/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.1239 - val_loss: 176.3801\n",
      "Epoch 3035/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.7533 - val_loss: 157.3338\n",
      "Epoch 3036/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.9232 - val_loss: 136.9070\n",
      "Epoch 3037/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.4564 - val_loss: 192.7094\n",
      "Epoch 3038/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.3272 - val_loss: 142.8032\n",
      "Epoch 3039/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.6895 - val_loss: 148.1413\n",
      "Epoch 3040/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 184.8321 - val_loss: 225.1468\n",
      "Epoch 3041/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.8021 - val_loss: 189.5448\n",
      "Epoch 3042/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.7624 - val_loss: 135.7528\n",
      "Epoch 3043/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 115.2199 - val_loss: 149.9573\n",
      "Epoch 3044/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 118.2342 - val_loss: 160.9949\n",
      "Epoch 3045/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 116.9870 - val_loss: 146.7887\n",
      "Epoch 3046/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.3091 - val_loss: 140.9097\n",
      "Epoch 3047/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.5488 - val_loss: 151.4548\n",
      "Epoch 3048/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.4256 - val_loss: 138.6298\n",
      "Epoch 3049/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 116.6220 - val_loss: 146.3028\n",
      "Epoch 3050/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 120.1451 - val_loss: 142.0930\n",
      "Epoch 3051/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.0179 - val_loss: 193.3423\n",
      "Epoch 3052/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.1490 - val_loss: 150.3757\n",
      "Epoch 3053/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.6912 - val_loss: 146.7965\n",
      "Epoch 3054/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.3832 - val_loss: 149.2851\n",
      "Epoch 3055/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 45us/step - loss: 120.7317 - val_loss: 144.5579\n",
      "Epoch 3056/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.0348 - val_loss: 142.4676\n",
      "Epoch 3057/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.6109 - val_loss: 150.4888\n",
      "Epoch 3058/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.2397 - val_loss: 165.1721\n",
      "Epoch 3059/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.3912 - val_loss: 146.7456\n",
      "Epoch 3060/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.8512 - val_loss: 209.9490\n",
      "Epoch 3061/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.1073 - val_loss: 149.2235\n",
      "Epoch 3062/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.6238 - val_loss: 152.1175\n",
      "Epoch 3063/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.0935 - val_loss: 142.6492\n",
      "Epoch 3064/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 143.1666 - val_loss: 193.4569\n",
      "Epoch 3065/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.4958 - val_loss: 175.0457\n",
      "Epoch 3066/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 125.9748 - val_loss: 141.3892\n",
      "Epoch 3067/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 115.3965 - val_loss: 151.3315\n",
      "Epoch 3068/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.4860 - val_loss: 162.6950\n",
      "Epoch 3069/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.9334 - val_loss: 148.2760\n",
      "Epoch 3070/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.7151 - val_loss: 142.1852\n",
      "Epoch 3071/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.4149 - val_loss: 150.1546\n",
      "Epoch 3072/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.4421 - val_loss: 144.7213\n",
      "Epoch 3073/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.0455 - val_loss: 138.7079\n",
      "Epoch 3074/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.5571 - val_loss: 141.8224\n",
      "Epoch 3075/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.3658 - val_loss: 154.1398\n",
      "Epoch 3076/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.9587 - val_loss: 142.3990\n",
      "Epoch 3077/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 119.8493 - val_loss: 137.7077\n",
      "Epoch 3078/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.2774 - val_loss: 192.8813\n",
      "Epoch 3079/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 129.2828 - val_loss: 162.1016\n",
      "Epoch 3080/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.1834 - val_loss: 151.9732\n",
      "Epoch 3081/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 121.7189 - val_loss: 162.7131\n",
      "Epoch 3082/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.8155 - val_loss: 170.5917\n",
      "Epoch 3083/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.0244 - val_loss: 140.4180\n",
      "Epoch 3084/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.2904 - val_loss: 149.9503\n",
      "Epoch 3085/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 142.4488 - val_loss: 194.6761\n",
      "Epoch 3086/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.3179 - val_loss: 139.7342\n",
      "Epoch 3087/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 129.1832 - val_loss: 190.7891\n",
      "Epoch 3088/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.4981 - val_loss: 139.8562\n",
      "Epoch 3089/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 119.2712 - val_loss: 191.4854\n",
      "Epoch 3090/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.5891 - val_loss: 176.8903\n",
      "Epoch 3091/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 122.4171 - val_loss: 141.1547\n",
      "Epoch 3092/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.2806 - val_loss: 162.6111\n",
      "Epoch 3093/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.5529 - val_loss: 148.5884\n",
      "Epoch 3094/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.2877 - val_loss: 148.1878\n",
      "Epoch 3095/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 118.2304 - val_loss: 138.2342\n",
      "Epoch 3096/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.3093 - val_loss: 139.9698\n",
      "Epoch 3097/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.8897 - val_loss: 167.5266\n",
      "Epoch 3098/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.3642 - val_loss: 152.3132\n",
      "Epoch 3099/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.0016 - val_loss: 198.6733\n",
      "Epoch 3100/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.8230 - val_loss: 137.7572\n",
      "Epoch 3101/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.9501 - val_loss: 157.7418\n",
      "Epoch 3102/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.2419 - val_loss: 158.5935\n",
      "Epoch 3103/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.4149 - val_loss: 138.9286\n",
      "Epoch 3104/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.7171 - val_loss: 143.2585\n",
      "Epoch 3105/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 123.8221 - val_loss: 144.9622\n",
      "Epoch 3106/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.2688 - val_loss: 139.4275\n",
      "Epoch 3107/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.9576 - val_loss: 150.3054\n",
      "Epoch 3108/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.6654 - val_loss: 139.5466\n",
      "Epoch 3109/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.8372 - val_loss: 144.3779\n",
      "Epoch 3110/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.9592 - val_loss: 145.5724\n",
      "Epoch 3111/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.2622 - val_loss: 145.0983\n",
      "Epoch 3112/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.7873 - val_loss: 195.1357\n",
      "Epoch 3113/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.6610 - val_loss: 143.9981\n",
      "Epoch 3114/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.9035 - val_loss: 153.0549\n",
      "Epoch 3115/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.8281 - val_loss: 164.5814\n",
      "Epoch 3116/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.4233 - val_loss: 141.4798\n",
      "Epoch 3117/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 118.5851 - val_loss: 175.6538\n",
      "Epoch 3118/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.9904 - val_loss: 143.2767\n",
      "Epoch 3119/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 178.6482 - val_loss: 202.1649\n",
      "Epoch 3120/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.5886 - val_loss: 140.1792\n",
      "Epoch 3121/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 115.4577 - val_loss: 146.6322\n",
      "Epoch 3122/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.6651 - val_loss: 192.4763\n",
      "Epoch 3123/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.1872 - val_loss: 136.1064\n",
      "Epoch 3124/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.1182 - val_loss: 137.7340\n",
      "Epoch 3125/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.9282 - val_loss: 150.0765\n",
      "Epoch 3126/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.4761 - val_loss: 138.8104\n",
      "Epoch 3127/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.4930 - val_loss: 150.7622\n",
      "Epoch 3128/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.1602 - val_loss: 151.4441\n",
      "Epoch 3129/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.5896 - val_loss: 137.7328\n",
      "Epoch 3130/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.3376 - val_loss: 142.3301\n",
      "Epoch 3131/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.5603 - val_loss: 228.2866\n",
      "Epoch 3132/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.6663 - val_loss: 142.8075\n",
      "Epoch 3133/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.8804 - val_loss: 140.9742\n",
      "Epoch 3134/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.5326 - val_loss: 150.1988\n",
      "Epoch 3135/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.4321 - val_loss: 146.8972\n",
      "Epoch 3136/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 116.5383 - val_loss: 166.7628\n",
      "Epoch 3137/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.6677 - val_loss: 167.0063\n",
      "Epoch 3138/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.2442 - val_loss: 141.3514\n",
      "Epoch 3139/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.6697 - val_loss: 154.3830\n",
      "Epoch 3140/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.1651 - val_loss: 153.9570\n",
      "Epoch 3141/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 115.9964 - val_loss: 140.3146\n",
      "Epoch 3142/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.1616 - val_loss: 140.4296\n",
      "Epoch 3143/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.7991 - val_loss: 156.6647\n",
      "Epoch 3144/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 125.2254 - val_loss: 142.5130\n",
      "Epoch 3145/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 127.8868 - val_loss: 145.2045\n",
      "Epoch 3146/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.9255 - val_loss: 143.0559\n",
      "Epoch 3147/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 118.5050 - val_loss: 149.6534\n",
      "Epoch 3148/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.8527 - val_loss: 204.8747\n",
      "Epoch 3149/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.0823 - val_loss: 140.1631\n",
      "Epoch 3150/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.9474 - val_loss: 140.5583\n",
      "Epoch 3151/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.3363 - val_loss: 145.0177\n",
      "Epoch 3152/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.4971 - val_loss: 145.0702\n",
      "Epoch 3153/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.0624 - val_loss: 154.5285\n",
      "Epoch 3154/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.5477 - val_loss: 149.8951\n",
      "Epoch 3155/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 116.5652 - val_loss: 147.5436\n",
      "Epoch 3156/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.7533 - val_loss: 142.2844\n",
      "Epoch 3157/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.5227 - val_loss: 152.0603\n",
      "Epoch 3158/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 126.5063 - val_loss: 138.4744\n",
      "Epoch 3159/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.6833 - val_loss: 164.9808\n",
      "Epoch 3160/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.9650 - val_loss: 163.7976\n",
      "Epoch 3161/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 123.0731 - val_loss: 148.5757\n",
      "Epoch 3162/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.0266 - val_loss: 144.9930\n",
      "Epoch 3163/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.2869 - val_loss: 142.3528\n",
      "Epoch 3164/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.0589 - val_loss: 146.4948\n",
      "Epoch 3165/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.6970 - val_loss: 173.7957\n",
      "Epoch 3166/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.9697 - val_loss: 142.8441\n",
      "Epoch 3167/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.4954 - val_loss: 141.2192\n",
      "Epoch 3168/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 118.4890 - val_loss: 148.3678\n",
      "Epoch 3169/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.6183 - val_loss: 168.4917\n",
      "Epoch 3170/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.1043 - val_loss: 144.0697\n",
      "Epoch 3171/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 118.9493 - val_loss: 148.9418\n",
      "Epoch 3172/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.5978 - val_loss: 147.6420\n",
      "Epoch 3173/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.1887 - val_loss: 165.4591\n",
      "Epoch 3174/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 127.8917 - val_loss: 136.5590\n",
      "Epoch 3175/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.7825 - val_loss: 169.0295\n",
      "Epoch 3176/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.5516 - val_loss: 138.7361\n",
      "Epoch 3177/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.7673 - val_loss: 159.5716\n",
      "Epoch 3178/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.4646 - val_loss: 152.8228\n",
      "Epoch 3179/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 119.9273 - val_loss: 146.5310\n",
      "Epoch 3180/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.2710 - val_loss: 185.2078\n",
      "Epoch 3181/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 123.8120 - val_loss: 148.0298\n",
      "Epoch 3182/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.0822 - val_loss: 135.7326\n",
      "Epoch 3183/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.8481 - val_loss: 162.3131\n",
      "Epoch 3184/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.2720 - val_loss: 139.9852\n",
      "Epoch 3185/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.6491 - val_loss: 153.3601\n",
      "Epoch 3186/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.2852 - val_loss: 184.9473\n",
      "Epoch 3187/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.6774 - val_loss: 165.2464\n",
      "Epoch 3188/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 120.9938 - val_loss: 159.4792\n",
      "Epoch 3189/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.5361 - val_loss: 205.1435\n",
      "Epoch 3190/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.3918 - val_loss: 136.7925\n",
      "Epoch 3191/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.9363 - val_loss: 147.8112\n",
      "Epoch 3192/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 126.0734 - val_loss: 155.8791\n",
      "Epoch 03192: early stopping\n",
      "Fold score (RMSE): 12.292957305908203\n",
      "Fold #3\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 5418.4893 - val_loss: 5479.4621\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 4497.4342 - val_loss: 4633.6278\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 4464.1678 - val_loss: 4422.0177\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 4231.7315 - val_loss: 4314.6574\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 4216.5675 - val_loss: 4246.7401\n",
      "Epoch 6/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step - loss: 4195.1876 - val_loss: 4237.9962\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 4081.5812 - val_loss: 4097.1410\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 4006.4373 - val_loss: 4399.8570\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 3745.3051 - val_loss: 3975.5179\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 3524.4532 - val_loss: 4797.7876\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 3352.9014 - val_loss: 3456.7541\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 2961.0821 - val_loss: 2182.5165\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 2850.2453 - val_loss: 2279.6850\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 2173.2031 - val_loss: 1553.7651\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 1672.8014 - val_loss: 3399.4708\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 1943.8679 - val_loss: 1669.8620\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 1390.6529 - val_loss: 2740.7240\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 1125.9160 - val_loss: 892.6715\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 1022.7415 - val_loss: 1740.4698\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 1046.1092 - val_loss: 653.2737\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 760.7573 - val_loss: 632.7857\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 673.8351 - val_loss: 520.3512\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 848.9938 - val_loss: 550.6160\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 752.2973 - val_loss: 613.9798\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 702.7775 - val_loss: 2483.1684\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 606.1406 - val_loss: 580.4155\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 632.1744 - val_loss: 1375.1655\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 600.7056 - val_loss: 536.9189\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 570.7922 - val_loss: 515.5825\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 556.8869 - val_loss: 557.5935\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 556.1757 - val_loss: 800.3568\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 624.7417 - val_loss: 406.6576\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 542.1709 - val_loss: 503.6872\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 494.9540 - val_loss: 368.7592\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 425.7181 - val_loss: 538.1315\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 571.8068 - val_loss: 417.2079\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 542.0570 - val_loss: 391.2131\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 504.1199 - val_loss: 349.0629\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 398.8590 - val_loss: 351.7667\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 451.0689 - val_loss: 574.6488\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 531.3134 - val_loss: 523.6395\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 422.8443 - val_loss: 604.5608\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 494.9585 - val_loss: 399.8791\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 405.2615 - val_loss: 507.2986\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 467.5206 - val_loss: 592.5787\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 696.6526 - val_loss: 396.6652\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 474.6507 - val_loss: 335.6266\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 422.8942 - val_loss: 318.3677\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 545.1146 - val_loss: 398.1760\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 448.3030 - val_loss: 406.9745\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 423.0906 - val_loss: 588.5951\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 400.5189 - val_loss: 627.6921\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 459.5124 - val_loss: 1200.8699\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 470.1654 - val_loss: 477.8648\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 371.2992 - val_loss: 312.6131\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 459.7660 - val_loss: 591.1287\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 499.6173 - val_loss: 623.1944\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 642.5050 - val_loss: 433.8473\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 374.8702 - val_loss: 387.2383\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 345.2680 - val_loss: 564.3541\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 417.5438 - val_loss: 285.0080\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 430.3859 - val_loss: 268.0967\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 351.4319 - val_loss: 730.9578\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 407.6966 - val_loss: 300.2977\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 337.3583 - val_loss: 396.5077\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 390.5030 - val_loss: 338.1518\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 354.4913 - val_loss: 245.5034\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 285.1830 - val_loss: 293.3258\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 331.2428 - val_loss: 379.3423\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 376.4901 - val_loss: 299.4081\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 336.9661 - val_loss: 326.7387\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 334.5994 - val_loss: 253.5358\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 338.8856 - val_loss: 331.9589\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 369.1091 - val_loss: 272.0690\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 291.8767 - val_loss: 569.8166\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 384.0918 - val_loss: 252.6785\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 357.5474 - val_loss: 313.2879\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 321.2873 - val_loss: 906.1412\n",
      "Epoch 79/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step - loss: 344.2915 - val_loss: 270.8164\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 281.9827 - val_loss: 360.7637\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 343.0150 - val_loss: 360.9103\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 398.0903 - val_loss: 224.1369\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 330.5668 - val_loss: 292.5060\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 285.1592 - val_loss: 236.8731\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 296.2915 - val_loss: 234.0042\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 326.4437 - val_loss: 405.4657\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 457.0131 - val_loss: 268.6222\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 311.9583 - val_loss: 487.0683\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 302.4237 - val_loss: 219.0689\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 299.7800 - val_loss: 210.6315\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 282.0159 - val_loss: 241.1378\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 336.4655 - val_loss: 238.5488\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 274.2426 - val_loss: 497.0735\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 286.3993 - val_loss: 214.5738\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 414.5223 - val_loss: 713.6581\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 323.3056 - val_loss: 236.1120\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 295.3407 - val_loss: 628.8304\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 377.8042 - val_loss: 296.7268\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 252.4963 - val_loss: 291.7356\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 304.1948 - val_loss: 218.0786\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 310.5959 - val_loss: 227.2320\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 265.1725 - val_loss: 342.2086\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 318.6931 - val_loss: 223.3987\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 373.3555 - val_loss: 281.8084\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 240.3828 - val_loss: 330.2247\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 277.3752 - val_loss: 745.8262\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 268.6630 - val_loss: 893.6014\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 318.9053 - val_loss: 296.5906\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 257.7535 - val_loss: 216.7166\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 302.9151 - val_loss: 205.7774\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 328.8977 - val_loss: 228.1961\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 385.5179 - val_loss: 343.6836\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 264.6533 - val_loss: 375.1778\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 315.7068 - val_loss: 276.6067\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 259.1450 - val_loss: 245.6499\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 253.4961 - val_loss: 190.0457\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 264.5250 - val_loss: 511.6981\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 280.2898 - val_loss: 427.2471\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 398.4279 - val_loss: 495.1493\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 328.4131 - val_loss: 357.1266\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 224.0084 - val_loss: 298.6362\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 261.6839 - val_loss: 204.6924\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 199.9249 - val_loss: 332.4777\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 281.3676 - val_loss: 520.9254\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 317.4314 - val_loss: 207.4718\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 243.9819 - val_loss: 255.2509\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 261.6363 - val_loss: 198.6551\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 255.4176 - val_loss: 210.0493\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 280.1643 - val_loss: 209.9261\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 250.0516 - val_loss: 242.6108\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 281.2520 - val_loss: 279.0503\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 244.5317 - val_loss: 212.6472\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 256.4726 - val_loss: 360.1781\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 257.5123 - val_loss: 215.4369\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 279.5908 - val_loss: 262.1647\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 207.3018 - val_loss: 309.2801\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 228.1452 - val_loss: 194.8017\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 299.2234 - val_loss: 236.5954\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 232.2095 - val_loss: 209.1813\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 222.7948 - val_loss: 232.7085\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 297.7585 - val_loss: 488.9626\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 325.4283 - val_loss: 211.2375\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 213.6178 - val_loss: 230.6664\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 236.8960 - val_loss: 326.3835\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 239.9839 - val_loss: 191.6394\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 215.2751 - val_loss: 220.4082\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 251.6134 - val_loss: 188.3583\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 254.7677 - val_loss: 198.4577\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 245.6659 - val_loss: 246.4977\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 245.9639 - val_loss: 825.9803\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 230.4542 - val_loss: 206.5002\n",
      "Epoch 152/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 244.5241 - val_loss: 169.1971\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 219.3442 - val_loss: 234.9127\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 209.3323 - val_loss: 691.3083\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 271.8995 - val_loss: 204.0335\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 218.9369 - val_loss: 296.8753\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 219.6599 - val_loss: 170.1540\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 218.2632 - val_loss: 364.0696\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 363.0383 - val_loss: 266.1924\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 244.7190 - val_loss: 170.3005\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 241.7929 - val_loss: 209.1552\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 231.4768 - val_loss: 293.5642\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 204.2295 - val_loss: 168.5479\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 256.0826 - val_loss: 283.5460\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 315.2332 - val_loss: 307.6289\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 200.8268 - val_loss: 231.2426\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 275.3553 - val_loss: 188.5508\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 213.7147 - val_loss: 196.0260\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 208.5718 - val_loss: 183.0902\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 249.7220 - val_loss: 190.6455\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 207.2142 - val_loss: 160.8820\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 213.2098 - val_loss: 213.3601\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 280.5924 - val_loss: 292.9567\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 244.7350 - val_loss: 175.6482\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 228.0098 - val_loss: 398.5353\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 207.8247 - val_loss: 216.4073\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 237.5605 - val_loss: 171.5633\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 218.1697 - val_loss: 174.4631\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 247.9827 - val_loss: 397.8674\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 265.0978 - val_loss: 910.4484\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 246.7714 - val_loss: 224.8140\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 191.7246 - val_loss: 171.9367\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 184.0747 - val_loss: 166.1837\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 211.0406 - val_loss: 188.0697\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 194.6309 - val_loss: 180.0493\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 191.8537 - val_loss: 181.8695\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 227.6919 - val_loss: 257.6806\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 188.5421 - val_loss: 253.9120\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 221.8946 - val_loss: 206.3854\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 231.3964 - val_loss: 165.3679\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 290.5672 - val_loss: 218.6988\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 192.9218 - val_loss: 225.2006\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 184.6078 - val_loss: 379.8162\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 224.6102 - val_loss: 180.3897\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 307.3442 - val_loss: 172.3791\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 198.5439 - val_loss: 420.7316\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 272.6122 - val_loss: 163.5510\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 182.7947 - val_loss: 160.7342\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 249.3645 - val_loss: 204.2442\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 200.0113 - val_loss: 257.8480\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 222.9882 - val_loss: 215.8081\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 223.9794 - val_loss: 253.6810\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 250.0674 - val_loss: 186.1751\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 172.9702 - val_loss: 159.3695\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 204.8994 - val_loss: 584.8405\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 205.7552 - val_loss: 173.8999\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 186.8596 - val_loss: 198.3849\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 225.9914 - val_loss: 302.8833\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 230.3658 - val_loss: 443.3481\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 339.0228 - val_loss: 159.3599\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 182.8913 - val_loss: 172.3788\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 200.2681 - val_loss: 181.1501\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 204.1931 - val_loss: 167.6796\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 170.2219 - val_loss: 191.3791\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 207.8853 - val_loss: 286.7894\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 214.0929 - val_loss: 454.8727\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 202.1179 - val_loss: 277.0667\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 185.9558 - val_loss: 370.7023\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 282.1430 - val_loss: 174.3527\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 251.0373 - val_loss: 212.4864\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 168.5924 - val_loss: 167.7032\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 199.2137 - val_loss: 152.5851\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 189.0307 - val_loss: 233.8948\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 182.6604 - val_loss: 207.2125\n",
      "Epoch 225/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 195.7777 - val_loss: 224.7216\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 189.6805 - val_loss: 167.3901\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 192.9784 - val_loss: 149.0135\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 166.4678 - val_loss: 181.7081\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 216.0053 - val_loss: 231.7371\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 200.6463 - val_loss: 193.2665\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 234.4355 - val_loss: 288.6092\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 194.0000 - val_loss: 214.5148\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 167.5387 - val_loss: 204.0224\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 183.5168 - val_loss: 153.8259\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 224.8797 - val_loss: 200.1130\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 237.5657 - val_loss: 211.6995\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 185.7139 - val_loss: 245.5660\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 290.3279 - val_loss: 270.6024\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 191.6468 - val_loss: 165.1260\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 182.7075 - val_loss: 177.1171\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 175.7177 - val_loss: 149.9887\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 198.7310 - val_loss: 185.8049\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 181.2920 - val_loss: 253.3559\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 174.2701 - val_loss: 379.8870\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 210.9326 - val_loss: 165.0145\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 205.1174 - val_loss: 213.0713\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 189.5124 - val_loss: 437.4326\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 201.2062 - val_loss: 164.4382\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 281.8887 - val_loss: 175.3517\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 165.9003 - val_loss: 192.6042\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 172.6461 - val_loss: 154.9488\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 203.9101 - val_loss: 214.6436\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 226.8264 - val_loss: 180.4655\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 176.3833 - val_loss: 189.7932\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 207.2119 - val_loss: 220.3266\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 175.3808 - val_loss: 166.0279\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 174.2439 - val_loss: 217.2273\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 175.1881 - val_loss: 232.1592\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 263.2791 - val_loss: 172.2843\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 163.4765 - val_loss: 161.3799\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 282.5311 - val_loss: 175.8778\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 169.3497 - val_loss: 153.5512\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 238.0112 - val_loss: 313.8893\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 215.0284 - val_loss: 159.8047\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 204.0792 - val_loss: 241.6776\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 190.1184 - val_loss: 258.8801\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 175.4941 - val_loss: 151.6198\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 190.5374 - val_loss: 148.5993\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 180.1303 - val_loss: 150.9588\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 170.8121 - val_loss: 224.1332\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 176.0688 - val_loss: 157.8562\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 381.6982 - val_loss: 210.1697\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 179.4343 - val_loss: 161.2334\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 192.6531 - val_loss: 196.6416\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 174.3302 - val_loss: 183.8903\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 180.6662 - val_loss: 189.9867\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 232.5789 - val_loss: 186.6506\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 184.9076 - val_loss: 143.0963\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 173.2710 - val_loss: 312.8048\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 199.7603 - val_loss: 279.5707\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 193.8726 - val_loss: 175.3802\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 232.8621 - val_loss: 149.8219\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 180.5762 - val_loss: 253.8935\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 163.4049 - val_loss: 152.5546\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 191.2694 - val_loss: 143.1056\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 171.3184 - val_loss: 229.8665\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 174.1416 - val_loss: 184.4101\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 171.1366 - val_loss: 150.5826\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 168.3195 - val_loss: 160.3151\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 260.7861 - val_loss: 177.1685\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 183.4396 - val_loss: 139.8012\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.7875 - val_loss: 193.0561\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 219.2685 - val_loss: 742.7726\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 320.3537 - val_loss: 258.4240\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 165.0142 - val_loss: 158.7252\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.0515 - val_loss: 148.8782\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 177.7679 - val_loss: 150.4853\n",
      "Epoch 298/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 352.3815 - val_loss: 162.1328\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 164.5160 - val_loss: 180.9469\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.5463 - val_loss: 158.0304\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 193.2382 - val_loss: 316.7664\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 161.2354 - val_loss: 153.4497\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 162.6104 - val_loss: 152.1016\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 153.1506 - val_loss: 237.2734\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 217.1530 - val_loss: 164.1253\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.9570 - val_loss: 182.0841\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 146.7875 - val_loss: 182.1579\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 176.6636 - val_loss: 147.6508\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 215.9547 - val_loss: 181.4007\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 174.2476 - val_loss: 214.2605\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 157.1118 - val_loss: 155.6529\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 161.7851 - val_loss: 360.8399\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 244.1449 - val_loss: 205.1977\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 163.1084 - val_loss: 159.5112\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 211.8944 - val_loss: 151.9180\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.3125 - val_loss: 142.9353\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 162.8276 - val_loss: 146.7648\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 174.4688 - val_loss: 167.7146\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.5698 - val_loss: 191.4322\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 163.1859 - val_loss: 153.7210\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 190.4475 - val_loss: 143.3278\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 164.8581 - val_loss: 147.7122\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 219.5141 - val_loss: 145.1272\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 165.9728 - val_loss: 247.4189\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 231.0615 - val_loss: 175.5369\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.5788 - val_loss: 142.3676\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.5497 - val_loss: 139.4969\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 165.7866 - val_loss: 312.2499\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.9665 - val_loss: 151.2573\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 171.0753 - val_loss: 168.6613\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 155.8236 - val_loss: 155.2510\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 207.5829 - val_loss: 141.4607\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 165.5765 - val_loss: 259.0496\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 167.4412 - val_loss: 176.9113\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 151.4904 - val_loss: 134.7230\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 166.8077 - val_loss: 142.2654\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.6287 - val_loss: 168.2518\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 167.3300 - val_loss: 143.6751\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 188.8973 - val_loss: 142.0055\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 175.2027 - val_loss: 199.2035\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 245.0885 - val_loss: 182.5904\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 157.5304 - val_loss: 149.3040\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 148.2257 - val_loss: 136.7185\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 167.9817 - val_loss: 153.5971\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 218.8327 - val_loss: 164.0416\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 165.0025 - val_loss: 140.4250\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 219.4595 - val_loss: 144.4953\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 161.1854 - val_loss: 157.4752\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.4486 - val_loss: 156.6335\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 195.3349 - val_loss: 182.9699\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 164.6184 - val_loss: 134.4091\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.3670 - val_loss: 152.9540\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 165.1189 - val_loss: 143.8775\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.3807 - val_loss: 216.9847\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 167.2080 - val_loss: 172.7405\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 181.6471 - val_loss: 135.5633\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.8146 - val_loss: 172.3340\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.9782 - val_loss: 136.1867\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 232.9478 - val_loss: 206.9468\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 191.9951 - val_loss: 157.2547\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 206.6905 - val_loss: 222.4982\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.9124 - val_loss: 208.9992\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 174.8773 - val_loss: 170.5311\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.3022 - val_loss: 137.9500\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 187.6986 - val_loss: 137.9699\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 175.4308 - val_loss: 191.0354\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 206.3607 - val_loss: 149.1192\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.8883 - val_loss: 227.4149\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 158.5720 - val_loss: 202.2742\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 185.5648 - val_loss: 167.4894\n",
      "Epoch 371/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.0438 - val_loss: 142.0753\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.6231 - val_loss: 138.5959\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 161.5189 - val_loss: 189.9624\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 231.4184 - val_loss: 148.1915\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 145.6597 - val_loss: 156.4489\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.6881 - val_loss: 150.9344\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.3900 - val_loss: 135.2915\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.0593 - val_loss: 167.5629\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.6161 - val_loss: 139.6011\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 156.7209 - val_loss: 153.0252\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 158.3229 - val_loss: 160.2683\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 173.9272 - val_loss: 155.4643\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 235.6475 - val_loss: 143.7280\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 175.5675 - val_loss: 218.5934\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.6885 - val_loss: 149.2717\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 230.9845 - val_loss: 149.1678\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.0731 - val_loss: 139.2072\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.7519 - val_loss: 180.1940\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.3614 - val_loss: 150.9482\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 165.1348 - val_loss: 162.3844\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 163.7989 - val_loss: 165.3552\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.6708 - val_loss: 155.8976\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 309.3047 - val_loss: 224.0110\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 213.8024 - val_loss: 136.6994\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.1868 - val_loss: 143.1423\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.4404 - val_loss: 233.2671\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.3821 - val_loss: 187.6317\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.3356 - val_loss: 137.4313\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 198.4089 - val_loss: 157.4172\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 168.7149 - val_loss: 219.6300\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 205.7151 - val_loss: 625.0767\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 183.8183 - val_loss: 154.7780\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.1341 - val_loss: 136.7660\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.0267 - val_loss: 138.0321\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 154.0402 - val_loss: 143.0322\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 158.4571 - val_loss: 149.4635\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 282.3919 - val_loss: 266.8617\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 158.6604 - val_loss: 176.2251\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.5970 - val_loss: 322.9631\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.6419 - val_loss: 134.4227\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 171.5764 - val_loss: 143.1132\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 166.7605 - val_loss: 148.7190\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 183.3517 - val_loss: 136.4036\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.4202 - val_loss: 143.0425\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.1360 - val_loss: 139.9397\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.6518 - val_loss: 213.4448\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.7726 - val_loss: 141.9303\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 189.3886 - val_loss: 181.7005\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 156.2556 - val_loss: 230.2587\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 219.7655 - val_loss: 179.7336\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.4055 - val_loss: 178.7131\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.6505 - val_loss: 218.0992\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.0034 - val_loss: 138.3134\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.0540 - val_loss: 174.2079\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.5868 - val_loss: 143.5629\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.1978 - val_loss: 165.2794\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.9483 - val_loss: 132.5915\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.7574 - val_loss: 316.4720\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 220.5639 - val_loss: 379.1366\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 211.9783 - val_loss: 152.2338\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.5107 - val_loss: 148.6616\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 160.7047 - val_loss: 186.5658\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 153.7371 - val_loss: 153.4610\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 154.3197 - val_loss: 182.4857\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.7388 - val_loss: 133.9107\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 197.5852 - val_loss: 152.8739\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.4891 - val_loss: 137.3828\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.2372 - val_loss: 249.4721\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 172.4710 - val_loss: 273.3674\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 158.1169 - val_loss: 134.2715\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.9301 - val_loss: 143.1857\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 154.1054 - val_loss: 157.5749\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.3177 - val_loss: 162.0417\n",
      "Epoch 444/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 180.5444 - val_loss: 141.2789\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.3411 - val_loss: 139.0625\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 160.0959 - val_loss: 208.1497\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 162.4186 - val_loss: 167.2770\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 154.9369 - val_loss: 134.5863\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 146.6869 - val_loss: 198.9172\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 173.9929 - val_loss: 132.6311\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.4044 - val_loss: 141.6149\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.4922 - val_loss: 172.7693\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 167.2740 - val_loss: 141.8124\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 179.7505 - val_loss: 173.4761\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 169.6623 - val_loss: 173.7771\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 170.7582 - val_loss: 139.0578\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 284.0797 - val_loss: 205.5276\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.7093 - val_loss: 136.3031\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.4973 - val_loss: 137.2921\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.0889 - val_loss: 171.6294\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.8013 - val_loss: 140.0879\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 203.7821 - val_loss: 169.9703\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.8453 - val_loss: 154.4439\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 163.2507 - val_loss: 136.0728\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.0431 - val_loss: 145.5582\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.6389 - val_loss: 213.5497\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 200.3181 - val_loss: 376.1450\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 173.4483 - val_loss: 154.8040\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.5176 - val_loss: 170.3857\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.6491 - val_loss: 135.3658\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.3043 - val_loss: 134.7865\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.7656 - val_loss: 471.9371\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 159.2380 - val_loss: 149.3084\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 162.8705 - val_loss: 235.2155\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.0361 - val_loss: 161.3494\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 167.8905 - val_loss: 142.4067\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 228.0868 - val_loss: 236.1309\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 158.4349 - val_loss: 212.3217\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 164.1568 - val_loss: 150.7921\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.4440 - val_loss: 146.0082\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.7683 - val_loss: 174.9387\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 220.5610 - val_loss: 326.9106\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.1435 - val_loss: 163.8488\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 240.3324 - val_loss: 155.6523\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.2550 - val_loss: 133.9643\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.3608 - val_loss: 131.2106\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.1485 - val_loss: 141.2171\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.4256 - val_loss: 149.6835\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.8251 - val_loss: 163.7897\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 199.9558 - val_loss: 156.4154\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.9841 - val_loss: 191.8772\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.2706 - val_loss: 142.8978\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 162.6966 - val_loss: 235.1346\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.6991 - val_loss: 157.0149\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 179.9603 - val_loss: 144.0136\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.8438 - val_loss: 146.0084\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.4675 - val_loss: 157.4931\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 143.9222 - val_loss: 144.5744\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 690.9074 - val_loss: 364.0732\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 286.8524 - val_loss: 209.0684\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 211.2591 - val_loss: 205.6864\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 234.1435 - val_loss: 182.9132\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 208.5170 - val_loss: 167.1179\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 255.3980 - val_loss: 223.5623\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 230.0338 - val_loss: 163.4675\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 191.0934 - val_loss: 441.8472\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 199.8707 - val_loss: 183.0873\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 184.4147 - val_loss: 174.9157\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 181.8243 - val_loss: 209.3504\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 198.7661 - val_loss: 176.5406\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 200.1087 - val_loss: 213.2246\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 199.6439 - val_loss: 196.6636\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 171.4149 - val_loss: 177.8908\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 200.3725 - val_loss: 312.7857\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 191.9744 - val_loss: 167.9360\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 188.3611 - val_loss: 160.9729\n",
      "Epoch 517/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 49us/step - loss: 183.9607 - val_loss: 245.5413\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 198.5310 - val_loss: 485.6630\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 170.6179 - val_loss: 196.9720\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 171.6147 - val_loss: 172.8800\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 166.6357 - val_loss: 172.8900\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 163.3845 - val_loss: 164.2434\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 179.3764 - val_loss: 162.1003\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 172.9282 - val_loss: 172.7125\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 180.2158 - val_loss: 158.6133\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 239.9814 - val_loss: 185.7265\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 161.4634 - val_loss: 155.5996\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 186.5305 - val_loss: 151.3371\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 161.4271 - val_loss: 154.8324\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 156.4161 - val_loss: 150.3337\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 215.1159 - val_loss: 151.6220\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 165.0854 - val_loss: 181.5411\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 158.3043 - val_loss: 155.6268\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 178.4231 - val_loss: 179.7672\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 170.1568 - val_loss: 171.0827\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 172.0341 - val_loss: 158.5814\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 205.3216 - val_loss: 190.9582\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.4850 - val_loss: 146.5344\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 178.6156 - val_loss: 195.6064\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 171.9212 - val_loss: 167.8457\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 164.3318 - val_loss: 150.2026\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 211.5617 - val_loss: 196.9375\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 174.0508 - val_loss: 188.5701\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 159.3783 - val_loss: 158.6220\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 166.3881 - val_loss: 207.2859\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 163.7086 - val_loss: 153.1667\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 154.3213 - val_loss: 184.9597\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 163.4677 - val_loss: 145.9661\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 377.1163 - val_loss: 636.3823\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 226.7658 - val_loss: 254.0500\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 173.5861 - val_loss: 144.4307\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 167.5219 - val_loss: 228.2126\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 172.4950 - val_loss: 158.7959\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 186.2930 - val_loss: 166.0573\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.7260 - val_loss: 145.1361\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.6100 - val_loss: 177.1042\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.5832 - val_loss: 204.7121\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 192.2449 - val_loss: 211.3238\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 152.8114 - val_loss: 151.0058\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 160.9635 - val_loss: 155.4229\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 162.8790 - val_loss: 146.3689\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.4413 - val_loss: 219.2450\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 159.5108 - val_loss: 436.0779\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 169.9106 - val_loss: 175.0687\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 163.9001 - val_loss: 140.3589\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 172.3556 - val_loss: 167.8418\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 272.7464 - val_loss: 209.7751\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.6012 - val_loss: 297.5730\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 164.6929 - val_loss: 156.0985\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.3559 - val_loss: 197.1520\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.9259 - val_loss: 147.3385\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.7248 - val_loss: 142.3221\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 151.6068 - val_loss: 159.5381\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 167.9485 - val_loss: 154.0084\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 171.5841 - val_loss: 150.3726\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 160.4876 - val_loss: 317.9373\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 203.9980 - val_loss: 165.4547\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 147.3612 - val_loss: 172.7127\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 153.4729 - val_loss: 144.4983\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 174.3092 - val_loss: 174.7964\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 171.8525 - val_loss: 144.9774\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.2395 - val_loss: 150.0244\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 163.1869 - val_loss: 161.3331\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.5361 - val_loss: 165.4052\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.7611 - val_loss: 313.4720\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 219.3445 - val_loss: 154.0772\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.4593 - val_loss: 149.0476\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 175.6574 - val_loss: 174.8428\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 154.0816 - val_loss: 148.3847\n",
      "Epoch 590/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 203.1112 - val_loss: 313.6513\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 161.7225 - val_loss: 147.3780\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.9303 - val_loss: 148.4218\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.6174 - val_loss: 146.2566\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.2356 - val_loss: 137.6825\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.4447 - val_loss: 143.9474\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.5377 - val_loss: 142.7866\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.7517 - val_loss: 135.9879\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.8695 - val_loss: 207.6905\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 182.6067 - val_loss: 141.2225\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.3805 - val_loss: 137.7976\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.7897 - val_loss: 144.3613\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 163.0063 - val_loss: 141.7481\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.4935 - val_loss: 148.0531\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.6561 - val_loss: 142.0472\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 161.2581 - val_loss: 146.5652\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 312.1522 - val_loss: 268.8728\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.9625 - val_loss: 144.3173\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.9567 - val_loss: 138.3690\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.8716 - val_loss: 135.3683\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 160.1224 - val_loss: 140.6425\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.6621 - val_loss: 140.1673\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.6690 - val_loss: 182.8659\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.1065 - val_loss: 166.6627\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 158.6119 - val_loss: 157.7314\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 152.6302 - val_loss: 133.8596\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.6744 - val_loss: 245.2055\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 550.2582 - val_loss: 521.9367\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 327.7843 - val_loss: 219.0091\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 187.6377 - val_loss: 200.1113\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 198.7061 - val_loss: 443.4537\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 202.4761 - val_loss: 151.5292\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 171.0167 - val_loss: 157.0959\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 209.8707 - val_loss: 348.3999\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 177.3329 - val_loss: 167.0725\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 164.4908 - val_loss: 149.3980\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.3326 - val_loss: 175.5095\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 166.1211 - val_loss: 154.3980\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.4654 - val_loss: 316.3431\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 190.7989 - val_loss: 155.8833\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 160.0659 - val_loss: 142.8999\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.5222 - val_loss: 144.1089\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 148.3228 - val_loss: 142.9491\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.0959 - val_loss: 196.8181\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 160.0827 - val_loss: 168.5804\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.3220 - val_loss: 159.6872\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 172.4237 - val_loss: 224.4576\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 165.3207 - val_loss: 155.6881\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.3572 - val_loss: 140.2983\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 226.7430 - val_loss: 257.3156\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 169.3515 - val_loss: 143.6782\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.7487 - val_loss: 142.8640\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 156.3620 - val_loss: 171.8894\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.3199 - val_loss: 367.1458\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.0440 - val_loss: 157.7196\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 158.5639 - val_loss: 238.6501\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.0361 - val_loss: 162.0688\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 155.7200 - val_loss: 187.4763\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 178.8965 - val_loss: 160.8324\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 158.7478 - val_loss: 141.5413\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.9210 - val_loss: 146.0545\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.4434 - val_loss: 136.9257\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 246.5563 - val_loss: 203.6621\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.7761 - val_loss: 143.4580\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.7670 - val_loss: 151.2053\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 155.8590 - val_loss: 146.3012\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 142.8578 - val_loss: 198.4450\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 142.5056 - val_loss: 147.3014\n",
      "Epoch 658/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 144.8927 - val_loss: 237.3380\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.6958 - val_loss: 172.0119\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 160.6865 - val_loss: 144.5942\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.6735 - val_loss: 137.7220\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 157.0340 - val_loss: 131.7675\n",
      "Epoch 663/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.3010 - val_loss: 160.4995\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 160.8697 - val_loss: 293.3002\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.6245 - val_loss: 156.8383\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 169.2598 - val_loss: 144.9215\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.3995 - val_loss: 146.4048\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 147.6671 - val_loss: 153.3225\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 152.5146 - val_loss: 192.4166\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.8859 - val_loss: 149.4224\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 160.4599 - val_loss: 156.9707\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 161.4568 - val_loss: 280.8555\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.4050 - val_loss: 177.3001\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.2430 - val_loss: 192.3478\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 208.3610 - val_loss: 146.5422\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.7204 - val_loss: 136.9404\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 165.0352 - val_loss: 146.9209\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.4608 - val_loss: 161.0449\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.2733 - val_loss: 155.1508\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.1877 - val_loss: 138.2739\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 163.2564 - val_loss: 146.3586\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.2899 - val_loss: 143.1956\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.6629 - val_loss: 138.3541\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 149.1951 - val_loss: 149.4353\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 259.3161 - val_loss: 210.3888\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.5049 - val_loss: 147.5235\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 143.1375 - val_loss: 130.5051\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.1333 - val_loss: 136.6029\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.6508 - val_loss: 173.6815\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.0525 - val_loss: 150.8629\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.2686 - val_loss: 181.9776\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 150.6520 - val_loss: 170.0414\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 146.7460 - val_loss: 301.8603\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.1896 - val_loss: 145.0678\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 153.5873 - val_loss: 140.5020\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.3222 - val_loss: 146.2899\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 160.2885 - val_loss: 170.8166\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 151.8234 - val_loss: 322.5731\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 177.1468 - val_loss: 158.5583\n",
      "Epoch 700/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.0264 - val_loss: 130.1320\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.7151 - val_loss: 136.0908\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.3301 - val_loss: 138.8494\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.6892 - val_loss: 149.5684\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.7524 - val_loss: 137.4756\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 269.1105 - val_loss: 150.1701\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.1173 - val_loss: 134.1383\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.9596 - val_loss: 137.2170\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.9641 - val_loss: 142.2676\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.9737 - val_loss: 134.4456\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.0799 - val_loss: 153.0142\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.3069 - val_loss: 138.6676\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.7309 - val_loss: 235.4790\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.9766 - val_loss: 147.0497\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 152.1458 - val_loss: 150.8050\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.5721 - val_loss: 187.3820\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 194.4319 - val_loss: 149.7463\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.4619 - val_loss: 147.6318\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.1035 - val_loss: 144.0519\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.1467 - val_loss: 162.1255\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.0450 - val_loss: 140.7038\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.7977 - val_loss: 194.9809\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.6028 - val_loss: 136.8339\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.7361 - val_loss: 143.6004\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.6525 - val_loss: 142.6993\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.8921 - val_loss: 157.7052\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.4438 - val_loss: 166.9561\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 172.2369 - val_loss: 155.3089\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.5496 - val_loss: 340.3166\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 199.9894 - val_loss: 257.5823\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 267.3378 - val_loss: 138.1900\n",
      "Epoch 731/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.3121 - val_loss: 140.5536\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.9165 - val_loss: 156.4426\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.8949 - val_loss: 147.6133\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.4915 - val_loss: 145.9567\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 175.9539 - val_loss: 148.4772\n",
      "Epoch 736/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.7372 - val_loss: 136.8840\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.7820 - val_loss: 141.1853\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.3486 - val_loss: 149.9301\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.6931 - val_loss: 149.6750\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.3473 - val_loss: 148.5829\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.8722 - val_loss: 189.4452\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.3848 - val_loss: 160.4176\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.0169 - val_loss: 197.8400\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 160.3135 - val_loss: 157.6051\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.9635 - val_loss: 145.8791\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.6410 - val_loss: 147.8965\n",
      "Epoch 747/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 198.9238 - val_loss: 158.6952\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.5149 - val_loss: 137.1813\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.1270 - val_loss: 155.9974\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.2733 - val_loss: 142.2241\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 201.6122 - val_loss: 136.4407\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.5820 - val_loss: 177.0054\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.1924 - val_loss: 146.8152\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.4260 - val_loss: 140.6826\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.8492 - val_loss: 147.4657\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.6251 - val_loss: 147.6341\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.2093 - val_loss: 131.2404\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.7290 - val_loss: 147.6233\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.4982 - val_loss: 150.5576\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.8785 - val_loss: 136.2191\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.6548 - val_loss: 155.3878\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.6126 - val_loss: 155.3784\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.5016 - val_loss: 154.3088\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.9291 - val_loss: 137.4996\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.4006 - val_loss: 136.6369\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 155.7656 - val_loss: 133.2836\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.8349 - val_loss: 133.5586\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.3793 - val_loss: 161.6541\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.8130 - val_loss: 142.8726\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.9464 - val_loss: 138.6969\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.5223 - val_loss: 176.2376\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.8275 - val_loss: 140.0843\n",
      "Epoch 773/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.4793 - val_loss: 201.1487\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.1500 - val_loss: 142.3125\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.9563 - val_loss: 137.6525\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.5500 - val_loss: 129.5733\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.4327 - val_loss: 133.2062\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.2839 - val_loss: 218.2070\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 165.7639 - val_loss: 137.0306\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.8390 - val_loss: 146.2544\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.5317 - val_loss: 152.8960\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.0374 - val_loss: 305.0722\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.1695 - val_loss: 140.4263\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.4590 - val_loss: 133.3947\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.9667 - val_loss: 133.0728\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 174.4246 - val_loss: 1122.0069\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 199.5895 - val_loss: 147.8715\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.8797 - val_loss: 140.8336\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.4023 - val_loss: 147.5232\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.4350 - val_loss: 262.6294\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.2772 - val_loss: 131.0718\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.3058 - val_loss: 139.1933\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.9501 - val_loss: 143.8787\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.3140 - val_loss: 265.8319\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.9415 - val_loss: 135.6256\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.8762 - val_loss: 154.9534\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.7274 - val_loss: 161.4217\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.7431 - val_loss: 131.3119\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 246.7695 - val_loss: 138.3971\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.8738 - val_loss: 163.4801\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.1423 - val_loss: 131.4462\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.3529 - val_loss: 180.7138\n",
      "Epoch 803/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.1939 - val_loss: 140.6438\n",
      "Epoch 804/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.0036 - val_loss: 146.9819\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.8783 - val_loss: 137.9243\n",
      "Epoch 806/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.3447 - val_loss: 140.8914\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.2033 - val_loss: 336.2584\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.3861 - val_loss: 181.2445\n",
      "Epoch 809/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 49us/step - loss: 148.9917 - val_loss: 131.1124\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 270.8767 - val_loss: 196.5741\n",
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.3511 - val_loss: 145.2934\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.4937 - val_loss: 139.3441\n",
      "Epoch 813/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 130.3088 - val_loss: 144.0385\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 136.0430 - val_loss: 162.8738\n",
      "Epoch 815/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 139.4940 - val_loss: 169.4552\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 160.3692 - val_loss: 137.5218\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.3918 - val_loss: 134.4022\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.1700 - val_loss: 138.5056\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.4322 - val_loss: 147.4672\n",
      "Epoch 820/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.3230 - val_loss: 141.7786\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.8310 - val_loss: 130.0730\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 154.6359 - val_loss: 371.2843\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 179.4404 - val_loss: 134.5873\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.0859 - val_loss: 237.5061\n",
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 310.6704 - val_loss: 304.8120\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.7113 - val_loss: 139.9980\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.3308 - val_loss: 140.1425\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.7300 - val_loss: 131.6184\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.2192 - val_loss: 130.2052\n",
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.7037 - val_loss: 179.4560\n",
      "Epoch 831/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.8284 - val_loss: 139.8512\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.4207 - val_loss: 137.2260\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.7624 - val_loss: 129.6167\n",
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.2260 - val_loss: 134.0009\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.1231 - val_loss: 139.2296\n",
      "Epoch 836/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.8502 - val_loss: 177.6470\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.2228 - val_loss: 133.4059\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 176.6180 - val_loss: 138.3974\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 141.9180 - val_loss: 142.2068\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.8236 - val_loss: 145.9853\n",
      "Epoch 841/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.1367 - val_loss: 137.1232\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.2040 - val_loss: 135.1084\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.8070 - val_loss: 183.0141\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 157.0097 - val_loss: 141.7575\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.2369 - val_loss: 133.3901\n",
      "Epoch 846/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 166.3022 - val_loss: 170.9350\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.0811 - val_loss: 180.9191\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.0529 - val_loss: 130.0537\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.0863 - val_loss: 159.7703\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.9106 - val_loss: 184.0583\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 328.1317 - val_loss: 134.1993\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.6112 - val_loss: 161.4204\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.4506 - val_loss: 134.6865\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.0646 - val_loss: 138.5546\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.3591 - val_loss: 138.0305\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.0908 - val_loss: 204.8763\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.8087 - val_loss: 147.1494\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.8735 - val_loss: 141.2773\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.1274 - val_loss: 134.6735\n",
      "Epoch 860/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.2620 - val_loss: 128.4935\n",
      "Epoch 861/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.6244 - val_loss: 157.8543\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.0807 - val_loss: 129.7610\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.1121 - val_loss: 154.5147\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.5156 - val_loss: 134.2501\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.3688 - val_loss: 219.6040\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 174.6432 - val_loss: 133.0110\n",
      "Epoch 867/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.4040 - val_loss: 135.1857\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.2061 - val_loss: 206.3084\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.8343 - val_loss: 158.5477\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 451.9093 - val_loss: 638.6434\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 265.0809 - val_loss: 193.0536\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 212.7189 - val_loss: 167.9745\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 193.0216 - val_loss: 612.7855\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 226.1805 - val_loss: 216.6961\n",
      "Epoch 875/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 174.3906 - val_loss: 169.4917\n",
      "Epoch 876/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 168.6525 - val_loss: 186.1585\n",
      "Epoch 877/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.0876 - val_loss: 161.8866\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 163.2634 - val_loss: 232.6955\n",
      "Epoch 879/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 167.5859 - val_loss: 159.7246\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 177.7047 - val_loss: 173.7225\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.4185 - val_loss: 177.4159\n",
      "Epoch 882/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 163.2811 - val_loss: 179.3605\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.3969 - val_loss: 273.7138\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 165.0598 - val_loss: 146.5813\n",
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 158.6274 - val_loss: 225.6067\n",
      "Epoch 886/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 168.2842 - val_loss: 150.5395\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.2533 - val_loss: 229.0959\n",
      "Epoch 888/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.8895 - val_loss: 148.3066\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.1877 - val_loss: 179.9443\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 205.6413 - val_loss: 208.9853\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 168.4320 - val_loss: 193.0525\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.0128 - val_loss: 177.5830\n",
      "Epoch 893/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 141.3940 - val_loss: 146.7669\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 147.8701 - val_loss: 135.5376\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 173.8128 - val_loss: 159.6958\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 165.6574 - val_loss: 135.6335\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.5939 - val_loss: 161.8579\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.9435 - val_loss: 165.3125\n",
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 221.1094 - val_loss: 169.1591\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.2008 - val_loss: 141.5808\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 161.7148 - val_loss: 166.4930\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.1252 - val_loss: 137.9670\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 151.3339 - val_loss: 190.3435\n",
      "Epoch 904/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 149.6238 - val_loss: 149.7437\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.6595 - val_loss: 173.0409\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 182.9167 - val_loss: 150.3379\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 163.3930 - val_loss: 190.3941\n",
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 173.7401 - val_loss: 136.9132\n",
      "Epoch 909/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.2746 - val_loss: 167.9881\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.3547 - val_loss: 138.7012\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.7040 - val_loss: 223.5124\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 164.6048 - val_loss: 145.1697\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 173.9513 - val_loss: 162.7266\n",
      "Epoch 914/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.2246 - val_loss: 142.3306\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 157.1136 - val_loss: 222.1329\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.9253 - val_loss: 143.9579\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 148.0499 - val_loss: 148.7163\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.9092 - val_loss: 178.2322\n",
      "Epoch 919/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.8921 - val_loss: 177.2493\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.9473 - val_loss: 218.7636\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.3724 - val_loss: 166.5987\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.2354 - val_loss: 153.8531\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 147.5612 - val_loss: 150.3870\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 160.7836 - val_loss: 146.2540\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.0739 - val_loss: 133.0454\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.1610 - val_loss: 281.9158\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.6401 - val_loss: 132.2724\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 166.2302 - val_loss: 206.8469\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.0827 - val_loss: 139.7158\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.9749 - val_loss: 273.7432\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.6481 - val_loss: 133.6859\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.8468 - val_loss: 232.7327\n",
      "Epoch 933/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 217.8281 - val_loss: 151.8035\n",
      "Epoch 934/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.2296 - val_loss: 153.0995\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 150.0560 - val_loss: 133.3604\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.3264 - val_loss: 137.0798\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.3359 - val_loss: 136.7701\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.7909 - val_loss: 140.8295\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.1552 - val_loss: 180.0252\n",
      "Epoch 940/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.9933 - val_loss: 138.4610\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.1115 - val_loss: 140.5218\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.9183 - val_loss: 214.1475\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 153.7622 - val_loss: 143.7454\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 363.0347 - val_loss: 200.4848\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.4231 - val_loss: 133.5784\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.1752 - val_loss: 134.0808\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.6222 - val_loss: 142.5162\n",
      "Epoch 948/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.5667 - val_loss: 142.1262\n",
      "Epoch 949/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.0038 - val_loss: 142.3635\n",
      "Epoch 950/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.2376 - val_loss: 142.0756\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.2990 - val_loss: 138.9994\n",
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.9000 - val_loss: 162.8968\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.2293 - val_loss: 145.0573\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.5083 - val_loss: 214.2160\n",
      "Epoch 955/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 55us/step - loss: 147.7041 - val_loss: 148.0499\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.4894 - val_loss: 212.1604\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.2402 - val_loss: 150.1615\n",
      "Epoch 958/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 172.7532 - val_loss: 170.4683\n",
      "Epoch 959/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.9094 - val_loss: 158.4669\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.0817 - val_loss: 133.3167\n",
      "Epoch 961/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.4443 - val_loss: 138.4919\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.8566 - val_loss: 135.1528\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 226.3729 - val_loss: 139.1617\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.1408 - val_loss: 160.2663\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.0266 - val_loss: 133.6236\n",
      "Epoch 966/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.3855 - val_loss: 202.0111\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.5032 - val_loss: 190.7069\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 171.6687 - val_loss: 216.0473\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.5583 - val_loss: 133.1400\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 171.7877 - val_loss: 137.5453\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.7869 - val_loss: 134.1829\n",
      "Epoch 972/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 144.6996 - val_loss: 163.2533\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.4278 - val_loss: 136.2729\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.6757 - val_loss: 137.2013\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.3578 - val_loss: 175.9560\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 169.5050 - val_loss: 131.5616\n",
      "Epoch 977/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.7580 - val_loss: 132.9476\n",
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.1689 - val_loss: 137.4413\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.1377 - val_loss: 138.6030\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.4372 - val_loss: 210.9800\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.8311 - val_loss: 224.0793\n",
      "Epoch 982/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.1346 - val_loss: 142.5125\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.1300 - val_loss: 132.6457\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 248.0274 - val_loss: 301.0985\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 177.5665 - val_loss: 169.9285\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.7468 - val_loss: 136.1454\n",
      "Epoch 987/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.3605 - val_loss: 144.4773\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.5136 - val_loss: 136.8171\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.8270 - val_loss: 137.7095\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 141.2826 - val_loss: 185.2250\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 323.3017 - val_loss: 178.9918\n",
      "Epoch 992/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.8253 - val_loss: 151.7013\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 164.2866 - val_loss: 137.5153\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 158.7683 - val_loss: 154.3730\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.8514 - val_loss: 233.2727\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.9550 - val_loss: 143.5642\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.6248 - val_loss: 160.5512\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.0434 - val_loss: 142.1406\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.5165 - val_loss: 136.6217\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.9830 - val_loss: 146.3782\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 324.8094 - val_loss: 132.3423\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.0707 - val_loss: 143.8528\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.7468 - val_loss: 152.5879\n",
      "Epoch 1004/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.8190 - val_loss: 130.2460\n",
      "Epoch 1005/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.2155 - val_loss: 189.3099\n",
      "Epoch 1006/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.5704 - val_loss: 137.2982\n",
      "Epoch 1007/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.3439 - val_loss: 130.2882\n",
      "Epoch 1008/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 176.4979 - val_loss: 148.9367\n",
      "Epoch 1009/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.3447 - val_loss: 133.3509\n",
      "Epoch 1010/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.8956 - val_loss: 163.3415\n",
      "Epoch 1011/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.7304 - val_loss: 133.3631\n",
      "Epoch 1012/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.3035 - val_loss: 145.5745\n",
      "Epoch 1013/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 171.5793 - val_loss: 1305.3809\n",
      "Epoch 1014/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 172.9071 - val_loss: 143.7960\n",
      "Epoch 1015/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.1226 - val_loss: 208.1867\n",
      "Epoch 1016/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.9296 - val_loss: 196.0858\n",
      "Epoch 1017/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.1249 - val_loss: 132.7272\n",
      "Epoch 1018/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.9636 - val_loss: 190.2240\n",
      "Epoch 1019/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.0645 - val_loss: 146.4345\n",
      "Epoch 1020/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.1901 - val_loss: 156.1198\n",
      "Epoch 1021/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 358.4519 - val_loss: 322.1958\n",
      "Epoch 1022/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 295.8594 - val_loss: 229.5988\n",
      "Epoch 1023/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 241.9647 - val_loss: 329.6735\n",
      "Epoch 1024/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 188.6339 - val_loss: 163.8521\n",
      "Epoch 1025/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 175.7243 - val_loss: 169.1333\n",
      "Epoch 1026/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 164.6187 - val_loss: 223.3081\n",
      "Epoch 1027/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 168.1746 - val_loss: 169.4179\n",
      "Epoch 1028/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 175.0944 - val_loss: 230.5596\n",
      "Epoch 1029/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 164.3654 - val_loss: 172.1527\n",
      "Epoch 1030/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 161.1497 - val_loss: 164.2726\n",
      "Epoch 1031/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 182.2607 - val_loss: 142.9568\n",
      "Epoch 1032/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.6977 - val_loss: 157.1869\n",
      "Epoch 1033/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.5066 - val_loss: 191.4167\n",
      "Epoch 1034/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.6117 - val_loss: 167.0934\n",
      "Epoch 1035/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.9913 - val_loss: 176.3823\n",
      "Epoch 1036/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.1191 - val_loss: 166.2961\n",
      "Epoch 1037/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.7105 - val_loss: 216.6056\n",
      "Epoch 1038/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 169.3009 - val_loss: 227.1824\n",
      "Epoch 1039/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 151.1770 - val_loss: 221.6615\n",
      "Epoch 1040/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 158.7806 - val_loss: 137.4682\n",
      "Epoch 1041/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.6257 - val_loss: 222.0826\n",
      "Epoch 1042/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.3880 - val_loss: 168.3907\n",
      "Epoch 1043/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 159.1927 - val_loss: 192.6354\n",
      "Epoch 1044/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.1174 - val_loss: 179.9591\n",
      "Epoch 1045/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 155.7312 - val_loss: 141.9599\n",
      "Epoch 1046/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.2054 - val_loss: 159.3009\n",
      "Epoch 1047/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.9439 - val_loss: 151.9229\n",
      "Epoch 1048/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 160.2333 - val_loss: 134.8918\n",
      "Epoch 1049/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 151.2921 - val_loss: 136.6446\n",
      "Epoch 1050/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 170.0569 - val_loss: 156.1654\n",
      "Epoch 1051/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 147.2887 - val_loss: 146.0796\n",
      "Epoch 1052/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 165.9810 - val_loss: 152.9113\n",
      "Epoch 1053/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.4402 - val_loss: 136.2633\n",
      "Epoch 1054/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.1863 - val_loss: 205.4479\n",
      "Epoch 1055/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.1471 - val_loss: 136.2597\n",
      "Epoch 1056/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.6876 - val_loss: 154.3262\n",
      "Epoch 1057/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.3913 - val_loss: 173.4973\n",
      "Epoch 1058/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.5184 - val_loss: 151.0194\n",
      "Epoch 1059/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.5621 - val_loss: 138.2664\n",
      "Epoch 1060/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.0656 - val_loss: 200.7414\n",
      "Epoch 1061/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.2107 - val_loss: 132.5712\n",
      "Epoch 1062/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.6990 - val_loss: 133.6543\n",
      "Epoch 1063/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.9610 - val_loss: 132.1611\n",
      "Epoch 1064/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.3221 - val_loss: 139.8575\n",
      "Epoch 1065/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.3344 - val_loss: 148.8544\n",
      "Epoch 1066/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.5978 - val_loss: 231.6354\n",
      "Epoch 1067/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 171.6077 - val_loss: 155.2597\n",
      "Epoch 1068/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.8703 - val_loss: 156.8551\n",
      "Epoch 1069/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.5924 - val_loss: 243.5320\n",
      "Epoch 1070/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.7894 - val_loss: 148.7704\n",
      "Epoch 1071/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 170.1648 - val_loss: 230.7704\n",
      "Epoch 1072/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 192.1020 - val_loss: 133.7139\n",
      "Epoch 1073/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.0451 - val_loss: 173.6262\n",
      "Epoch 1074/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.8172 - val_loss: 134.1836\n",
      "Epoch 1075/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.9902 - val_loss: 173.4902\n",
      "Epoch 1076/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.5342 - val_loss: 131.0180\n",
      "Epoch 1077/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 142.4065 - val_loss: 151.0144\n",
      "Epoch 1078/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.6982 - val_loss: 139.3204\n",
      "Epoch 1079/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.5173 - val_loss: 138.0763\n",
      "Epoch 1080/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.2876 - val_loss: 161.1407\n",
      "Epoch 1081/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.5186 - val_loss: 137.4567\n",
      "Epoch 1082/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.3390 - val_loss: 134.2529\n",
      "Epoch 1083/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.1465 - val_loss: 331.0231\n",
      "Epoch 1084/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 209.1271 - val_loss: 136.0070\n",
      "Epoch 1085/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.5732 - val_loss: 133.6274\n",
      "Epoch 1086/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.3772 - val_loss: 173.3172\n",
      "Epoch 1087/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.1357 - val_loss: 136.8246\n",
      "Epoch 1088/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.6711 - val_loss: 133.7163\n",
      "Epoch 1089/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 185.8154 - val_loss: 131.7222\n",
      "Epoch 1090/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.6706 - val_loss: 142.2634\n",
      "Epoch 1091/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.6741 - val_loss: 143.1200\n",
      "Epoch 1092/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 165.8976 - val_loss: 149.4102\n",
      "Epoch 1093/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.8128 - val_loss: 132.9199\n",
      "Epoch 1094/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.4311 - val_loss: 156.3514\n",
      "Epoch 1095/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.1962 - val_loss: 143.5669\n",
      "Epoch 1096/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.2115 - val_loss: 131.1294\n",
      "Epoch 1097/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.6669 - val_loss: 148.0142\n",
      "Epoch 1098/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.3893 - val_loss: 168.0667\n",
      "Epoch 1099/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.3523 - val_loss: 154.7651\n",
      "Epoch 1100/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.3442 - val_loss: 139.6651\n",
      "Epoch 1101/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.6352 - val_loss: 153.0307\n",
      "Epoch 1102/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.2839 - val_loss: 155.0385\n",
      "Epoch 1103/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.6501 - val_loss: 163.8380\n",
      "Epoch 1104/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 136.0330 - val_loss: 133.7202\n",
      "Epoch 1105/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.1306 - val_loss: 149.9461\n",
      "Epoch 1106/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.6582 - val_loss: 145.5276\n",
      "Epoch 1107/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.9924 - val_loss: 150.8279\n",
      "Epoch 1108/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.3083 - val_loss: 210.4863\n",
      "Epoch 1109/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.9327 - val_loss: 141.3144\n",
      "Epoch 1110/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.0249 - val_loss: 154.5705\n",
      "Epoch 1111/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.1718 - val_loss: 134.0507\n",
      "Epoch 1112/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.6679 - val_loss: 147.9738\n",
      "Epoch 1113/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.2745 - val_loss: 193.9080\n",
      "Epoch 1114/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.6976 - val_loss: 180.3624\n",
      "Epoch 1115/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 160.5952 - val_loss: 135.1153\n",
      "Epoch 1116/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 158.1255 - val_loss: 131.0551\n",
      "Epoch 1117/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.0301 - val_loss: 134.2194\n",
      "Epoch 1118/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.9064 - val_loss: 132.1247\n",
      "Epoch 1119/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.4835 - val_loss: 194.1330\n",
      "Epoch 1120/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.3583 - val_loss: 147.5189\n",
      "Epoch 1121/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.6788 - val_loss: 168.6136\n",
      "Epoch 1122/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.6011 - val_loss: 133.6223\n",
      "Epoch 1123/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.3159 - val_loss: 134.4942\n",
      "Epoch 1124/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.7937 - val_loss: 185.0476\n",
      "Epoch 1125/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.1305 - val_loss: 139.7903\n",
      "Epoch 1126/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 138.0757 - val_loss: 222.1847\n",
      "Epoch 1127/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 142.4092 - val_loss: 138.8977\n",
      "Epoch 1128/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.2029 - val_loss: 135.0649\n",
      "Epoch 1129/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.6539 - val_loss: 169.9982\n",
      "Epoch 1130/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.6733 - val_loss: 150.9464\n",
      "Epoch 1131/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.2703 - val_loss: 134.0917\n",
      "Epoch 1132/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.1770 - val_loss: 143.8628\n",
      "Epoch 1133/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.2747 - val_loss: 137.9805\n",
      "Epoch 1134/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.4386 - val_loss: 471.0639\n",
      "Epoch 1135/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 174.5457 - val_loss: 158.7542\n",
      "Epoch 1136/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.1689 - val_loss: 132.7313\n",
      "Epoch 1137/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.7077 - val_loss: 139.2074\n",
      "Epoch 1138/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.6355 - val_loss: 171.2675\n",
      "Epoch 1139/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.6288 - val_loss: 133.4725\n",
      "Epoch 1140/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.5308 - val_loss: 177.6106\n",
      "Epoch 1141/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.0453 - val_loss: 157.4561\n",
      "Epoch 1142/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.7956 - val_loss: 143.6181\n",
      "Epoch 1143/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.3066 - val_loss: 136.1914\n",
      "Epoch 1144/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.9897 - val_loss: 186.8556\n",
      "Epoch 1145/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.5811 - val_loss: 141.9122\n",
      "Epoch 1146/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.2264 - val_loss: 183.2445\n",
      "Epoch 1147/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.6671 - val_loss: 160.8069\n",
      "Epoch 1148/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.8087 - val_loss: 135.6143\n",
      "Epoch 1149/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.5651 - val_loss: 145.8795\n",
      "Epoch 1150/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.1241 - val_loss: 136.3761\n",
      "Epoch 1151/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.3772 - val_loss: 165.7234\n",
      "Epoch 1152/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.1984 - val_loss: 223.8720\n",
      "Epoch 1153/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.8125 - val_loss: 132.9284\n",
      "Epoch 1154/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.9197 - val_loss: 135.3287\n",
      "Epoch 1155/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.9524 - val_loss: 154.8695\n",
      "Epoch 1156/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.5994 - val_loss: 146.7809\n",
      "Epoch 1157/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.0447 - val_loss: 183.3864\n",
      "Epoch 1158/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.9820 - val_loss: 151.6010\n",
      "Epoch 1159/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 180.4862 - val_loss: 273.6291\n",
      "Epoch 1160/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.3580 - val_loss: 152.1411\n",
      "Epoch 1161/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.2856 - val_loss: 141.1022\n",
      "Epoch 1162/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.7923 - val_loss: 176.8144\n",
      "Epoch 1163/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.0971 - val_loss: 175.9747\n",
      "Epoch 1164/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.0276 - val_loss: 156.6976\n",
      "Epoch 1165/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.8744 - val_loss: 136.8295\n",
      "Epoch 1166/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.4505 - val_loss: 150.9216\n",
      "Epoch 1167/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.5614 - val_loss: 130.9712\n",
      "Epoch 1168/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.1171 - val_loss: 139.7670\n",
      "Epoch 1169/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.3083 - val_loss: 139.5414\n",
      "Epoch 1170/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.7479 - val_loss: 159.5357\n",
      "Epoch 1171/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.8101 - val_loss: 145.0737\n",
      "Epoch 1172/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.9402 - val_loss: 133.5729\n",
      "Epoch 1173/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.1306 - val_loss: 133.9057\n",
      "Epoch 1174/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 176.5350 - val_loss: 245.8994\n",
      "Epoch 1175/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 190.2646 - val_loss: 151.1617\n",
      "Epoch 1176/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.6477 - val_loss: 144.5432\n",
      "Epoch 1177/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.7053 - val_loss: 144.1061\n",
      "Epoch 1178/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.2970 - val_loss: 141.1216\n",
      "Epoch 1179/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.0443 - val_loss: 142.4961\n",
      "Epoch 1180/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 152.0681 - val_loss: 148.6825\n",
      "Epoch 1181/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.1609 - val_loss: 134.6137\n",
      "Epoch 1182/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.3524 - val_loss: 138.1801\n",
      "Epoch 1183/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.6318 - val_loss: 135.5437\n",
      "Epoch 1184/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.2588 - val_loss: 137.7323\n",
      "Epoch 1185/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.4713 - val_loss: 129.8591\n",
      "Epoch 1186/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.7136 - val_loss: 155.5578\n",
      "Epoch 1187/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.7816 - val_loss: 192.7609\n",
      "Epoch 1188/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.5423 - val_loss: 132.7654\n",
      "Epoch 1189/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 135.6308 - val_loss: 135.4134\n",
      "Epoch 1190/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.0594 - val_loss: 133.0370\n",
      "Epoch 1191/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.9840 - val_loss: 132.1022\n",
      "Epoch 1192/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.5301 - val_loss: 137.9186\n",
      "Epoch 1193/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.9881 - val_loss: 154.0714\n",
      "Epoch 1194/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.9621 - val_loss: 163.9921\n",
      "Epoch 1195/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 177.7896 - val_loss: 140.1853\n",
      "Epoch 1196/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.9289 - val_loss: 128.0286\n",
      "Epoch 1197/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.2954 - val_loss: 161.2369\n",
      "Epoch 1198/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.7753 - val_loss: 138.9775\n",
      "Epoch 1199/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.5389 - val_loss: 197.8292\n",
      "Epoch 1200/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.4167 - val_loss: 155.2161\n",
      "Epoch 1201/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 151.1117 - val_loss: 147.1651\n",
      "Epoch 1202/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.6230 - val_loss: 149.3384\n",
      "Epoch 1203/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.4542 - val_loss: 134.5848\n",
      "Epoch 1204/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.8480 - val_loss: 146.2553\n",
      "Epoch 1205/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.6908 - val_loss: 140.3508\n",
      "Epoch 1206/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 252.1543 - val_loss: 138.6381\n",
      "Epoch 1207/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.6266 - val_loss: 167.6635\n",
      "Epoch 1208/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.5194 - val_loss: 134.4288\n",
      "Epoch 1209/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.7015 - val_loss: 154.1605\n",
      "Epoch 1210/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.5594 - val_loss: 132.8126\n",
      "Epoch 1211/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.9776 - val_loss: 158.8273\n",
      "Epoch 1212/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.1844 - val_loss: 128.3188\n",
      "Epoch 1213/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.0377 - val_loss: 142.8318\n",
      "Epoch 1214/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.0322 - val_loss: 146.5974\n",
      "Epoch 1215/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.0252 - val_loss: 467.6125\n",
      "Epoch 1216/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 210.1587 - val_loss: 140.4505\n",
      "Epoch 1217/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.0266 - val_loss: 170.0876\n",
      "Epoch 1218/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 129.7689 - val_loss: 131.2236\n",
      "Epoch 1219/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.0665 - val_loss: 143.3838\n",
      "Epoch 1220/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.6737 - val_loss: 131.6446\n",
      "Epoch 1221/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.5847 - val_loss: 156.5767\n",
      "Epoch 1222/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.9690 - val_loss: 152.9580\n",
      "Epoch 1223/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.6745 - val_loss: 129.9596\n",
      "Epoch 1224/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.3708 - val_loss: 143.3870\n",
      "Epoch 1225/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.0102 - val_loss: 386.3576\n",
      "Epoch 1226/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 177.8808 - val_loss: 258.0879\n",
      "Epoch 1227/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.0887 - val_loss: 133.8165\n",
      "Epoch 1228/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.8572 - val_loss: 184.0329\n",
      "Epoch 1229/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.4219 - val_loss: 158.0711\n",
      "Epoch 1230/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.8428 - val_loss: 144.3132\n",
      "Epoch 1231/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.3067 - val_loss: 166.2580\n",
      "Epoch 1232/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.3641 - val_loss: 129.3333\n",
      "Epoch 1233/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.8966 - val_loss: 144.0198\n",
      "Epoch 1234/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.7579 - val_loss: 145.6061\n",
      "Epoch 1235/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.5378 - val_loss: 146.1066\n",
      "Epoch 1236/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.8798 - val_loss: 160.3748\n",
      "Epoch 1237/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.7138 - val_loss: 186.6321\n",
      "Epoch 1238/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.3982 - val_loss: 132.6850\n",
      "Epoch 1239/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.6805 - val_loss: 135.7761\n",
      "Epoch 1240/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.9048 - val_loss: 155.7155\n",
      "Epoch 1241/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 208.4820 - val_loss: 788.7408\n",
      "Epoch 1242/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.7102 - val_loss: 132.4899\n",
      "Epoch 1243/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.2337 - val_loss: 128.0016\n",
      "Epoch 1244/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.8190 - val_loss: 146.4198\n",
      "Epoch 1245/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.7023 - val_loss: 168.0980\n",
      "Epoch 1246/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.8045 - val_loss: 181.8796\n",
      "Epoch 1247/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.6409 - val_loss: 186.8494\n",
      "Epoch 1248/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.0308 - val_loss: 129.2389\n",
      "Epoch 1249/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.5128 - val_loss: 130.6609\n",
      "Epoch 1250/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.3116 - val_loss: 177.5290\n",
      "Epoch 1251/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.3006 - val_loss: 128.2423\n",
      "Epoch 1252/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.3896 - val_loss: 175.8553\n",
      "Epoch 1253/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.1931 - val_loss: 138.6940\n",
      "Epoch 1254/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 146.6228 - val_loss: 156.7533\n",
      "Epoch 1255/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.8508 - val_loss: 138.3561\n",
      "Epoch 1256/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.3739 - val_loss: 248.5871\n",
      "Epoch 1257/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.3797 - val_loss: 140.0798\n",
      "Epoch 1258/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.2044 - val_loss: 135.7292\n",
      "Epoch 1259/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.1373 - val_loss: 139.1835\n",
      "Epoch 1260/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.9244 - val_loss: 132.6318\n",
      "Epoch 1261/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.9319 - val_loss: 174.7708\n",
      "Epoch 1262/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.3755 - val_loss: 155.4538\n",
      "Epoch 1263/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.0389 - val_loss: 169.6820\n",
      "Epoch 1264/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 216.2223 - val_loss: 405.2516\n",
      "Epoch 1265/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.9991 - val_loss: 190.3682\n",
      "Epoch 1266/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.8088 - val_loss: 200.6234\n",
      "Epoch 1267/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.3478 - val_loss: 133.1727\n",
      "Epoch 1268/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.3148 - val_loss: 146.0078\n",
      "Epoch 1269/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.9946 - val_loss: 127.5595\n",
      "Epoch 1270/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.0993 - val_loss: 151.6664\n",
      "Epoch 1271/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.4087 - val_loss: 131.5773\n",
      "Epoch 1272/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.7518 - val_loss: 137.5689\n",
      "Epoch 1273/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.2260 - val_loss: 159.8473\n",
      "Epoch 1274/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.8194 - val_loss: 143.3716\n",
      "Epoch 1275/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.1665 - val_loss: 132.0802\n",
      "Epoch 1276/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.0434 - val_loss: 152.3703\n",
      "Epoch 1277/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.3898 - val_loss: 129.8458\n",
      "Epoch 1278/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.3662 - val_loss: 133.1702\n",
      "Epoch 1279/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.0979 - val_loss: 160.6834\n",
      "Epoch 1280/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.7361 - val_loss: 130.0128\n",
      "Epoch 1281/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.8881 - val_loss: 136.0823\n",
      "Epoch 1282/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.8298 - val_loss: 259.0948\n",
      "Epoch 1283/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 136.1711 - val_loss: 156.8290\n",
      "Epoch 1284/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 126.9104 - val_loss: 130.6419\n",
      "Epoch 1285/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.4374 - val_loss: 147.6015\n",
      "Epoch 1286/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 147.7841 - val_loss: 139.7703\n",
      "Epoch 1287/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.3348 - val_loss: 133.6929\n",
      "Epoch 1288/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.9741 - val_loss: 138.6279\n",
      "Epoch 1289/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.4640 - val_loss: 135.0267\n",
      "Epoch 1290/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.5066 - val_loss: 136.1943\n",
      "Epoch 1291/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.3272 - val_loss: 144.8120\n",
      "Epoch 1292/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.1105 - val_loss: 160.8633\n",
      "Epoch 1293/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.1007 - val_loss: 139.1555\n",
      "Epoch 1294/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 156.8968 - val_loss: 156.6735\n",
      "Epoch 1295/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.3273 - val_loss: 126.9013\n",
      "Epoch 1296/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 152.6420 - val_loss: 132.2142\n",
      "Epoch 1297/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.4373 - val_loss: 132.7282\n",
      "Epoch 1298/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 147.7963 - val_loss: 178.9873\n",
      "Epoch 1299/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.2555 - val_loss: 136.4058\n",
      "Epoch 1300/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.9269 - val_loss: 144.0509\n",
      "Epoch 1301/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.6533 - val_loss: 135.6208\n",
      "Epoch 1302/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.7792 - val_loss: 132.2570\n",
      "Epoch 1303/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.0370 - val_loss: 168.7383\n",
      "Epoch 1304/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.0709 - val_loss: 128.7946\n",
      "Epoch 1305/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.6109 - val_loss: 136.4749\n",
      "Epoch 1306/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.2726 - val_loss: 136.9138\n",
      "Epoch 1307/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.1287 - val_loss: 166.7007\n",
      "Epoch 1308/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.0556 - val_loss: 156.8091\n",
      "Epoch 1309/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.6801 - val_loss: 135.8257\n",
      "Epoch 1310/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.1440 - val_loss: 179.1115\n",
      "Epoch 1311/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 139.3676 - val_loss: 128.2692\n",
      "Epoch 1312/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.0842 - val_loss: 146.8239\n",
      "Epoch 1313/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.1386 - val_loss: 132.5655\n",
      "Epoch 1314/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.5843 - val_loss: 128.1944\n",
      "Epoch 1315/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.9148 - val_loss: 138.7029\n",
      "Epoch 1316/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.7279 - val_loss: 140.9905\n",
      "Epoch 1317/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.1101 - val_loss: 138.3598\n",
      "Epoch 1318/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.0876 - val_loss: 163.9811\n",
      "Epoch 1319/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.7444 - val_loss: 135.1569\n",
      "Epoch 1320/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.3487 - val_loss: 193.0545\n",
      "Epoch 1321/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.3415 - val_loss: 182.6098\n",
      "Epoch 1322/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.0275 - val_loss: 145.0520\n",
      "Epoch 1323/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.8340 - val_loss: 190.9657\n",
      "Epoch 1324/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.0806 - val_loss: 130.5419\n",
      "Epoch 1325/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.4800 - val_loss: 246.7719\n",
      "Epoch 1326/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.6707 - val_loss: 129.0428\n",
      "Epoch 1327/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.7253 - val_loss: 162.3035\n",
      "Epoch 1328/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.0564 - val_loss: 139.3517\n",
      "Epoch 1329/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.8188 - val_loss: 133.8034\n",
      "Epoch 1330/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.3015 - val_loss: 248.8545\n",
      "Epoch 1331/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.9242 - val_loss: 190.6910\n",
      "Epoch 1332/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.4910 - val_loss: 136.6038\n",
      "Epoch 1333/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.5757 - val_loss: 173.9499\n",
      "Epoch 1334/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.3549 - val_loss: 135.2224\n",
      "Epoch 1335/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.5662 - val_loss: 145.0558\n",
      "Epoch 1336/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.3503 - val_loss: 127.4095\n",
      "Epoch 1337/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.6632 - val_loss: 140.4573\n",
      "Epoch 1338/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.6006 - val_loss: 157.0978\n",
      "Epoch 1339/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.4357 - val_loss: 127.9944\n",
      "Epoch 1340/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.9184 - val_loss: 138.5143\n",
      "Epoch 1341/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.8281 - val_loss: 128.5319\n",
      "Epoch 1342/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.9501 - val_loss: 133.1660\n",
      "Epoch 1343/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.9019 - val_loss: 150.5250\n",
      "Epoch 1344/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.9613 - val_loss: 132.6887\n",
      "Epoch 1345/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.3731 - val_loss: 167.0014\n",
      "Epoch 1346/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.5476 - val_loss: 138.4395\n",
      "Epoch 1347/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.5556 - val_loss: 133.8294\n",
      "Epoch 1348/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.6522 - val_loss: 137.6643\n",
      "Epoch 1349/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.2947 - val_loss: 146.8722\n",
      "Epoch 1350/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 136.1058 - val_loss: 143.0467\n",
      "Epoch 1351/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.4900 - val_loss: 138.5038\n",
      "Epoch 1352/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 129.9674 - val_loss: 130.6299\n",
      "Epoch 1353/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.6586 - val_loss: 128.3974\n",
      "Epoch 1354/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.7877 - val_loss: 130.7814\n",
      "Epoch 1355/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.9894 - val_loss: 168.9876\n",
      "Epoch 1356/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.9394 - val_loss: 128.6887\n",
      "Epoch 1357/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.1001 - val_loss: 142.7828\n",
      "Epoch 1358/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.2281 - val_loss: 129.5027\n",
      "Epoch 1359/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.4659 - val_loss: 139.4085\n",
      "Epoch 1360/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.0083 - val_loss: 143.2793\n",
      "Epoch 1361/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.7920 - val_loss: 135.6807\n",
      "Epoch 1362/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.9074 - val_loss: 151.9814\n",
      "Epoch 1363/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 135.5159 - val_loss: 134.1749\n",
      "Epoch 1364/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.3557 - val_loss: 143.2695\n",
      "Epoch 1365/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.2043 - val_loss: 164.6500\n",
      "Epoch 1366/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.0445 - val_loss: 136.3508\n",
      "Epoch 1367/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 184.9481 - val_loss: 150.1262\n",
      "Epoch 1368/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.8687 - val_loss: 132.2014\n",
      "Epoch 1369/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.5006 - val_loss: 149.9061\n",
      "Epoch 1370/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 140.0625 - val_loss: 141.1376\n",
      "Epoch 1371/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.2676 - val_loss: 262.1768\n",
      "Epoch 1372/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.1224 - val_loss: 129.0464\n",
      "Epoch 1373/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.1028 - val_loss: 164.2977\n",
      "Epoch 1374/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.4259 - val_loss: 156.1556\n",
      "Epoch 1375/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.2968 - val_loss: 173.9606\n",
      "Epoch 1376/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.6174 - val_loss: 129.9308\n",
      "Epoch 1377/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.3608 - val_loss: 149.1725\n",
      "Epoch 1378/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.7849 - val_loss: 178.2218\n",
      "Epoch 1379/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.7276 - val_loss: 128.7396\n",
      "Epoch 1380/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.5250 - val_loss: 305.7666\n",
      "Epoch 1381/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 251.7476 - val_loss: 148.9637\n",
      "Epoch 1382/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.8631 - val_loss: 128.8205\n",
      "Epoch 1383/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.7357 - val_loss: 141.0238\n",
      "Epoch 1384/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.4103 - val_loss: 140.1566\n",
      "Epoch 1385/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.8980 - val_loss: 128.4718\n",
      "Epoch 1386/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.3119 - val_loss: 161.6492\n",
      "Epoch 1387/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 127.8029 - val_loss: 129.3844\n",
      "Epoch 1388/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.1330 - val_loss: 138.3157\n",
      "Epoch 1389/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.7969 - val_loss: 133.6326\n",
      "Epoch 1390/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.8317 - val_loss: 139.7828\n",
      "Epoch 1391/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.4990 - val_loss: 146.9688\n",
      "Epoch 1392/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.7342 - val_loss: 130.3964\n",
      "Epoch 1393/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.6542 - val_loss: 183.9593\n",
      "Epoch 1394/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 194.2177 - val_loss: 139.4564\n",
      "Epoch 1395/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.7171 - val_loss: 128.7721\n",
      "Epoch 1396/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.3935 - val_loss: 143.5074\n",
      "Epoch 1397/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.5949 - val_loss: 135.7693\n",
      "Epoch 1398/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.7323 - val_loss: 144.6552\n",
      "Epoch 1399/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.9525 - val_loss: 132.3395\n",
      "Epoch 1400/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.8343 - val_loss: 128.8861\n",
      "Epoch 1401/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.2365 - val_loss: 129.1241\n",
      "Epoch 1402/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.9731 - val_loss: 198.3010\n",
      "Epoch 1403/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.5804 - val_loss: 131.9739\n",
      "Epoch 1404/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.9157 - val_loss: 135.7766\n",
      "Epoch 1405/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.8441 - val_loss: 134.4434\n",
      "Epoch 1406/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.8528 - val_loss: 144.0289\n",
      "Epoch 1407/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.9873 - val_loss: 136.5634\n",
      "Epoch 1408/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.9994 - val_loss: 149.1664\n",
      "Epoch 1409/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.5149 - val_loss: 130.8889\n",
      "Epoch 1410/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.0347 - val_loss: 144.1307\n",
      "Epoch 1411/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.5182 - val_loss: 130.8298\n",
      "Epoch 1412/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.3105 - val_loss: 204.2185\n",
      "Epoch 1413/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.0188 - val_loss: 159.5353\n",
      "Epoch 1414/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.2150 - val_loss: 165.7374\n",
      "Epoch 1415/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.2440 - val_loss: 135.3763\n",
      "Epoch 1416/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.8524 - val_loss: 149.1556\n",
      "Epoch 1417/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.1669 - val_loss: 208.5341\n",
      "Epoch 1418/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.9008 - val_loss: 250.9715\n",
      "Epoch 1419/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.5900 - val_loss: 141.4986\n",
      "Epoch 1420/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.1428 - val_loss: 175.5872\n",
      "Epoch 1421/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.8599 - val_loss: 148.7719\n",
      "Epoch 1422/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.5146 - val_loss: 129.8680\n",
      "Epoch 1423/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.1539 - val_loss: 154.5050\n",
      "Epoch 1424/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.5070 - val_loss: 135.5281\n",
      "Epoch 1425/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.4251 - val_loss: 165.2468\n",
      "Epoch 1426/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.8987 - val_loss: 135.2125\n",
      "Epoch 1427/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.8573 - val_loss: 144.9714\n",
      "Epoch 1428/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.3584 - val_loss: 142.0862\n",
      "Epoch 1429/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.3067 - val_loss: 131.7909\n",
      "Epoch 1430/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.3466 - val_loss: 205.5186\n",
      "Epoch 1431/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.2910 - val_loss: 131.3748\n",
      "Epoch 1432/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.4550 - val_loss: 132.5480\n",
      "Epoch 1433/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.1794 - val_loss: 140.2007\n",
      "Epoch 1434/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.8849 - val_loss: 151.5240\n",
      "Epoch 1435/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.1238 - val_loss: 132.0782\n",
      "Epoch 1436/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.5549 - val_loss: 142.6306\n",
      "Epoch 1437/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.0149 - val_loss: 191.0333\n",
      "Epoch 1438/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.1721 - val_loss: 140.1595\n",
      "Epoch 1439/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.5114 - val_loss: 141.4400\n",
      "Epoch 1440/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.4915 - val_loss: 136.0698\n",
      "Epoch 1441/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 131.0111 - val_loss: 139.5633\n",
      "Epoch 1442/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.4459 - val_loss: 163.1716\n",
      "Epoch 1443/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.9330 - val_loss: 128.9623\n",
      "Epoch 1444/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 136.0472 - val_loss: 135.3633\n",
      "Epoch 1445/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.9613 - val_loss: 130.9223\n",
      "Epoch 1446/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 130.2207 - val_loss: 158.5524\n",
      "Epoch 1447/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.3119 - val_loss: 127.7967\n",
      "Epoch 1448/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 268.4758 - val_loss: 144.3330\n",
      "Epoch 1449/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.5071 - val_loss: 142.2366\n",
      "Epoch 1450/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.4144 - val_loss: 130.9849\n",
      "Epoch 1451/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 123.9425 - val_loss: 133.5133\n",
      "Epoch 1452/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.7231 - val_loss: 170.3150\n",
      "Epoch 1453/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.3467 - val_loss: 155.3935\n",
      "Epoch 1454/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.7536 - val_loss: 188.8581\n",
      "Epoch 1455/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.4619 - val_loss: 158.1148\n",
      "Epoch 1456/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.7824 - val_loss: 135.1921\n",
      "Epoch 1457/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.0277 - val_loss: 143.0169\n",
      "Epoch 1458/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.1791 - val_loss: 173.4217\n",
      "Epoch 1459/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.4415 - val_loss: 133.4118\n",
      "Epoch 1460/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.8796 - val_loss: 128.7335\n",
      "Epoch 1461/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.1097 - val_loss: 144.8722\n",
      "Epoch 1462/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.1780 - val_loss: 129.7487\n",
      "Epoch 1463/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.9922 - val_loss: 131.8832\n",
      "Epoch 1464/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.8821 - val_loss: 417.5082\n",
      "Epoch 1465/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.9564 - val_loss: 136.4262\n",
      "Epoch 1466/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.0396 - val_loss: 190.4266\n",
      "Epoch 1467/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.5448 - val_loss: 130.9628\n",
      "Epoch 1468/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.4557 - val_loss: 135.7472\n",
      "Epoch 1469/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.8490 - val_loss: 135.1936\n",
      "Epoch 1470/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.8015 - val_loss: 132.6646\n",
      "Epoch 1471/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.0291 - val_loss: 130.0738\n",
      "Epoch 1472/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.7909 - val_loss: 149.7924\n",
      "Epoch 1473/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.3941 - val_loss: 132.6162\n",
      "Epoch 1474/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.9935 - val_loss: 175.6201\n",
      "Epoch 1475/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.8276 - val_loss: 188.0626\n",
      "Epoch 1476/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.6196 - val_loss: 133.6297\n",
      "Epoch 1477/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.8747 - val_loss: 132.2637\n",
      "Epoch 1478/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.9599 - val_loss: 142.1393\n",
      "Epoch 1479/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.9891 - val_loss: 142.9501\n",
      "Epoch 1480/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.8905 - val_loss: 140.5341\n",
      "Epoch 1481/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.5746 - val_loss: 137.1929\n",
      "Epoch 1482/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.2988 - val_loss: 186.4876\n",
      "Epoch 1483/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 151.1101 - val_loss: 254.2354\n",
      "Epoch 1484/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 133.6533 - val_loss: 144.6885\n",
      "Epoch 1485/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.2938 - val_loss: 130.6864\n",
      "Epoch 1486/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.5871 - val_loss: 129.0339\n",
      "Epoch 1487/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.9932 - val_loss: 144.0482\n",
      "Epoch 1488/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.3506 - val_loss: 135.5552\n",
      "Epoch 1489/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.2216 - val_loss: 167.0885\n",
      "Epoch 1490/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.8090 - val_loss: 137.2799\n",
      "Epoch 1491/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.8888 - val_loss: 149.7375\n",
      "Epoch 1492/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.8792 - val_loss: 144.0655\n",
      "Epoch 1493/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.2104 - val_loss: 130.7916\n",
      "Epoch 1494/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.7809 - val_loss: 169.1668\n",
      "Epoch 1495/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.1400 - val_loss: 132.9181\n",
      "Epoch 1496/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.7066 - val_loss: 140.1175\n",
      "Epoch 1497/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.8546 - val_loss: 129.0018\n",
      "Epoch 1498/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.6894 - val_loss: 130.4246\n",
      "Epoch 1499/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.9111 - val_loss: 130.5423\n",
      "Epoch 1500/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.0257 - val_loss: 158.8715\n",
      "Epoch 1501/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.1490 - val_loss: 158.5915\n",
      "Epoch 1502/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 132.7768 - val_loss: 132.0671\n",
      "Epoch 1503/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.2431 - val_loss: 143.7419\n",
      "Epoch 1504/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 176.1063 - val_loss: 148.1242\n",
      "Epoch 1505/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.4274 - val_loss: 153.3114\n",
      "Epoch 1506/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.7913 - val_loss: 185.8335\n",
      "Epoch 1507/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.7307 - val_loss: 128.3131\n",
      "Epoch 1508/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.5311 - val_loss: 152.8277\n",
      "Epoch 1509/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.2211 - val_loss: 131.2734\n",
      "Epoch 1510/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.1292 - val_loss: 166.7849\n",
      "Epoch 1511/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.8846 - val_loss: 143.3960\n",
      "Epoch 1512/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.3906 - val_loss: 152.1448\n",
      "Epoch 1513/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.3658 - val_loss: 137.6146\n",
      "Epoch 1514/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.2460 - val_loss: 135.4010\n",
      "Epoch 1515/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.5657 - val_loss: 140.0411\n",
      "Epoch 1516/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.8733 - val_loss: 132.7136\n",
      "Epoch 1517/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.5212 - val_loss: 239.4397\n",
      "Epoch 1518/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.3091 - val_loss: 134.7616\n",
      "Epoch 1519/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 139.2647 - val_loss: 146.9666\n",
      "Epoch 1520/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 132.7411 - val_loss: 145.5062\n",
      "Epoch 1521/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 129.3493 - val_loss: 133.4650\n",
      "Epoch 1522/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.9741 - val_loss: 130.7950\n",
      "Epoch 1523/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.7871 - val_loss: 137.1142\n",
      "Epoch 1524/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.9636 - val_loss: 189.4177\n",
      "Epoch 1525/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 193.8672 - val_loss: 148.1028\n",
      "Epoch 1526/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.1345 - val_loss: 149.7716\n",
      "Epoch 1527/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.7630 - val_loss: 145.0299\n",
      "Epoch 1528/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.3582 - val_loss: 155.2867\n",
      "Epoch 1529/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.4376 - val_loss: 145.1719\n",
      "Epoch 1530/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.0082 - val_loss: 151.0297\n",
      "Epoch 1531/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.6684 - val_loss: 141.4554\n",
      "Epoch 1532/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.7431 - val_loss: 131.8958\n",
      "Epoch 1533/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.2618 - val_loss: 168.8747\n",
      "Epoch 1534/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.6414 - val_loss: 135.0517\n",
      "Epoch 1535/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.6274 - val_loss: 159.1906\n",
      "Epoch 1536/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.7803 - val_loss: 142.4564\n",
      "Epoch 1537/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.2567 - val_loss: 135.3948\n",
      "Epoch 1538/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.3214 - val_loss: 140.6543\n",
      "Epoch 1539/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.4554 - val_loss: 133.4346\n",
      "Epoch 1540/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.6580 - val_loss: 153.4604\n",
      "Epoch 1541/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.2359 - val_loss: 141.8489\n",
      "Epoch 1542/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.7430 - val_loss: 132.0610\n",
      "Epoch 1543/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.8468 - val_loss: 142.6449\n",
      "Epoch 1544/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.3735 - val_loss: 131.2545\n",
      "Epoch 1545/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.2471 - val_loss: 132.7456\n",
      "Epoch 1546/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.4352 - val_loss: 129.4734\n",
      "Epoch 1547/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.4635 - val_loss: 137.1194\n",
      "Epoch 1548/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.9253 - val_loss: 147.4120\n",
      "Epoch 1549/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.6750 - val_loss: 135.8281\n",
      "Epoch 1550/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.1842 - val_loss: 173.2279\n",
      "Epoch 1551/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.1243 - val_loss: 169.4995\n",
      "Epoch 1552/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.0042 - val_loss: 134.3082\n",
      "Epoch 1553/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 132.2629 - val_loss: 143.3304\n",
      "Epoch 1554/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.4051 - val_loss: 145.3479\n",
      "Epoch 1555/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 213.3704 - val_loss: 139.9342\n",
      "Epoch 1556/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.0518 - val_loss: 143.2858\n",
      "Epoch 1557/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.2759 - val_loss: 132.9826\n",
      "Epoch 1558/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.8367 - val_loss: 159.6981\n",
      "Epoch 1559/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.8446 - val_loss: 129.2253\n",
      "Epoch 1560/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.9580 - val_loss: 130.4809\n",
      "Epoch 1561/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.6664 - val_loss: 135.9683\n",
      "Epoch 1562/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.8798 - val_loss: 143.5472\n",
      "Epoch 1563/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.6147 - val_loss: 155.3958\n",
      "Epoch 1564/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.8655 - val_loss: 137.9085\n",
      "Epoch 1565/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.6501 - val_loss: 132.6877\n",
      "Epoch 1566/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.4618 - val_loss: 151.5553\n",
      "Epoch 1567/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.2030 - val_loss: 131.5437\n",
      "Epoch 1568/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.9540 - val_loss: 133.6407\n",
      "Epoch 1569/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 320.1788 - val_loss: 147.1013\n",
      "Epoch 1570/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.9820 - val_loss: 136.3496\n",
      "Epoch 1571/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.2196 - val_loss: 135.6038\n",
      "Epoch 1572/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 129.1574 - val_loss: 138.1859\n",
      "Epoch 1573/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.2463 - val_loss: 136.3914\n",
      "Epoch 1574/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.8597 - val_loss: 131.1608\n",
      "Epoch 1575/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.6817 - val_loss: 131.8718\n",
      "Epoch 1576/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.4091 - val_loss: 182.1019\n",
      "Epoch 1577/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.8809 - val_loss: 138.1257\n",
      "Epoch 1578/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.0492 - val_loss: 159.4619\n",
      "Epoch 1579/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.9224 - val_loss: 141.1165\n",
      "Epoch 1580/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.8451 - val_loss: 140.3300\n",
      "Epoch 1581/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.0931 - val_loss: 153.4636\n",
      "Epoch 1582/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.4470 - val_loss: 145.1158\n",
      "Epoch 1583/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.0784 - val_loss: 139.4163\n",
      "Epoch 1584/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 158.7137 - val_loss: 134.5456\n",
      "Epoch 1585/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.8041 - val_loss: 138.1095\n",
      "Epoch 1586/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.7396 - val_loss: 139.6930\n",
      "Epoch 1587/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.9883 - val_loss: 136.7581\n",
      "Epoch 1588/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.5147 - val_loss: 178.1775\n",
      "Epoch 1589/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.8028 - val_loss: 178.4141\n",
      "Epoch 1590/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.2918 - val_loss: 134.4314\n",
      "Epoch 1591/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.7805 - val_loss: 136.4209\n",
      "Epoch 1592/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.0302 - val_loss: 131.3772\n",
      "Epoch 1593/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.4894 - val_loss: 139.1238\n",
      "Epoch 1594/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.1843 - val_loss: 139.4600\n",
      "Epoch 1595/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.3932 - val_loss: 140.2601\n",
      "Epoch 1596/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.6085 - val_loss: 129.2572\n",
      "Epoch 1597/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 299.3714 - val_loss: 210.7649\n",
      "Epoch 1598/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 140.0734 - val_loss: 174.6602\n",
      "Epoch 1599/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 128.6750 - val_loss: 149.7344\n",
      "Epoch 1600/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.6120 - val_loss: 150.7851\n",
      "Epoch 1601/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.1931 - val_loss: 138.6105\n",
      "Epoch 1602/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.2148 - val_loss: 148.8897\n",
      "Epoch 1603/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.0113 - val_loss: 137.3104\n",
      "Epoch 1604/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.6928 - val_loss: 132.8294\n",
      "Epoch 1605/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.5984 - val_loss: 133.1961\n",
      "Epoch 1606/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.9464 - val_loss: 145.7947\n",
      "Epoch 1607/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.1989 - val_loss: 135.0215\n",
      "Epoch 1608/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.2776 - val_loss: 133.6552\n",
      "Epoch 1609/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.9020 - val_loss: 166.6404\n",
      "Epoch 1610/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.1047 - val_loss: 160.5145\n",
      "Epoch 1611/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.8754 - val_loss: 162.3774\n",
      "Epoch 1612/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.9221 - val_loss: 140.9959\n",
      "Epoch 1613/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 142.2829 - val_loss: 138.3646\n",
      "Epoch 1614/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.2274 - val_loss: 143.7457\n",
      "Epoch 1615/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.8120 - val_loss: 138.8821\n",
      "Epoch 1616/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.2071 - val_loss: 133.3518\n",
      "Epoch 1617/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.2178 - val_loss: 189.4830\n",
      "Epoch 1618/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.8678 - val_loss: 137.8667\n",
      "Epoch 1619/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.3828 - val_loss: 132.0778\n",
      "Epoch 1620/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.2935 - val_loss: 162.7389\n",
      "Epoch 1621/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.5496 - val_loss: 130.7839\n",
      "Epoch 1622/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.2285 - val_loss: 157.4944\n",
      "Epoch 1623/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 151.3646 - val_loss: 143.9457\n",
      "Epoch 1624/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.2247 - val_loss: 145.4586\n",
      "Epoch 1625/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.6120 - val_loss: 130.9172\n",
      "Epoch 1626/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.5352 - val_loss: 179.5330\n",
      "Epoch 1627/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.4441 - val_loss: 137.8976\n",
      "Epoch 1628/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.4601 - val_loss: 149.3904\n",
      "Epoch 1629/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.6668 - val_loss: 245.2279\n",
      "Epoch 1630/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 167.7780 - val_loss: 143.1687\n",
      "Epoch 1631/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.3250 - val_loss: 131.2006\n",
      "Epoch 1632/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.8756 - val_loss: 138.8638\n",
      "Epoch 1633/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.9891 - val_loss: 135.6124\n",
      "Epoch 1634/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.3609 - val_loss: 149.4206\n",
      "Epoch 1635/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.1500 - val_loss: 139.1078\n",
      "Epoch 1636/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.8770 - val_loss: 144.7484\n",
      "Epoch 1637/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.4491 - val_loss: 148.6085\n",
      "Epoch 1638/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.8951 - val_loss: 194.8007\n",
      "Epoch 1639/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.2187 - val_loss: 141.0587\n",
      "Epoch 1640/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.8456 - val_loss: 153.1982\n",
      "Epoch 1641/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.8876 - val_loss: 167.2474\n",
      "Epoch 1642/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.0069 - val_loss: 142.8811\n",
      "Epoch 1643/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.4556 - val_loss: 134.9736\n",
      "Epoch 1644/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.6455 - val_loss: 140.0719\n",
      "Epoch 1645/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.1944 - val_loss: 131.8346\n",
      "Epoch 1646/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.8615 - val_loss: 158.4617\n",
      "Epoch 1647/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.6229 - val_loss: 136.3584\n",
      "Epoch 1648/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.8368 - val_loss: 133.7975\n",
      "Epoch 1649/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.6842 - val_loss: 149.0880\n",
      "Epoch 1650/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.9644 - val_loss: 158.5036\n",
      "Epoch 1651/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.8992 - val_loss: 194.1229\n",
      "Epoch 1652/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.5053 - val_loss: 130.9904\n",
      "Epoch 1653/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.7509 - val_loss: 131.2130\n",
      "Epoch 1654/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.1836 - val_loss: 220.3272\n",
      "Epoch 1655/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.3690 - val_loss: 182.8852\n",
      "Epoch 1656/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.9336 - val_loss: 193.1181\n",
      "Epoch 1657/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.7194 - val_loss: 128.5971\n",
      "Epoch 1658/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.7731 - val_loss: 143.5915\n",
      "Epoch 1659/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.3221 - val_loss: 133.8913\n",
      "Epoch 1660/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.1508 - val_loss: 133.3841\n",
      "Epoch 1661/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.2261 - val_loss: 140.5041\n",
      "Epoch 1662/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.8884 - val_loss: 148.5373\n",
      "Epoch 1663/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.8363 - val_loss: 136.6642\n",
      "Epoch 1664/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.4058 - val_loss: 132.4440\n",
      "Epoch 1665/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.8684 - val_loss: 130.0796\n",
      "Epoch 1666/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.7125 - val_loss: 129.3578\n",
      "Epoch 1667/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.8094 - val_loss: 140.5756\n",
      "Epoch 1668/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.8178 - val_loss: 158.5661\n",
      "Epoch 1669/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.5265 - val_loss: 137.8521\n",
      "Epoch 1670/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.5367 - val_loss: 134.7194\n",
      "Epoch 1671/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.9359 - val_loss: 162.1729\n",
      "Epoch 1672/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.3416 - val_loss: 189.2517\n",
      "Epoch 1673/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 127.4250 - val_loss: 138.2878\n",
      "Epoch 1674/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 126.7996 - val_loss: 156.3838\n",
      "Epoch 1675/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 132.2247 - val_loss: 165.7442\n",
      "Epoch 1676/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 126.4452 - val_loss: 140.7039\n",
      "Epoch 1677/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.3634 - val_loss: 135.1339\n",
      "Epoch 1678/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.7512 - val_loss: 156.4239\n",
      "Epoch 1679/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.6840 - val_loss: 149.7078\n",
      "Epoch 1680/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.6620 - val_loss: 142.3223\n",
      "Epoch 1681/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.1074 - val_loss: 153.4038\n",
      "Epoch 1682/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.5209 - val_loss: 147.3993\n",
      "Epoch 1683/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.5720 - val_loss: 127.1930\n",
      "Epoch 1684/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.8336 - val_loss: 131.1255\n",
      "Epoch 1685/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.9787 - val_loss: 133.1513\n",
      "Epoch 1686/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.7715 - val_loss: 130.4752\n",
      "Epoch 1687/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.4977 - val_loss: 165.2403\n",
      "Epoch 1688/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.3294 - val_loss: 133.7904\n",
      "Epoch 1689/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.4628 - val_loss: 139.6952\n",
      "Epoch 1690/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.3213 - val_loss: 129.3653\n",
      "Epoch 1691/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.6213 - val_loss: 139.4876\n",
      "Epoch 1692/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.1305 - val_loss: 153.4183\n",
      "Epoch 1693/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.6487 - val_loss: 204.9544\n",
      "Epoch 1694/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.5328 - val_loss: 205.8559\n",
      "Epoch 1695/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.5914 - val_loss: 133.1018\n",
      "Epoch 1696/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.1973 - val_loss: 142.6687\n",
      "Epoch 1697/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.3846 - val_loss: 136.7553\n",
      "Epoch 1698/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.2189 - val_loss: 153.8449\n",
      "Epoch 1699/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.9974 - val_loss: 137.3516\n",
      "Epoch 1700/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.8942 - val_loss: 134.9713\n",
      "Epoch 1701/10000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 121.8051 - val_loss: 197.1484\n",
      "Epoch 1702/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.5315 - val_loss: 140.3895\n",
      "Epoch 1703/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 197.0201 - val_loss: 148.9206\n",
      "Epoch 1704/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.7703 - val_loss: 132.2550\n",
      "Epoch 1705/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.8947 - val_loss: 132.5327\n",
      "Epoch 1706/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.1629 - val_loss: 135.9844\n",
      "Epoch 1707/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.4180 - val_loss: 192.2807\n",
      "Epoch 1708/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.6682 - val_loss: 135.8725\n",
      "Epoch 1709/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.8940 - val_loss: 138.4505\n",
      "Epoch 1710/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.5554 - val_loss: 144.1998\n",
      "Epoch 1711/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.0547 - val_loss: 132.5520\n",
      "Epoch 1712/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.8007 - val_loss: 137.2598\n",
      "Epoch 1713/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.2249 - val_loss: 136.8967\n",
      "Epoch 1714/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.3365 - val_loss: 247.6308\n",
      "Epoch 1715/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.7551 - val_loss: 141.1571\n",
      "Epoch 1716/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.2723 - val_loss: 153.7570\n",
      "Epoch 1717/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.9088 - val_loss: 370.8536\n",
      "Epoch 1718/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.2327 - val_loss: 130.7300\n",
      "Epoch 1719/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.4716 - val_loss: 130.5693\n",
      "Epoch 1720/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.7313 - val_loss: 133.0502\n",
      "Epoch 1721/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.6190 - val_loss: 144.7382\n",
      "Epoch 1722/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.2396 - val_loss: 155.1724\n",
      "Epoch 1723/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.2411 - val_loss: 133.8947\n",
      "Epoch 1724/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.3061 - val_loss: 135.3416\n",
      "Epoch 1725/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.4884 - val_loss: 130.6887\n",
      "Epoch 1726/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.1613 - val_loss: 135.9666\n",
      "Epoch 1727/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.1397 - val_loss: 185.5809\n",
      "Epoch 1728/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.4296 - val_loss: 136.6027\n",
      "Epoch 1729/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.0698 - val_loss: 134.1969\n",
      "Epoch 1730/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.3592 - val_loss: 142.4440\n",
      "Epoch 1731/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 127.3356 - val_loss: 126.3187\n",
      "Epoch 1732/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.9847 - val_loss: 141.1210\n",
      "Epoch 1733/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.7976 - val_loss: 155.1574\n",
      "Epoch 1734/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.1402 - val_loss: 133.0224\n",
      "Epoch 1735/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.8372 - val_loss: 130.6849\n",
      "Epoch 1736/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.8776 - val_loss: 171.8656\n",
      "Epoch 1737/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.5673 - val_loss: 150.6186\n",
      "Epoch 1738/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.1571 - val_loss: 130.8125\n",
      "Epoch 1739/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.7581 - val_loss: 130.7404\n",
      "Epoch 1740/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.0254 - val_loss: 205.6998\n",
      "Epoch 1741/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 151.1559 - val_loss: 163.6741\n",
      "Epoch 1742/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.9368 - val_loss: 128.4514\n",
      "Epoch 1743/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.2334 - val_loss: 127.9330\n",
      "Epoch 1744/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.9122 - val_loss: 158.5510\n",
      "Epoch 1745/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.6808 - val_loss: 177.5512\n",
      "Epoch 1746/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.7251 - val_loss: 141.9500\n",
      "Epoch 1747/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.6613 - val_loss: 139.4366\n",
      "Epoch 1748/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.6134 - val_loss: 130.7678\n",
      "Epoch 1749/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.4841 - val_loss: 156.6031\n",
      "Epoch 1750/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.8624 - val_loss: 131.2420\n",
      "Epoch 1751/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.2650 - val_loss: 144.9835\n",
      "Epoch 1752/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 121.8323 - val_loss: 136.3806\n",
      "Epoch 1753/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 128.0576 - val_loss: 180.3583\n",
      "Epoch 1754/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 139.1085 - val_loss: 130.9943\n",
      "Epoch 1755/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.0912 - val_loss: 136.5033\n",
      "Epoch 1756/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.6607 - val_loss: 130.9426\n",
      "Epoch 1757/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.9993 - val_loss: 132.5978\n",
      "Epoch 1758/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.0650 - val_loss: 154.6258\n",
      "Epoch 1759/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.6344 - val_loss: 149.2244\n",
      "Epoch 1760/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.2977 - val_loss: 155.8044\n",
      "Epoch 1761/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.3563 - val_loss: 197.6024\n",
      "Epoch 1762/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.5759 - val_loss: 195.8461\n",
      "Epoch 1763/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.5607 - val_loss: 133.9486\n",
      "Epoch 1764/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.5415 - val_loss: 132.5156\n",
      "Epoch 1765/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.7753 - val_loss: 142.7373\n",
      "Epoch 1766/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.9498 - val_loss: 129.9603\n",
      "Epoch 1767/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.1868 - val_loss: 134.7237\n",
      "Epoch 1768/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.8403 - val_loss: 142.9315\n",
      "Epoch 1769/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.7342 - val_loss: 136.7871\n",
      "Epoch 1770/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.4756 - val_loss: 151.4567\n",
      "Epoch 1771/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 191.5170 - val_loss: 138.2940\n",
      "Epoch 1772/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.2637 - val_loss: 133.0559\n",
      "Epoch 1773/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.2897 - val_loss: 125.8766\n",
      "Epoch 1774/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.4839 - val_loss: 135.6160\n",
      "Epoch 1775/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.6537 - val_loss: 136.7550\n",
      "Epoch 1776/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.5737 - val_loss: 194.5150\n",
      "Epoch 1777/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.4791 - val_loss: 132.9592\n",
      "Epoch 1778/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.2763 - val_loss: 129.9914\n",
      "Epoch 1779/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.5226 - val_loss: 137.8908\n",
      "Epoch 1780/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.5836 - val_loss: 186.3335\n",
      "Epoch 1781/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.6810 - val_loss: 149.3855\n",
      "Epoch 1782/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.1624 - val_loss: 131.4934\n",
      "Epoch 1783/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.5335 - val_loss: 129.9862\n",
      "Epoch 1784/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.9960 - val_loss: 157.2053\n",
      "Epoch 1785/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.8925 - val_loss: 130.7941\n",
      "Epoch 1786/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.6193 - val_loss: 144.6923\n",
      "Epoch 1787/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.1307 - val_loss: 135.8125\n",
      "Epoch 1788/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.0546 - val_loss: 132.3608\n",
      "Epoch 1789/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.2783 - val_loss: 162.7930\n",
      "Epoch 1790/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.9572 - val_loss: 129.6450\n",
      "Epoch 1791/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.7525 - val_loss: 145.4930\n",
      "Epoch 1792/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.0438 - val_loss: 139.1646\n",
      "Epoch 1793/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 126.9422 - val_loss: 129.8170\n",
      "Epoch 1794/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.4584 - val_loss: 141.1210\n",
      "Epoch 1795/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.0486 - val_loss: 185.4294\n",
      "Epoch 1796/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.1020 - val_loss: 141.5110\n",
      "Epoch 1797/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.7264 - val_loss: 179.6530\n",
      "Epoch 1798/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.8555 - val_loss: 133.3691\n",
      "Epoch 1799/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.5162 - val_loss: 170.6628\n",
      "Epoch 1800/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.2553 - val_loss: 168.1148\n",
      "Epoch 1801/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.4549 - val_loss: 137.6026\n",
      "Epoch 1802/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.1273 - val_loss: 146.4242\n",
      "Epoch 1803/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.5464 - val_loss: 127.5680\n",
      "Epoch 1804/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.2696 - val_loss: 140.1962\n",
      "Epoch 1805/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.6268 - val_loss: 131.5073\n",
      "Epoch 1806/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.0530 - val_loss: 140.4239\n",
      "Epoch 1807/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.5313 - val_loss: 149.5290\n",
      "Epoch 1808/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.3485 - val_loss: 141.1935\n",
      "Epoch 1809/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.0534 - val_loss: 146.3060\n",
      "Epoch 1810/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.5831 - val_loss: 145.6872\n",
      "Epoch 1811/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.0905 - val_loss: 162.8126\n",
      "Epoch 1812/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.5357 - val_loss: 143.7081\n",
      "Epoch 1813/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.3495 - val_loss: 171.5280\n",
      "Epoch 1814/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 142.5313 - val_loss: 135.4934\n",
      "Epoch 1815/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.3724 - val_loss: 132.5900\n",
      "Epoch 1816/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.7327 - val_loss: 133.9788\n",
      "Epoch 1817/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.1904 - val_loss: 142.3145\n",
      "Epoch 1818/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.4629 - val_loss: 128.1965\n",
      "Epoch 1819/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.7334 - val_loss: 133.3737\n",
      "Epoch 1820/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.5491 - val_loss: 158.4809\n",
      "Epoch 1821/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.9640 - val_loss: 134.4284\n",
      "Epoch 1822/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.8711 - val_loss: 143.3034\n",
      "Epoch 1823/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.9132 - val_loss: 133.2612\n",
      "Epoch 1824/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.1370 - val_loss: 146.0402\n",
      "Epoch 1825/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.8654 - val_loss: 130.6273\n",
      "Epoch 1826/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.6768 - val_loss: 148.6943\n",
      "Epoch 1827/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.2529 - val_loss: 219.4951\n",
      "Epoch 1828/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.7535 - val_loss: 139.7300\n",
      "Epoch 1829/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.9153 - val_loss: 135.3052\n",
      "Epoch 1830/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 134.2657 - val_loss: 130.1135\n",
      "Epoch 1831/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.7881 - val_loss: 146.0119\n",
      "Epoch 1832/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.8816 - val_loss: 138.7629\n",
      "Epoch 1833/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.7177 - val_loss: 149.3270\n",
      "Epoch 1834/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.1419 - val_loss: 198.2181\n",
      "Epoch 1835/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.5353 - val_loss: 134.9789\n",
      "Epoch 1836/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.7790 - val_loss: 142.9100\n",
      "Epoch 1837/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.6676 - val_loss: 134.6051\n",
      "Epoch 1838/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.0342 - val_loss: 134.8600\n",
      "Epoch 1839/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.9736 - val_loss: 166.9090\n",
      "Epoch 1840/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.6925 - val_loss: 132.1589\n",
      "Epoch 1841/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.9642 - val_loss: 128.8065\n",
      "Epoch 1842/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.5727 - val_loss: 144.4830\n",
      "Epoch 1843/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.7581 - val_loss: 135.7596\n",
      "Epoch 1844/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.1887 - val_loss: 131.9543\n",
      "Epoch 1845/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.5837 - val_loss: 156.4108\n",
      "Epoch 1846/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 158.7208 - val_loss: 140.2048\n",
      "Epoch 1847/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.1365 - val_loss: 133.5104\n",
      "Epoch 1848/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.8983 - val_loss: 134.7294\n",
      "Epoch 1849/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.5640 - val_loss: 136.9232\n",
      "Epoch 1850/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.4769 - val_loss: 135.6842\n",
      "Epoch 1851/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.6266 - val_loss: 130.8622\n",
      "Epoch 1852/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.8543 - val_loss: 140.3481\n",
      "Epoch 1853/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.8937 - val_loss: 164.0568\n",
      "Epoch 1854/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.5473 - val_loss: 137.7447\n",
      "Epoch 1855/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.8922 - val_loss: 134.6288\n",
      "Epoch 1856/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.0375 - val_loss: 139.4070\n",
      "Epoch 1857/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.7955 - val_loss: 132.1542\n",
      "Epoch 1858/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.2603 - val_loss: 132.5299\n",
      "Epoch 1859/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 182.6691 - val_loss: 130.6932\n",
      "Epoch 1860/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.9837 - val_loss: 174.4048\n",
      "Epoch 1861/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.9514 - val_loss: 136.9380\n",
      "Epoch 1862/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.4456 - val_loss: 140.8091\n",
      "Epoch 1863/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.3264 - val_loss: 185.4506\n",
      "Epoch 1864/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.7206 - val_loss: 162.6291\n",
      "Epoch 1865/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.6772 - val_loss: 209.1155\n",
      "Epoch 1866/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.6060 - val_loss: 155.8693\n",
      "Epoch 1867/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.6148 - val_loss: 136.6932\n",
      "Epoch 1868/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.5375 - val_loss: 139.4286\n",
      "Epoch 1869/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.9258 - val_loss: 128.7207\n",
      "Epoch 1870/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.7421 - val_loss: 167.1902\n",
      "Epoch 1871/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.6216 - val_loss: 128.9579\n",
      "Epoch 1872/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.4088 - val_loss: 139.4922\n",
      "Epoch 1873/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.6509 - val_loss: 135.9952\n",
      "Epoch 1874/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 213.3457 - val_loss: 138.7020\n",
      "Epoch 1875/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.2622 - val_loss: 130.1876\n",
      "Epoch 1876/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.4663 - val_loss: 139.0444\n",
      "Epoch 1877/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.3195 - val_loss: 132.3950\n",
      "Epoch 1878/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.6191 - val_loss: 127.1697\n",
      "Epoch 1879/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.7601 - val_loss: 138.9714\n",
      "Epoch 1880/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.3727 - val_loss: 150.9211\n",
      "Epoch 1881/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.5997 - val_loss: 150.1045\n",
      "Epoch 1882/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.6590 - val_loss: 140.4121\n",
      "Epoch 1883/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.4711 - val_loss: 132.4014\n",
      "Epoch 1884/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 127.0726 - val_loss: 142.5450\n",
      "Epoch 1885/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.6065 - val_loss: 138.9377\n",
      "Epoch 1886/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.6006 - val_loss: 133.9839\n",
      "Epoch 1887/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.2343 - val_loss: 151.7209\n",
      "Epoch 1888/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.6363 - val_loss: 140.2878\n",
      "Epoch 1889/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.5192 - val_loss: 132.7943\n",
      "Epoch 1890/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.2841 - val_loss: 136.3873\n",
      "Epoch 1891/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.0942 - val_loss: 146.8130\n",
      "Epoch 1892/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.6538 - val_loss: 135.2186\n",
      "Epoch 1893/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.0929 - val_loss: 138.6116\n",
      "Epoch 1894/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.4516 - val_loss: 129.3449\n",
      "Epoch 1895/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.7794 - val_loss: 133.1736\n",
      "Epoch 1896/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.5100 - val_loss: 147.0523\n",
      "Epoch 1897/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.0120 - val_loss: 152.9798\n",
      "Epoch 1898/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.2884 - val_loss: 162.0677\n",
      "Epoch 1899/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.9369 - val_loss: 153.3606\n",
      "Epoch 1900/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 154.0491 - val_loss: 134.6688\n",
      "Epoch 1901/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.8054 - val_loss: 185.5602\n",
      "Epoch 1902/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.6511 - val_loss: 163.2390\n",
      "Epoch 1903/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.8864 - val_loss: 137.2087\n",
      "Epoch 1904/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 171.0557 - val_loss: 148.5333\n",
      "Epoch 1905/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.4513 - val_loss: 130.2436\n",
      "Epoch 1906/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.5888 - val_loss: 139.4471\n",
      "Epoch 1907/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.2270 - val_loss: 151.2079\n",
      "Epoch 1908/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.0454 - val_loss: 137.7822\n",
      "Epoch 1909/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 123.5806 - val_loss: 136.6470\n",
      "Epoch 1910/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 269.2335 - val_loss: 133.4475\n",
      "Epoch 1911/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 119.9529 - val_loss: 138.8303\n",
      "Epoch 1912/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.6085 - val_loss: 135.4167\n",
      "Epoch 1913/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.7415 - val_loss: 138.1443\n",
      "Epoch 1914/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.2473 - val_loss: 140.5934\n",
      "Epoch 1915/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.0511 - val_loss: 132.5866\n",
      "Epoch 1916/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.2194 - val_loss: 145.7387\n",
      "Epoch 1917/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.8191 - val_loss: 131.0038\n",
      "Epoch 1918/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.7027 - val_loss: 139.9450\n",
      "Epoch 1919/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.4692 - val_loss: 130.8824\n",
      "Epoch 1920/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.0558 - val_loss: 162.0342\n",
      "Epoch 1921/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.7255 - val_loss: 143.2162\n",
      "Epoch 1922/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.0800 - val_loss: 146.1803\n",
      "Epoch 1923/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.7228 - val_loss: 148.7179\n",
      "Epoch 1924/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.9921 - val_loss: 128.5157\n",
      "Epoch 1925/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.1520 - val_loss: 149.1631\n",
      "Epoch 1926/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.4369 - val_loss: 153.3495\n",
      "Epoch 1927/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.8155 - val_loss: 140.3949\n",
      "Epoch 1928/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.6954 - val_loss: 129.7114\n",
      "Epoch 1929/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.5127 - val_loss: 131.2227\n",
      "Epoch 1930/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.6360 - val_loss: 175.7397\n",
      "Epoch 1931/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.5514 - val_loss: 159.1727\n",
      "Epoch 1932/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.6510 - val_loss: 152.5896\n",
      "Epoch 1933/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.5999 - val_loss: 137.2126\n",
      "Epoch 1934/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.1072 - val_loss: 176.1171\n",
      "Epoch 1935/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.1146 - val_loss: 129.3847\n",
      "Epoch 1936/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.6696 - val_loss: 374.3664\n",
      "Epoch 1937/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.7331 - val_loss: 160.8958\n",
      "Epoch 1938/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.2155 - val_loss: 181.0516\n",
      "Epoch 1939/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.6373 - val_loss: 174.4703\n",
      "Epoch 1940/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.9318 - val_loss: 137.2248\n",
      "Epoch 1941/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.4304 - val_loss: 150.2457\n",
      "Epoch 1942/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.4660 - val_loss: 175.5826\n",
      "Epoch 1943/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.3644 - val_loss: 140.5422\n",
      "Epoch 1944/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.4870 - val_loss: 131.7116\n",
      "Epoch 1945/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.0768 - val_loss: 126.1297\n",
      "Epoch 1946/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.0117 - val_loss: 146.1844\n",
      "Epoch 1947/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.2335 - val_loss: 154.7131\n",
      "Epoch 1948/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.0098 - val_loss: 130.1761\n",
      "Epoch 1949/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.3959 - val_loss: 173.4849\n",
      "Epoch 1950/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.7318 - val_loss: 147.7281\n",
      "Epoch 1951/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.1627 - val_loss: 176.1555\n",
      "Epoch 1952/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.2697 - val_loss: 240.4418\n",
      "Epoch 1953/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.4408 - val_loss: 201.2932\n",
      "Epoch 1954/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.9774 - val_loss: 135.8581\n",
      "Epoch 1955/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.5341 - val_loss: 140.4038\n",
      "Epoch 1956/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.0658 - val_loss: 130.0085\n",
      "Epoch 1957/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.8603 - val_loss: 196.1243\n",
      "Epoch 1958/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.2486 - val_loss: 128.9724\n",
      "Epoch 1959/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.9169 - val_loss: 153.0740\n",
      "Epoch 1960/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.6056 - val_loss: 133.2637\n",
      "Epoch 1961/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.8576 - val_loss: 128.9374\n",
      "Epoch 1962/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.5179 - val_loss: 229.9497\n",
      "Epoch 1963/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.5931 - val_loss: 142.9379\n",
      "Epoch 1964/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.0055 - val_loss: 129.3854\n",
      "Epoch 1965/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.4578 - val_loss: 150.5376\n",
      "Epoch 1966/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.2540 - val_loss: 130.4251\n",
      "Epoch 1967/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.7556 - val_loss: 182.7859\n",
      "Epoch 1968/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.9733 - val_loss: 138.1523\n",
      "Epoch 1969/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.9565 - val_loss: 160.4812\n",
      "Epoch 1970/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.0032 - val_loss: 180.7084\n",
      "Epoch 1971/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.9894 - val_loss: 145.2041\n",
      "Epoch 1972/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.5937 - val_loss: 181.9336\n",
      "Epoch 1973/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.0044 - val_loss: 154.2143\n",
      "Epoch 1974/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.6372 - val_loss: 139.5289\n",
      "Epoch 1975/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.7380 - val_loss: 128.7913\n",
      "Epoch 1976/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.6298 - val_loss: 148.1947\n",
      "Epoch 1977/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.3323 - val_loss: 140.7356\n",
      "Epoch 1978/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.8832 - val_loss: 167.7391\n",
      "Epoch 1979/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.2595 - val_loss: 140.2525\n",
      "Epoch 1980/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 169.9247 - val_loss: 138.6501\n",
      "Epoch 1981/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.0800 - val_loss: 132.4499\n",
      "Epoch 1982/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.1861 - val_loss: 131.6395\n",
      "Epoch 1983/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.9556 - val_loss: 138.6040\n",
      "Epoch 1984/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.9204 - val_loss: 221.2451\n",
      "Epoch 1985/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.4164 - val_loss: 137.8093\n",
      "Epoch 1986/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.5465 - val_loss: 131.3846\n",
      "Epoch 1987/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.5421 - val_loss: 132.8653\n",
      "Epoch 1988/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.0002 - val_loss: 165.9127\n",
      "Epoch 1989/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.7485 - val_loss: 147.0860\n",
      "Epoch 1990/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.1402 - val_loss: 142.3166\n",
      "Epoch 1991/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.9100 - val_loss: 146.5190\n",
      "Epoch 1992/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.1404 - val_loss: 135.3791\n",
      "Epoch 1993/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.8761 - val_loss: 130.0638\n",
      "Epoch 1994/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.2980 - val_loss: 169.8854\n",
      "Epoch 1995/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.8549 - val_loss: 137.3274\n",
      "Epoch 1996/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.9133 - val_loss: 136.5506\n",
      "Epoch 1997/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.0300 - val_loss: 161.0244\n",
      "Epoch 1998/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.0544 - val_loss: 151.1587\n",
      "Epoch 1999/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 125.0447 - val_loss: 144.5156\n",
      "Epoch 2000/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 135.1044 - val_loss: 160.9480\n",
      "Epoch 2001/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 124.6919 - val_loss: 135.9406\n",
      "Epoch 2002/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 141.6591 - val_loss: 169.8101\n",
      "Epoch 2003/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 162.0293 - val_loss: 162.3405\n",
      "Epoch 2004/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.3754 - val_loss: 138.9211\n",
      "Epoch 2005/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.2055 - val_loss: 175.8602\n",
      "Epoch 2006/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.2077 - val_loss: 155.5060\n",
      "Epoch 2007/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.4736 - val_loss: 133.7821\n",
      "Epoch 2008/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.9129 - val_loss: 154.2733\n",
      "Epoch 2009/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.5264 - val_loss: 132.2131\n",
      "Epoch 2010/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.9875 - val_loss: 165.4599\n",
      "Epoch 2011/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.8951 - val_loss: 201.7491\n",
      "Epoch 2012/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.0083 - val_loss: 140.4064\n",
      "Epoch 2013/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.9852 - val_loss: 155.6026\n",
      "Epoch 2014/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.2404 - val_loss: 155.0387\n",
      "Epoch 2015/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.1625 - val_loss: 175.8574\n",
      "Epoch 2016/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.2803 - val_loss: 135.0322\n",
      "Epoch 2017/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.7468 - val_loss: 175.6609\n",
      "Epoch 2018/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.1245 - val_loss: 153.1697\n",
      "Epoch 2019/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.7626 - val_loss: 161.4543\n",
      "Epoch 2020/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.5619 - val_loss: 135.4551\n",
      "Epoch 2021/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.7982 - val_loss: 165.8720\n",
      "Epoch 2022/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.1011 - val_loss: 137.8029\n",
      "Epoch 2023/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.0149 - val_loss: 141.4542\n",
      "Epoch 2024/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.8978 - val_loss: 337.9208\n",
      "Epoch 2025/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 173.9467 - val_loss: 163.7115\n",
      "Epoch 2026/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.9128 - val_loss: 136.1475\n",
      "Epoch 2027/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.2481 - val_loss: 144.8951\n",
      "Epoch 2028/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.0640 - val_loss: 128.3301\n",
      "Epoch 2029/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.9388 - val_loss: 149.2452\n",
      "Epoch 2030/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.7772 - val_loss: 133.7407\n",
      "Epoch 2031/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.9651 - val_loss: 157.8105\n",
      "Epoch 2032/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.0007 - val_loss: 131.0814\n",
      "Epoch 2033/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.8790 - val_loss: 131.6375\n",
      "Epoch 2034/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.4789 - val_loss: 195.0680\n",
      "Epoch 2035/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 180.2668 - val_loss: 127.0684\n",
      "Epoch 2036/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.5701 - val_loss: 155.2807\n",
      "Epoch 2037/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.3887 - val_loss: 145.3770\n",
      "Epoch 2038/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.9032 - val_loss: 128.5811\n",
      "Epoch 2039/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.1033 - val_loss: 132.0685\n",
      "Epoch 2040/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.9877 - val_loss: 144.6505\n",
      "Epoch 2041/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.9482 - val_loss: 139.5252\n",
      "Epoch 2042/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.9655 - val_loss: 136.8101\n",
      "Epoch 2043/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.8509 - val_loss: 219.4640\n",
      "Epoch 2044/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.2390 - val_loss: 146.8498\n",
      "Epoch 2045/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.8147 - val_loss: 140.5924\n",
      "Epoch 2046/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.0709 - val_loss: 139.9594\n",
      "Epoch 2047/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.5306 - val_loss: 134.8707\n",
      "Epoch 2048/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.1900 - val_loss: 135.2298\n",
      "Epoch 2049/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.0791 - val_loss: 129.6251\n",
      "Epoch 2050/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.8520 - val_loss: 141.8704\n",
      "Epoch 2051/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.2004 - val_loss: 185.3767\n",
      "Epoch 2052/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.2054 - val_loss: 129.8361\n",
      "Epoch 2053/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.7148 - val_loss: 161.0426\n",
      "Epoch 2054/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.4423 - val_loss: 135.7088\n",
      "Epoch 2055/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.2991 - val_loss: 138.8037\n",
      "Epoch 2056/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.6149 - val_loss: 144.7970\n",
      "Epoch 2057/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.0975 - val_loss: 147.5596\n",
      "Epoch 2058/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.2900 - val_loss: 140.9051\n",
      "Epoch 2059/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.4195 - val_loss: 134.3747\n",
      "Epoch 2060/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.4564 - val_loss: 137.2652\n",
      "Epoch 2061/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.1683 - val_loss: 137.0216\n",
      "Epoch 2062/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 144.4643 - val_loss: 160.5709\n",
      "Epoch 2063/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.6458 - val_loss: 164.4715\n",
      "Epoch 2064/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.7834 - val_loss: 136.5562\n",
      "Epoch 2065/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.0884 - val_loss: 136.9140\n",
      "Epoch 2066/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 130.4591 - val_loss: 129.4846\n",
      "Epoch 2067/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 126.0522 - val_loss: 161.0388\n",
      "Epoch 2068/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.2940 - val_loss: 139.6180\n",
      "Epoch 2069/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 137.5203 - val_loss: 132.8476\n",
      "Epoch 2070/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.3425 - val_loss: 130.5004\n",
      "Epoch 2071/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.6255 - val_loss: 146.1165\n",
      "Epoch 2072/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.8539 - val_loss: 139.2543\n",
      "Epoch 2073/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.0032 - val_loss: 133.8687\n",
      "Epoch 2074/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.2853 - val_loss: 128.0413\n",
      "Epoch 2075/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.2204 - val_loss: 139.8408\n",
      "Epoch 2076/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.8067 - val_loss: 133.3637\n",
      "Epoch 2077/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.9910 - val_loss: 141.5468\n",
      "Epoch 2078/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.6118 - val_loss: 129.9209\n",
      "Epoch 2079/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.4743 - val_loss: 141.9384\n",
      "Epoch 2080/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.7707 - val_loss: 153.9319\n",
      "Epoch 2081/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.0473 - val_loss: 166.0530\n",
      "Epoch 2082/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.0169 - val_loss: 136.9061\n",
      "Epoch 2083/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.0322 - val_loss: 201.9564\n",
      "Epoch 2084/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.4968 - val_loss: 140.3337\n",
      "Epoch 2085/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.7217 - val_loss: 144.3527\n",
      "Epoch 2086/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.5453 - val_loss: 131.0836\n",
      "Epoch 2087/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.4917 - val_loss: 176.8535\n",
      "Epoch 2088/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.2965 - val_loss: 130.3521\n",
      "Epoch 2089/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.2334 - val_loss: 144.6673\n",
      "Epoch 2090/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.8014 - val_loss: 179.2985\n",
      "Epoch 2091/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.0031 - val_loss: 132.5427\n",
      "Epoch 2092/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.3417 - val_loss: 170.7916\n",
      "Epoch 2093/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.1051 - val_loss: 130.2604\n",
      "Epoch 2094/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.0297 - val_loss: 136.4847\n",
      "Epoch 2095/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.8715 - val_loss: 147.0983\n",
      "Epoch 2096/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.5755 - val_loss: 128.0150\n",
      "Epoch 2097/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.7445 - val_loss: 143.0555\n",
      "Epoch 2098/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.8348 - val_loss: 178.8256\n",
      "Epoch 2099/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 172.5853 - val_loss: 142.6768\n",
      "Epoch 2100/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.4584 - val_loss: 141.3150\n",
      "Epoch 2101/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.7247 - val_loss: 133.1100\n",
      "Epoch 2102/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.6676 - val_loss: 138.6335\n",
      "Epoch 2103/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.7135 - val_loss: 157.2466\n",
      "Epoch 2104/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.0376 - val_loss: 146.0259\n",
      "Epoch 2105/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.6320 - val_loss: 139.5159\n",
      "Epoch 2106/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.9214 - val_loss: 139.5021\n",
      "Epoch 2107/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.5711 - val_loss: 136.4264\n",
      "Epoch 2108/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.9055 - val_loss: 136.1375\n",
      "Epoch 2109/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.0012 - val_loss: 128.5582\n",
      "Epoch 2110/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.7689 - val_loss: 132.2114\n",
      "Epoch 2111/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.4136 - val_loss: 133.0606\n",
      "Epoch 2112/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 205.2083 - val_loss: 160.7556\n",
      "Epoch 2113/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.3101 - val_loss: 161.9422\n",
      "Epoch 2114/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 118.3263 - val_loss: 158.3899\n",
      "Epoch 2115/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.0002 - val_loss: 137.8522\n",
      "Epoch 2116/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.7645 - val_loss: 133.5705\n",
      "Epoch 2117/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.5399 - val_loss: 142.0052\n",
      "Epoch 2118/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.2886 - val_loss: 130.1347\n",
      "Epoch 2119/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.9198 - val_loss: 144.1658\n",
      "Epoch 2120/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.2347 - val_loss: 136.8899\n",
      "Epoch 2121/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.8724 - val_loss: 132.9923\n",
      "Epoch 2122/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.0482 - val_loss: 154.5423\n",
      "Epoch 2123/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.2974 - val_loss: 134.7598\n",
      "Epoch 2124/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.2265 - val_loss: 132.0467\n",
      "Epoch 2125/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.1313 - val_loss: 138.7944\n",
      "Epoch 2126/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.0533 - val_loss: 222.0313\n",
      "Epoch 2127/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.4011 - val_loss: 158.2848\n",
      "Epoch 2128/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.4152 - val_loss: 139.9358\n",
      "Epoch 2129/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.8400 - val_loss: 130.6780\n",
      "Epoch 2130/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.2987 - val_loss: 136.8716\n",
      "Epoch 2131/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.6043 - val_loss: 137.5977\n",
      "Epoch 2132/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.1954 - val_loss: 133.0407\n",
      "Epoch 2133/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.5331 - val_loss: 148.2578\n",
      "Epoch 2134/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 119.9609 - val_loss: 131.4870\n",
      "Epoch 2135/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.4808 - val_loss: 140.2940\n",
      "Epoch 2136/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.2197 - val_loss: 134.6562\n",
      "Epoch 2137/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.1659 - val_loss: 144.1055\n",
      "Epoch 2138/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.9033 - val_loss: 199.7967\n",
      "Epoch 2139/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.1124 - val_loss: 128.7306\n",
      "Epoch 2140/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.5853 - val_loss: 137.5773\n",
      "Epoch 2141/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.3143 - val_loss: 129.7964\n",
      "Epoch 2142/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.0312 - val_loss: 167.4722\n",
      "Epoch 2143/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.5749 - val_loss: 285.9375\n",
      "Epoch 2144/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 164.9520 - val_loss: 139.4745\n",
      "Epoch 2145/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 119.0022 - val_loss: 138.5063\n",
      "Epoch 2146/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 121.2670 - val_loss: 135.9166\n",
      "Epoch 2147/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 123.2612 - val_loss: 143.5534\n",
      "Epoch 2148/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 126.1051 - val_loss: 150.3100\n",
      "Epoch 2149/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 123.9681 - val_loss: 186.3763\n",
      "Epoch 2150/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 136.2074 - val_loss: 132.8752\n",
      "Epoch 2151/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.0072 - val_loss: 185.1203\n",
      "Epoch 2152/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.7014 - val_loss: 133.7712\n",
      "Epoch 2153/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.8659 - val_loss: 159.1225\n",
      "Epoch 2154/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.7477 - val_loss: 139.1675\n",
      "Epoch 2155/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.0343 - val_loss: 129.3882\n",
      "Epoch 2156/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.8395 - val_loss: 136.9603\n",
      "Epoch 2157/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.5278 - val_loss: 147.2099\n",
      "Epoch 2158/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.2017 - val_loss: 131.5025\n",
      "Epoch 2159/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.0780 - val_loss: 210.4756\n",
      "Epoch 2160/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.3371 - val_loss: 132.0654\n",
      "Epoch 2161/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.0366 - val_loss: 139.0165\n",
      "Epoch 2162/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 127.3733 - val_loss: 133.7299\n",
      "Epoch 2163/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.2038 - val_loss: 136.9132\n",
      "Epoch 2164/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.8774 - val_loss: 133.9457\n",
      "Epoch 2165/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.0220 - val_loss: 136.5889\n",
      "Epoch 2166/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.8140 - val_loss: 138.4528\n",
      "Epoch 2167/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.3762 - val_loss: 131.2667\n",
      "Epoch 2168/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.9490 - val_loss: 143.9000\n",
      "Epoch 2169/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.9666 - val_loss: 146.9060\n",
      "Epoch 2170/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.8516 - val_loss: 132.8636\n",
      "Epoch 2171/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.0596 - val_loss: 135.2117\n",
      "Epoch 2172/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.5289 - val_loss: 137.1398\n",
      "Epoch 2173/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 166.5393 - val_loss: 248.8858\n",
      "Epoch 2174/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 163.6612 - val_loss: 136.5964\n",
      "Epoch 2175/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.4966 - val_loss: 140.0830\n",
      "Epoch 2176/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.8441 - val_loss: 128.2696\n",
      "Epoch 2177/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.2413 - val_loss: 136.8697\n",
      "Epoch 2178/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.1984 - val_loss: 136.2106\n",
      "Epoch 2179/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.0406 - val_loss: 132.6364\n",
      "Epoch 2180/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.5227 - val_loss: 129.4481\n",
      "Epoch 2181/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.3795 - val_loss: 153.7457\n",
      "Epoch 2182/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.7470 - val_loss: 131.4846\n",
      "Epoch 2183/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.7376 - val_loss: 159.8363\n",
      "Epoch 2184/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.2844 - val_loss: 135.2460\n",
      "Epoch 2185/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.7225 - val_loss: 140.0828\n",
      "Epoch 2186/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.8849 - val_loss: 159.0792\n",
      "Epoch 2187/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.2738 - val_loss: 130.7005\n",
      "Epoch 2188/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.6728 - val_loss: 148.1833\n",
      "Epoch 2189/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.7340 - val_loss: 140.8776\n",
      "Epoch 2190/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.8662 - val_loss: 135.1784\n",
      "Epoch 2191/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.0205 - val_loss: 166.3004\n",
      "Epoch 2192/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.5464 - val_loss: 140.4786\n",
      "Epoch 2193/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.9610 - val_loss: 144.3438\n",
      "Epoch 2194/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.9107 - val_loss: 137.2735\n",
      "Epoch 2195/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.6723 - val_loss: 131.6328\n",
      "Epoch 2196/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.2537 - val_loss: 133.9569\n",
      "Epoch 2197/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.1948 - val_loss: 171.9577\n",
      "Epoch 2198/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.9672 - val_loss: 155.0518\n",
      "Epoch 2199/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.7682 - val_loss: 131.3368\n",
      "Epoch 2200/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.6286 - val_loss: 130.7215\n",
      "Epoch 2201/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.0086 - val_loss: 144.5987\n",
      "Epoch 2202/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.3483 - val_loss: 155.5761\n",
      "Epoch 2203/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.1528 - val_loss: 138.3024\n",
      "Epoch 2204/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.6815 - val_loss: 140.6062\n",
      "Epoch 2205/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.8507 - val_loss: 141.7781\n",
      "Epoch 2206/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.7261 - val_loss: 152.1019\n",
      "Epoch 2207/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.6405 - val_loss: 168.9758\n",
      "Epoch 2208/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.9572 - val_loss: 168.6667\n",
      "Epoch 2209/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.0305 - val_loss: 148.1374\n",
      "Epoch 2210/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.6325 - val_loss: 127.4287\n",
      "Epoch 2211/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.6333 - val_loss: 155.4915\n",
      "Epoch 2212/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.2502 - val_loss: 330.2476\n",
      "Epoch 2213/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.2847 - val_loss: 133.3852\n",
      "Epoch 2214/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.7884 - val_loss: 127.9562\n",
      "Epoch 2215/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.5571 - val_loss: 131.8305\n",
      "Epoch 2216/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.6295 - val_loss: 180.5118\n",
      "Epoch 2217/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.0174 - val_loss: 143.0629\n",
      "Epoch 2218/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.0755 - val_loss: 132.8917\n",
      "Epoch 2219/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.4715 - val_loss: 164.9597\n",
      "Epoch 2220/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.1247 - val_loss: 136.1986\n",
      "Epoch 2221/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.1451 - val_loss: 148.7058\n",
      "Epoch 2222/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.3965 - val_loss: 134.7526\n",
      "Epoch 2223/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.6126 - val_loss: 198.5229\n",
      "Epoch 2224/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 121.8174 - val_loss: 143.6350\n",
      "Epoch 2225/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.3991 - val_loss: 145.9938\n",
      "Epoch 2226/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 126.7900 - val_loss: 129.6962\n",
      "Epoch 2227/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.1861 - val_loss: 135.5613\n",
      "Epoch 2228/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.4696 - val_loss: 142.5807\n",
      "Epoch 2229/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.4356 - val_loss: 141.5015\n",
      "Epoch 2230/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.7836 - val_loss: 139.7804\n",
      "Epoch 2231/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.2589 - val_loss: 177.4650\n",
      "Epoch 2232/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.8353 - val_loss: 142.9312\n",
      "Epoch 2233/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.3573 - val_loss: 135.3499\n",
      "Epoch 2234/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.3269 - val_loss: 150.4826\n",
      "Epoch 2235/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.4155 - val_loss: 155.3327\n",
      "Epoch 2236/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.8040 - val_loss: 133.5433\n",
      "Epoch 2237/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.8041 - val_loss: 128.3103\n",
      "Epoch 2238/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.9920 - val_loss: 138.6375\n",
      "Epoch 2239/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.2974 - val_loss: 128.4622\n",
      "Epoch 2240/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.1505 - val_loss: 136.9120\n",
      "Epoch 2241/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.2678 - val_loss: 147.5295\n",
      "Epoch 2242/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.5367 - val_loss: 130.3272\n",
      "Epoch 2243/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.5533 - val_loss: 134.5009\n",
      "Epoch 2244/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.8186 - val_loss: 139.1014\n",
      "Epoch 2245/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.9066 - val_loss: 133.1770\n",
      "Epoch 2246/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.3273 - val_loss: 162.8037\n",
      "Epoch 2247/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.0270 - val_loss: 129.8756\n",
      "Epoch 2248/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.3652 - val_loss: 152.2745\n",
      "Epoch 2249/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.2169 - val_loss: 132.0101\n",
      "Epoch 2250/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.1883 - val_loss: 139.7606\n",
      "Epoch 2251/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.6271 - val_loss: 146.8711\n",
      "Epoch 2252/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.6371 - val_loss: 132.7302\n",
      "Epoch 2253/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.4303 - val_loss: 131.2437\n",
      "Epoch 2254/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.5605 - val_loss: 137.7740\n",
      "Epoch 2255/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.8887 - val_loss: 134.5001\n",
      "Epoch 2256/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.6012 - val_loss: 133.9367\n",
      "Epoch 2257/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.5981 - val_loss: 127.9439\n",
      "Epoch 2258/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.6848 - val_loss: 135.1222\n",
      "Epoch 2259/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.5484 - val_loss: 143.4953\n",
      "Epoch 2260/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.3091 - val_loss: 208.3501\n",
      "Epoch 2261/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 222.0760 - val_loss: 133.6113\n",
      "Epoch 2262/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.4796 - val_loss: 132.6744\n",
      "Epoch 2263/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.1343 - val_loss: 144.5445\n",
      "Epoch 2264/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.3961 - val_loss: 251.7955\n",
      "Epoch 2265/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.9522 - val_loss: 155.3918\n",
      "Epoch 2266/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 116.2484 - val_loss: 132.3235\n",
      "Epoch 2267/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.4953 - val_loss: 147.8334\n",
      "Epoch 2268/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.0072 - val_loss: 197.7842\n",
      "Epoch 2269/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.0421 - val_loss: 140.4746\n",
      "Epoch 2270/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.9845 - val_loss: 140.7786\n",
      "Epoch 2271/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.4132 - val_loss: 225.2881\n",
      "Epoch 2272/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.6321 - val_loss: 135.1191\n",
      "Epoch 2273/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.0604 - val_loss: 161.1777\n",
      "Epoch 2274/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.9583 - val_loss: 133.3450\n",
      "Epoch 2275/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.0466 - val_loss: 130.8201\n",
      "Epoch 2276/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.3594 - val_loss: 132.0424\n",
      "Epoch 2277/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.0314 - val_loss: 128.3916\n",
      "Epoch 2278/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.8437 - val_loss: 139.5004\n",
      "Epoch 2279/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.3617 - val_loss: 156.9561\n",
      "Epoch 2280/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.2392 - val_loss: 183.6013\n",
      "Epoch 2281/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.7996 - val_loss: 132.9379\n",
      "Epoch 2282/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.2084 - val_loss: 135.6386\n",
      "Epoch 2283/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.8485 - val_loss: 183.8199\n",
      "Epoch 2284/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.8751 - val_loss: 171.1027\n",
      "Epoch 2285/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.8336 - val_loss: 188.0061\n",
      "Epoch 2286/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.7833 - val_loss: 135.9473\n",
      "Epoch 2287/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.6718 - val_loss: 129.7658\n",
      "Epoch 2288/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.7510 - val_loss: 135.8939\n",
      "Epoch 2289/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.0205 - val_loss: 133.2569\n",
      "Epoch 2290/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.5990 - val_loss: 133.7669\n",
      "Epoch 2291/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.8776 - val_loss: 133.8060\n",
      "Epoch 2292/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.0967 - val_loss: 149.4114\n",
      "Epoch 2293/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.8624 - val_loss: 139.1888\n",
      "Epoch 2294/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.4354 - val_loss: 134.9931\n",
      "Epoch 2295/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.6096 - val_loss: 140.4106\n",
      "Epoch 2296/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.2042 - val_loss: 132.9855\n",
      "Epoch 2297/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.9137 - val_loss: 136.1511\n",
      "Epoch 2298/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.7178 - val_loss: 132.0778\n",
      "Epoch 2299/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.3161 - val_loss: 128.3376\n",
      "Epoch 2300/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.2141 - val_loss: 134.5119\n",
      "Epoch 2301/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.3603 - val_loss: 134.6728\n",
      "Epoch 2302/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.9604 - val_loss: 140.2743\n",
      "Epoch 2303/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 126.5871 - val_loss: 132.5535\n",
      "Epoch 2304/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 124.6584 - val_loss: 132.3589\n",
      "Epoch 2305/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 129.5470 - val_loss: 147.2384\n",
      "Epoch 2306/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.5788 - val_loss: 131.1107\n",
      "Epoch 2307/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.5194 - val_loss: 135.6254\n",
      "Epoch 2308/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.7844 - val_loss: 143.9025\n",
      "Epoch 2309/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.6669 - val_loss: 143.7308\n",
      "Epoch 2310/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.5955 - val_loss: 166.2551\n",
      "Epoch 2311/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.0204 - val_loss: 150.6900\n",
      "Epoch 2312/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.0390 - val_loss: 160.5986\n",
      "Epoch 2313/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.2261 - val_loss: 130.8204\n",
      "Epoch 2314/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.2534 - val_loss: 142.6957\n",
      "Epoch 2315/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 189.9523 - val_loss: 132.7707\n",
      "Epoch 2316/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.4233 - val_loss: 140.2255\n",
      "Epoch 2317/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 121.7775 - val_loss: 136.8045\n",
      "Epoch 2318/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 119.3384 - val_loss: 139.9383\n",
      "Epoch 2319/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.6788 - val_loss: 137.3172\n",
      "Epoch 2320/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.6613 - val_loss: 137.1895\n",
      "Epoch 2321/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 121.6531 - val_loss: 138.3851\n",
      "Epoch 2322/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.5976 - val_loss: 135.5388\n",
      "Epoch 2323/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.9289 - val_loss: 134.3194\n",
      "Epoch 2324/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.9629 - val_loss: 179.8800\n",
      "Epoch 2325/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.4491 - val_loss: 133.5143\n",
      "Epoch 2326/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.4926 - val_loss: 136.6154\n",
      "Epoch 2327/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.5181 - val_loss: 169.5027\n",
      "Epoch 2328/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.2370 - val_loss: 138.9154\n",
      "Epoch 2329/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 121.1175 - val_loss: 150.2003\n",
      "Epoch 2330/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.5352 - val_loss: 146.3533\n",
      "Epoch 2331/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.5284 - val_loss: 130.4476\n",
      "Epoch 2332/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.1110 - val_loss: 156.4723\n",
      "Epoch 2333/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.0592 - val_loss: 143.5587\n",
      "Epoch 2334/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.4411 - val_loss: 142.1098\n",
      "Epoch 2335/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.4631 - val_loss: 163.1955\n",
      "Epoch 2336/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.3392 - val_loss: 138.3968\n",
      "Epoch 2337/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.7259 - val_loss: 157.9081\n",
      "Epoch 2338/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.2485 - val_loss: 137.5587\n",
      "Epoch 2339/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.5895 - val_loss: 144.6578\n",
      "Epoch 2340/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.0290 - val_loss: 136.7720\n",
      "Epoch 2341/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.7036 - val_loss: 129.4617\n",
      "Epoch 2342/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.6096 - val_loss: 138.5425\n",
      "Epoch 2343/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 156.5455 - val_loss: 147.8619\n",
      "Epoch 2344/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.3563 - val_loss: 131.7158\n",
      "Epoch 2345/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.7452 - val_loss: 132.5197\n",
      "Epoch 2346/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.4871 - val_loss: 133.3552\n",
      "Epoch 2347/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.7656 - val_loss: 203.3981\n",
      "Epoch 2348/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.5142 - val_loss: 174.3024\n",
      "Epoch 2349/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.1032 - val_loss: 132.2499\n",
      "Epoch 2350/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.9904 - val_loss: 133.6487\n",
      "Epoch 2351/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.8955 - val_loss: 132.3524\n",
      "Epoch 2352/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.4038 - val_loss: 135.3511\n",
      "Epoch 2353/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.0754 - val_loss: 140.7694\n",
      "Epoch 2354/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.1883 - val_loss: 149.7099\n",
      "Epoch 2355/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.8845 - val_loss: 133.7210\n",
      "Epoch 2356/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.4682 - val_loss: 131.4793\n",
      "Epoch 2357/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.9414 - val_loss: 130.9446\n",
      "Epoch 2358/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.1504 - val_loss: 139.0922\n",
      "Epoch 2359/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.1253 - val_loss: 127.4030\n",
      "Epoch 2360/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 170.4413 - val_loss: 157.1144\n",
      "Epoch 2361/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.9906 - val_loss: 142.4539\n",
      "Epoch 2362/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.9276 - val_loss: 133.4667\n",
      "Epoch 2363/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.5418 - val_loss: 142.5214\n",
      "Epoch 2364/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.2259 - val_loss: 139.0109\n",
      "Epoch 2365/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.1163 - val_loss: 131.4363\n",
      "Epoch 2366/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.4524 - val_loss: 137.4390\n",
      "Epoch 2367/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.8594 - val_loss: 133.2873\n",
      "Epoch 2368/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.3268 - val_loss: 149.8031\n",
      "Epoch 2369/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.4623 - val_loss: 140.5073\n",
      "Epoch 2370/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.8381 - val_loss: 238.1880\n",
      "Epoch 2371/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.7339 - val_loss: 132.7929\n",
      "Epoch 2372/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.1873 - val_loss: 134.8399\n",
      "Epoch 2373/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.1775 - val_loss: 142.0523\n",
      "Epoch 2374/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.6817 - val_loss: 134.3345\n",
      "Epoch 2375/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.9265 - val_loss: 161.8936\n",
      "Epoch 2376/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.8091 - val_loss: 130.3207\n",
      "Epoch 2377/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.0188 - val_loss: 139.3887\n",
      "Epoch 2378/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 162.8756 - val_loss: 130.1432\n",
      "Epoch 2379/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.9026 - val_loss: 133.3120\n",
      "Epoch 2380/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.2768 - val_loss: 140.2230\n",
      "Epoch 2381/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 124.1534 - val_loss: 140.5930\n",
      "Epoch 2382/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 126.9650 - val_loss: 132.4677\n",
      "Epoch 2383/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 125.0897 - val_loss: 130.6802\n",
      "Epoch 2384/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 128.6647 - val_loss: 170.9977\n",
      "Epoch 2385/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.4111 - val_loss: 139.5589\n",
      "Epoch 2386/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.4914 - val_loss: 131.8172\n",
      "Epoch 2387/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.2732 - val_loss: 131.8829\n",
      "Epoch 2388/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.2299 - val_loss: 163.1641\n",
      "Epoch 2389/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.7315 - val_loss: 158.5604\n",
      "Epoch 2390/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.8811 - val_loss: 139.1201\n",
      "Epoch 2391/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.5920 - val_loss: 130.2009\n",
      "Epoch 2392/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.2344 - val_loss: 147.6105\n",
      "Epoch 2393/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.5735 - val_loss: 153.3444\n",
      "Epoch 2394/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.3059 - val_loss: 138.6393\n",
      "Epoch 2395/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.8815 - val_loss: 140.6139\n",
      "Epoch 2396/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.6878 - val_loss: 139.6018\n",
      "Epoch 2397/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.4177 - val_loss: 149.8423\n",
      "Epoch 2398/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.4375 - val_loss: 136.0395\n",
      "Epoch 2399/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.0660 - val_loss: 149.0566\n",
      "Epoch 2400/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.2769 - val_loss: 139.8004\n",
      "Epoch 2401/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.8595 - val_loss: 140.9562\n",
      "Epoch 2402/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.5902 - val_loss: 132.4163\n",
      "Epoch 2403/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.3901 - val_loss: 139.7171\n",
      "Epoch 2404/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.7015 - val_loss: 134.9076\n",
      "Epoch 2405/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.2611 - val_loss: 136.7020\n",
      "Epoch 2406/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.8252 - val_loss: 133.4566\n",
      "Epoch 2407/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.2389 - val_loss: 136.4164\n",
      "Epoch 2408/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.5684 - val_loss: 143.5692\n",
      "Epoch 2409/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.1339 - val_loss: 152.8532\n",
      "Epoch 2410/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.8836 - val_loss: 154.0708\n",
      "Epoch 2411/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.4501 - val_loss: 141.2012\n",
      "Epoch 2412/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.4927 - val_loss: 176.5927\n",
      "Epoch 2413/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.0252 - val_loss: 131.5728\n",
      "Epoch 2414/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 166.5793 - val_loss: 129.0500\n",
      "Epoch 2415/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.7259 - val_loss: 150.3624\n",
      "Epoch 2416/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.8226 - val_loss: 190.8724\n",
      "Epoch 2417/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.0540 - val_loss: 132.5937\n",
      "Epoch 2418/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.8614 - val_loss: 131.6001\n",
      "Epoch 2419/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.2008 - val_loss: 136.9413\n",
      "Epoch 2420/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.3899 - val_loss: 136.7211\n",
      "Epoch 2421/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.9214 - val_loss: 145.8205\n",
      "Epoch 2422/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.0602 - val_loss: 230.7056\n",
      "Epoch 2423/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.1278 - val_loss: 167.3689\n",
      "Epoch 2424/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.4490 - val_loss: 127.1241\n",
      "Epoch 2425/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.6720 - val_loss: 168.5365\n",
      "Epoch 2426/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 162.7685 - val_loss: 136.2747\n",
      "Epoch 2427/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.9203 - val_loss: 136.0568\n",
      "Epoch 2428/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.4526 - val_loss: 133.9203\n",
      "Epoch 2429/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.7607 - val_loss: 152.0545\n",
      "Epoch 2430/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.3269 - val_loss: 137.2304\n",
      "Epoch 2431/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.8351 - val_loss: 138.3213\n",
      "Epoch 2432/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.1131 - val_loss: 169.8081\n",
      "Epoch 2433/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.5403 - val_loss: 201.2008\n",
      "Epoch 2434/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.3353 - val_loss: 143.2275\n",
      "Epoch 2435/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.0425 - val_loss: 143.2574\n",
      "Epoch 2436/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.6309 - val_loss: 148.0732\n",
      "Epoch 2437/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.8816 - val_loss: 192.9889\n",
      "Epoch 2438/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.7462 - val_loss: 133.8960\n",
      "Epoch 2439/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.6008 - val_loss: 132.9875\n",
      "Epoch 2440/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.4941 - val_loss: 132.1488\n",
      "Epoch 2441/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.2294 - val_loss: 140.1084\n",
      "Epoch 2442/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.8913 - val_loss: 133.0387\n",
      "Epoch 2443/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.6398 - val_loss: 142.6606\n",
      "Epoch 2444/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.6010 - val_loss: 153.1804\n",
      "Epoch 2445/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.9427 - val_loss: 157.6327\n",
      "Epoch 2446/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.2601 - val_loss: 133.7420\n",
      "Epoch 2447/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.8420 - val_loss: 143.6497\n",
      "Epoch 2448/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.8481 - val_loss: 132.0193\n",
      "Epoch 2449/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.6109 - val_loss: 137.2774\n",
      "Epoch 2450/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.9408 - val_loss: 142.7149\n",
      "Epoch 2451/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.8105 - val_loss: 131.8345\n",
      "Epoch 2452/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.2954 - val_loss: 132.6718\n",
      "Epoch 2453/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.0963 - val_loss: 142.0406\n",
      "Epoch 2454/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.3293 - val_loss: 129.9276\n",
      "Epoch 2455/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.1929 - val_loss: 133.2115\n",
      "Epoch 2456/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.6403 - val_loss: 143.2633\n",
      "Epoch 2457/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.4062 - val_loss: 145.1055\n",
      "Epoch 2458/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.7925 - val_loss: 132.5590\n",
      "Epoch 2459/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.9290 - val_loss: 130.6500\n",
      "Epoch 2460/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 123.7786 - val_loss: 207.1379\n",
      "Epoch 2461/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 121.6342 - val_loss: 128.7398\n",
      "Epoch 2462/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 122.1230 - val_loss: 131.8249\n",
      "Epoch 2463/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 124.5754 - val_loss: 136.2054\n",
      "Epoch 2464/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.2884 - val_loss: 136.3449\n",
      "Epoch 2465/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.3057 - val_loss: 208.8466\n",
      "Epoch 2466/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.6523 - val_loss: 163.3475\n",
      "Epoch 2467/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.6932 - val_loss: 136.5625\n",
      "Epoch 2468/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.2674 - val_loss: 129.7339\n",
      "Epoch 2469/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.8082 - val_loss: 161.3330\n",
      "Epoch 2470/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.1625 - val_loss: 163.3155\n",
      "Epoch 2471/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.8187 - val_loss: 152.2737\n",
      "Epoch 2472/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.5456 - val_loss: 138.2830\n",
      "Epoch 2473/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.2589 - val_loss: 161.1270\n",
      "Epoch 2474/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.6633 - val_loss: 149.7968\n",
      "Epoch 2475/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.1893 - val_loss: 138.0833\n",
      "Epoch 2476/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.4347 - val_loss: 150.3369\n",
      "Epoch 2477/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.7294 - val_loss: 143.4507\n",
      "Epoch 2478/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.8579 - val_loss: 170.6882\n",
      "Epoch 2479/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.2321 - val_loss: 138.3930\n",
      "Epoch 2480/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.6402 - val_loss: 138.6800\n",
      "Epoch 2481/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.3722 - val_loss: 139.5517\n",
      "Epoch 2482/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.0972 - val_loss: 167.5534\n",
      "Epoch 2483/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.0993 - val_loss: 131.5685\n",
      "Epoch 2484/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.3197 - val_loss: 153.0549\n",
      "Epoch 2485/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.3261 - val_loss: 139.3829\n",
      "Epoch 2486/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.2662 - val_loss: 139.8425\n",
      "Epoch 2487/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.7174 - val_loss: 174.6621\n",
      "Epoch 2488/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.9896 - val_loss: 130.9937\n",
      "Epoch 2489/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.2088 - val_loss: 138.0616\n",
      "Epoch 2490/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.0562 - val_loss: 126.5800\n",
      "Epoch 2491/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.4734 - val_loss: 131.2928\n",
      "Epoch 2492/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.5070 - val_loss: 169.2090\n",
      "Epoch 2493/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.4967 - val_loss: 132.3287\n",
      "Epoch 2494/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.2740 - val_loss: 132.3044\n",
      "Epoch 2495/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.1839 - val_loss: 138.0874\n",
      "Epoch 2496/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.7778 - val_loss: 136.4018\n",
      "Epoch 2497/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.0120 - val_loss: 130.2774\n",
      "Epoch 2498/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.8490 - val_loss: 195.8977\n",
      "Epoch 2499/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.2790 - val_loss: 136.6503\n",
      "Epoch 2500/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.7648 - val_loss: 184.7115\n",
      "Epoch 2501/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.1699 - val_loss: 146.7417\n",
      "Epoch 2502/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.3808 - val_loss: 169.2747\n",
      "Epoch 2503/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.7566 - val_loss: 144.9820\n",
      "Epoch 2504/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 132.3215 - val_loss: 137.0057\n",
      "Epoch 2505/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.2065 - val_loss: 132.6438\n",
      "Epoch 2506/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.5493 - val_loss: 236.6112\n",
      "Epoch 2507/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.5852 - val_loss: 134.6165\n",
      "Epoch 2508/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.7351 - val_loss: 138.4078\n",
      "Epoch 2509/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.4492 - val_loss: 134.7595\n",
      "Epoch 2510/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.2504 - val_loss: 136.8096\n",
      "Epoch 2511/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.6372 - val_loss: 127.5689\n",
      "Epoch 2512/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.6265 - val_loss: 168.9448\n",
      "Epoch 2513/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.2678 - val_loss: 156.3601\n",
      "Epoch 2514/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.0632 - val_loss: 183.5828\n",
      "Epoch 2515/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 171.2958 - val_loss: 130.9196\n",
      "Epoch 2516/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.6145 - val_loss: 179.2802\n",
      "Epoch 2517/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.1776 - val_loss: 138.1929\n",
      "Epoch 2518/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.5618 - val_loss: 139.4153\n",
      "Epoch 2519/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.2092 - val_loss: 144.3003\n",
      "Epoch 2520/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.7665 - val_loss: 148.8750\n",
      "Epoch 2521/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.9400 - val_loss: 135.7996\n",
      "Epoch 2522/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.4776 - val_loss: 132.5985\n",
      "Epoch 2523/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.5596 - val_loss: 136.6443\n",
      "Epoch 2524/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.6135 - val_loss: 145.9039\n",
      "Epoch 2525/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.0139 - val_loss: 145.0787\n",
      "Epoch 2526/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.8132 - val_loss: 135.9862\n",
      "Epoch 2527/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.5264 - val_loss: 169.4267\n",
      "Epoch 2528/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.3265 - val_loss: 152.2168\n",
      "Epoch 2529/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.0371 - val_loss: 132.0074\n",
      "Epoch 2530/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.1505 - val_loss: 135.2901\n",
      "Epoch 2531/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.6780 - val_loss: 135.9738\n",
      "Epoch 2532/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.5723 - val_loss: 143.1173\n",
      "Epoch 2533/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.8865 - val_loss: 137.1403\n",
      "Epoch 2534/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.2151 - val_loss: 130.9827\n",
      "Epoch 2535/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.6906 - val_loss: 202.1310\n",
      "Epoch 2536/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.0928 - val_loss: 134.1665\n",
      "Epoch 2537/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.7904 - val_loss: 151.2604\n",
      "Epoch 2538/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.5660 - val_loss: 138.8639\n",
      "Epoch 2539/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 121.3944 - val_loss: 148.3371\n",
      "Epoch 2540/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 121.9405 - val_loss: 127.2059\n",
      "Epoch 2541/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 129.4293 - val_loss: 157.1823\n",
      "Epoch 2542/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 122.9115 - val_loss: 154.1885\n",
      "Epoch 2543/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.9381 - val_loss: 145.0963\n",
      "Epoch 2544/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.9372 - val_loss: 133.9574\n",
      "Epoch 2545/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.0590 - val_loss: 133.6102\n",
      "Epoch 2546/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.9298 - val_loss: 140.3848\n",
      "Epoch 2547/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.7859 - val_loss: 150.7337\n",
      "Epoch 2548/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.1127 - val_loss: 132.9039\n",
      "Epoch 2549/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.5300 - val_loss: 137.7943\n",
      "Epoch 2550/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.1364 - val_loss: 134.9917\n",
      "Epoch 2551/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.3605 - val_loss: 145.1616\n",
      "Epoch 2552/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.7439 - val_loss: 138.4033\n",
      "Epoch 2553/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.8659 - val_loss: 135.4442\n",
      "Epoch 2554/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.0957 - val_loss: 142.2030\n",
      "Epoch 2555/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.7287 - val_loss: 170.8273\n",
      "Epoch 2556/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.4747 - val_loss: 144.2542\n",
      "Epoch 2557/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.3172 - val_loss: 142.0079\n",
      "Epoch 2558/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 131.6995 - val_loss: 130.6510\n",
      "Epoch 2559/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.7848 - val_loss: 205.1946\n",
      "Epoch 2560/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.5307 - val_loss: 186.1222\n",
      "Epoch 2561/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.8945 - val_loss: 129.2767\n",
      "Epoch 2562/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.0287 - val_loss: 140.9720\n",
      "Epoch 2563/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.4642 - val_loss: 137.5794\n",
      "Epoch 2564/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.8925 - val_loss: 141.7277\n",
      "Epoch 2565/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.4014 - val_loss: 167.9771\n",
      "Epoch 2566/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.7591 - val_loss: 162.0130\n",
      "Epoch 2567/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.7992 - val_loss: 158.8905\n",
      "Epoch 2568/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.0615 - val_loss: 139.1603\n",
      "Epoch 2569/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.7715 - val_loss: 133.2075\n",
      "Epoch 2570/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 119.4667 - val_loss: 142.2696\n",
      "Epoch 2571/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.2994 - val_loss: 144.3809\n",
      "Epoch 2572/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.1913 - val_loss: 180.4476\n",
      "Epoch 2573/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.0214 - val_loss: 134.9648\n",
      "Epoch 2574/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.1925 - val_loss: 130.7146\n",
      "Epoch 2575/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.7841 - val_loss: 137.9334\n",
      "Epoch 2576/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.2504 - val_loss: 132.0126\n",
      "Epoch 2577/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.6868 - val_loss: 137.6398\n",
      "Epoch 2578/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.4876 - val_loss: 130.8562\n",
      "Epoch 2579/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.9437 - val_loss: 135.0043\n",
      "Epoch 2580/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.2437 - val_loss: 165.2217\n",
      "Epoch 2581/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.1042 - val_loss: 208.4442\n",
      "Epoch 2582/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.1187 - val_loss: 131.5873\n",
      "Epoch 2583/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.4323 - val_loss: 137.6174\n",
      "Epoch 2584/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.3202 - val_loss: 145.7010\n",
      "Epoch 2585/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.0450 - val_loss: 129.3087\n",
      "Epoch 2586/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.8364 - val_loss: 209.5023\n",
      "Epoch 2587/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.7653 - val_loss: 144.9908\n",
      "Epoch 2588/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.7935 - val_loss: 149.3128\n",
      "Epoch 2589/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.1246 - val_loss: 132.8529\n",
      "Epoch 2590/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.3597 - val_loss: 141.4134\n",
      "Epoch 2591/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.8793 - val_loss: 168.3024\n",
      "Epoch 2592/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.7372 - val_loss: 164.1012\n",
      "Epoch 2593/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.2512 - val_loss: 139.8627\n",
      "Epoch 2594/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.8578 - val_loss: 137.5553\n",
      "Epoch 2595/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.6075 - val_loss: 150.1095\n",
      "Epoch 2596/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.5399 - val_loss: 132.4998\n",
      "Epoch 2597/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.3182 - val_loss: 134.7733\n",
      "Epoch 2598/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.8096 - val_loss: 132.5961\n",
      "Epoch 2599/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.0954 - val_loss: 136.9835\n",
      "Epoch 2600/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.0810 - val_loss: 155.4428\n",
      "Epoch 2601/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.5007 - val_loss: 135.9908\n",
      "Epoch 2602/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.9293 - val_loss: 150.6699\n",
      "Epoch 2603/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.0134 - val_loss: 129.2356\n",
      "Epoch 2604/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.8783 - val_loss: 281.9137\n",
      "Epoch 2605/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.5154 - val_loss: 135.5991\n",
      "Epoch 2606/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.1120 - val_loss: 128.6702\n",
      "Epoch 2607/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.7116 - val_loss: 177.0572\n",
      "Epoch 2608/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.8908 - val_loss: 150.9807\n",
      "Epoch 2609/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 121.6396 - val_loss: 132.6325\n",
      "Epoch 2610/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.0800 - val_loss: 154.7709\n",
      "Epoch 2611/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.5419 - val_loss: 140.6685\n",
      "Epoch 2612/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.1378 - val_loss: 137.7747\n",
      "Epoch 2613/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.9289 - val_loss: 132.2802\n",
      "Epoch 2614/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.3099 - val_loss: 145.7294\n",
      "Epoch 2615/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.5879 - val_loss: 153.3526\n",
      "Epoch 2616/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.3146 - val_loss: 145.7374\n",
      "Epoch 2617/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.3084 - val_loss: 150.5674\n",
      "Epoch 2618/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 121.6301 - val_loss: 173.1639\n",
      "Epoch 2619/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 120.3489 - val_loss: 139.2204\n",
      "Epoch 2620/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 121.3834 - val_loss: 131.2733\n",
      "Epoch 2621/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.4754 - val_loss: 128.3475\n",
      "Epoch 2622/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.3188 - val_loss: 139.5914\n",
      "Epoch 2623/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.5256 - val_loss: 140.2314\n",
      "Epoch 2624/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 154.7822 - val_loss: 165.5155\n",
      "Epoch 2625/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.4507 - val_loss: 131.5128\n",
      "Epoch 2626/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.3863 - val_loss: 142.2984\n",
      "Epoch 2627/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.3120 - val_loss: 130.8289\n",
      "Epoch 2628/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.9774 - val_loss: 133.7711\n",
      "Epoch 2629/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.1490 - val_loss: 128.4701\n",
      "Epoch 2630/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.6595 - val_loss: 137.4303\n",
      "Epoch 2631/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.1160 - val_loss: 162.0162\n",
      "Epoch 2632/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.8338 - val_loss: 129.7318\n",
      "Epoch 2633/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.7857 - val_loss: 140.9950\n",
      "Epoch 2634/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.6899 - val_loss: 145.8088\n",
      "Epoch 2635/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.3487 - val_loss: 133.0977\n",
      "Epoch 2636/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.8238 - val_loss: 215.4524\n",
      "Epoch 2637/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.6677 - val_loss: 148.0867\n",
      "Epoch 2638/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.1160 - val_loss: 135.9002\n",
      "Epoch 2639/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.4987 - val_loss: 139.9735\n",
      "Epoch 2640/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.1709 - val_loss: 148.5687\n",
      "Epoch 2641/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.1595 - val_loss: 142.7437\n",
      "Epoch 2642/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.5247 - val_loss: 166.1453\n",
      "Epoch 2643/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.7985 - val_loss: 130.5995\n",
      "Epoch 2644/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.6886 - val_loss: 129.7151\n",
      "Epoch 2645/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.6472 - val_loss: 133.6769\n",
      "Epoch 2646/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.3384 - val_loss: 134.2354\n",
      "Epoch 2647/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.3410 - val_loss: 139.7927\n",
      "Epoch 2648/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.2795 - val_loss: 169.9230\n",
      "Epoch 2649/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.2626 - val_loss: 139.9867\n",
      "Epoch 2650/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.0146 - val_loss: 135.7143\n",
      "Epoch 2651/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.0584 - val_loss: 169.0670\n",
      "Epoch 2652/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.0551 - val_loss: 166.2544\n",
      "Epoch 2653/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.3202 - val_loss: 177.8296\n",
      "Epoch 2654/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.0483 - val_loss: 137.5180\n",
      "Epoch 2655/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.3715 - val_loss: 133.9380\n",
      "Epoch 2656/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.2239 - val_loss: 164.0468\n",
      "Epoch 2657/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.2777 - val_loss: 135.5918\n",
      "Epoch 2658/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.6199 - val_loss: 140.0507\n",
      "Epoch 2659/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.8201 - val_loss: 133.2547\n",
      "Epoch 2660/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.3457 - val_loss: 203.4542\n",
      "Epoch 2661/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 186.0977 - val_loss: 139.7262\n",
      "Epoch 2662/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.9763 - val_loss: 133.5541\n",
      "Epoch 2663/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.8046 - val_loss: 135.9050\n",
      "Epoch 2664/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.5736 - val_loss: 134.3782\n",
      "Epoch 2665/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.2868 - val_loss: 130.7040\n",
      "Epoch 2666/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.4538 - val_loss: 130.0589\n",
      "Epoch 2667/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.0717 - val_loss: 141.3159\n",
      "Epoch 2668/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.7743 - val_loss: 151.5308\n",
      "Epoch 2669/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.7620 - val_loss: 134.0897\n",
      "Epoch 2670/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.3929 - val_loss: 144.6071\n",
      "Epoch 2671/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.6576 - val_loss: 129.0221\n",
      "Epoch 2672/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.7679 - val_loss: 155.1940\n",
      "Epoch 2673/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.5048 - val_loss: 132.8870\n",
      "Epoch 2674/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.4032 - val_loss: 161.4349\n",
      "Epoch 2675/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.2387 - val_loss: 161.8786\n",
      "Epoch 2676/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.8948 - val_loss: 136.2008\n",
      "Epoch 2677/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.1688 - val_loss: 168.4286\n",
      "Epoch 2678/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.6279 - val_loss: 147.3992\n",
      "Epoch 2679/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.2473 - val_loss: 141.1732\n",
      "Epoch 2680/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.7167 - val_loss: 133.9216\n",
      "Epoch 2681/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.8103 - val_loss: 140.5204\n",
      "Epoch 2682/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.3071 - val_loss: 138.5180\n",
      "Epoch 2683/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.1836 - val_loss: 127.9580\n",
      "Epoch 2684/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.8797 - val_loss: 156.3454\n",
      "Epoch 2685/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.5002 - val_loss: 137.7958\n",
      "Epoch 2686/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.3435 - val_loss: 131.8868\n",
      "Epoch 2687/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.7550 - val_loss: 138.9941\n",
      "Epoch 2688/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 127.9778 - val_loss: 174.0386\n",
      "Epoch 2689/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.8565 - val_loss: 136.1241\n",
      "Epoch 2690/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.0570 - val_loss: 134.6816\n",
      "Epoch 2691/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.4407 - val_loss: 202.3177\n",
      "Epoch 2692/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.1689 - val_loss: 125.7764\n",
      "Epoch 2693/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.5442 - val_loss: 136.7750\n",
      "Epoch 2694/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.4176 - val_loss: 161.3078\n",
      "Epoch 2695/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 125.5011 - val_loss: 134.7482\n",
      "Epoch 2696/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 118.5484 - val_loss: 165.3674\n",
      "Epoch 2697/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 124.1957 - val_loss: 139.4774\n",
      "Epoch 2698/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 122.5547 - val_loss: 150.3902\n",
      "Epoch 2699/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 122.7371 - val_loss: 133.6744\n",
      "Epoch 2700/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.4674 - val_loss: 138.7104\n",
      "Epoch 2701/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.7695 - val_loss: 140.3801\n",
      "Epoch 2702/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.9731 - val_loss: 144.9439\n",
      "Epoch 2703/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.7655 - val_loss: 147.0655\n",
      "Epoch 2704/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 116.8106 - val_loss: 152.2737\n",
      "Epoch 2705/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.2432 - val_loss: 139.1364\n",
      "Epoch 2706/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 116.2074 - val_loss: 145.1557\n",
      "Epoch 2707/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.7630 - val_loss: 133.4881\n",
      "Epoch 2708/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.8055 - val_loss: 159.7048\n",
      "Epoch 2709/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.0494 - val_loss: 134.6559\n",
      "Epoch 2710/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.8849 - val_loss: 131.5326\n",
      "Epoch 2711/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.3952 - val_loss: 135.3550\n",
      "Epoch 2712/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.6791 - val_loss: 135.8826\n",
      "Epoch 2713/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 121.3809 - val_loss: 162.0784\n",
      "Epoch 2714/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.6500 - val_loss: 148.5117\n",
      "Epoch 2715/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.2327 - val_loss: 156.6101\n",
      "Epoch 2716/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.2811 - val_loss: 183.0658\n",
      "Epoch 2717/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.6910 - val_loss: 130.3530\n",
      "Epoch 2718/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.7943 - val_loss: 137.5254\n",
      "Epoch 2719/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.6493 - val_loss: 149.3478\n",
      "Epoch 2720/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.6861 - val_loss: 216.5935\n",
      "Epoch 2721/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.7815 - val_loss: 143.5947\n",
      "Epoch 2722/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.7450 - val_loss: 186.2816\n",
      "Epoch 2723/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.6725 - val_loss: 127.5225\n",
      "Epoch 2724/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.6787 - val_loss: 151.5252\n",
      "Epoch 2725/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.2683 - val_loss: 134.2796\n",
      "Epoch 2726/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 115.2156 - val_loss: 139.0568\n",
      "Epoch 2727/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.2018 - val_loss: 137.2759\n",
      "Epoch 2728/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.0632 - val_loss: 130.1031\n",
      "Epoch 2729/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.3815 - val_loss: 150.5113\n",
      "Epoch 2730/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.3501 - val_loss: 133.4803\n",
      "Epoch 2731/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.6311 - val_loss: 132.4268\n",
      "Epoch 2732/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.2943 - val_loss: 180.0268\n",
      "Epoch 2733/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.9596 - val_loss: 131.8851\n",
      "Epoch 2734/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.8197 - val_loss: 131.7406\n",
      "Epoch 2735/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.8342 - val_loss: 133.9010\n",
      "Epoch 2736/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.0093 - val_loss: 180.2137\n",
      "Epoch 2737/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.4150 - val_loss: 141.5286\n",
      "Epoch 2738/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.0425 - val_loss: 131.5839\n",
      "Epoch 2739/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.8457 - val_loss: 151.4633\n",
      "Epoch 2740/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.8995 - val_loss: 130.2029\n",
      "Epoch 2741/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.5259 - val_loss: 154.9350\n",
      "Epoch 2742/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.9247 - val_loss: 162.3331\n",
      "Epoch 2743/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.7040 - val_loss: 137.1157\n",
      "Epoch 2744/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.0190 - val_loss: 132.0879\n",
      "Epoch 2745/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.1424 - val_loss: 204.5763\n",
      "Epoch 2746/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.4263 - val_loss: 136.5713\n",
      "Epoch 2747/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 158.0869 - val_loss: 159.0285\n",
      "Epoch 2748/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.5334 - val_loss: 135.5305\n",
      "Epoch 2749/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.9783 - val_loss: 129.3261\n",
      "Epoch 2750/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.2942 - val_loss: 178.4394\n",
      "Epoch 2751/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.8629 - val_loss: 135.6116\n",
      "Epoch 2752/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.8391 - val_loss: 138.3151\n",
      "Epoch 2753/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.0581 - val_loss: 137.5851\n",
      "Epoch 2754/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.1227 - val_loss: 145.0177\n",
      "Epoch 2755/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.3129 - val_loss: 142.2353\n",
      "Epoch 2756/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.9028 - val_loss: 162.0591\n",
      "Epoch 2757/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.2649 - val_loss: 163.9092\n",
      "Epoch 2758/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.0272 - val_loss: 131.0618\n",
      "Epoch 2759/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.0302 - val_loss: 153.5845\n",
      "Epoch 2760/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.2009 - val_loss: 136.6840\n",
      "Epoch 2761/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.6999 - val_loss: 131.6983\n",
      "Epoch 2762/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.4678 - val_loss: 176.2184\n",
      "Epoch 2763/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.4705 - val_loss: 151.9904\n",
      "Epoch 2764/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.6178 - val_loss: 156.2344\n",
      "Epoch 2765/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.7483 - val_loss: 132.7529\n",
      "Epoch 2766/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.0169 - val_loss: 136.9871\n",
      "Epoch 2767/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.9843 - val_loss: 135.1368\n",
      "Epoch 2768/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.9534 - val_loss: 139.0350\n",
      "Epoch 2769/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.0612 - val_loss: 140.0682\n",
      "Epoch 2770/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.6491 - val_loss: 167.3035\n",
      "Epoch 2771/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.6353 - val_loss: 148.6000\n",
      "Epoch 2772/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.8692 - val_loss: 140.8630\n",
      "Epoch 2773/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.8086 - val_loss: 160.3626\n",
      "Epoch 2774/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.6486 - val_loss: 162.8566\n",
      "Epoch 2775/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 116.8526 - val_loss: 130.4154\n",
      "Epoch 2776/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.4111 - val_loss: 133.4377\n",
      "Epoch 2777/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.8540 - val_loss: 148.6021\n",
      "Epoch 2778/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.5407 - val_loss: 151.9156\n",
      "Epoch 2779/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 158.9970 - val_loss: 139.2566\n",
      "Epoch 2780/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.5565 - val_loss: 136.8671\n",
      "Epoch 2781/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.5751 - val_loss: 138.7973\n",
      "Epoch 2782/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.4221 - val_loss: 127.9216\n",
      "Epoch 2783/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.8819 - val_loss: 224.8586\n",
      "Epoch 2784/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.9215 - val_loss: 131.6255\n",
      "Epoch 2785/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.4238 - val_loss: 141.2894\n",
      "Epoch 2786/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 119.3046 - val_loss: 137.6008\n",
      "Epoch 2787/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 188.9628 - val_loss: 151.9740\n",
      "Epoch 2788/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 121.7788 - val_loss: 138.2902\n",
      "Epoch 2789/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 117.3092 - val_loss: 131.5211\n",
      "Epoch 2790/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 115.5934 - val_loss: 143.0354\n",
      "Epoch 2791/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.5114 - val_loss: 155.7357\n",
      "Epoch 2792/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.0871 - val_loss: 145.9443\n",
      "Epoch 2793/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.7564 - val_loss: 138.7386\n",
      "Epoch 2794/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.8532 - val_loss: 174.3739\n",
      "Epoch 2795/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.4866 - val_loss: 129.4625\n",
      "Epoch 2796/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.2859 - val_loss: 127.6545\n",
      "Epoch 2797/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.6835 - val_loss: 186.3667\n",
      "Epoch 2798/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.4643 - val_loss: 135.2204\n",
      "Epoch 2799/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.2796 - val_loss: 149.5276\n",
      "Epoch 2800/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.4001 - val_loss: 169.7863\n",
      "Epoch 2801/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.1440 - val_loss: 143.3904\n",
      "Epoch 2802/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.2499 - val_loss: 171.6819\n",
      "Epoch 2803/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.6070 - val_loss: 136.4907\n",
      "Epoch 2804/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.5054 - val_loss: 126.1052\n",
      "Epoch 2805/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.1028 - val_loss: 135.3282\n",
      "Epoch 2806/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.9036 - val_loss: 132.2659\n",
      "Epoch 2807/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.2866 - val_loss: 140.8729\n",
      "Epoch 2808/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.9842 - val_loss: 137.6373\n",
      "Epoch 2809/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.0685 - val_loss: 201.2068\n",
      "Epoch 2810/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.4350 - val_loss: 139.7328\n",
      "Epoch 2811/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.1430 - val_loss: 152.8954\n",
      "Epoch 2812/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.7038 - val_loss: 140.8462\n",
      "Epoch 2813/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.7655 - val_loss: 132.7752\n",
      "Epoch 2814/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.9889 - val_loss: 188.4454\n",
      "Epoch 2815/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.0656 - val_loss: 180.5341\n",
      "Epoch 2816/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.6739 - val_loss: 141.5978\n",
      "Epoch 2817/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.2417 - val_loss: 134.5584\n",
      "Epoch 2818/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.3332 - val_loss: 129.7494\n",
      "Epoch 2819/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.8203 - val_loss: 132.5668\n",
      "Epoch 2820/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.5473 - val_loss: 131.4365\n",
      "Epoch 2821/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.7363 - val_loss: 147.5537\n",
      "Epoch 2822/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.9853 - val_loss: 132.5503\n",
      "Epoch 2823/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.4483 - val_loss: 132.3621\n",
      "Epoch 2824/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.8411 - val_loss: 139.5668\n",
      "Epoch 2825/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.9978 - val_loss: 139.4106\n",
      "Epoch 2826/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.6603 - val_loss: 148.3287\n",
      "Epoch 2827/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.5939 - val_loss: 147.6480\n",
      "Epoch 2828/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.7941 - val_loss: 178.5214\n",
      "Epoch 2829/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.6456 - val_loss: 170.1510\n",
      "Epoch 2830/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.5178 - val_loss: 236.2234\n",
      "Epoch 2831/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.9270 - val_loss: 167.3476\n",
      "Epoch 2832/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.2936 - val_loss: 129.4156\n",
      "Epoch 2833/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.1411 - val_loss: 139.2097\n",
      "Epoch 2834/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.4310 - val_loss: 173.1823\n",
      "Epoch 2835/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.4177 - val_loss: 139.4150\n",
      "Epoch 2836/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.8248 - val_loss: 132.2475\n",
      "Epoch 2837/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.2355 - val_loss: 144.2839\n",
      "Epoch 2838/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.6553 - val_loss: 155.2594\n",
      "Epoch 2839/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.3783 - val_loss: 139.0438\n",
      "Epoch 2840/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.6296 - val_loss: 136.0810\n",
      "Epoch 2841/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.0652 - val_loss: 135.9162\n",
      "Epoch 2842/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.9017 - val_loss: 133.9916\n",
      "Epoch 2843/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.3570 - val_loss: 164.8090\n",
      "Epoch 2844/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.4286 - val_loss: 149.9271\n",
      "Epoch 2845/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 128.2626 - val_loss: 133.3321\n",
      "Epoch 2846/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.0743 - val_loss: 149.3163\n",
      "Epoch 2847/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.4317 - val_loss: 156.7842\n",
      "Epoch 2848/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.7259 - val_loss: 140.7243\n",
      "Epoch 2849/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.8228 - val_loss: 156.4636\n",
      "Epoch 2850/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.8238 - val_loss: 131.4192\n",
      "Epoch 2851/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.0707 - val_loss: 135.5026\n",
      "Epoch 2852/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.8357 - val_loss: 154.3104\n",
      "Epoch 2853/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.6392 - val_loss: 132.9936\n",
      "Epoch 2854/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 120.8914 - val_loss: 147.6990\n",
      "Epoch 2855/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 127.9015 - val_loss: 130.3643\n",
      "Epoch 2856/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 123.5770 - val_loss: 141.6617\n",
      "Epoch 2857/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 119.9945 - val_loss: 128.9187\n",
      "Epoch 2858/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.8341 - val_loss: 130.3398\n",
      "Epoch 2859/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.9641 - val_loss: 131.0697\n",
      "Epoch 2860/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.3075 - val_loss: 142.2750\n",
      "Epoch 2861/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.1971 - val_loss: 133.7061\n",
      "Epoch 2862/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.0430 - val_loss: 135.2137\n",
      "Epoch 2863/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.4714 - val_loss: 197.7832\n",
      "Epoch 2864/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.6852 - val_loss: 138.7068\n",
      "Epoch 2865/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.3389 - val_loss: 131.3481\n",
      "Epoch 2866/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.6709 - val_loss: 152.2064\n",
      "Epoch 2867/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.7490 - val_loss: 178.5120\n",
      "Epoch 2868/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.9578 - val_loss: 159.5185\n",
      "Epoch 2869/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.9818 - val_loss: 164.7289\n",
      "Epoch 2870/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.3040 - val_loss: 129.4222\n",
      "Epoch 2871/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.0703 - val_loss: 146.9349\n",
      "Epoch 2872/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.6601 - val_loss: 127.3688\n",
      "Epoch 2873/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.5019 - val_loss: 146.9203\n",
      "Epoch 2874/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.7490 - val_loss: 155.3281\n",
      "Epoch 2875/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.0328 - val_loss: 136.6024\n",
      "Epoch 2876/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.6869 - val_loss: 136.0838\n",
      "Epoch 2877/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.1116 - val_loss: 131.6340\n",
      "Epoch 2878/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.7212 - val_loss: 168.7558\n",
      "Epoch 2879/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.8712 - val_loss: 160.1390\n",
      "Epoch 2880/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.2885 - val_loss: 143.6049\n",
      "Epoch 2881/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.1133 - val_loss: 137.7108\n",
      "Epoch 2882/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.2794 - val_loss: 129.9972\n",
      "Epoch 2883/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.6935 - val_loss: 143.0280\n",
      "Epoch 2884/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.0543 - val_loss: 204.5563\n",
      "Epoch 2885/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.4442 - val_loss: 149.8597\n",
      "Epoch 2886/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.1365 - val_loss: 142.3069\n",
      "Epoch 2887/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.0389 - val_loss: 149.6062\n",
      "Epoch 2888/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.0581 - val_loss: 135.6606\n",
      "Epoch 2889/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.2372 - val_loss: 147.6880\n",
      "Epoch 2890/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.0419 - val_loss: 133.6969\n",
      "Epoch 2891/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.3293 - val_loss: 145.0692\n",
      "Epoch 2892/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.7640 - val_loss: 138.9425\n",
      "Epoch 2893/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.5414 - val_loss: 132.2406\n",
      "Epoch 2894/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.3900 - val_loss: 156.7147\n",
      "Epoch 2895/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.9311 - val_loss: 136.5863\n",
      "Epoch 2896/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.5644 - val_loss: 133.8128\n",
      "Epoch 2897/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.3473 - val_loss: 140.2978\n",
      "Epoch 2898/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.6885 - val_loss: 142.0317\n",
      "Epoch 2899/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.8676 - val_loss: 131.9985\n",
      "Epoch 2900/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.5331 - val_loss: 137.6012\n",
      "Epoch 2901/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.5427 - val_loss: 141.5144\n",
      "Epoch 2902/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.6250 - val_loss: 135.5831\n",
      "Epoch 2903/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.8194 - val_loss: 138.3598\n",
      "Epoch 2904/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.5484 - val_loss: 145.1681\n",
      "Epoch 2905/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 115.5592 - val_loss: 143.5117\n",
      "Epoch 2906/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.6229 - val_loss: 153.4036\n",
      "Epoch 2907/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.0007 - val_loss: 145.9160\n",
      "Epoch 2908/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.8481 - val_loss: 151.9891\n",
      "Epoch 2909/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.7574 - val_loss: 219.6572\n",
      "Epoch 2910/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.9031 - val_loss: 137.9489\n",
      "Epoch 2911/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.6212 - val_loss: 138.9500\n",
      "Epoch 2912/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.2338 - val_loss: 138.8726\n",
      "Epoch 2913/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.2406 - val_loss: 147.4085\n",
      "Epoch 2914/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.0036 - val_loss: 132.0467\n",
      "Epoch 2915/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.9636 - val_loss: 149.1577\n",
      "Epoch 2916/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.5447 - val_loss: 129.8821\n",
      "Epoch 2917/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.3812 - val_loss: 156.0963\n",
      "Epoch 2918/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.3334 - val_loss: 133.9985\n",
      "Epoch 2919/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.3836 - val_loss: 143.0327\n",
      "Epoch 2920/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 118.4614 - val_loss: 134.5072\n",
      "Epoch 2921/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.7701 - val_loss: 158.8282\n",
      "Epoch 2922/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.0321 - val_loss: 147.7648\n",
      "Epoch 2923/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.3143 - val_loss: 135.2835\n",
      "Epoch 2924/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.3404 - val_loss: 141.2359\n",
      "Epoch 2925/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.5175 - val_loss: 200.5288\n",
      "Epoch 2926/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.7935 - val_loss: 141.9786\n",
      "Epoch 2927/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 116.9210 - val_loss: 132.1107\n",
      "Epoch 2928/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.9145 - val_loss: 135.3128\n",
      "Epoch 2929/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.2027 - val_loss: 221.1054\n",
      "Epoch 2930/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.1811 - val_loss: 134.8170\n",
      "Epoch 2931/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.1909 - val_loss: 140.6082\n",
      "Epoch 2932/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.8649 - val_loss: 175.7501\n",
      "Epoch 2933/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 120.9208 - val_loss: 128.9895\n",
      "Epoch 2934/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 119.7200 - val_loss: 146.1200\n",
      "Epoch 2935/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 118.7191 - val_loss: 138.7782\n",
      "Epoch 2936/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.3939 - val_loss: 135.8248\n",
      "Epoch 2937/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.2931 - val_loss: 134.4917\n",
      "Epoch 2938/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 124.5667 - val_loss: 163.2445\n",
      "Epoch 2939/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.1900 - val_loss: 147.6346\n",
      "Epoch 2940/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.2986 - val_loss: 160.7522\n",
      "Epoch 2941/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.1898 - val_loss: 145.9889\n",
      "Epoch 2942/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 123.6779 - val_loss: 136.8613\n",
      "Epoch 2943/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.3135 - val_loss: 134.0239\n",
      "Epoch 2944/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.7652 - val_loss: 146.9845\n",
      "Epoch 2945/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.4008 - val_loss: 139.2706\n",
      "Epoch 2946/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.8182 - val_loss: 146.2357\n",
      "Epoch 2947/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.5965 - val_loss: 144.6613\n",
      "Epoch 2948/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.3187 - val_loss: 152.7584\n",
      "Epoch 2949/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 117.9320 - val_loss: 134.0939\n",
      "Epoch 2950/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 120.4092 - val_loss: 130.3614\n",
      "Epoch 2951/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.7060 - val_loss: 129.0942\n",
      "Epoch 2952/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.4950 - val_loss: 185.1109\n",
      "Epoch 2953/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.4853 - val_loss: 144.6594\n",
      "Epoch 2954/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.1930 - val_loss: 134.5184\n",
      "Epoch 2955/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.9289 - val_loss: 158.2746\n",
      "Epoch 2956/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 119.8503 - val_loss: 133.9978\n",
      "Epoch 2957/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.7702 - val_loss: 157.0109\n",
      "Epoch 2958/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.7019 - val_loss: 136.0786\n",
      "Epoch 2959/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.0531 - val_loss: 133.2046\n",
      "Epoch 2960/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.4735 - val_loss: 145.0731\n",
      "Epoch 2961/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 119.8998 - val_loss: 129.4054\n",
      "Epoch 2962/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.5651 - val_loss: 142.9612\n",
      "Epoch 2963/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.8750 - val_loss: 151.3505\n",
      "Epoch 2964/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 116.2133 - val_loss: 135.1151\n",
      "Epoch 2965/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.5867 - val_loss: 149.5445\n",
      "Epoch 2966/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.9085 - val_loss: 149.1246\n",
      "Epoch 2967/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 119.3873 - val_loss: 134.2756\n",
      "Epoch 2968/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.6951 - val_loss: 142.6642\n",
      "Epoch 2969/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.0844 - val_loss: 132.9529\n",
      "Epoch 2970/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.8385 - val_loss: 127.5982\n",
      "Epoch 2971/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.5930 - val_loss: 140.5225\n",
      "Epoch 2972/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.8998 - val_loss: 141.8188\n",
      "Epoch 2973/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.9406 - val_loss: 144.0094\n",
      "Epoch 2974/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.5753 - val_loss: 134.8091\n",
      "Epoch 2975/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.8585 - val_loss: 131.5940\n",
      "Epoch 2976/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.8169 - val_loss: 142.9213\n",
      "Epoch 2977/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.6949 - val_loss: 136.1254\n",
      "Epoch 2978/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.6694 - val_loss: 130.4181\n",
      "Epoch 2979/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.5388 - val_loss: 158.8819\n",
      "Epoch 2980/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.8813 - val_loss: 131.7141\n",
      "Epoch 2981/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.3706 - val_loss: 133.5600\n",
      "Epoch 2982/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.5439 - val_loss: 127.1566\n",
      "Epoch 2983/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.0469 - val_loss: 139.7346\n",
      "Epoch 2984/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.6379 - val_loss: 136.9037\n",
      "Epoch 2985/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.7340 - val_loss: 149.5863\n",
      "Epoch 2986/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.1921 - val_loss: 135.3270\n",
      "Epoch 2987/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.6271 - val_loss: 135.6633\n",
      "Epoch 2988/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 118.4806 - val_loss: 135.8667\n",
      "Epoch 2989/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 117.9432 - val_loss: 136.0957\n",
      "Epoch 2990/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 118.3915 - val_loss: 139.4979\n",
      "Epoch 2991/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 119.9098 - val_loss: 159.6787\n",
      "Epoch 2992/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.7544 - val_loss: 146.0539\n",
      "Epoch 2993/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.2351 - val_loss: 141.1414\n",
      "Epoch 2994/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.0495 - val_loss: 184.7591\n",
      "Epoch 2995/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 264.4775 - val_loss: 149.2785\n",
      "Epoch 2996/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 116.6879 - val_loss: 127.9397\n",
      "Epoch 2997/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.0425 - val_loss: 130.4423\n",
      "Epoch 2998/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 117.0724 - val_loss: 139.9302\n",
      "Epoch 2999/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 116.4342 - val_loss: 131.3049\n",
      "Epoch 3000/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.3049 - val_loss: 140.4067\n",
      "Epoch 3001/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.7087 - val_loss: 135.1898\n",
      "Epoch 3002/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.9648 - val_loss: 134.2249\n",
      "Epoch 3003/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.5094 - val_loss: 130.3252\n",
      "Epoch 3004/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.7242 - val_loss: 138.1907\n",
      "Epoch 3005/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.9831 - val_loss: 133.8969\n",
      "Epoch 3006/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.0136 - val_loss: 130.3963\n",
      "Epoch 3007/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.8709 - val_loss: 155.6607\n",
      "Epoch 3008/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.3841 - val_loss: 135.9582\n",
      "Epoch 3009/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 119.7422 - val_loss: 180.4148\n",
      "Epoch 3010/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 124.5577 - val_loss: 136.7207\n",
      "Epoch 3011/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 122.0701 - val_loss: 130.3770\n",
      "Epoch 3012/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 119.5767 - val_loss: 140.3939\n",
      "Epoch 3013/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 124.3573 - val_loss: 148.3360\n",
      "Epoch 3014/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.8291 - val_loss: 144.0279\n",
      "Epoch 3015/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.8947 - val_loss: 138.4828\n",
      "Epoch 3016/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.2851 - val_loss: 157.0854\n",
      "Epoch 3017/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.4789 - val_loss: 131.0224\n",
      "Epoch 3018/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.2759 - val_loss: 156.5780\n",
      "Epoch 3019/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.0649 - val_loss: 138.3840\n",
      "Epoch 3020/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 191.4527 - val_loss: 139.3952\n",
      "Epoch 3021/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 115.2296 - val_loss: 132.5347\n",
      "Epoch 3022/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 114.4474 - val_loss: 134.4044\n",
      "Epoch 3023/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.7881 - val_loss: 142.6339\n",
      "Epoch 3024/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.2796 - val_loss: 127.8582\n",
      "Epoch 3025/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.3179 - val_loss: 140.4384\n",
      "Epoch 3026/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.2905 - val_loss: 133.1825\n",
      "Epoch 3027/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.9300 - val_loss: 138.3251\n",
      "Epoch 3028/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.6675 - val_loss: 142.8513\n",
      "Epoch 3029/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.8996 - val_loss: 135.2448\n",
      "Epoch 3030/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.3328 - val_loss: 203.3628\n",
      "Epoch 3031/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.1803 - val_loss: 145.3863\n",
      "Epoch 3032/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.1701 - val_loss: 130.1163\n",
      "Epoch 3033/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.6399 - val_loss: 211.3032\n",
      "Epoch 3034/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.8871 - val_loss: 152.3839\n",
      "Epoch 3035/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.8693 - val_loss: 138.0686\n",
      "Epoch 3036/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.5925 - val_loss: 162.7377\n",
      "Epoch 3037/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.5032 - val_loss: 142.4210\n",
      "Epoch 3038/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.6784 - val_loss: 133.4788\n",
      "Epoch 3039/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 116.0001 - val_loss: 132.8215\n",
      "Epoch 3040/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.9857 - val_loss: 146.7376\n",
      "Epoch 3041/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.2309 - val_loss: 138.9876\n",
      "Epoch 3042/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 120.0867 - val_loss: 147.2057\n",
      "Epoch 3043/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.2577 - val_loss: 142.2203\n",
      "Epoch 3044/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.0846 - val_loss: 131.9940\n",
      "Epoch 3045/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.5481 - val_loss: 136.9020\n",
      "Epoch 3046/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.0703 - val_loss: 167.7727\n",
      "Epoch 3047/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.3558 - val_loss: 140.0779\n",
      "Epoch 3048/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.1512 - val_loss: 148.9201\n",
      "Epoch 3049/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.7961 - val_loss: 151.0013\n",
      "Epoch 3050/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.9554 - val_loss: 306.0113\n",
      "Epoch 3051/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.3946 - val_loss: 128.4395\n",
      "Epoch 3052/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.1722 - val_loss: 136.8398\n",
      "Epoch 3053/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.9445 - val_loss: 163.0390\n",
      "Epoch 3054/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.3081 - val_loss: 139.0206\n",
      "Epoch 3055/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.7879 - val_loss: 133.7407\n",
      "Epoch 3056/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.3639 - val_loss: 131.6092\n",
      "Epoch 3057/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.6311 - val_loss: 143.3539\n",
      "Epoch 3058/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.3672 - val_loss: 133.2003\n",
      "Epoch 3059/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.4176 - val_loss: 133.2343\n",
      "Epoch 3060/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.1702 - val_loss: 145.1596\n",
      "Epoch 3061/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 125.9348 - val_loss: 133.0488\n",
      "Epoch 3062/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.6480 - val_loss: 151.3594\n",
      "Epoch 3063/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.1084 - val_loss: 143.5033\n",
      "Epoch 3064/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.8360 - val_loss: 130.6234\n",
      "Epoch 3065/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.3185 - val_loss: 147.2305\n",
      "Epoch 3066/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.6975 - val_loss: 129.8504\n",
      "Epoch 3067/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.0367 - val_loss: 138.8961\n",
      "Epoch 3068/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.1797 - val_loss: 148.0444\n",
      "Epoch 3069/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.1385 - val_loss: 152.1866\n",
      "Epoch 3070/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.0772 - val_loss: 155.5333\n",
      "Epoch 3071/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.8343 - val_loss: 136.1754\n",
      "Epoch 3072/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.7110 - val_loss: 145.8732\n",
      "Epoch 3073/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.6740 - val_loss: 141.0388\n",
      "Epoch 3074/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.8654 - val_loss: 139.9763\n",
      "Epoch 3075/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.9010 - val_loss: 147.2596\n",
      "Epoch 3076/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.0352 - val_loss: 168.6089\n",
      "Epoch 3077/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 122.0793 - val_loss: 142.0804\n",
      "Epoch 3078/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.9293 - val_loss: 151.1419\n",
      "Epoch 3079/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.3588 - val_loss: 132.3838\n",
      "Epoch 3080/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.8631 - val_loss: 222.7448\n",
      "Epoch 3081/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.0610 - val_loss: 150.4599\n",
      "Epoch 3082/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.2825 - val_loss: 131.8814\n",
      "Epoch 3083/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.3759 - val_loss: 152.0570\n",
      "Epoch 3084/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.3465 - val_loss: 144.0757\n",
      "Epoch 3085/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.6492 - val_loss: 154.7493\n",
      "Epoch 3086/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.5971 - val_loss: 129.8119\n",
      "Epoch 3087/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.1015 - val_loss: 133.1603\n",
      "Epoch 3088/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.6645 - val_loss: 129.4662\n",
      "Epoch 3089/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 126.0268 - val_loss: 134.6262\n",
      "Epoch 3090/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 123.7673 - val_loss: 129.0176\n",
      "Epoch 3091/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 123.5437 - val_loss: 144.7078\n",
      "Epoch 3092/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.6918 - val_loss: 128.5212\n",
      "Epoch 3093/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 355.3655 - val_loss: 137.4180\n",
      "Epoch 3094/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 117.9966 - val_loss: 130.6019\n",
      "Epoch 3095/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 117.1139 - val_loss: 128.5298\n",
      "Epoch 3096/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 119.6788 - val_loss: 131.0699\n",
      "Epoch 3097/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 115.8992 - val_loss: 136.2505\n",
      "Epoch 3098/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 116.9001 - val_loss: 145.6506\n",
      "Epoch 3099/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.4953 - val_loss: 136.4547\n",
      "Epoch 3100/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 116.0814 - val_loss: 132.2753\n",
      "Epoch 3101/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.9349 - val_loss: 139.3536\n",
      "Epoch 3102/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.4261 - val_loss: 136.2247\n",
      "Epoch 3103/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.6450 - val_loss: 141.0738\n",
      "Epoch 3104/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 116.8033 - val_loss: 148.2654\n",
      "Epoch 3105/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.1339 - val_loss: 147.1932\n",
      "Epoch 3106/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.4372 - val_loss: 135.2002\n",
      "Epoch 3107/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.4978 - val_loss: 130.7263\n",
      "Epoch 3108/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.4682 - val_loss: 149.8965\n",
      "Epoch 3109/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.7221 - val_loss: 164.0082\n",
      "Epoch 3110/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.3390 - val_loss: 129.1173\n",
      "Epoch 3111/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.2638 - val_loss: 140.4634\n",
      "Epoch 3112/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.0641 - val_loss: 131.3255\n",
      "Epoch 3113/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.5456 - val_loss: 139.3076\n",
      "Epoch 3114/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.7439 - val_loss: 148.9179\n",
      "Epoch 3115/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.2782 - val_loss: 146.7992\n",
      "Epoch 3116/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 121.2627 - val_loss: 131.9693\n",
      "Epoch 3117/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.9090 - val_loss: 133.1091\n",
      "Epoch 3118/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.4574 - val_loss: 137.4269\n",
      "Epoch 3119/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 117.8700 - val_loss: 142.0845\n",
      "Epoch 3120/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.5495 - val_loss: 131.3152\n",
      "Epoch 3121/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.3613 - val_loss: 146.3306\n",
      "Epoch 3122/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.0593 - val_loss: 138.5224\n",
      "Epoch 3123/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.1016 - val_loss: 132.5141\n",
      "Epoch 3124/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.4273 - val_loss: 137.3748\n",
      "Epoch 3125/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.9033 - val_loss: 159.9356\n",
      "Epoch 3126/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.2978 - val_loss: 148.5557\n",
      "Epoch 3127/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.1594 - val_loss: 146.6246\n",
      "Epoch 3128/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.6869 - val_loss: 147.6217\n",
      "Epoch 3129/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.9340 - val_loss: 141.8619\n",
      "Epoch 3130/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.3784 - val_loss: 151.4656\n",
      "Epoch 3131/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.1918 - val_loss: 152.6440\n",
      "Epoch 3132/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.4133 - val_loss: 145.2968\n",
      "Epoch 3133/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.3167 - val_loss: 129.3070\n",
      "Epoch 3134/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.4882 - val_loss: 128.3341\n",
      "Epoch 3135/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.4811 - val_loss: 144.1734\n",
      "Epoch 3136/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.3087 - val_loss: 153.8942\n",
      "Epoch 3137/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.6966 - val_loss: 134.5736\n",
      "Epoch 3138/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.9642 - val_loss: 133.2003\n",
      "Epoch 3139/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.9166 - val_loss: 129.1484\n",
      "Epoch 3140/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.4446 - val_loss: 149.6170\n",
      "Epoch 3141/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.2934 - val_loss: 130.9819\n",
      "Epoch 3142/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.7730 - val_loss: 133.7500\n",
      "Epoch 3143/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.8939 - val_loss: 151.9873\n",
      "Epoch 3144/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.1553 - val_loss: 140.8182\n",
      "Epoch 3145/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.4666 - val_loss: 126.9159\n",
      "Epoch 3146/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.6907 - val_loss: 130.5291\n",
      "Epoch 3147/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.0302 - val_loss: 130.5775\n",
      "Epoch 3148/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 119.1100 - val_loss: 130.3419\n",
      "Epoch 3149/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.4300 - val_loss: 147.5836\n",
      "Epoch 3150/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.0832 - val_loss: 138.4399\n",
      "Epoch 3151/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 117.9987 - val_loss: 177.6817\n",
      "Epoch 3152/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.1329 - val_loss: 134.5912\n",
      "Epoch 3153/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.2438 - val_loss: 155.0021\n",
      "Epoch 3154/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.1142 - val_loss: 144.3313\n",
      "Epoch 3155/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.3117 - val_loss: 158.1196\n",
      "Epoch 3156/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 116.8050 - val_loss: 132.5415\n",
      "Epoch 3157/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.1896 - val_loss: 143.6210\n",
      "Epoch 3158/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 116.6615 - val_loss: 128.1726\n",
      "Epoch 3159/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.0218 - val_loss: 139.3128\n",
      "Epoch 3160/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.7908 - val_loss: 142.9003\n",
      "Epoch 3161/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 117.9444 - val_loss: 137.9547\n",
      "Epoch 3162/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.3361 - val_loss: 132.8614\n",
      "Epoch 3163/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.6611 - val_loss: 140.5805\n",
      "Epoch 3164/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.0083 - val_loss: 134.5580\n",
      "Epoch 3165/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.5964 - val_loss: 130.1850\n",
      "Epoch 3166/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 123.8141 - val_loss: 130.8225\n",
      "Epoch 3167/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 118.1062 - val_loss: 171.8617\n",
      "Epoch 3168/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 122.2762 - val_loss: 177.8678\n",
      "Epoch 3169/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.9408 - val_loss: 129.2038\n",
      "Epoch 3170/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.4852 - val_loss: 136.2337\n",
      "Epoch 3171/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 115.1870 - val_loss: 133.4770\n",
      "Epoch 3172/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.1837 - val_loss: 136.1692\n",
      "Epoch 3173/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.5327 - val_loss: 230.1986\n",
      "Epoch 3174/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.3130 - val_loss: 140.7803\n",
      "Epoch 3175/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.0765 - val_loss: 143.2576\n",
      "Epoch 3176/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.2960 - val_loss: 141.3511\n",
      "Epoch 3177/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.9562 - val_loss: 130.6708\n",
      "Epoch 3178/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 117.7854 - val_loss: 141.2273\n",
      "Epoch 3179/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.4974 - val_loss: 142.8675\n",
      "Epoch 3180/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.2751 - val_loss: 132.2795\n",
      "Epoch 3181/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.3812 - val_loss: 135.9435\n",
      "Epoch 3182/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.3388 - val_loss: 130.4168\n",
      "Epoch 3183/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.4869 - val_loss: 163.6092\n",
      "Epoch 3184/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.4117 - val_loss: 138.2164\n",
      "Epoch 3185/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 119.0783 - val_loss: 160.3433\n",
      "Epoch 3186/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.4472 - val_loss: 153.2212\n",
      "Epoch 3187/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.6510 - val_loss: 143.7858\n",
      "Epoch 3188/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.1048 - val_loss: 133.1144\n",
      "Epoch 3189/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.3844 - val_loss: 132.9969\n",
      "Epoch 3190/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.6361 - val_loss: 133.7496\n",
      "Epoch 3191/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 120.7309 - val_loss: 132.4083\n",
      "Epoch 3192/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.4299 - val_loss: 151.1306\n",
      "Epoch 3193/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.5191 - val_loss: 136.4132\n",
      "Epoch 3194/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.9636 - val_loss: 142.5120\n",
      "Epoch 3195/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 119.3631 - val_loss: 130.9068\n",
      "Epoch 3196/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.4433 - val_loss: 134.5743\n",
      "Epoch 3197/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.2192 - val_loss: 157.8629\n",
      "Epoch 3198/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.9835 - val_loss: 137.8295\n",
      "Epoch 3199/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.4086 - val_loss: 136.8435\n",
      "Epoch 3200/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.5034 - val_loss: 131.1128\n",
      "Epoch 3201/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.1726 - val_loss: 137.3896\n",
      "Epoch 3202/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 117.2139 - val_loss: 132.0881\n",
      "Epoch 3203/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.3735 - val_loss: 132.9363\n",
      "Epoch 3204/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 118.9436 - val_loss: 144.1829\n",
      "Epoch 3205/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.9592 - val_loss: 140.7648\n",
      "Epoch 3206/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.7914 - val_loss: 138.9632\n",
      "Epoch 3207/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.9633 - val_loss: 144.1206\n",
      "Epoch 3208/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.1956 - val_loss: 150.7372\n",
      "Epoch 3209/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.4483 - val_loss: 189.7979\n",
      "Epoch 3210/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.4084 - val_loss: 144.4896\n",
      "Epoch 3211/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.9107 - val_loss: 135.7146\n",
      "Epoch 3212/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.9738 - val_loss: 139.6060\n",
      "Epoch 3213/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.4788 - val_loss: 132.0159\n",
      "Epoch 3214/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.5763 - val_loss: 134.4756\n",
      "Epoch 3215/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.3245 - val_loss: 133.6636\n",
      "Epoch 3216/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.8979 - val_loss: 143.2598\n",
      "Epoch 3217/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.3387 - val_loss: 199.5122\n",
      "Epoch 3218/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.4072 - val_loss: 167.7837\n",
      "Epoch 3219/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.8228 - val_loss: 156.0847\n",
      "Epoch 3220/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.5413 - val_loss: 149.9932\n",
      "Epoch 3221/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.6158 - val_loss: 142.5257\n",
      "Epoch 3222/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.8627 - val_loss: 172.3180\n",
      "Epoch 3223/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 118.2415 - val_loss: 138.8606\n",
      "Epoch 3224/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 116.5826 - val_loss: 132.2786\n",
      "Epoch 3225/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 118.0368 - val_loss: 138.1839\n",
      "Epoch 3226/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.8129 - val_loss: 135.7959\n",
      "Epoch 3227/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.4284 - val_loss: 133.8463\n",
      "Epoch 3228/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.7631 - val_loss: 156.9713\n",
      "Epoch 3229/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.9763 - val_loss: 131.2361\n",
      "Epoch 3230/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 119.6502 - val_loss: 138.8016\n",
      "Epoch 3231/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.1192 - val_loss: 132.2670\n",
      "Epoch 3232/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.9559 - val_loss: 153.6194\n",
      "Epoch 3233/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.9899 - val_loss: 161.5175\n",
      "Epoch 3234/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.2921 - val_loss: 218.5570\n",
      "Epoch 3235/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.4764 - val_loss: 140.9833\n",
      "Epoch 3236/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.5930 - val_loss: 143.1846\n",
      "Epoch 3237/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.6235 - val_loss: 136.8455\n",
      "Epoch 3238/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.7296 - val_loss: 131.1173\n",
      "Epoch 3239/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.0310 - val_loss: 127.2215\n",
      "Epoch 3240/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.7419 - val_loss: 144.2185\n",
      "Epoch 3241/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.5866 - val_loss: 131.9641\n",
      "Epoch 3242/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 119.2689 - val_loss: 145.2581\n",
      "Epoch 3243/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 120.3758 - val_loss: 141.6339\n",
      "Epoch 3244/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 201.6513 - val_loss: 177.7370\n",
      "Epoch 3245/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 115.3216 - val_loss: 130.1604\n",
      "Epoch 3246/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 119.1680 - val_loss: 131.4974\n",
      "Epoch 3247/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 119.0709 - val_loss: 135.9375\n",
      "Epoch 3248/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 116.4836 - val_loss: 139.0928\n",
      "Epoch 3249/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.0202 - val_loss: 134.5917\n",
      "Epoch 3250/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.3340 - val_loss: 131.7751\n",
      "Epoch 3251/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.4436 - val_loss: 136.0467\n",
      "Epoch 3252/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 117.1191 - val_loss: 137.5082\n",
      "Epoch 3253/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 116.7877 - val_loss: 144.6362\n",
      "Epoch 3254/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.8942 - val_loss: 130.3136\n",
      "Epoch 3255/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.9130 - val_loss: 138.3098\n",
      "Epoch 3256/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.7569 - val_loss: 198.0590\n",
      "Epoch 3257/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.1539 - val_loss: 136.8826\n",
      "Epoch 3258/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.2630 - val_loss: 170.8451\n",
      "Epoch 3259/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.2127 - val_loss: 164.6209\n",
      "Epoch 3260/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.3840 - val_loss: 154.5529\n",
      "Epoch 3261/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.5346 - val_loss: 149.5050\n",
      "Epoch 3262/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.2435 - val_loss: 134.6787\n",
      "Epoch 3263/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 157.3142 - val_loss: 172.5209\n",
      "Epoch 3264/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.9543 - val_loss: 135.2366\n",
      "Epoch 3265/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.7720 - val_loss: 134.4201\n",
      "Epoch 3266/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.8632 - val_loss: 132.7621\n",
      "Epoch 3267/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.7169 - val_loss: 135.0685\n",
      "Epoch 3268/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.4765 - val_loss: 129.7496\n",
      "Epoch 3269/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.3217 - val_loss: 133.3307\n",
      "Epoch 3270/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.7681 - val_loss: 129.2674\n",
      "Epoch 3271/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.9942 - val_loss: 184.0595\n",
      "Epoch 3272/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.0269 - val_loss: 134.9634\n",
      "Epoch 3273/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.8391 - val_loss: 137.2148\n",
      "Epoch 3274/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.7421 - val_loss: 159.0564\n",
      "Epoch 3275/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.9679 - val_loss: 145.4583\n",
      "Epoch 3276/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.5167 - val_loss: 138.2234\n",
      "Epoch 3277/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.5924 - val_loss: 131.9801\n",
      "Epoch 3278/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.2133 - val_loss: 140.1936\n",
      "Epoch 3279/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.9315 - val_loss: 134.4322\n",
      "Epoch 3280/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 114.2002 - val_loss: 137.7770\n",
      "Epoch 3281/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.1770 - val_loss: 154.2808\n",
      "Epoch 3282/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 117.3142 - val_loss: 137.6278\n",
      "Epoch 3283/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.5073 - val_loss: 130.6962\n",
      "Epoch 3284/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.5742 - val_loss: 136.1733\n",
      "Epoch 3285/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.0096 - val_loss: 138.5398\n",
      "Epoch 3286/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.0012 - val_loss: 129.4697\n",
      "Epoch 3287/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.1932 - val_loss: 138.4706\n",
      "Epoch 3288/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.8873 - val_loss: 207.5519\n",
      "Epoch 3289/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.2768 - val_loss: 172.3423\n",
      "Epoch 3290/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.7581 - val_loss: 137.5627\n",
      "Epoch 3291/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.5840 - val_loss: 130.1747\n",
      "Epoch 3292/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.1049 - val_loss: 136.0387\n",
      "Epoch 3293/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.0256 - val_loss: 144.4541\n",
      "Epoch 3294/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 119.7199 - val_loss: 131.2725\n",
      "Epoch 3295/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.5923 - val_loss: 164.0556\n",
      "Epoch 3296/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.5209 - val_loss: 134.8859\n",
      "Epoch 3297/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.0980 - val_loss: 178.0715\n",
      "Epoch 3298/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.4707 - val_loss: 133.9651\n",
      "Epoch 3299/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.5777 - val_loss: 147.4901\n",
      "Epoch 3300/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.3064 - val_loss: 142.7517\n",
      "Epoch 3301/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.0743 - val_loss: 130.6760\n",
      "Epoch 3302/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 118.7218 - val_loss: 137.8208\n",
      "Epoch 3303/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.2548 - val_loss: 134.7926\n",
      "Epoch 3304/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.0748 - val_loss: 160.4827\n",
      "Epoch 3305/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.8997 - val_loss: 132.2721\n",
      "Epoch 3306/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.4178 - val_loss: 129.9241\n",
      "Epoch 3307/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.7322 - val_loss: 135.8915\n",
      "Epoch 3308/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.9090 - val_loss: 139.1304\n",
      "Epoch 3309/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 118.7876 - val_loss: 139.7484\n",
      "Epoch 3310/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.0298 - val_loss: 131.6829\n",
      "Epoch 3311/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.5000 - val_loss: 137.4523\n",
      "Epoch 3312/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.9609 - val_loss: 147.8661\n",
      "Epoch 3313/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.8086 - val_loss: 213.2011\n",
      "Epoch 3314/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.2630 - val_loss: 130.3298\n",
      "Epoch 3315/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.8350 - val_loss: 135.3996\n",
      "Epoch 3316/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.4318 - val_loss: 137.3869\n",
      "Epoch 3317/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.3905 - val_loss: 130.6373\n",
      "Epoch 3318/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.8405 - val_loss: 240.5184\n",
      "Epoch 3319/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.5783 - val_loss: 163.5369\n",
      "Epoch 3320/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 123.6382 - val_loss: 192.3087\n",
      "Epoch 3321/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 126.5304 - val_loss: 144.6111\n",
      "Epoch 3322/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 119.8516 - val_loss: 135.2610\n",
      "Epoch 3323/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.2925 - val_loss: 134.8783\n",
      "Epoch 3324/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 122.4978 - val_loss: 135.5986\n",
      "Epoch 3325/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 118.4007 - val_loss: 142.7102\n",
      "Epoch 3326/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.8646 - val_loss: 137.8937\n",
      "Epoch 3327/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.2649 - val_loss: 150.9549\n",
      "Epoch 3328/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.4972 - val_loss: 142.4879\n",
      "Epoch 3329/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.3294 - val_loss: 144.1039\n",
      "Epoch 3330/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 177.5243 - val_loss: 138.1613\n",
      "Epoch 3331/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 115.5772 - val_loss: 140.3758\n",
      "Epoch 3332/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 115.4849 - val_loss: 130.4718\n",
      "Epoch 3333/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.8593 - val_loss: 143.2352\n",
      "Epoch 3334/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 117.0453 - val_loss: 134.3330\n",
      "Epoch 3335/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.7873 - val_loss: 149.8273\n",
      "Epoch 3336/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.6543 - val_loss: 137.5997\n",
      "Epoch 3337/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.2337 - val_loss: 135.5622\n",
      "Epoch 3338/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 123.0152 - val_loss: 136.5725\n",
      "Epoch 3339/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.7220 - val_loss: 159.9452\n",
      "Epoch 3340/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.7675 - val_loss: 144.3136\n",
      "Epoch 3341/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.7949 - val_loss: 152.6306\n",
      "Epoch 3342/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.3952 - val_loss: 129.1456\n",
      "Epoch 3343/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.2868 - val_loss: 136.9834\n",
      "Epoch 3344/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.8158 - val_loss: 130.7772\n",
      "Epoch 3345/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.6101 - val_loss: 294.4378\n",
      "Epoch 3346/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 152.2577 - val_loss: 153.6510\n",
      "Epoch 3347/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 117.9299 - val_loss: 136.7468\n",
      "Epoch 3348/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.9551 - val_loss: 206.6021\n",
      "Epoch 3349/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.2428 - val_loss: 146.8574\n",
      "Epoch 3350/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.4042 - val_loss: 138.1773\n",
      "Epoch 3351/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 116.7396 - val_loss: 130.1072\n",
      "Epoch 3352/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.3285 - val_loss: 137.4326\n",
      "Epoch 3353/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.8378 - val_loss: 147.8910\n",
      "Epoch 3354/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.9924 - val_loss: 132.5731\n",
      "Epoch 3355/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.8582 - val_loss: 141.9847\n",
      "Epoch 3356/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.3275 - val_loss: 142.0217\n",
      "Epoch 3357/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.6060 - val_loss: 130.6423\n",
      "Epoch 3358/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 115.9090 - val_loss: 158.1490\n",
      "Epoch 3359/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.4018 - val_loss: 185.6571\n",
      "Epoch 3360/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.3669 - val_loss: 257.6871\n",
      "Epoch 3361/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.8516 - val_loss: 130.7162\n",
      "Epoch 3362/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 115.8606 - val_loss: 135.9397\n",
      "Epoch 3363/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.9746 - val_loss: 134.7970\n",
      "Epoch 3364/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 113.9324 - val_loss: 132.9793\n",
      "Epoch 3365/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.6705 - val_loss: 178.4882\n",
      "Epoch 3366/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.6382 - val_loss: 142.7803\n",
      "Epoch 3367/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 116.8327 - val_loss: 136.7652\n",
      "Epoch 3368/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.8988 - val_loss: 137.1593\n",
      "Epoch 3369/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.5239 - val_loss: 127.9173\n",
      "Epoch 3370/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.9367 - val_loss: 144.4781\n",
      "Epoch 3371/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.5396 - val_loss: 142.2986\n",
      "Epoch 3372/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.5291 - val_loss: 219.7713\n",
      "Epoch 3373/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.3141 - val_loss: 149.9628\n",
      "Epoch 3374/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.5778 - val_loss: 136.7148\n",
      "Epoch 3375/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 114.6943 - val_loss: 159.0706\n",
      "Epoch 3376/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.7325 - val_loss: 157.7491\n",
      "Epoch 3377/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.7421 - val_loss: 147.0281\n",
      "Epoch 3378/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.7337 - val_loss: 153.1083\n",
      "Epoch 3379/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.2879 - val_loss: 142.0206\n",
      "Epoch 3380/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.5721 - val_loss: 138.3616\n",
      "Epoch 3381/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 116.6858 - val_loss: 128.8950\n",
      "Epoch 3382/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.8991 - val_loss: 130.7038\n",
      "Epoch 3383/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 116.7383 - val_loss: 136.8233\n",
      "Epoch 3384/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.5863 - val_loss: 148.6899\n",
      "Epoch 3385/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.1715 - val_loss: 131.8238\n",
      "Epoch 3386/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 114.5418 - val_loss: 130.6087\n",
      "Epoch 3387/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.9315 - val_loss: 138.7528\n",
      "Epoch 3388/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.9296 - val_loss: 149.4859\n",
      "Epoch 3389/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.5429 - val_loss: 150.3829\n",
      "Epoch 3390/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.5499 - val_loss: 135.0619\n",
      "Epoch 3391/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 147.6870 - val_loss: 186.5921\n",
      "Epoch 3392/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 118.4643 - val_loss: 142.0733\n",
      "Epoch 3393/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.8892 - val_loss: 134.6184\n",
      "Epoch 3394/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.8343 - val_loss: 138.2439\n",
      "Epoch 3395/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.1440 - val_loss: 151.9068\n",
      "Epoch 3396/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.0742 - val_loss: 130.2230\n",
      "Epoch 3397/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.7133 - val_loss: 165.0499\n",
      "Epoch 3398/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.3343 - val_loss: 135.6129\n",
      "Epoch 3399/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 125.3567 - val_loss: 136.4791\n",
      "Epoch 3400/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 122.1825 - val_loss: 138.2625\n",
      "Epoch 3401/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 116.3924 - val_loss: 151.9196\n",
      "Epoch 3402/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 119.2469 - val_loss: 201.8546\n",
      "Epoch 3403/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.7034 - val_loss: 138.5495\n",
      "Epoch 3404/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.5279 - val_loss: 129.4712\n",
      "Epoch 3405/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 115.4299 - val_loss: 154.3874\n",
      "Epoch 3406/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.5307 - val_loss: 167.9071\n",
      "Epoch 3407/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.9665 - val_loss: 149.4675\n",
      "Epoch 3408/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.3668 - val_loss: 137.3668\n",
      "Epoch 3409/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.5792 - val_loss: 152.9147\n",
      "Epoch 3410/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.1675 - val_loss: 131.0194\n",
      "Epoch 3411/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.2929 - val_loss: 168.7725\n",
      "Epoch 3412/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.0554 - val_loss: 150.6254\n",
      "Epoch 3413/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 158.3170 - val_loss: 530.2661\n",
      "Epoch 3414/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 149.1715 - val_loss: 130.9193\n",
      "Epoch 3415/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 118.9719 - val_loss: 139.3248\n",
      "Epoch 3416/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 116.5541 - val_loss: 130.0196\n",
      "Epoch 3417/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.3083 - val_loss: 130.6619\n",
      "Epoch 3418/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.9257 - val_loss: 153.2697\n",
      "Epoch 3419/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.1079 - val_loss: 132.6749\n",
      "Epoch 3420/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.8628 - val_loss: 145.2858\n",
      "Epoch 3421/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.0902 - val_loss: 133.6616\n",
      "Epoch 3422/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.9374 - val_loss: 127.9770\n",
      "Epoch 3423/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.5065 - val_loss: 135.7490\n",
      "Epoch 3424/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.2183 - val_loss: 148.9939\n",
      "Epoch 3425/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.5677 - val_loss: 138.4915\n",
      "Epoch 3426/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.1190 - val_loss: 138.8583\n",
      "Epoch 3427/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 114.5269 - val_loss: 146.2290\n",
      "Epoch 3428/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.7355 - val_loss: 138.2019\n",
      "Epoch 3429/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 247.3070 - val_loss: 131.5471\n",
      "Epoch 3430/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 114.3394 - val_loss: 148.3398\n",
      "Epoch 3431/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 116.6286 - val_loss: 146.6199\n",
      "Epoch 3432/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 116.0719 - val_loss: 138.1271\n",
      "Epoch 3433/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 114.1290 - val_loss: 137.1014\n",
      "Epoch 3434/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.5906 - val_loss: 134.5825\n",
      "Epoch 3435/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 112.1455 - val_loss: 136.4504\n",
      "Epoch 3436/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.5730 - val_loss: 139.8568\n",
      "Epoch 3437/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.2856 - val_loss: 144.3832\n",
      "Epoch 3438/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.1689 - val_loss: 154.2970\n",
      "Epoch 3439/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.8980 - val_loss: 133.2216\n",
      "Epoch 3440/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.8067 - val_loss: 138.0599\n",
      "Epoch 3441/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.7511 - val_loss: 127.8955\n",
      "Epoch 3442/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.1861 - val_loss: 260.5527\n",
      "Epoch 3443/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.3057 - val_loss: 146.6851\n",
      "Epoch 3444/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.2933 - val_loss: 136.3827\n",
      "Epoch 3445/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.5724 - val_loss: 134.9200\n",
      "Epoch 3446/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.4104 - val_loss: 138.6412\n",
      "Epoch 3447/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.6822 - val_loss: 141.5299\n",
      "Epoch 3448/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.2415 - val_loss: 128.2884\n",
      "Epoch 3449/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.0239 - val_loss: 132.8626\n",
      "Epoch 3450/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.0016 - val_loss: 145.0349\n",
      "Epoch 3451/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.6726 - val_loss: 139.3042\n",
      "Epoch 3452/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.8666 - val_loss: 151.1908\n",
      "Epoch 3453/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.3776 - val_loss: 127.2134\n",
      "Epoch 3454/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.7915 - val_loss: 144.5627\n",
      "Epoch 3455/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.7827 - val_loss: 133.1208\n",
      "Epoch 3456/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.0431 - val_loss: 136.5661\n",
      "Epoch 3457/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 116.5603 - val_loss: 141.3320\n",
      "Epoch 3458/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.9812 - val_loss: 144.0783\n",
      "Epoch 3459/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.6271 - val_loss: 148.8173\n",
      "Epoch 3460/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.4081 - val_loss: 144.5267\n",
      "Epoch 3461/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.8824 - val_loss: 150.3912\n",
      "Epoch 3462/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.7981 - val_loss: 130.3099\n",
      "Epoch 3463/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 115.9307 - val_loss: 132.9369\n",
      "Epoch 3464/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.5608 - val_loss: 148.1721\n",
      "Epoch 3465/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.1075 - val_loss: 133.8507\n",
      "Epoch 3466/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.1050 - val_loss: 127.9698\n",
      "Epoch 3467/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.8539 - val_loss: 129.4136\n",
      "Epoch 3468/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.2851 - val_loss: 139.4197\n",
      "Epoch 3469/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.7743 - val_loss: 154.2862\n",
      "Epoch 3470/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.2086 - val_loss: 138.7894\n",
      "Epoch 3471/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.5617 - val_loss: 131.6844\n",
      "Epoch 3472/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.5043 - val_loss: 140.7707\n",
      "Epoch 3473/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.1508 - val_loss: 135.3224\n",
      "Epoch 3474/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.1748 - val_loss: 131.2103\n",
      "Epoch 3475/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.1268 - val_loss: 142.3535\n",
      "Epoch 3476/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.2510 - val_loss: 140.5124\n",
      "Epoch 3477/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 116.9600 - val_loss: 136.9579\n",
      "Epoch 3478/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 128.4816 - val_loss: 187.9146\n",
      "Epoch 3479/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 125.0871 - val_loss: 155.0857\n",
      "Epoch 3480/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 120.8321 - val_loss: 162.4742\n",
      "Epoch 3481/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.3013 - val_loss: 148.4695\n",
      "Epoch 3482/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.1348 - val_loss: 133.8361\n",
      "Epoch 3483/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.4600 - val_loss: 151.8832\n",
      "Epoch 3484/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.9743 - val_loss: 152.8731\n",
      "Epoch 3485/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.4308 - val_loss: 133.9143\n",
      "Epoch 3486/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.0725 - val_loss: 136.8186\n",
      "Epoch 3487/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.4457 - val_loss: 173.0185\n",
      "Epoch 3488/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.8758 - val_loss: 148.6019\n",
      "Epoch 3489/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.1820 - val_loss: 136.9762\n",
      "Epoch 3490/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.0060 - val_loss: 153.0866\n",
      "Epoch 3491/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.0156 - val_loss: 138.1361\n",
      "Epoch 3492/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.1696 - val_loss: 149.5353\n",
      "Epoch 3493/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.4176 - val_loss: 154.0054\n",
      "Epoch 3494/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.6306 - val_loss: 185.5505\n",
      "Epoch 3495/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.1615 - val_loss: 134.0622\n",
      "Epoch 3496/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 118.8777 - val_loss: 129.7627\n",
      "Epoch 3497/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.3598 - val_loss: 148.3668\n",
      "Epoch 3498/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.0302 - val_loss: 151.9636\n",
      "Epoch 3499/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.8139 - val_loss: 159.3047\n",
      "Epoch 3500/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.8448 - val_loss: 195.6772\n",
      "Epoch 3501/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.8132 - val_loss: 133.7862\n",
      "Epoch 3502/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.5988 - val_loss: 152.5940\n",
      "Epoch 3503/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.6206 - val_loss: 129.5099\n",
      "Epoch 3504/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.9186 - val_loss: 138.4781\n",
      "Epoch 3505/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.1963 - val_loss: 142.7660\n",
      "Epoch 3506/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.6523 - val_loss: 140.3875\n",
      "Epoch 3507/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.5431 - val_loss: 128.6158\n",
      "Epoch 3508/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.4573 - val_loss: 135.1474\n",
      "Epoch 3509/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.6288 - val_loss: 140.5097\n",
      "Epoch 3510/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.5110 - val_loss: 137.2475\n",
      "Epoch 3511/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.1254 - val_loss: 136.8664\n",
      "Epoch 3512/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.1680 - val_loss: 141.3366\n",
      "Epoch 3513/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.3402 - val_loss: 156.2182\n",
      "Epoch 3514/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.7524 - val_loss: 173.2080\n",
      "Epoch 3515/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.4463 - val_loss: 136.4123\n",
      "Epoch 3516/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.2532 - val_loss: 137.3303\n",
      "Epoch 3517/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.0293 - val_loss: 134.4203\n",
      "Epoch 3518/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.8080 - val_loss: 132.1649\n",
      "Epoch 3519/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.6499 - val_loss: 155.5070\n",
      "Epoch 3520/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.1556 - val_loss: 128.5933\n",
      "Epoch 3521/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.6821 - val_loss: 134.1771\n",
      "Epoch 3522/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.7796 - val_loss: 140.8750\n",
      "Epoch 3523/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 119.3961 - val_loss: 134.7607\n",
      "Epoch 3524/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.8362 - val_loss: 126.7485\n",
      "Epoch 3525/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.6576 - val_loss: 162.7261\n",
      "Epoch 3526/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.3770 - val_loss: 171.8545\n",
      "Epoch 3527/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.5571 - val_loss: 158.6548\n",
      "Epoch 3528/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.6402 - val_loss: 154.3127\n",
      "Epoch 3529/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.0245 - val_loss: 146.1577\n",
      "Epoch 3530/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.9571 - val_loss: 188.4426\n",
      "Epoch 3531/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.6854 - val_loss: 211.7420\n",
      "Epoch 3532/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.3246 - val_loss: 151.7872\n",
      "Epoch 3533/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.8032 - val_loss: 132.5021\n",
      "Epoch 3534/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.8468 - val_loss: 226.2490\n",
      "Epoch 3535/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.2225 - val_loss: 137.7260\n",
      "Epoch 3536/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.6764 - val_loss: 133.5291\n",
      "Epoch 3537/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 115.0416 - val_loss: 146.8006\n",
      "Epoch 3538/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.5852 - val_loss: 134.8828\n",
      "Epoch 3539/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.3222 - val_loss: 137.3138\n",
      "Epoch 3540/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.1967 - val_loss: 131.6958\n",
      "Epoch 3541/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 115.8906 - val_loss: 141.5826\n",
      "Epoch 3542/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.5249 - val_loss: 175.1710\n",
      "Epoch 3543/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.8895 - val_loss: 166.0797\n",
      "Epoch 3544/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.5974 - val_loss: 158.5834\n",
      "Epoch 3545/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.7068 - val_loss: 139.9644\n",
      "Epoch 3546/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.3434 - val_loss: 158.8717\n",
      "Epoch 3547/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 115.6198 - val_loss: 134.6424\n",
      "Epoch 3548/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.4861 - val_loss: 151.7195\n",
      "Epoch 3549/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.3527 - val_loss: 131.6200\n",
      "Epoch 3550/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.3745 - val_loss: 137.7374\n",
      "Epoch 3551/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.9034 - val_loss: 132.0708\n",
      "Epoch 3552/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.1898 - val_loss: 143.6563\n",
      "Epoch 3553/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.6398 - val_loss: 133.8231\n",
      "Epoch 3554/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.1636 - val_loss: 151.2580\n",
      "Epoch 3555/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 117.6606 - val_loss: 145.0229\n",
      "Epoch 3556/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.5103 - val_loss: 131.7535\n",
      "Epoch 3557/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.1593 - val_loss: 137.0478\n",
      "Epoch 3558/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.4917 - val_loss: 131.9597\n",
      "Epoch 3559/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.2240 - val_loss: 129.6711\n",
      "Epoch 3560/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.5465 - val_loss: 153.3300\n",
      "Epoch 3561/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.7515 - val_loss: 155.6384\n",
      "Epoch 3562/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.1835 - val_loss: 166.9880\n",
      "Epoch 3563/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.6032 - val_loss: 149.9897\n",
      "Epoch 3564/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.7755 - val_loss: 150.5727\n",
      "Epoch 3565/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.5674 - val_loss: 138.6867\n",
      "Epoch 3566/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.6532 - val_loss: 136.8190\n",
      "Epoch 3567/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.6590 - val_loss: 133.2417\n",
      "Epoch 3568/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 147.2014 - val_loss: 246.7131\n",
      "Epoch 3569/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 118.3108 - val_loss: 161.4609\n",
      "Epoch 3570/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 116.7576 - val_loss: 134.3021\n",
      "Epoch 3571/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.6511 - val_loss: 165.4591\n",
      "Epoch 3572/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 115.4785 - val_loss: 132.6528\n",
      "Epoch 3573/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 115.1906 - val_loss: 132.7675\n",
      "Epoch 3574/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.2969 - val_loss: 142.0523\n",
      "Epoch 3575/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.8867 - val_loss: 148.0640\n",
      "Epoch 3576/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 116.0585 - val_loss: 139.5350\n",
      "Epoch 3577/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 116.5330 - val_loss: 144.0166\n",
      "Epoch 3578/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.3540 - val_loss: 158.6228\n",
      "Epoch 3579/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.6241 - val_loss: 155.3561\n",
      "Epoch 3580/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.6129 - val_loss: 140.8097\n",
      "Epoch 3581/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.0796 - val_loss: 133.1466\n",
      "Epoch 3582/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.9638 - val_loss: 134.5634\n",
      "Epoch 3583/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.4840 - val_loss: 148.7649\n",
      "Epoch 3584/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.9075 - val_loss: 129.7644\n",
      "Epoch 3585/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.7917 - val_loss: 125.9459\n",
      "Epoch 3586/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.5122 - val_loss: 127.0611\n",
      "Epoch 3587/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.2433 - val_loss: 151.9073\n",
      "Epoch 3588/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.4606 - val_loss: 132.8273\n",
      "Epoch 3589/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.0010 - val_loss: 136.9678\n",
      "Epoch 3590/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.9782 - val_loss: 146.8144\n",
      "Epoch 3591/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.7803 - val_loss: 150.6125\n",
      "Epoch 3592/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.2307 - val_loss: 133.7105\n",
      "Epoch 3593/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.4757 - val_loss: 139.8772\n",
      "Epoch 3594/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.3698 - val_loss: 147.9862\n",
      "Epoch 3595/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.3435 - val_loss: 194.6642\n",
      "Epoch 3596/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 119.7802 - val_loss: 131.2863\n",
      "Epoch 3597/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.2653 - val_loss: 150.7162\n",
      "Epoch 3598/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.7621 - val_loss: 132.2818\n",
      "Epoch 3599/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.4877 - val_loss: 140.5213\n",
      "Epoch 3600/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 116.2077 - val_loss: 133.8918\n",
      "Epoch 3601/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.9721 - val_loss: 150.2735\n",
      "Epoch 3602/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.8916 - val_loss: 135.7994\n",
      "Epoch 3603/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 113.5425 - val_loss: 150.9276\n",
      "Epoch 3604/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.4743 - val_loss: 144.9465\n",
      "Epoch 3605/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.0357 - val_loss: 138.8229\n",
      "Epoch 3606/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.8253 - val_loss: 133.3325\n",
      "Epoch 3607/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.7746 - val_loss: 146.0596\n",
      "Epoch 3608/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.2849 - val_loss: 143.6586\n",
      "Epoch 3609/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 120.0474 - val_loss: 138.0719\n",
      "Epoch 3610/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.6083 - val_loss: 132.6144\n",
      "Epoch 3611/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.4699 - val_loss: 142.9762\n",
      "Epoch 3612/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.6262 - val_loss: 132.1142\n",
      "Epoch 3613/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.2747 - val_loss: 155.1274\n",
      "Epoch 3614/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.1263 - val_loss: 178.0711\n",
      "Epoch 3615/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.3347 - val_loss: 233.4468\n",
      "Epoch 3616/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.9365 - val_loss: 137.3634\n",
      "Epoch 3617/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.6401 - val_loss: 131.2195\n",
      "Epoch 3618/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.4997 - val_loss: 143.3953\n",
      "Epoch 3619/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.7007 - val_loss: 138.0890\n",
      "Epoch 3620/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.4298 - val_loss: 129.8709\n",
      "Epoch 3621/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.5257 - val_loss: 139.8422\n",
      "Epoch 3622/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.5005 - val_loss: 135.3998\n",
      "Epoch 3623/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.2609 - val_loss: 130.1522\n",
      "Epoch 3624/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.1699 - val_loss: 136.8727\n",
      "Epoch 3625/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.4957 - val_loss: 132.6374\n",
      "Epoch 3626/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.2886 - val_loss: 146.9412\n",
      "Epoch 3627/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.1250 - val_loss: 195.4386\n",
      "Epoch 3628/10000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 119.3110 - val_loss: 142.4298\n",
      "Epoch 3629/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.0509 - val_loss: 144.9396\n",
      "Epoch 3630/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.6937 - val_loss: 133.6637\n",
      "Epoch 3631/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 114.8746 - val_loss: 141.4853\n",
      "Epoch 3632/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.5655 - val_loss: 146.3068\n",
      "Epoch 3633/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.6435 - val_loss: 134.2101\n",
      "Epoch 3634/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 118.5714 - val_loss: 155.5293\n",
      "Epoch 3635/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 121.5292 - val_loss: 133.2700\n",
      "Epoch 3636/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 117.0871 - val_loss: 131.6298\n",
      "Epoch 3637/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 116.9191 - val_loss: 174.6998\n",
      "Epoch 3638/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 120.9595 - val_loss: 130.5476\n",
      "Epoch 3639/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.7307 - val_loss: 150.5994\n",
      "Epoch 3640/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.2981 - val_loss: 170.8311\n",
      "Epoch 3641/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.4535 - val_loss: 144.7412\n",
      "Epoch 3642/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.8373 - val_loss: 138.5976\n",
      "Epoch 3643/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.2981 - val_loss: 158.0275\n",
      "Epoch 3644/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.6596 - val_loss: 136.4900\n",
      "Epoch 3645/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.2872 - val_loss: 136.4172\n",
      "Epoch 3646/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.3585 - val_loss: 130.6238\n",
      "Epoch 3647/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.1556 - val_loss: 142.2569\n",
      "Epoch 3648/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 117.7705 - val_loss: 139.3449\n",
      "Epoch 3649/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.9259 - val_loss: 134.9112\n",
      "Epoch 3650/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.1108 - val_loss: 157.1237\n",
      "Epoch 3651/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.7935 - val_loss: 141.4837\n",
      "Epoch 3652/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.8935 - val_loss: 141.3368\n",
      "Epoch 3653/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 117.4501 - val_loss: 151.2520\n",
      "Epoch 3654/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.1777 - val_loss: 134.8834\n",
      "Epoch 3655/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.5471 - val_loss: 181.6695\n",
      "Epoch 3656/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.0583 - val_loss: 163.3728\n",
      "Epoch 3657/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.8968 - val_loss: 144.0135\n",
      "Epoch 3658/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.8032 - val_loss: 143.9706\n",
      "Epoch 3659/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.2281 - val_loss: 179.3085\n",
      "Epoch 3660/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.1002 - val_loss: 146.1164\n",
      "Epoch 3661/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.4664 - val_loss: 162.7987\n",
      "Epoch 3662/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.3077 - val_loss: 135.9357\n",
      "Epoch 3663/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.7385 - val_loss: 142.5207\n",
      "Epoch 3664/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.4130 - val_loss: 150.2804\n",
      "Epoch 3665/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 116.2564 - val_loss: 141.7409\n",
      "Epoch 3666/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.0687 - val_loss: 169.1391\n",
      "Epoch 3667/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.8705 - val_loss: 159.2721\n",
      "Epoch 3668/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.5501 - val_loss: 162.7401\n",
      "Epoch 3669/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.3634 - val_loss: 185.7763\n",
      "Epoch 3670/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 165.7576 - val_loss: 161.6167\n",
      "Epoch 3671/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.8079 - val_loss: 137.1090\n",
      "Epoch 3672/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.7839 - val_loss: 151.1882\n",
      "Epoch 3673/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 114.7645 - val_loss: 135.9700\n",
      "Epoch 3674/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 114.8322 - val_loss: 207.7701\n",
      "Epoch 3675/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 115.8807 - val_loss: 140.8833\n",
      "Epoch 3676/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.3420 - val_loss: 263.7769\n",
      "Epoch 3677/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.4594 - val_loss: 146.1350\n",
      "Epoch 3678/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 116.6200 - val_loss: 145.7762\n",
      "Epoch 3679/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.9022 - val_loss: 131.0306\n",
      "Epoch 3680/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.3144 - val_loss: 166.7202\n",
      "Epoch 3681/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.1294 - val_loss: 134.2025\n",
      "Epoch 3682/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.0080 - val_loss: 137.6483\n",
      "Epoch 3683/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 115.5898 - val_loss: 143.3161\n",
      "Epoch 3684/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.8103 - val_loss: 216.8186\n",
      "Epoch 3685/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.8285 - val_loss: 147.6938\n",
      "Epoch 3686/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.5991 - val_loss: 142.4059\n",
      "Epoch 3687/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.2514 - val_loss: 162.1348\n",
      "Epoch 3688/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.5422 - val_loss: 146.0790\n",
      "Epoch 3689/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.6087 - val_loss: 130.2889\n",
      "Epoch 3690/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 117.4668 - val_loss: 130.0572\n",
      "Epoch 3691/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.1242 - val_loss: 155.0294\n",
      "Epoch 3692/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 117.7731 - val_loss: 133.1703\n",
      "Epoch 03692: early stopping\n",
      "Fold score (RMSE): 11.324384689331055\n",
      "Fold #4\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 7101.6001 - val_loss: 5410.9119\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 4927.0283 - val_loss: 4999.8705\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 4556.3288 - val_loss: 4377.8522\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 4421.5967 - val_loss: 4545.4695\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 4365.4663 - val_loss: 4595.8445\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 4205.4639 - val_loss: 4171.6242\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 4098.6891 - val_loss: 4122.7185\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 3962.0811 - val_loss: 4071.9026\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 4014.0831 - val_loss: 3894.7529\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 3831.0795 - val_loss: 3961.2586\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 3530.6435 - val_loss: 3245.9331\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 3280.2992 - val_loss: 2589.2132\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 3276.3213 - val_loss: 4065.2686\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 2822.4199 - val_loss: 2530.1735\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 2380.6174 - val_loss: 2591.6177\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 1763.9921 - val_loss: 882.5656\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 1964.9301 - val_loss: 1772.6212\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 1595.9744 - val_loss: 1589.7980\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 1200.4622 - val_loss: 884.6309\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 1237.5785 - val_loss: 773.3703\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 923.4886 - val_loss: 839.6452\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 1057.6850 - val_loss: 773.8752\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 1055.6076 - val_loss: 1198.2085\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 818.0721 - val_loss: 1165.9911\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 667.5764 - val_loss: 449.5410\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 630.6906 - val_loss: 464.7947\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 643.1246 - val_loss: 437.2794\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 671.0914 - val_loss: 423.0181\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 596.2279 - val_loss: 735.3229\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 486.9438 - val_loss: 1150.2402\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 685.5634 - val_loss: 333.0371\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 575.8265 - val_loss: 355.5818\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 561.6682 - val_loss: 647.2281\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 522.3991 - val_loss: 387.4290\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 447.6987 - val_loss: 335.0714\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 637.0793 - val_loss: 663.4753\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 511.9049 - val_loss: 544.8276\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 559.9182 - val_loss: 324.1490\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 525.5730 - val_loss: 361.9868\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 466.6602 - val_loss: 310.7903\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 382.7811 - val_loss: 389.3351\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 522.5320 - val_loss: 481.4468\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 457.7985 - val_loss: 280.1003\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 793.9432 - val_loss: 407.6635\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 453.9844 - val_loss: 353.4478\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 417.3167 - val_loss: 275.4695\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 512.8646 - val_loss: 296.1720\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 509.7378 - val_loss: 492.6570\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 396.9961 - val_loss: 266.1834\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 527.1385 - val_loss: 333.9464\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 509.0382 - val_loss: 676.6631\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 495.2517 - val_loss: 395.0614\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 387.2883 - val_loss: 374.3014\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 454.0098 - val_loss: 389.5158\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 366.3246 - val_loss: 274.4148\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 464.2176 - val_loss: 282.2721\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 393.4545 - val_loss: 287.4723\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 386.8881 - val_loss: 315.2957\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 368.3558 - val_loss: 279.8436\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 375.5684 - val_loss: 726.4610\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 444.2077 - val_loss: 311.4918\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 368.7918 - val_loss: 248.9513\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 397.2928 - val_loss: 246.2566\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 366.6180 - val_loss: 411.8856\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 372.6619 - val_loss: 769.5145\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 368.2086 - val_loss: 219.8849\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 346.0160 - val_loss: 320.8867\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 333.7468 - val_loss: 325.1577\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 376.6394 - val_loss: 880.0011\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 340.1269 - val_loss: 244.1299\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 339.0988 - val_loss: 365.0764\n",
      "Epoch 72/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 370.8961 - val_loss: 220.2423\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 324.9927 - val_loss: 649.6335\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 374.9067 - val_loss: 216.1424\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 325.2782 - val_loss: 478.6103\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 387.1259 - val_loss: 318.0784\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 335.8803 - val_loss: 383.3779\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 366.8528 - val_loss: 305.8011\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 355.9513 - val_loss: 191.8398\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 377.8245 - val_loss: 248.1034\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 302.4508 - val_loss: 192.1341\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 469.9233 - val_loss: 280.7566\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 328.8512 - val_loss: 355.2073\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 276.9934 - val_loss: 188.4229\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 374.5521 - val_loss: 187.3017\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 396.0560 - val_loss: 203.9057\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 314.1581 - val_loss: 214.7114\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 318.2344 - val_loss: 310.5203\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 270.5196 - val_loss: 227.5246\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 275.0975 - val_loss: 264.7624\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 608.7244 - val_loss: 212.2407\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 405.3761 - val_loss: 236.1835\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 327.5656 - val_loss: 184.3162\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 315.6414 - val_loss: 265.3555\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 363.0931 - val_loss: 250.6727\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 286.7804 - val_loss: 240.0538\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 247.9120 - val_loss: 189.0782\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 452.6381 - val_loss: 361.0328\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 253.2791 - val_loss: 182.9881\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 300.0797 - val_loss: 255.1044\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 313.9114 - val_loss: 833.0259\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 323.7638 - val_loss: 179.1667\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 324.8123 - val_loss: 238.7118\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 308.7298 - val_loss: 330.3080\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 333.6509 - val_loss: 197.0007\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 273.2025 - val_loss: 371.9000\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 316.3405 - val_loss: 257.6886\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 335.8491 - val_loss: 226.5107\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 243.1502 - val_loss: 187.0785\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 301.8238 - val_loss: 375.6930\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 319.8625 - val_loss: 246.7994\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 297.5785 - val_loss: 554.8327\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 330.0736 - val_loss: 218.5754\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 252.8128 - val_loss: 282.1340\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 427.6630 - val_loss: 205.4809\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 259.9123 - val_loss: 188.4030\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 294.5169 - val_loss: 235.9228\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 266.8599 - val_loss: 201.8109\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 337.7113 - val_loss: 259.3083\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 252.1302 - val_loss: 292.9241\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 301.5947 - val_loss: 228.8521\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 312.4852 - val_loss: 267.5628\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 368.5624 - val_loss: 236.0270\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 312.2887 - val_loss: 222.6780\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 299.6863 - val_loss: 311.8905\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 374.9041 - val_loss: 274.9551\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 236.9911 - val_loss: 192.0820\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 245.3039 - val_loss: 184.9685\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 282.0098 - val_loss: 213.1305\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 316.3415 - val_loss: 304.5775\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 236.8331 - val_loss: 273.2877\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 305.4004 - val_loss: 386.9158\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 260.6358 - val_loss: 306.9145\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 287.3915 - val_loss: 187.4772\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 300.8056 - val_loss: 264.1183\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 274.2215 - val_loss: 383.6980\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 277.4573 - val_loss: 166.4316\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 229.3795 - val_loss: 186.2297\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 308.4547 - val_loss: 177.0730\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 389.6208 - val_loss: 161.2454\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 278.4245 - val_loss: 188.5194\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 219.0705 - val_loss: 183.2378\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 244.0278 - val_loss: 168.0372\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 231.0926 - val_loss: 246.8573\n",
      "Epoch 145/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 235.7594 - val_loss: 296.4949\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 235.6628 - val_loss: 304.0486\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 240.5458 - val_loss: 163.9540\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 243.9445 - val_loss: 177.4490\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 266.2900 - val_loss: 186.6641\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 435.4417 - val_loss: 272.7416\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 330.6296 - val_loss: 234.9998\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 260.0019 - val_loss: 165.2603\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 228.5289 - val_loss: 175.1276\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 248.4249 - val_loss: 209.0040\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 216.5465 - val_loss: 167.0275\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 275.3843 - val_loss: 207.9636\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 274.1342 - val_loss: 169.9242\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 460.2977 - val_loss: 551.6334\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 284.1616 - val_loss: 547.3891\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 280.6228 - val_loss: 509.0462\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 241.0874 - val_loss: 192.3988\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 354.8545 - val_loss: 192.5630\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 234.8857 - val_loss: 159.3348\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 247.9899 - val_loss: 212.4198\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 214.9000 - val_loss: 167.8927\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 379.6299 - val_loss: 175.2007\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 265.4530 - val_loss: 171.2750\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 221.8201 - val_loss: 163.2055\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 262.6990 - val_loss: 173.4042\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 229.2508 - val_loss: 520.3082\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 304.2268 - val_loss: 192.6111\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 236.1122 - val_loss: 156.5545\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 239.0199 - val_loss: 166.6523\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 214.5045 - val_loss: 190.8244\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 266.8516 - val_loss: 275.1348\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 282.1948 - val_loss: 486.2574\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 242.6076 - val_loss: 273.3774\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 213.9399 - val_loss: 178.7389\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 301.2476 - val_loss: 216.3124\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 258.3339 - val_loss: 280.4022\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 253.0285 - val_loss: 154.2001\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 289.7724 - val_loss: 183.7073\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 234.2363 - val_loss: 192.6810\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 237.2925 - val_loss: 341.5064\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 225.7154 - val_loss: 159.1911\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 209.0091 - val_loss: 207.5332\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 242.5217 - val_loss: 180.7667\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 273.3238 - val_loss: 247.9247\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 208.2227 - val_loss: 270.8805\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 278.5009 - val_loss: 248.1263\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 250.9409 - val_loss: 220.6475\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 193.5602 - val_loss: 206.3837\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 241.2798 - val_loss: 219.8162\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 222.1758 - val_loss: 318.5913\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 241.1496 - val_loss: 207.4386\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 212.2546 - val_loss: 193.7948\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 279.7793 - val_loss: 173.9711\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 221.1901 - val_loss: 160.7980\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 245.5521 - val_loss: 145.9636\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 279.1514 - val_loss: 161.8632\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 224.9799 - val_loss: 220.5197\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 196.6994 - val_loss: 147.7212\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 198.5492 - val_loss: 192.5970\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 283.6449 - val_loss: 181.3201\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 227.5849 - val_loss: 148.5947\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 222.5517 - val_loss: 182.8950\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 267.7869 - val_loss: 321.8522\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 297.7639 - val_loss: 148.0767\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 228.8977 - val_loss: 283.7906\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 267.0076 - val_loss: 143.4570\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 222.0993 - val_loss: 354.0494\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 243.3278 - val_loss: 159.4809\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 245.1724 - val_loss: 192.5696\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 254.6525 - val_loss: 145.6475\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 179.6525 - val_loss: 155.4924\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 239.3293 - val_loss: 147.3256\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 317.0029 - val_loss: 200.1077\n",
      "Epoch 218/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 223.7524 - val_loss: 211.3459\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 249.3432 - val_loss: 145.1129\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 210.1454 - val_loss: 178.2657\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 227.3425 - val_loss: 152.0390\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 246.6054 - val_loss: 634.3067\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 271.6900 - val_loss: 152.3550\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 203.2192 - val_loss: 142.4523\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 189.9167 - val_loss: 148.0394\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 232.2207 - val_loss: 176.8706\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 217.8595 - val_loss: 152.6984\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 213.8610 - val_loss: 150.8615\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 239.0010 - val_loss: 149.6758\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 198.3039 - val_loss: 146.5696\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 331.6137 - val_loss: 190.0986\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 262.2208 - val_loss: 149.5421\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 221.6771 - val_loss: 153.2278\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 209.9224 - val_loss: 150.1216\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 191.3521 - val_loss: 253.7773\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 184.3391 - val_loss: 147.2077\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 230.6514 - val_loss: 181.0673\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 227.7794 - val_loss: 140.7294\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 221.7951 - val_loss: 323.9402\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 257.0579 - val_loss: 160.0831\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 285.2528 - val_loss: 247.4878\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 225.8703 - val_loss: 656.7735\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 218.2669 - val_loss: 195.9677\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 209.6664 - val_loss: 137.6083\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 196.7290 - val_loss: 148.0129\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 199.1976 - val_loss: 138.3911\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 208.5860 - val_loss: 142.0724\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 184.7167 - val_loss: 216.5690\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 205.3699 - val_loss: 143.0811\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 249.7258 - val_loss: 175.9607\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 201.0988 - val_loss: 201.0087\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 239.3671 - val_loss: 163.1887\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 208.1844 - val_loss: 135.5373\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 245.9433 - val_loss: 187.0101\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 232.6278 - val_loss: 205.8036\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 222.0585 - val_loss: 152.4526\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 184.4472 - val_loss: 174.1544\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 260.0935 - val_loss: 213.8916\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 189.1792 - val_loss: 191.3773\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 192.4239 - val_loss: 136.3125\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 218.2444 - val_loss: 140.0914\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 232.5109 - val_loss: 320.7028\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 238.0003 - val_loss: 304.7206\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 221.7995 - val_loss: 177.6959\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 193.5161 - val_loss: 132.6982\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 201.6428 - val_loss: 304.2156\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 221.6921 - val_loss: 175.1963\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 202.4858 - val_loss: 175.8814\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 213.2305 - val_loss: 227.8759\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 252.9886 - val_loss: 163.5892\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 240.7260 - val_loss: 158.5816\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 192.1484 - val_loss: 163.3320\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 224.2610 - val_loss: 235.4499\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 205.0381 - val_loss: 420.4501\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 222.4328 - val_loss: 147.7776\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 195.5567 - val_loss: 241.5857\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 188.6390 - val_loss: 208.5145\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 183.4640 - val_loss: 184.7602\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 205.6141 - val_loss: 304.5266\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 227.6310 - val_loss: 146.3534\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 182.8638 - val_loss: 168.9319\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 199.6597 - val_loss: 188.5065\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 236.2898 - val_loss: 231.0090\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 230.9691 - val_loss: 190.8730\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 234.2517 - val_loss: 184.2197\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 180.8403 - val_loss: 200.0470\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 190.2523 - val_loss: 132.5946\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 278.2027 - val_loss: 202.2749\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 198.7397 - val_loss: 165.8396\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 190.4055 - val_loss: 161.8145\n",
      "Epoch 291/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 201.9954 - val_loss: 199.9538\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 220.6596 - val_loss: 239.1315\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 194.5137 - val_loss: 150.1186\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 201.7168 - val_loss: 137.2369\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 176.2951 - val_loss: 216.4336\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 315.6639 - val_loss: 144.4818\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 186.4935 - val_loss: 159.1198\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 171.9097 - val_loss: 131.0378\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 200.0144 - val_loss: 166.6590\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 186.6599 - val_loss: 134.1376\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 184.0423 - val_loss: 146.8002\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 230.5426 - val_loss: 198.7764\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 219.0853 - val_loss: 139.4987\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 183.0691 - val_loss: 141.1261\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 187.0869 - val_loss: 251.3356\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 202.4483 - val_loss: 155.5520\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 223.3290 - val_loss: 277.1051\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 199.6692 - val_loss: 185.0524\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 248.6543 - val_loss: 324.3822\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 177.0393 - val_loss: 152.1411\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.7798 - val_loss: 134.9719\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 177.7606 - val_loss: 143.3034\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 177.2618 - val_loss: 233.2418\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 279.5280 - val_loss: 632.0516\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 250.7825 - val_loss: 744.6814\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 181.8419 - val_loss: 139.2130\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 180.3470 - val_loss: 142.6606\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 199.7308 - val_loss: 164.9444\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 193.8752 - val_loss: 135.5519\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 173.4458 - val_loss: 394.2902\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 279.1062 - val_loss: 211.5177\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 194.8967 - val_loss: 154.4748\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 168.6226 - val_loss: 144.4746\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 176.9134 - val_loss: 146.1467\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 184.1641 - val_loss: 123.9980\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 205.7183 - val_loss: 163.7553\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 211.5201 - val_loss: 182.2433\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 189.1407 - val_loss: 304.6425\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 190.4987 - val_loss: 154.6574\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 213.4618 - val_loss: 617.7329\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 185.3098 - val_loss: 547.0580\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 187.7712 - val_loss: 133.7793\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 210.3730 - val_loss: 135.0236\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 182.4391 - val_loss: 340.7290\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 270.9118 - val_loss: 127.9557\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 195.6551 - val_loss: 130.6764\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 178.1571 - val_loss: 148.3653\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 182.8959 - val_loss: 149.0977\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 208.7001 - val_loss: 232.1155\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 231.1165 - val_loss: 131.6483\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 167.2993 - val_loss: 142.5865\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 224.4037 - val_loss: 171.1645\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 180.3541 - val_loss: 236.6015\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 279.0681 - val_loss: 170.9042\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 171.3604 - val_loss: 126.8993\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.1767 - val_loss: 144.3850\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 193.3989 - val_loss: 137.2316\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 202.4434 - val_loss: 141.0780\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 167.8048 - val_loss: 141.1296\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 160.9753 - val_loss: 133.7809\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 166.3279 - val_loss: 140.7723\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 188.3929 - val_loss: 185.1435\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 175.5396 - val_loss: 179.1716\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 178.2987 - val_loss: 125.4531\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 204.8743 - val_loss: 190.3612\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 203.8017 - val_loss: 149.9321\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 181.3042 - val_loss: 158.9123\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 212.7626 - val_loss: 174.7536\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 165.8049 - val_loss: 131.8324\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 170.2862 - val_loss: 134.0322\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 230.4213 - val_loss: 169.5341\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.4106 - val_loss: 133.2117\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 182.7695 - val_loss: 139.4761\n",
      "Epoch 364/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 200.7595 - val_loss: 219.3727\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 239.4977 - val_loss: 231.1762\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 165.9890 - val_loss: 130.0065\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 169.6295 - val_loss: 125.5042\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 163.3228 - val_loss: 144.0085\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 223.2802 - val_loss: 184.4870\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 158.8240 - val_loss: 156.5337\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 294.2405 - val_loss: 131.4012\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.4262 - val_loss: 168.4923\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 174.6590 - val_loss: 141.6310\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 177.3532 - val_loss: 136.8691\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 199.3417 - val_loss: 184.6951\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 175.3389 - val_loss: 189.7851\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 178.7303 - val_loss: 148.9517\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 176.0966 - val_loss: 330.2387\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 177.2411 - val_loss: 133.0723\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 174.6518 - val_loss: 128.1377\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 166.3540 - val_loss: 131.5412\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 190.9151 - val_loss: 135.5119\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 174.5956 - val_loss: 145.4092\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 246.8508 - val_loss: 148.0769\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 161.7029 - val_loss: 154.2764\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 173.6336 - val_loss: 131.7167\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 166.4508 - val_loss: 136.8379\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 246.0463 - val_loss: 126.3780\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 179.1054 - val_loss: 141.0969\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 171.7681 - val_loss: 156.2156\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 163.1415 - val_loss: 181.0220\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 209.8655 - val_loss: 158.9493\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 175.3487 - val_loss: 141.8885\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.8919 - val_loss: 319.8490\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 186.3563 - val_loss: 131.1990\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 173.5556 - val_loss: 340.8687\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 182.3370 - val_loss: 127.1791\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 182.1863 - val_loss: 310.5248\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 190.1098 - val_loss: 133.0488\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 156.0346 - val_loss: 138.7154\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 188.2972 - val_loss: 128.5634\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.3006 - val_loss: 147.2892\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 160.4232 - val_loss: 139.7619\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 164.8220 - val_loss: 149.4992\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.3840 - val_loss: 136.1695\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 196.3092 - val_loss: 188.7257\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 168.2445 - val_loss: 120.0517\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 217.1639 - val_loss: 134.7032\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 168.9404 - val_loss: 129.3203\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 195.0896 - val_loss: 192.3625\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 169.4338 - val_loss: 120.4839\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 159.6884 - val_loss: 161.3263\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 179.5804 - val_loss: 161.2481\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 166.4911 - val_loss: 136.0278\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 680.6418 - val_loss: 379.9822\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 224.4588 - val_loss: 240.0540\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 208.0497 - val_loss: 144.2407\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 182.2439 - val_loss: 131.5154\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 201.0048 - val_loss: 126.5603\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 197.7807 - val_loss: 128.8748\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 156.6620 - val_loss: 137.7692\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 194.1112 - val_loss: 165.5634\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.2342 - val_loss: 146.2823\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 166.1556 - val_loss: 128.7100\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 220.4867 - val_loss: 163.8656\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 211.9147 - val_loss: 146.6610\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 180.9487 - val_loss: 138.6493\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 187.0959 - val_loss: 128.4030\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 173.6292 - val_loss: 132.1094\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 213.2410 - val_loss: 140.2466\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.5620 - val_loss: 184.9174\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 175.8154 - val_loss: 152.9894\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 200.7881 - val_loss: 127.9488\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 168.7758 - val_loss: 120.8576\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 193.0953 - val_loss: 123.1909\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 181.7111 - val_loss: 164.8999\n",
      "Epoch 437/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 168.4794 - val_loss: 175.3413\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 226.4292 - val_loss: 158.1455\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 164.617 - 0s 51us/step - loss: 163.4071 - val_loss: 127.3744\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 161.3563 - val_loss: 142.8985\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 165.0296 - val_loss: 141.4571\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 302.4784 - val_loss: 161.9153\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 168.0885 - val_loss: 138.9689\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 159.3391 - val_loss: 133.2471\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 165.3379 - val_loss: 127.4659\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 164.2074 - val_loss: 124.3760\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 157.7269 - val_loss: 180.9801\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 173.1867 - val_loss: 220.2007\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 173.9539 - val_loss: 178.9374\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 255.5650 - val_loss: 129.8530\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 195.9459 - val_loss: 221.4561\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.5013 - val_loss: 175.9980\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 152.7243 - val_loss: 138.5735\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 162.9500 - val_loss: 156.1999\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 153.5598 - val_loss: 150.1233\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 231.5842 - val_loss: 196.0798\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 158.3200 - val_loss: 124.8625\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.8346 - val_loss: 119.8421\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 160.5414 - val_loss: 119.4962\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 158.6926 - val_loss: 149.2100\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 187.1243 - val_loss: 132.4478\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 156.0486 - val_loss: 177.2939\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 158.1597 - val_loss: 123.3761\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 182.7062 - val_loss: 150.7265\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 176.2722 - val_loss: 133.8378\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 162.0725 - val_loss: 143.5060\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 167.7112 - val_loss: 189.5396\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 278.0105 - val_loss: 123.6864\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 171.2761 - val_loss: 154.2381\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 176.3373 - val_loss: 122.2443\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 182.8461 - val_loss: 122.7142\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 166.0742 - val_loss: 120.2680\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 235.1333 - val_loss: 123.9903\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 157.0216 - val_loss: 150.6069\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 161.5096 - val_loss: 142.4575\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 153.6408 - val_loss: 123.1689\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 146.0673 - val_loss: 229.7103\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 159.1186 - val_loss: 169.3776\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 173.9425 - val_loss: 259.3814\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 174.4667 - val_loss: 120.9281\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 191.0330 - val_loss: 137.5097\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.3666 - val_loss: 121.7766\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 154.8516 - val_loss: 137.4088\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 291.2827 - val_loss: 131.3005\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 158.6939 - val_loss: 138.2665\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 168.9818 - val_loss: 164.2021\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 163.4719 - val_loss: 130.4599\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 150.1956 - val_loss: 123.3633\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 159.2127 - val_loss: 133.1341\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 196.9747 - val_loss: 162.1047\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 174.0890 - val_loss: 123.7790\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 166.1000 - val_loss: 127.7633\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 275.2099 - val_loss: 122.1850\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.1675 - val_loss: 125.2761\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 144.7702 - val_loss: 119.8030\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 166.4542 - val_loss: 129.4721\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 164.1949 - val_loss: 137.6653\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 169.1662 - val_loss: 134.1786\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 171.2185 - val_loss: 337.6589\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 175.7431 - val_loss: 173.2759\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 166.6133 - val_loss: 191.9157\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 176.7227 - val_loss: 129.4836\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 158.4861 - val_loss: 203.2124\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 181.2053 - val_loss: 133.7512\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 162.6669 - val_loss: 182.9504\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 164.8753 - val_loss: 124.4582\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 184.3635 - val_loss: 123.8538\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 149.6681 - val_loss: 136.4124\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 155.3971 - val_loss: 131.6522\n",
      "Epoch 510/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 81us/step - loss: 155.9713 - val_loss: 142.9695\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 193.8135 - val_loss: 149.9034\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 159.1609 - val_loss: 137.3432\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 146.4548 - val_loss: 122.9800\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 166.2392 - val_loss: 152.1215\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 262.2812 - val_loss: 117.0834\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 152.4594 - val_loss: 127.0717\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 156.0596 - val_loss: 142.5757\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 156.3872 - val_loss: 133.5962\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 147.9716 - val_loss: 137.5943\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 149.6552 - val_loss: 119.3073\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.9455 - val_loss: 128.1769\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 148.3633 - val_loss: 134.1018\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 159.9788 - val_loss: 135.2980\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 216.1582 - val_loss: 124.1507\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 189.9185 - val_loss: 314.6977\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 150.7643 - val_loss: 132.5216\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 173.2752 - val_loss: 123.2413\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 156.1653 - val_loss: 119.6923\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 146.7573 - val_loss: 118.9287\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 151.1967 - val_loss: 121.7865\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 181.9449 - val_loss: 135.0151\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 150.4449 - val_loss: 226.7870\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 148.0686 - val_loss: 119.1420\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 266.0894 - val_loss: 121.9857\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 154.1603 - val_loss: 117.8580\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 158.1695 - val_loss: 131.7713\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.6341 - val_loss: 153.6139\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 162.4111 - val_loss: 156.8278\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 151.0710 - val_loss: 123.1759\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 161.7664 - val_loss: 121.6921\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 177.4254 - val_loss: 137.7540\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 229.8130 - val_loss: 114.9902\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 156.7322 - val_loss: 127.9035\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 148.2044 - val_loss: 123.6349\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 153.0301 - val_loss: 237.9198\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 172.3735 - val_loss: 127.8818\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 172.5187 - val_loss: 133.0372\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 198.0435 - val_loss: 117.2545\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 149.8672 - val_loss: 151.0757\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 150.5020 - val_loss: 175.0892\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 261.0008 - val_loss: 129.5100\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 162.7220 - val_loss: 183.0830\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 153.6888 - val_loss: 123.2150\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 148.6831 - val_loss: 128.6613\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 154.2724 - val_loss: 132.4195\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 190.2287 - val_loss: 156.6221\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 156.7480 - val_loss: 116.8933\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 148.2339 - val_loss: 140.0493\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 177.7410 - val_loss: 495.4424\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 239.9887 - val_loss: 169.3339\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 144.3048 - val_loss: 141.9440\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 154.5360 - val_loss: 143.6453\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.6134 - val_loss: 125.3989\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 148.5352 - val_loss: 123.6537\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 154.3594 - val_loss: 146.3357\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 177.6654 - val_loss: 138.3465\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 190.6008 - val_loss: 146.5295\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 158.7825 - val_loss: 119.9407\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 158.9400 - val_loss: 249.4099\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 169.8249 - val_loss: 203.4607\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.1673 - val_loss: 121.0457\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 284.3404 - val_loss: 171.4737\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 161.7774 - val_loss: 116.5197\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 158.8358 - val_loss: 142.8816\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 147.0287 - val_loss: 124.2305\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 150.9856 - val_loss: 118.2027\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 157.4770 - val_loss: 134.4944\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 142.9790 - val_loss: 158.1236\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 153.7196 - val_loss: 129.1269\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 146.8981 - val_loss: 116.9410\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 397.4645 - val_loss: 155.2732\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 153.0103 - val_loss: 123.7700\n",
      "Epoch 583/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 156us/step - loss: 147.8100 - val_loss: 132.5264\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 1s 150us/step - loss: 145.8618 - val_loss: 137.9893\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 152.9296 - val_loss: 117.5342\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 151.1574 - val_loss: 128.1675\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 151.7575 - val_loss: 125.8427\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 156.3494 - val_loss: 124.5831\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 144.9069 - val_loss: 116.9912\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 158.7313 - val_loss: 137.4470\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 158.9372 - val_loss: 122.7592\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.6788 - val_loss: 128.1241\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 166.5666 - val_loss: 119.2745\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 195.9398 - val_loss: 244.5548\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.1313 - val_loss: 130.1135\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 150.9442 - val_loss: 119.4487\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.9632 - val_loss: 179.7353\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 162.0181 - val_loss: 144.0053\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 160.8652 - val_loss: 115.2195\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 167.1567 - val_loss: 133.3439\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 160.4746 - val_loss: 133.7112\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.1378 - val_loss: 134.1303\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 178.4437 - val_loss: 135.4490\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 154.0835 - val_loss: 129.6232\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 170.2395 - val_loss: 120.3831\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 158.2948 - val_loss: 123.6270\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 243.1550 - val_loss: 131.7501\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 171.7056 - val_loss: 236.6568\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 155.5247 - val_loss: 124.1689\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 151.6125 - val_loss: 135.3345\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 152.6910 - val_loss: 146.7511\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 163.6263 - val_loss: 227.7433\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 419.3326 - val_loss: 163.2082\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 169.8238 - val_loss: 180.2896\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 185.3986 - val_loss: 213.4824\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 159.9588 - val_loss: 123.9616\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 177.1433 - val_loss: 126.2503\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.4789 - val_loss: 124.0628\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 218.6866 - val_loss: 148.2730\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 182.5687 - val_loss: 127.5694\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.4954 - val_loss: 191.0846\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 153.2344 - val_loss: 176.0946\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 148.1646 - val_loss: 128.8324\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.7099 - val_loss: 125.1134\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 160.8580 - val_loss: 128.2828\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 162.2855 - val_loss: 207.1215\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 193.6064 - val_loss: 125.8060\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 351.1128 - val_loss: 297.0524\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.5414 - val_loss: 138.7204\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 150.3115 - val_loss: 121.7268\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.5561 - val_loss: 117.9197\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 154.3104 - val_loss: 145.4272\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.4665 - val_loss: 125.0187\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 158.3929 - val_loss: 153.7259\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 177.5013 - val_loss: 153.8160\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 147.0315 - val_loss: 124.1047\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 169.2275 - val_loss: 125.7886\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 168.9566 - val_loss: 127.8634\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 162.7858 - val_loss: 121.2024\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 148.3999 - val_loss: 158.2269\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.6985 - val_loss: 393.1050\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 152.8447 - val_loss: 119.5382\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 311.4003 - val_loss: 125.6793\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.0382 - val_loss: 116.8790\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 147.3443 - val_loss: 118.6551\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.4945 - val_loss: 129.0259\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 153.2489 - val_loss: 126.4141\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 166.2278 - val_loss: 138.3331\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 153.7814 - val_loss: 116.3375\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 159.1688 - val_loss: 126.4214\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 148.0311 - val_loss: 127.2716\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 159.7069 - val_loss: 143.6195\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 165.7560 - val_loss: 123.2143\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 152.8040 - val_loss: 128.5401\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 196.4259 - val_loss: 116.9296\n",
      "Epoch 656/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.9457 - val_loss: 241.3473\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.3005 - val_loss: 120.3174\n",
      "Epoch 658/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 154.151 - 0s 51us/step - loss: 154.5678 - val_loss: 126.5789\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 193.5465 - val_loss: 118.6386\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.5393 - val_loss: 136.0263\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 180.0704 - val_loss: 119.7258\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.4790 - val_loss: 120.6147\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 145.3797 - val_loss: 118.3332\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 151.9521 - val_loss: 127.3257\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 172.0827 - val_loss: 159.9085\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 168.4217 - val_loss: 129.4177\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 146.1037 - val_loss: 125.4164\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.3260 - val_loss: 145.8334\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.4971 - val_loss: 139.0357\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 176.8760 - val_loss: 159.7383\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 173.9551 - val_loss: 716.1535\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 198.6709 - val_loss: 130.4760\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 148.1953 - val_loss: 116.3559\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.2339 - val_loss: 152.9309\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.2629 - val_loss: 256.9925\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 157.6807 - val_loss: 150.0479\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.3234 - val_loss: 121.5328\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.6041 - val_loss: 161.7504\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 161.2017 - val_loss: 140.6648\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 267.1501 - val_loss: 116.8972\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.9322 - val_loss: 136.4299\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.7710 - val_loss: 123.3491\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.5222 - val_loss: 118.5693\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.8159 - val_loss: 123.2276\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.0869 - val_loss: 136.1268\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.0309 - val_loss: 148.3414\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 142.3673 - val_loss: 118.4050\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 187.5763 - val_loss: 129.2566\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.5711 - val_loss: 129.3629\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 163.1932 - val_loss: 122.1498\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.8861 - val_loss: 142.7428\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.5956 - val_loss: 129.4931\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 152.9087 - val_loss: 126.0151\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 162.8107 - val_loss: 133.2672\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 264.7096 - val_loss: 133.1798\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 148.0170 - val_loss: 273.5512\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.5057 - val_loss: 129.1363\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.4198 - val_loss: 124.8582\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.1146 - val_loss: 160.8520\n",
      "Epoch 700/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.8509 - val_loss: 129.2143\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 192.9911 - val_loss: 165.1830\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.4094 - val_loss: 127.3708\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.7730 - val_loss: 136.5979\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 148.2028 - val_loss: 127.8302\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 155.3464 - val_loss: 123.6444\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 167.5537 - val_loss: 117.3502\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 278.8925 - val_loss: 136.4742\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 164.0858 - val_loss: 235.1644\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.9626 - val_loss: 120.6093\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 155.8254 - val_loss: 142.0443\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.6142 - val_loss: 131.2956\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 146.2296 - val_loss: 126.3675\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.4060 - val_loss: 175.0844\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 192.6707 - val_loss: 127.6419\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.2255 - val_loss: 128.7741\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 146.7369 - val_loss: 118.3693\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.4638 - val_loss: 129.5486\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 238.7436 - val_loss: 161.0606\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 145.6693 - val_loss: 119.4437\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.8918 - val_loss: 122.3488\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 149.0677 - val_loss: 118.8681\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 241.1809 - val_loss: 130.1447\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.4779 - val_loss: 120.0809\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.9919 - val_loss: 170.4390\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 356.2169 - val_loss: 135.4712\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.9996 - val_loss: 138.5426\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 139.5062 - val_loss: 160.4023\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 143.0289 - val_loss: 148.1623\n",
      "Epoch 729/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 49us/step - loss: 146.9024 - val_loss: 165.5288\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.7280 - val_loss: 128.1574\n",
      "Epoch 731/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 144.9973 - val_loss: 127.1869\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 348.8271 - val_loss: 170.8297\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.1877 - val_loss: 167.8594\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.7929 - val_loss: 155.8320\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.1895 - val_loss: 150.8041\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.8330 - val_loss: 115.1485\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 140.4104 - val_loss: 161.0549\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 150.548 - 0s 57us/step - loss: 151.6264 - val_loss: 122.0004\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 141.1295 - val_loss: 137.6082\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 150.7328 - val_loss: 133.9546\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 197.1494 - val_loss: 123.7156\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 164.4053 - val_loss: 127.9972\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 154.8265 - val_loss: 160.0422\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.2964 - val_loss: 121.7356\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 136.7148 - val_loss: 128.8220\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 140.3045 - val_loss: 122.1536\n",
      "Epoch 747/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 146.4567 - val_loss: 146.6518\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 178.2498 - val_loss: 150.4803\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 203.1773 - val_loss: 137.4125\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 146.6157 - val_loss: 125.4730\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.1458 - val_loss: 120.7267\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.4698 - val_loss: 128.3194\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 148.1809 - val_loss: 133.9921\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 149.2855 - val_loss: 336.3824\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 176.3278 - val_loss: 116.2402\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 185.7386 - val_loss: 248.3526\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 220.8513 - val_loss: 126.2030\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 142.1553 - val_loss: 117.6117\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 151.3554 - val_loss: 117.5591\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 152.1681 - val_loss: 121.2616\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 139.6839 - val_loss: 117.5228\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 141.8086 - val_loss: 118.8312\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 153.5869 - val_loss: 134.9368\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 140.5004 - val_loss: 136.8163\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 148.6713 - val_loss: 131.8085\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 150.9357 - val_loss: 161.3315\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 154.3922 - val_loss: 146.3880\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 187.8838 - val_loss: 126.0972\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 150.7618 - val_loss: 128.8075\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 148.0868 - val_loss: 116.6714\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.3120 - val_loss: 118.9617\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.4607 - val_loss: 125.3186\n",
      "Epoch 773/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 163.1396 - val_loss: 303.7408\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 153.3877 - val_loss: 171.8255\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 199.7422 - val_loss: 148.7574\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.4362 - val_loss: 136.5776\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 152.6534 - val_loss: 128.0068\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.0551 - val_loss: 119.5507\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 144.1097 - val_loss: 117.7295\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 154.7333 - val_loss: 170.5204\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 154.8078 - val_loss: 122.4788\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.8405 - val_loss: 184.2225\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 161.2715 - val_loss: 134.7070\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 216.8642 - val_loss: 131.4214\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.1037 - val_loss: 137.3284\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 164.3276 - val_loss: 122.3900\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.2424 - val_loss: 118.2983\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.5348 - val_loss: 125.5697\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.8705 - val_loss: 122.3226\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.6186 - val_loss: 168.0387\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.9408 - val_loss: 137.8744\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 149.1346 - val_loss: 114.8716\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 171.5614 - val_loss: 125.3501\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.0646 - val_loss: 121.4333\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.0150 - val_loss: 221.7890\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 183.1364 - val_loss: 127.0371\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.1161 - val_loss: 166.2931\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.6098 - val_loss: 121.9876\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.0619 - val_loss: 136.6671\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 192.2437 - val_loss: 128.7731\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.9364 - val_loss: 120.8263\n",
      "Epoch 802/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.2755 - val_loss: 139.1497\n",
      "Epoch 803/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 150.2886 - val_loss: 137.2317\n",
      "Epoch 804/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.2619 - val_loss: 149.1762\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.8667 - val_loss: 181.3040\n",
      "Epoch 806/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.1020 - val_loss: 135.6971\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 186.5225 - val_loss: 194.5248\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.6961 - val_loss: 125.8329\n",
      "Epoch 809/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 154.0646 - val_loss: 166.4433\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.1649 - val_loss: 118.1491\n",
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 187.6589 - val_loss: 160.2956\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.3946 - val_loss: 128.5568\n",
      "Epoch 813/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.6138 - val_loss: 131.6096\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 207.3326 - val_loss: 130.5854\n",
      "Epoch 815/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.2789 - val_loss: 168.1508\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.2767 - val_loss: 123.3207\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.4866 - val_loss: 184.8236\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.5795 - val_loss: 158.4274\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.4297 - val_loss: 155.6078\n",
      "Epoch 820/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.2386 - val_loss: 116.8165\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.1541 - val_loss: 142.7498\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.3094 - val_loss: 155.8372\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.8277 - val_loss: 193.3427\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 171.9382 - val_loss: 408.0512\n",
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 214.4025 - val_loss: 129.1810\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.0296 - val_loss: 172.6412\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 149.6140 - val_loss: 116.6539\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.1701 - val_loss: 176.9873\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.3884 - val_loss: 146.9843\n",
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.5923 - val_loss: 128.2004\n",
      "Epoch 831/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.1927 - val_loss: 144.7845\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.3885 - val_loss: 121.8245\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.9042 - val_loss: 120.4407\n",
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.5348 - val_loss: 124.2247\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.5840 - val_loss: 122.7905\n",
      "Epoch 836/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.0552 - val_loss: 133.1340\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.2183 - val_loss: 130.2937\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 252.6560 - val_loss: 138.3358\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.6348 - val_loss: 172.7327\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.8706 - val_loss: 120.8416\n",
      "Epoch 841/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.7148 - val_loss: 129.4671\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 149.2977 - val_loss: 161.6926\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.3433 - val_loss: 126.4692\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.9778 - val_loss: 124.3730\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.6112 - val_loss: 122.8206\n",
      "Epoch 846/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 168.8098 - val_loss: 142.5133\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.7044 - val_loss: 129.5794\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 152.6618 - val_loss: 138.8536\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 202.7532 - val_loss: 124.6285\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.5247 - val_loss: 137.0345\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.7298 - val_loss: 163.0389\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.5038 - val_loss: 138.0043\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 150.1368 - val_loss: 128.8679\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.9184 - val_loss: 122.1659\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 146.6688 - val_loss: 116.5714\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 145.0510 - val_loss: 119.0676\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 165.9178 - val_loss: 118.0865\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.3337 - val_loss: 150.6745\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 248.5752 - val_loss: 136.4442\n",
      "Epoch 860/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.6865 - val_loss: 156.4954\n",
      "Epoch 861/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 150.5879 - val_loss: 140.3437\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.7185 - val_loss: 197.6508\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.5983 - val_loss: 121.9660\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.0346 - val_loss: 141.0792\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 331.7940 - val_loss: 198.4207\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 205.6590 - val_loss: 188.9503\n",
      "Epoch 867/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.9694 - val_loss: 128.3314\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 193.6299 - val_loss: 141.9989\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 154.3603 - val_loss: 126.0683\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.6850 - val_loss: 163.0992\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.8149 - val_loss: 127.3747\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 151.3280 - val_loss: 148.7784\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 160.0427 - val_loss: 131.2485\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.3505 - val_loss: 130.5950\n",
      "Epoch 875/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.1855 - val_loss: 123.7314\n",
      "Epoch 876/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.1729 - val_loss: 130.6762\n",
      "Epoch 877/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 168.7604 - val_loss: 188.8085\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.6954 - val_loss: 136.6228\n",
      "Epoch 879/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.9133 - val_loss: 134.1240\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.9229 - val_loss: 173.1165\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.0215 - val_loss: 118.0099\n",
      "Epoch 882/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.9976 - val_loss: 150.1591\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 196.5045 - val_loss: 137.4148\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 152.9607 - val_loss: 129.8143\n",
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 158.5018 - val_loss: 119.2632\n",
      "Epoch 886/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.8368 - val_loss: 167.0514\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.9744 - val_loss: 126.6486\n",
      "Epoch 888/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.5913 - val_loss: 119.3081\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.3681 - val_loss: 133.1671\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.0109 - val_loss: 130.1115\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.2541 - val_loss: 123.3302\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 146.8802 - val_loss: 161.6332\n",
      "Epoch 893/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.8114 - val_loss: 132.1856\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 161.7084 - val_loss: 163.4300\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.4885 - val_loss: 144.3700\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 151.8022 - val_loss: 125.9480\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.6087 - val_loss: 120.3117\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 184.8458 - val_loss: 115.6393\n",
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.6920 - val_loss: 130.8492\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 158.5844 - val_loss: 137.6500\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.7351 - val_loss: 125.6207\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.3696 - val_loss: 126.6113\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.2729 - val_loss: 119.7975\n",
      "Epoch 904/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.6849 - val_loss: 126.6332\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 258.8587 - val_loss: 197.8129\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.3448 - val_loss: 132.3061\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.1075 - val_loss: 143.1368\n",
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.2754 - val_loss: 120.0014\n",
      "Epoch 909/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.5289 - val_loss: 136.3257\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.3494 - val_loss: 124.2737\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.4547 - val_loss: 123.1892\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.2007 - val_loss: 215.6246\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 149.2712 - val_loss: 122.2385\n",
      "Epoch 914/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 208.4007 - val_loss: 167.6301\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.4463 - val_loss: 149.8793\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.4037 - val_loss: 144.7019\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.1580 - val_loss: 128.1958\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.3819 - val_loss: 136.4018\n",
      "Epoch 919/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.0434 - val_loss: 148.4154\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 196.7962 - val_loss: 122.8860\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 156.5832 - val_loss: 158.4386\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.8714 - val_loss: 142.4110\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.8826 - val_loss: 143.3921\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.8633 - val_loss: 123.7296\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.9785 - val_loss: 127.3827\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.2595 - val_loss: 118.8264\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 253.6230 - val_loss: 370.5326\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 183.4856 - val_loss: 182.2687\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 165.0861 - val_loss: 135.5434\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 158.4174 - val_loss: 143.6549\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.3575 - val_loss: 136.0086\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 170.0134 - val_loss: 133.4915\n",
      "Epoch 933/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.4100 - val_loss: 130.8652\n",
      "Epoch 934/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.3379 - val_loss: 123.6722\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.3747 - val_loss: 155.0171\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 207.6426 - val_loss: 252.5236\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 164.6269 - val_loss: 323.7419\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 163.9980 - val_loss: 130.9403\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 165.0436 - val_loss: 245.8763\n",
      "Epoch 940/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.7738 - val_loss: 129.7553\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.8360 - val_loss: 120.7023\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.9877 - val_loss: 131.7567\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 417.0489 - val_loss: 147.7550\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 154.2022 - val_loss: 127.6466\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 166.1206 - val_loss: 117.1721\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.0056 - val_loss: 120.6554\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.2931 - val_loss: 132.4135\n",
      "Epoch 948/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step - loss: 149.6316 - val_loss: 132.1603\n",
      "Epoch 949/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.1494 - val_loss: 122.5900\n",
      "Epoch 950/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.4364 - val_loss: 127.3720\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.6389 - val_loss: 134.5737\n",
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.3312 - val_loss: 128.1866\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.5649 - val_loss: 129.3869\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 185.0445 - val_loss: 182.5542\n",
      "Epoch 955/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.9670 - val_loss: 252.7699\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 187.5894 - val_loss: 119.2599\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.4520 - val_loss: 137.6695\n",
      "Epoch 958/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 176.6607 - val_loss: 120.8332\n",
      "Epoch 959/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.5683 - val_loss: 198.0331\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.4082 - val_loss: 155.4333\n",
      "Epoch 961/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.2518 - val_loss: 273.2941\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 356.7891 - val_loss: 158.1358\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.4728 - val_loss: 119.0628\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.6864 - val_loss: 148.7743\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.3648 - val_loss: 141.2491\n",
      "Epoch 966/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.5548 - val_loss: 181.3824\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.8342 - val_loss: 122.0153\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.1455 - val_loss: 146.3102\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 147.6902 - val_loss: 123.1796\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 145.5096 - val_loss: 195.1717\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 148.1550 - val_loss: 117.1874\n",
      "Epoch 972/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.6882 - val_loss: 121.9487\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.1469 - val_loss: 124.9492\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 178.7433 - val_loss: 154.2079\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.0520 - val_loss: 164.1778\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.3424 - val_loss: 166.3297\n",
      "Epoch 977/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.1378 - val_loss: 126.4367\n",
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 165.9045 - val_loss: 115.9401\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.1328 - val_loss: 121.9392\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 163.0628 - val_loss: 128.2448\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 165.2031 - val_loss: 176.2394\n",
      "Epoch 982/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 154.6824 - val_loss: 118.2931\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.2524 - val_loss: 115.6893\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.3705 - val_loss: 117.2621\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 166.5467 - val_loss: 132.0952\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.6529 - val_loss: 122.9883\n",
      "Epoch 987/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.0415 - val_loss: 128.1339\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 186.5883 - val_loss: 122.5480\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.4262 - val_loss: 117.8795\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.6314 - val_loss: 135.0057\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.8845 - val_loss: 140.3420\n",
      "Epoch 992/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.8336 - val_loss: 122.3525\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.1877 - val_loss: 129.9702\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.6238 - val_loss: 118.0831\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 317.8006 - val_loss: 137.1814\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 148.8471 - val_loss: 140.7278\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 152.1807 - val_loss: 182.7589\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.9627 - val_loss: 121.3071\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.5287 - val_loss: 136.9714\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.1709 - val_loss: 190.0960\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.2680 - val_loss: 123.7700\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.9346 - val_loss: 139.7366\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 162.5393 - val_loss: 121.5774\n",
      "Epoch 1004/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.1928 - val_loss: 128.2274\n",
      "Epoch 1005/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.7011 - val_loss: 132.5094\n",
      "Epoch 1006/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.5084 - val_loss: 118.5459\n",
      "Epoch 1007/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.6363 - val_loss: 125.7378\n",
      "Epoch 1008/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 139.1634 - val_loss: 137.3699\n",
      "Epoch 1009/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.1280 - val_loss: 114.0103\n",
      "Epoch 1010/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 148.6666 - val_loss: 227.0993\n",
      "Epoch 1011/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 175.9530 - val_loss: 275.1064\n",
      "Epoch 1012/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.8083 - val_loss: 113.2976\n",
      "Epoch 1013/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 152.4084 - val_loss: 170.8161\n",
      "Epoch 1014/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.4099 - val_loss: 120.0324\n",
      "Epoch 1015/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.1017 - val_loss: 125.5035\n",
      "Epoch 1016/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 175.2662 - val_loss: 126.5865\n",
      "Epoch 1017/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 141.0713 - val_loss: 128.8253\n",
      "Epoch 1018/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.5392 - val_loss: 124.7541\n",
      "Epoch 1019/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.1365 - val_loss: 143.5635\n",
      "Epoch 1020/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.1864 - val_loss: 121.2724\n",
      "Epoch 1021/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 49us/step - loss: 147.4418 - val_loss: 117.3450\n",
      "Epoch 1022/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 156.2813 - val_loss: 187.5367\n",
      "Epoch 1023/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 299.2732 - val_loss: 135.7541\n",
      "Epoch 1024/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.6939 - val_loss: 133.4122\n",
      "Epoch 1025/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.2339 - val_loss: 154.5732\n",
      "Epoch 1026/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.3702 - val_loss: 124.3257\n",
      "Epoch 1027/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.4474 - val_loss: 130.1031\n",
      "Epoch 1028/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.8247 - val_loss: 125.3829\n",
      "Epoch 1029/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.8607 - val_loss: 126.1454\n",
      "Epoch 1030/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 159.3135 - val_loss: 121.7773\n",
      "Epoch 1031/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 149.4033 - val_loss: 203.5188\n",
      "Epoch 1032/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 154.1268 - val_loss: 120.2944\n",
      "Epoch 1033/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.4742 - val_loss: 122.0232\n",
      "Epoch 1034/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.5437 - val_loss: 140.0627\n",
      "Epoch 1035/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.5089 - val_loss: 124.8944\n",
      "Epoch 1036/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 148.3082 - val_loss: 125.3364\n",
      "Epoch 1037/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.8499 - val_loss: 131.7109\n",
      "Epoch 1038/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 143.1900 - val_loss: 157.7838\n",
      "Epoch 1039/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 166.1489 - val_loss: 117.8231\n",
      "Epoch 1040/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 143.0918 - val_loss: 150.4908\n",
      "Epoch 1041/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.4948 - val_loss: 126.4435\n",
      "Epoch 1042/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.5237 - val_loss: 122.2090\n",
      "Epoch 1043/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 156.4587 - val_loss: 128.0480\n",
      "Epoch 1044/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 149.7755 - val_loss: 119.2019\n",
      "Epoch 1045/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.5667 - val_loss: 126.3219\n",
      "Epoch 1046/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.2898 - val_loss: 128.9812\n",
      "Epoch 1047/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.2330 - val_loss: 127.5688\n",
      "Epoch 1048/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 156.9261 - val_loss: 120.2590\n",
      "Epoch 1049/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.8357 - val_loss: 132.0961\n",
      "Epoch 1050/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 139.7951 - val_loss: 117.2739\n",
      "Epoch 1051/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.8437 - val_loss: 142.5870\n",
      "Epoch 1052/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.6151 - val_loss: 124.8623\n",
      "Epoch 1053/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.8643 - val_loss: 118.3075\n",
      "Epoch 1054/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.0652 - val_loss: 119.2245\n",
      "Epoch 1055/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.3075 - val_loss: 127.5111\n",
      "Epoch 1056/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 163.8388 - val_loss: 596.2780\n",
      "Epoch 1057/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 172.4151 - val_loss: 132.6538\n",
      "Epoch 1058/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.7013 - val_loss: 119.3443\n",
      "Epoch 1059/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.6694 - val_loss: 119.1694\n",
      "Epoch 1060/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.7134 - val_loss: 145.5017\n",
      "Epoch 1061/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.4783 - val_loss: 131.6841\n",
      "Epoch 1062/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.7311 - val_loss: 212.1885\n",
      "Epoch 1063/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 153.7631 - val_loss: 119.3998\n",
      "Epoch 1064/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.0264 - val_loss: 130.9678\n",
      "Epoch 1065/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.6155 - val_loss: 140.2061\n",
      "Epoch 1066/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.3144 - val_loss: 121.6156\n",
      "Epoch 1067/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.2307 - val_loss: 139.6138\n",
      "Epoch 1068/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 174.0095 - val_loss: 583.3202\n",
      "Epoch 1069/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 159.1985 - val_loss: 120.0397\n",
      "Epoch 1070/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.4549 - val_loss: 128.2513\n",
      "Epoch 1071/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.0506 - val_loss: 138.5703\n",
      "Epoch 1072/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.6679 - val_loss: 126.0430\n",
      "Epoch 1073/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.6699 - val_loss: 136.5623\n",
      "Epoch 1074/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.1548 - val_loss: 127.5655\n",
      "Epoch 1075/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 143.6729 - val_loss: 112.0554\n",
      "Epoch 1076/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.6341 - val_loss: 150.6049\n",
      "Epoch 1077/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.0908 - val_loss: 127.0659\n",
      "Epoch 1078/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.9129 - val_loss: 131.2765\n",
      "Epoch 1079/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 180.4027 - val_loss: 114.7900\n",
      "Epoch 1080/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.1318 - val_loss: 124.1790\n",
      "Epoch 1081/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.1727 - val_loss: 126.2932\n",
      "Epoch 1082/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 173.6516 - val_loss: 151.2505\n",
      "Epoch 1083/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 183.9865 - val_loss: 197.5344\n",
      "Epoch 1084/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 141.549 - 1s 63us/step - loss: 140.2166 - val_loss: 131.0429\n",
      "Epoch 1085/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.0990 - val_loss: 179.6263\n",
      "Epoch 1086/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 149.2575 - val_loss: 156.7210\n",
      "Epoch 1087/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.5964 - val_loss: 169.2999\n",
      "Epoch 1088/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.3970 - val_loss: 131.6561\n",
      "Epoch 1089/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.6724 - val_loss: 120.4141\n",
      "Epoch 1090/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.1422 - val_loss: 114.6693\n",
      "Epoch 1091/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.8803 - val_loss: 120.5833\n",
      "Epoch 1092/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.1801 - val_loss: 128.9773\n",
      "Epoch 1093/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.2572 - val_loss: 145.5672\n",
      "Epoch 1094/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 201.9986 - val_loss: 118.6384\n",
      "Epoch 1095/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.4592 - val_loss: 116.2566\n",
      "Epoch 1096/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.3074 - val_loss: 122.7570\n",
      "Epoch 1097/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 157.9298 - val_loss: 148.0447\n",
      "Epoch 1098/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 165.4813 - val_loss: 137.8694\n",
      "Epoch 1099/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.4570 - val_loss: 137.9306\n",
      "Epoch 1100/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.6056 - val_loss: 138.2523\n",
      "Epoch 1101/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.9879 - val_loss: 118.8458\n",
      "Epoch 1102/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.1575 - val_loss: 140.5641\n",
      "Epoch 1103/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.4791 - val_loss: 124.9812\n",
      "Epoch 1104/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 157.9067 - val_loss: 126.1468\n",
      "Epoch 1105/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.7367 - val_loss: 130.3090\n",
      "Epoch 1106/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 161.1612 - val_loss: 117.9134\n",
      "Epoch 1107/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.7822 - val_loss: 296.3872\n",
      "Epoch 1108/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 180.2643 - val_loss: 120.6717\n",
      "Epoch 1109/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.6838 - val_loss: 128.0852\n",
      "Epoch 1110/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.1183 - val_loss: 144.8156\n",
      "Epoch 1111/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.4987 - val_loss: 117.1679\n",
      "Epoch 1112/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.1391 - val_loss: 155.1233\n",
      "Epoch 1113/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.6037 - val_loss: 123.5021\n",
      "Epoch 1114/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 198.8371 - val_loss: 124.8680\n",
      "Epoch 1115/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.3253 - val_loss: 115.2899\n",
      "Epoch 1116/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.2555 - val_loss: 147.5931\n",
      "Epoch 1117/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.4104 - val_loss: 122.3856\n",
      "Epoch 1118/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.7725 - val_loss: 124.0265\n",
      "Epoch 1119/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.2168 - val_loss: 125.8815\n",
      "Epoch 1120/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.0252 - val_loss: 139.9721\n",
      "Epoch 1121/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.6819 - val_loss: 124.6403\n",
      "Epoch 1122/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.0241 - val_loss: 125.7856\n",
      "Epoch 1123/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.8658 - val_loss: 121.4497\n",
      "Epoch 1124/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.9296 - val_loss: 116.2734\n",
      "Epoch 1125/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.9866 - val_loss: 132.8196\n",
      "Epoch 1126/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.4340 - val_loss: 126.8957\n",
      "Epoch 1127/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.1264 - val_loss: 134.9247\n",
      "Epoch 1128/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.5135 - val_loss: 123.9923\n",
      "Epoch 1129/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 203.1226 - val_loss: 120.9327\n",
      "Epoch 1130/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.5418 - val_loss: 124.9515\n",
      "Epoch 1131/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.4171 - val_loss: 159.8035\n",
      "Epoch 1132/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.9292 - val_loss: 118.8535\n",
      "Epoch 1133/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.3726 - val_loss: 127.3052\n",
      "Epoch 1134/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.3145 - val_loss: 146.4881\n",
      "Epoch 1135/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 142.1466 - val_loss: 218.7217\n",
      "Epoch 1136/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 143.0729 - val_loss: 120.0124\n",
      "Epoch 1137/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 142.8529 - val_loss: 121.1766\n",
      "Epoch 1138/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.2706 - val_loss: 141.0221\n",
      "Epoch 1139/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.9325 - val_loss: 132.3135\n",
      "Epoch 1140/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 157.5867 - val_loss: 125.9327\n",
      "Epoch 1141/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.8618 - val_loss: 119.6264\n",
      "Epoch 1142/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 143.5342 - val_loss: 131.3606\n",
      "Epoch 1143/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 175.9573 - val_loss: 171.5571\n",
      "Epoch 1144/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.9369 - val_loss: 131.9278\n",
      "Epoch 1145/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 142.4079 - val_loss: 122.8255\n",
      "Epoch 1146/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.7007 - val_loss: 181.8458\n",
      "Epoch 1147/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.5597 - val_loss: 126.2647\n",
      "Epoch 1148/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.9100 - val_loss: 127.9799\n",
      "Epoch 1149/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 193.8680 - val_loss: 122.6695\n",
      "Epoch 1150/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.8366 - val_loss: 124.2168\n",
      "Epoch 1151/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.0027 - val_loss: 133.6508\n",
      "Epoch 1152/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 148.7931 - val_loss: 152.7142\n",
      "Epoch 1153/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.8474 - val_loss: 118.8492\n",
      "Epoch 1154/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.2692 - val_loss: 130.2413\n",
      "Epoch 1155/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.9810 - val_loss: 125.7959\n",
      "Epoch 1156/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.7531 - val_loss: 123.8977\n",
      "Epoch 1157/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.5881 - val_loss: 176.0550\n",
      "Epoch 1158/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.4353 - val_loss: 130.6183\n",
      "Epoch 1159/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 141.7745 - val_loss: 115.7977\n",
      "Epoch 1160/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.3094 - val_loss: 158.6538\n",
      "Epoch 1161/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 176.0284 - val_loss: 125.4104\n",
      "Epoch 1162/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 132.5617 - val_loss: 119.1194\n",
      "Epoch 1163/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.4226 - val_loss: 137.4860\n",
      "Epoch 1164/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.5794 - val_loss: 127.6605\n",
      "Epoch 1165/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 159.2010 - val_loss: 126.7613\n",
      "Epoch 1166/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.7000 - val_loss: 145.4499\n",
      "Epoch 1167/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.4590 - val_loss: 115.1639\n",
      "Epoch 1168/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.0713 - val_loss: 117.9373\n",
      "Epoch 1169/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 201.9532 - val_loss: 142.3858\n",
      "Epoch 1170/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.4418 - val_loss: 125.4013\n",
      "Epoch 1171/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.7356 - val_loss: 115.8480\n",
      "Epoch 1172/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.2238 - val_loss: 138.0550\n",
      "Epoch 1173/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.1461 - val_loss: 127.1528\n",
      "Epoch 1174/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.7271 - val_loss: 123.4270\n",
      "Epoch 1175/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.6538 - val_loss: 125.6034\n",
      "Epoch 1176/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.3779 - val_loss: 187.1069\n",
      "Epoch 1177/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.4595 - val_loss: 136.3755\n",
      "Epoch 1178/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.0567 - val_loss: 127.7868\n",
      "Epoch 1179/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 190.7511 - val_loss: 139.5318\n",
      "Epoch 1180/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.6083 - val_loss: 116.5966\n",
      "Epoch 1181/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.6532 - val_loss: 118.2277\n",
      "Epoch 1182/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.4407 - val_loss: 132.7742\n",
      "Epoch 1183/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.3032 - val_loss: 192.2011\n",
      "Epoch 1184/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.9944 - val_loss: 143.1516\n",
      "Epoch 1185/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 151.8172 - val_loss: 135.0904\n",
      "Epoch 1186/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.4180 - val_loss: 127.1041\n",
      "Epoch 1187/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.8503 - val_loss: 116.3937\n",
      "Epoch 1188/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 155.2876 - val_loss: 116.4382\n",
      "Epoch 1189/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.2104 - val_loss: 172.1069\n",
      "Epoch 1190/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.4693 - val_loss: 137.7819\n",
      "Epoch 1191/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 142.5791 - val_loss: 193.0370\n",
      "Epoch 1192/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.8726 - val_loss: 119.9835\n",
      "Epoch 1193/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.3979 - val_loss: 140.2598\n",
      "Epoch 1194/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.2278 - val_loss: 132.0505\n",
      "Epoch 1195/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 155.5325 - val_loss: 122.1339\n",
      "Epoch 1196/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.8911 - val_loss: 123.4196\n",
      "Epoch 1197/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.5980 - val_loss: 132.7840\n",
      "Epoch 1198/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.0488 - val_loss: 120.1471\n",
      "Epoch 1199/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 140.7605 - val_loss: 115.7728\n",
      "Epoch 1200/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.3907 - val_loss: 114.8935\n",
      "Epoch 1201/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.1660 - val_loss: 126.1164\n",
      "Epoch 1202/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 336.8406 - val_loss: 155.4516\n",
      "Epoch 1203/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 166.1256 - val_loss: 138.5469\n",
      "Epoch 1204/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.9156 - val_loss: 150.2790\n",
      "Epoch 1205/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.0362 - val_loss: 114.8399\n",
      "Epoch 1206/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.0029 - val_loss: 116.5760\n",
      "Epoch 1207/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.2325 - val_loss: 117.0817\n",
      "Epoch 1208/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.0894 - val_loss: 120.4453\n",
      "Epoch 1209/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.1737 - val_loss: 150.3369\n",
      "Epoch 1210/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.7710 - val_loss: 121.8015\n",
      "Epoch 1211/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.6077 - val_loss: 122.7611\n",
      "Epoch 1212/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.9324 - val_loss: 135.0359\n",
      "Epoch 1213/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.3783 - val_loss: 119.1319\n",
      "Epoch 1214/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.2187 - val_loss: 121.4955\n",
      "Epoch 1215/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.4881 - val_loss: 119.6352\n",
      "Epoch 1216/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.8938 - val_loss: 158.6439\n",
      "Epoch 1217/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.7936 - val_loss: 132.8302\n",
      "Epoch 1218/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.1309 - val_loss: 167.0296\n",
      "Epoch 1219/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.9854 - val_loss: 143.7661\n",
      "Epoch 1220/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.9018 - val_loss: 140.9367\n",
      "Epoch 1221/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 161.8906 - val_loss: 118.9324\n",
      "Epoch 1222/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.7735 - val_loss: 142.8455\n",
      "Epoch 1223/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.8205 - val_loss: 138.4458\n",
      "Epoch 1224/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.0666 - val_loss: 117.6485\n",
      "Epoch 1225/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 244.8201 - val_loss: 141.0269\n",
      "Epoch 1226/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.8487 - val_loss: 132.2647\n",
      "Epoch 1227/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.5177 - val_loss: 140.9286\n",
      "Epoch 1228/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.8523 - val_loss: 124.5441\n",
      "Epoch 1229/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.8160 - val_loss: 126.9067\n",
      "Epoch 1230/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.5636 - val_loss: 182.5250\n",
      "Epoch 1231/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 155.0494 - val_loss: 126.4902\n",
      "Epoch 1232/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.9137 - val_loss: 124.4405\n",
      "Epoch 1233/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.4397 - val_loss: 151.0523\n",
      "Epoch 1234/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.4713 - val_loss: 139.6918\n",
      "Epoch 1235/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.3115 - val_loss: 123.3564\n",
      "Epoch 1236/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.0999 - val_loss: 163.6531\n",
      "Epoch 1237/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 63us/step - loss: 147.5924 - val_loss: 120.8201\n",
      "Epoch 1238/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 139.3029 - val_loss: 121.6107\n",
      "Epoch 1239/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 315.3056 - val_loss: 124.4032\n",
      "Epoch 1240/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 141.3284 - val_loss: 135.7052\n",
      "Epoch 1241/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.0975 - val_loss: 124.4508\n",
      "Epoch 1242/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.2089 - val_loss: 133.5325\n",
      "Epoch 1243/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.8099 - val_loss: 190.2558\n",
      "Epoch 1244/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.2111 - val_loss: 114.5887\n",
      "Epoch 1245/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 142.3024 - val_loss: 142.9252\n",
      "Epoch 1246/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.4056 - val_loss: 115.1269\n",
      "Epoch 1247/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.2693 - val_loss: 119.1664\n",
      "Epoch 1248/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.1933 - val_loss: 121.8040\n",
      "Epoch 1249/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.6508 - val_loss: 117.6079\n",
      "Epoch 1250/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.4358 - val_loss: 131.9525\n",
      "Epoch 1251/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.8076 - val_loss: 119.0902\n",
      "Epoch 1252/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.3765 - val_loss: 137.9825\n",
      "Epoch 1253/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.7373 - val_loss: 158.8751\n",
      "Epoch 1254/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.8473 - val_loss: 122.1840\n",
      "Epoch 1255/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 148.9476 - val_loss: 121.4529\n",
      "Epoch 1256/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.9568 - val_loss: 254.5991\n",
      "Epoch 1257/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 158.9605 - val_loss: 146.4401\n",
      "Epoch 1258/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.6289 - val_loss: 161.5932\n",
      "Epoch 1259/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.2007 - val_loss: 132.3709\n",
      "Epoch 1260/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.8855 - val_loss: 140.8685\n",
      "Epoch 1261/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.7410 - val_loss: 316.5349\n",
      "Epoch 1262/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 214.8773 - val_loss: 120.5211\n",
      "Epoch 1263/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.2172 - val_loss: 122.2269\n",
      "Epoch 1264/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.9706 - val_loss: 120.6629\n",
      "Epoch 1265/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.9430 - val_loss: 120.7498\n",
      "Epoch 1266/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.5934 - val_loss: 133.9492\n",
      "Epoch 1267/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.2573 - val_loss: 135.8379\n",
      "Epoch 1268/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.2890 - val_loss: 123.7827\n",
      "Epoch 1269/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.4994 - val_loss: 122.2960\n",
      "Epoch 1270/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.7751 - val_loss: 171.8422\n",
      "Epoch 1271/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.0015 - val_loss: 135.6271\n",
      "Epoch 1272/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.2916 - val_loss: 173.9126\n",
      "Epoch 1273/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.5378 - val_loss: 117.7600\n",
      "Epoch 1274/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.5604 - val_loss: 124.4884\n",
      "Epoch 1275/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.5013 - val_loss: 118.5783\n",
      "Epoch 1276/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 188.1277 - val_loss: 117.1323\n",
      "Epoch 1277/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.4075 - val_loss: 113.8854\n",
      "Epoch 1278/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.2623 - val_loss: 150.5698\n",
      "Epoch 1279/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.9891 - val_loss: 114.2989\n",
      "Epoch 1280/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.5412 - val_loss: 119.5567\n",
      "Epoch 1281/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.7433 - val_loss: 119.5595\n",
      "Epoch 1282/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 166.9919 - val_loss: 136.2139\n",
      "Epoch 1283/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.3361 - val_loss: 164.3070\n",
      "Epoch 1284/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.9771 - val_loss: 116.1638\n",
      "Epoch 1285/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 155.9417 - val_loss: 125.6794\n",
      "Epoch 1286/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 183.7498 - val_loss: 166.6521\n",
      "Epoch 1287/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.3094 - val_loss: 116.7479\n",
      "Epoch 1288/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.5612 - val_loss: 115.7369\n",
      "Epoch 1289/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.4045 - val_loss: 143.5615\n",
      "Epoch 1290/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.4723 - val_loss: 119.6725\n",
      "Epoch 1291/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.7261 - val_loss: 136.4836\n",
      "Epoch 1292/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.0140 - val_loss: 117.7177\n",
      "Epoch 1293/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.0432 - val_loss: 277.5343\n",
      "Epoch 1294/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.3526 - val_loss: 116.9773\n",
      "Epoch 1295/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.7217 - val_loss: 117.7844\n",
      "Epoch 1296/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.4757 - val_loss: 143.4586\n",
      "Epoch 1297/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.7343 - val_loss: 127.7168\n",
      "Epoch 1298/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.6469 - val_loss: 141.9520\n",
      "Epoch 1299/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.0466 - val_loss: 148.9445\n",
      "Epoch 1300/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 226.0442 - val_loss: 148.2224\n",
      "Epoch 1301/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.2836 - val_loss: 163.4757\n",
      "Epoch 1302/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.1583 - val_loss: 130.4426\n",
      "Epoch 1303/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.5986 - val_loss: 120.9646\n",
      "Epoch 1304/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.8591 - val_loss: 146.7471\n",
      "Epoch 1305/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.4481 - val_loss: 118.6122\n",
      "Epoch 1306/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.6563 - val_loss: 127.4471\n",
      "Epoch 1307/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.8582 - val_loss: 124.3250\n",
      "Epoch 1308/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.2035 - val_loss: 115.1711\n",
      "Epoch 1309/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.2317 - val_loss: 115.9342\n",
      "Epoch 1310/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 163.6028 - val_loss: 120.5066\n",
      "Epoch 1311/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.6706 - val_loss: 131.7919\n",
      "Epoch 1312/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.2220 - val_loss: 119.2939\n",
      "Epoch 1313/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.6557 - val_loss: 122.9591\n",
      "Epoch 1314/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 147.9390 - val_loss: 126.4180\n",
      "Epoch 1315/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.5143 - val_loss: 185.1127\n",
      "Epoch 1316/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.9037 - val_loss: 121.5049\n",
      "Epoch 1317/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.8081 - val_loss: 127.7091\n",
      "Epoch 1318/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.6229 - val_loss: 144.8930\n",
      "Epoch 1319/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.7839 - val_loss: 188.3181\n",
      "Epoch 1320/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 139.6985 - val_loss: 113.4754\n",
      "Epoch 1321/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 182.8643 - val_loss: 156.0229\n",
      "Epoch 1322/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.9631 - val_loss: 126.8994\n",
      "Epoch 1323/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.5683 - val_loss: 114.9042\n",
      "Epoch 1324/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.5094 - val_loss: 113.4672\n",
      "Epoch 1325/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.0980 - val_loss: 113.9448\n",
      "Epoch 1326/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.5316 - val_loss: 125.6210\n",
      "Epoch 1327/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 138.0176 - val_loss: 117.7843\n",
      "Epoch 1328/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.9359 - val_loss: 127.6120\n",
      "Epoch 1329/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.9710 - val_loss: 124.5225\n",
      "Epoch 1330/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.9828 - val_loss: 125.7419\n",
      "Epoch 1331/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.9748 - val_loss: 131.8747\n",
      "Epoch 1332/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.8450 - val_loss: 202.8419\n",
      "Epoch 1333/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 187.4563 - val_loss: 137.9682\n",
      "Epoch 1334/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.7858 - val_loss: 122.5782\n",
      "Epoch 1335/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.8976 - val_loss: 153.2263\n",
      "Epoch 1336/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.8503 - val_loss: 137.1413\n",
      "Epoch 1337/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.6597 - val_loss: 129.4825\n",
      "Epoch 1338/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 166.1238 - val_loss: 126.7174\n",
      "Epoch 1339/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.7069 - val_loss: 120.9240\n",
      "Epoch 1340/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.6670 - val_loss: 239.8489\n",
      "Epoch 1341/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.8434 - val_loss: 122.0745\n",
      "Epoch 1342/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.5818 - val_loss: 160.5144\n",
      "Epoch 1343/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.5981 - val_loss: 128.8667\n",
      "Epoch 1344/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.7296 - val_loss: 140.5865\n",
      "Epoch 1345/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.2815 - val_loss: 122.5976\n",
      "Epoch 1346/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.0035 - val_loss: 126.1669\n",
      "Epoch 1347/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.2074 - val_loss: 131.6443\n",
      "Epoch 1348/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.0102 - val_loss: 118.2393\n",
      "Epoch 1349/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.4981 - val_loss: 118.9033\n",
      "Epoch 1350/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 190.1924 - val_loss: 118.7610\n",
      "Epoch 1351/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.4008 - val_loss: 115.1552\n",
      "Epoch 1352/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.1729 - val_loss: 129.6837\n",
      "Epoch 1353/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.9721 - val_loss: 122.4304\n",
      "Epoch 1354/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.8661 - val_loss: 116.5517\n",
      "Epoch 1355/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.7618 - val_loss: 116.6109\n",
      "Epoch 1356/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.8941 - val_loss: 130.6202\n",
      "Epoch 1357/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.5930 - val_loss: 116.7212\n",
      "Epoch 1358/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.5254 - val_loss: 134.2503\n",
      "Epoch 1359/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 147.5675 - val_loss: 158.0161\n",
      "Epoch 1360/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.9388 - val_loss: 185.6034\n",
      "Epoch 1361/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 265.4467 - val_loss: 140.5496\n",
      "Epoch 1362/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.6753 - val_loss: 121.2822\n",
      "Epoch 1363/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.9898 - val_loss: 137.6072\n",
      "Epoch 1364/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.5261 - val_loss: 127.5846\n",
      "Epoch 1365/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.5863 - val_loss: 116.5094\n",
      "Epoch 1366/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.9524 - val_loss: 121.5773\n",
      "Epoch 1367/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.8346 - val_loss: 159.8883\n",
      "Epoch 1368/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.4183 - val_loss: 116.0571\n",
      "Epoch 1369/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.0917 - val_loss: 123.2271\n",
      "Epoch 1370/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.7997 - val_loss: 129.6141\n",
      "Epoch 1371/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.5043 - val_loss: 121.5655\n",
      "Epoch 1372/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.5048 - val_loss: 160.4160\n",
      "Epoch 1373/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.1936 - val_loss: 137.4489\n",
      "Epoch 1374/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.3743 - val_loss: 129.7798\n",
      "Epoch 1375/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.6212 - val_loss: 120.1448\n",
      "Epoch 1376/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.3490 - val_loss: 137.0102\n",
      "Epoch 1377/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 267.0925 - val_loss: 124.0653\n",
      "Epoch 1378/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.9856 - val_loss: 119.6143\n",
      "Epoch 1379/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.4186 - val_loss: 140.1893\n",
      "Epoch 1380/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.1363 - val_loss: 119.2935\n",
      "Epoch 1381/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.4641 - val_loss: 133.5488\n",
      "Epoch 1382/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.7749 - val_loss: 129.1913\n",
      "Epoch 1383/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.8232 - val_loss: 114.7625\n",
      "Epoch 1384/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.3910 - val_loss: 116.6539\n",
      "Epoch 1385/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 170.0768 - val_loss: 150.1057\n",
      "Epoch 1386/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.2519 - val_loss: 117.9416\n",
      "Epoch 1387/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.1586 - val_loss: 136.4053\n",
      "Epoch 1388/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.4969 - val_loss: 116.4017\n",
      "Epoch 1389/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.2616 - val_loss: 130.4449\n",
      "Epoch 1390/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.4773 - val_loss: 123.4525\n",
      "Epoch 1391/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 134.7569 - val_loss: 134.4200\n",
      "Epoch 1392/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.7723 - val_loss: 127.0934\n",
      "Epoch 1393/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 146.5116 - val_loss: 118.0408\n",
      "Epoch 1394/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 141.3518 - val_loss: 151.7707\n",
      "Epoch 1395/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.3984 - val_loss: 137.2813\n",
      "Epoch 1396/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 172.5509 - val_loss: 128.7978\n",
      "Epoch 1397/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.5757 - val_loss: 136.6058\n",
      "Epoch 1398/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.9014 - val_loss: 123.0579\n",
      "Epoch 1399/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.4523 - val_loss: 118.1306\n",
      "Epoch 1400/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.7418 - val_loss: 125.4819\n",
      "Epoch 1401/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.7924 - val_loss: 121.5470\n",
      "Epoch 1402/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.9142 - val_loss: 123.9714\n",
      "Epoch 1403/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.2040 - val_loss: 116.1023\n",
      "Epoch 1404/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.7186 - val_loss: 151.8540\n",
      "Epoch 1405/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.3338 - val_loss: 125.3888\n",
      "Epoch 1406/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.7760 - val_loss: 118.9788\n",
      "Epoch 1407/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.7988 - val_loss: 136.2072\n",
      "Epoch 1408/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 180.1198 - val_loss: 118.5095\n",
      "Epoch 1409/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.8175 - val_loss: 144.7725\n",
      "Epoch 1410/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.7609 - val_loss: 120.1322\n",
      "Epoch 1411/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.8213 - val_loss: 120.2306\n",
      "Epoch 1412/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.7965 - val_loss: 115.2471\n",
      "Epoch 1413/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.8003 - val_loss: 145.4342\n",
      "Epoch 1414/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.0252 - val_loss: 120.3130\n",
      "Epoch 1415/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.5202 - val_loss: 118.5819\n",
      "Epoch 1416/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.3535 - val_loss: 155.1603\n",
      "Epoch 1417/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.3234 - val_loss: 139.4481\n",
      "Epoch 1418/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 170.2383 - val_loss: 143.0152\n",
      "Epoch 1419/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.9735 - val_loss: 127.4389\n",
      "Epoch 1420/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.8536 - val_loss: 137.7417\n",
      "Epoch 1421/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.8173 - val_loss: 138.0241\n",
      "Epoch 1422/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.9564 - val_loss: 135.1122\n",
      "Epoch 1423/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 288.0496 - val_loss: 118.3102\n",
      "Epoch 1424/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.5756 - val_loss: 116.2351\n",
      "Epoch 1425/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.8015 - val_loss: 123.3284\n",
      "Epoch 1426/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.0704 - val_loss: 153.5253\n",
      "Epoch 1427/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.4903 - val_loss: 120.6895\n",
      "Epoch 1428/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.5573 - val_loss: 153.3333\n",
      "Epoch 1429/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.8595 - val_loss: 181.2683\n",
      "Epoch 1430/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.8162 - val_loss: 115.2510\n",
      "Epoch 1431/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.0248 - val_loss: 119.5154\n",
      "Epoch 1432/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.0621 - val_loss: 150.5932\n",
      "Epoch 1433/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.9052 - val_loss: 120.4127\n",
      "Epoch 1434/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.2705 - val_loss: 136.7110\n",
      "Epoch 1435/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.0007 - val_loss: 128.1717\n",
      "Epoch 1436/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.3836 - val_loss: 118.6539\n",
      "Epoch 1437/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.6639 - val_loss: 133.0263\n",
      "Epoch 1438/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.0009 - val_loss: 117.1300\n",
      "Epoch 1439/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.7070 - val_loss: 123.9412\n",
      "Epoch 1440/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.0851 - val_loss: 140.7613\n",
      "Epoch 1441/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.4010 - val_loss: 116.2464\n",
      "Epoch 1442/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.1847 - val_loss: 164.4772\n",
      "Epoch 1443/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 246.2806 - val_loss: 129.8886\n",
      "Epoch 1444/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.6279 - val_loss: 119.9200\n",
      "Epoch 1445/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.3665 - val_loss: 117.7171\n",
      "Epoch 1446/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.3321 - val_loss: 182.8165\n",
      "Epoch 1447/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.7326 - val_loss: 130.1127\n",
      "Epoch 1448/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.5474 - val_loss: 129.2267\n",
      "Epoch 1449/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.7362 - val_loss: 126.9645\n",
      "Epoch 1450/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.6524 - val_loss: 117.8326\n",
      "Epoch 1451/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.3062 - val_loss: 129.3786\n",
      "Epoch 1452/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.3887 - val_loss: 129.5421\n",
      "Epoch 1453/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.0157 - val_loss: 124.1704\n",
      "Epoch 1454/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.4915 - val_loss: 126.3040\n",
      "Epoch 1455/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 175.6109 - val_loss: 125.9276\n",
      "Epoch 1456/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.5384 - val_loss: 136.0223\n",
      "Epoch 1457/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.8726 - val_loss: 118.1477\n",
      "Epoch 1458/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.4120 - val_loss: 140.4426\n",
      "Epoch 1459/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.4992 - val_loss: 117.6233\n",
      "Epoch 1460/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.9894 - val_loss: 164.8629\n",
      "Epoch 1461/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.4415 - val_loss: 122.6698\n",
      "Epoch 1462/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.6862 - val_loss: 155.9071\n",
      "Epoch 1463/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 177.5823 - val_loss: 133.4710\n",
      "Epoch 1464/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.8442 - val_loss: 225.7938\n",
      "Epoch 1465/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.9198 - val_loss: 119.4195\n",
      "Epoch 1466/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.3612 - val_loss: 119.9246\n",
      "Epoch 1467/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.5537 - val_loss: 126.4600\n",
      "Epoch 1468/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 133.5876 - val_loss: 119.1078\n",
      "Epoch 1469/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 138.4439 - val_loss: 121.9929\n",
      "Epoch 1470/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 127.5175 - val_loss: 178.9754\n",
      "Epoch 1471/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 137.6817 - val_loss: 157.4690\n",
      "Epoch 1472/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.3142 - val_loss: 131.3837\n",
      "Epoch 1473/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.0345 - val_loss: 184.8763\n",
      "Epoch 1474/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 186.9699 - val_loss: 125.6708\n",
      "Epoch 1475/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.0570 - val_loss: 117.1143\n",
      "Epoch 1476/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.9025 - val_loss: 117.4195\n",
      "Epoch 1477/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.4837 - val_loss: 119.8368\n",
      "Epoch 1478/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.9474 - val_loss: 115.0497\n",
      "Epoch 1479/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.9267 - val_loss: 139.6872\n",
      "Epoch 1480/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.8656 - val_loss: 140.6334\n",
      "Epoch 1481/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 146.6468 - val_loss: 120.0242\n",
      "Epoch 1482/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.8351 - val_loss: 135.5189\n",
      "Epoch 1483/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.4988 - val_loss: 119.1915\n",
      "Epoch 1484/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.1931 - val_loss: 134.0272\n",
      "Epoch 1485/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.3613 - val_loss: 137.6403\n",
      "Epoch 1486/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 162.0860 - val_loss: 242.1177\n",
      "Epoch 1487/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 152.5399 - val_loss: 204.4823\n",
      "Epoch 1488/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.9872 - val_loss: 129.4515\n",
      "Epoch 1489/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.0211 - val_loss: 130.9816\n",
      "Epoch 1490/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.0024 - val_loss: 178.4687\n",
      "Epoch 1491/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 576.0676 - val_loss: 1356.8566\n",
      "Epoch 1492/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 765.6069 - val_loss: 403.3352\n",
      "Epoch 1493/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 222.4685 - val_loss: 218.2572\n",
      "Epoch 1494/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 191.9772 - val_loss: 162.5504\n",
      "Epoch 1495/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 182.2783 - val_loss: 182.1294\n",
      "Epoch 1496/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 169.5098 - val_loss: 165.3466\n",
      "Epoch 1497/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 177.0304 - val_loss: 150.6497\n",
      "Epoch 1498/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 170.2867 - val_loss: 144.6987\n",
      "Epoch 1499/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 159.0122 - val_loss: 138.4410\n",
      "Epoch 1500/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 158.9818 - val_loss: 217.3369\n",
      "Epoch 1501/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 157.5249 - val_loss: 168.3207\n",
      "Epoch 1502/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 163.0650 - val_loss: 135.0064\n",
      "Epoch 1503/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 164.6283 - val_loss: 224.3848\n",
      "Epoch 1504/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 158.4283 - val_loss: 149.4989\n",
      "Epoch 1505/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.5604 - val_loss: 258.6882\n",
      "Epoch 1506/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 174.8048 - val_loss: 128.7515\n",
      "Epoch 1507/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 160.6346 - val_loss: 194.7117\n",
      "Epoch 1508/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 148.3814 - val_loss: 120.2579\n",
      "Epoch 1509/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.2565 - val_loss: 174.1182\n",
      "Epoch 1510/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 168.8619 - val_loss: 133.0540\n",
      "Epoch 1511/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 154.3268 - val_loss: 162.1975\n",
      "Epoch 1512/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 157.8751 - val_loss: 135.9567\n",
      "Epoch 1513/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 152.7331 - val_loss: 157.3608\n",
      "Epoch 1514/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.7083 - val_loss: 154.8484\n",
      "Epoch 1515/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.0465 - val_loss: 130.8523\n",
      "Epoch 1516/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 176.0791 - val_loss: 166.2042\n",
      "Epoch 1517/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.6955 - val_loss: 126.8951\n",
      "Epoch 1518/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.0681 - val_loss: 123.0744\n",
      "Epoch 1519/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.9896 - val_loss: 126.0896\n",
      "Epoch 1520/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 156.6933 - val_loss: 136.1345\n",
      "Epoch 1521/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 156.9395 - val_loss: 202.4376\n",
      "Epoch 1522/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 160.8277 - val_loss: 144.4271\n",
      "Epoch 1523/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 150.7896 - val_loss: 175.1953\n",
      "Epoch 1524/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 160.3024 - val_loss: 119.4233\n",
      "Epoch 1525/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 161.6560 - val_loss: 154.5752\n",
      "Epoch 1526/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.0073 - val_loss: 187.0466\n",
      "Epoch 1527/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 158.5059 - val_loss: 122.8807\n",
      "Epoch 1528/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.9547 - val_loss: 219.8206\n",
      "Epoch 1529/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 156.1729 - val_loss: 143.5029\n",
      "Epoch 1530/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 152.2922 - val_loss: 144.3136\n",
      "Epoch 1531/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 141.5963 - val_loss: 134.3620\n",
      "Epoch 1532/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 178.8680 - val_loss: 122.1905\n",
      "Epoch 1533/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.0138 - val_loss: 130.8460\n",
      "Epoch 1534/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.3252 - val_loss: 127.2203\n",
      "Epoch 1535/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 156.6524 - val_loss: 132.2811\n",
      "Epoch 1536/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 147.7291 - val_loss: 193.4247\n",
      "Epoch 1537/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.8646 - val_loss: 116.7041\n",
      "Epoch 1538/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.3482 - val_loss: 138.4203\n",
      "Epoch 1539/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 161.4433 - val_loss: 132.7062\n",
      "Epoch 1540/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 162.9363 - val_loss: 135.3632\n",
      "Epoch 1541/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.5206 - val_loss: 146.0442\n",
      "Epoch 1542/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 151.8496 - val_loss: 157.6526\n",
      "Epoch 1543/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.6923 - val_loss: 152.9558\n",
      "Epoch 1544/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.5972 - val_loss: 139.2028\n",
      "Epoch 1545/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 143.4262 - val_loss: 137.1058\n",
      "Epoch 1546/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 160.4282 - val_loss: 333.2156\n",
      "Epoch 1547/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.2975 - val_loss: 124.8603\n",
      "Epoch 1548/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.1298 - val_loss: 122.6382\n",
      "Epoch 1549/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 147.0768 - val_loss: 123.9726\n",
      "Epoch 1550/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 142.9284 - val_loss: 117.8457\n",
      "Epoch 1551/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 154.4505 - val_loss: 141.7382\n",
      "Epoch 1552/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 204.8404 - val_loss: 935.7123\n",
      "Epoch 1553/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 186.9837 - val_loss: 126.8489\n",
      "Epoch 1554/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.4232 - val_loss: 129.4346\n",
      "Epoch 1555/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.6955 - val_loss: 185.6224\n",
      "Epoch 1556/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 157.2215 - val_loss: 116.6393\n",
      "Epoch 1557/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.6153 - val_loss: 126.0141\n",
      "Epoch 1558/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.9514 - val_loss: 166.2386\n",
      "Epoch 1559/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.1392 - val_loss: 178.4176\n",
      "Epoch 1560/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.4440 - val_loss: 132.7840\n",
      "Epoch 1561/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 140.4777 - val_loss: 119.6254\n",
      "Epoch 1562/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.5435 - val_loss: 140.2541\n",
      "Epoch 1563/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.4641 - val_loss: 256.3526\n",
      "Epoch 1564/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.4705 - val_loss: 117.8527\n",
      "Epoch 1565/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.5865 - val_loss: 130.9216\n",
      "Epoch 1566/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.4898 - val_loss: 176.9111\n",
      "Epoch 1567/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 169.5209 - val_loss: 127.0240\n",
      "Epoch 1568/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.0425 - val_loss: 150.4355\n",
      "Epoch 1569/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.5755 - val_loss: 117.1157\n",
      "Epoch 1570/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.1218 - val_loss: 128.9634\n",
      "Epoch 1571/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.6493 - val_loss: 204.4605\n",
      "Epoch 1572/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.7959 - val_loss: 120.4279\n",
      "Epoch 1573/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.1007 - val_loss: 147.9683\n",
      "Epoch 1574/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.2335 - val_loss: 132.0045\n",
      "Epoch 1575/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.0428 - val_loss: 124.4751\n",
      "Epoch 1576/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.1620 - val_loss: 131.2276\n",
      "Epoch 1577/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.2096 - val_loss: 120.0603\n",
      "Epoch 1578/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.7500 - val_loss: 125.2532\n",
      "Epoch 1579/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.1907 - val_loss: 148.4841\n",
      "Epoch 1580/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.3018 - val_loss: 119.4723\n",
      "Epoch 1581/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.5601 - val_loss: 115.5507\n",
      "Epoch 1582/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.7838 - val_loss: 127.4022\n",
      "Epoch 1583/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.6330 - val_loss: 118.0613\n",
      "Epoch 1584/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.0324 - val_loss: 167.7378\n",
      "Epoch 1585/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.4921 - val_loss: 132.1336\n",
      "Epoch 1586/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.5596 - val_loss: 114.9969\n",
      "Epoch 1587/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.2178 - val_loss: 118.3712\n",
      "Epoch 1588/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.1874 - val_loss: 116.4589\n",
      "Epoch 1589/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.8007 - val_loss: 163.0377\n",
      "Epoch 1590/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.9715 - val_loss: 135.8324\n",
      "Epoch 1591/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 166.3940 - val_loss: 131.7517\n",
      "Epoch 1592/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.4427 - val_loss: 127.7589\n",
      "Epoch 1593/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.4256 - val_loss: 122.9808\n",
      "Epoch 1594/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.4204 - val_loss: 134.9191\n",
      "Epoch 1595/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.1430 - val_loss: 154.2144\n",
      "Epoch 1596/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.8518 - val_loss: 142.2597\n",
      "Epoch 1597/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.9590 - val_loss: 219.3292\n",
      "Epoch 1598/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.4875 - val_loss: 122.7234\n",
      "Epoch 1599/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.1114 - val_loss: 117.9139\n",
      "Epoch 1600/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.6511 - val_loss: 115.2102\n",
      "Epoch 1601/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.2498 - val_loss: 155.6340\n",
      "Epoch 1602/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.3920 - val_loss: 221.0296\n",
      "Epoch 1603/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.0941 - val_loss: 194.0542\n",
      "Epoch 1604/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 164.0736 - val_loss: 123.0230\n",
      "Epoch 1605/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 169.2572 - val_loss: 119.4460\n",
      "Epoch 1606/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.2636 - val_loss: 116.8223\n",
      "Epoch 1607/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.9546 - val_loss: 114.5887\n",
      "Epoch 1608/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.9026 - val_loss: 114.0460\n",
      "Epoch 1609/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.6964 - val_loss: 205.5403\n",
      "Epoch 1610/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.8557 - val_loss: 178.8750\n",
      "Epoch 1611/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 159.8105 - val_loss: 209.6644\n",
      "Epoch 1612/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 176.1805 - val_loss: 117.4369\n",
      "Epoch 1613/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.5952 - val_loss: 133.7677\n",
      "Epoch 1614/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.6923 - val_loss: 115.1331\n",
      "Epoch 1615/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.0507 - val_loss: 115.3380\n",
      "Epoch 1616/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.4683 - val_loss: 120.0744\n",
      "Epoch 1617/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.7363 - val_loss: 118.1209\n",
      "Epoch 1618/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.8783 - val_loss: 133.2328\n",
      "Epoch 1619/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 133.0326 - val_loss: 131.6201\n",
      "Epoch 1620/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.9630 - val_loss: 124.3095\n",
      "Epoch 1621/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 135.7337 - val_loss: 185.7983\n",
      "Epoch 1622/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 152.8015 - val_loss: 134.2444\n",
      "Epoch 1623/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 140.8913 - val_loss: 114.4522\n",
      "Epoch 1624/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.4312 - val_loss: 155.8708\n",
      "Epoch 1625/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.4241 - val_loss: 113.9399\n",
      "Epoch 1626/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.0629 - val_loss: 118.5552\n",
      "Epoch 1627/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.6744 - val_loss: 124.3609\n",
      "Epoch 1628/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.9676 - val_loss: 121.1906\n",
      "Epoch 1629/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.2343 - val_loss: 113.8652\n",
      "Epoch 1630/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.7798 - val_loss: 119.4842\n",
      "Epoch 1631/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.1503 - val_loss: 142.6086\n",
      "Epoch 1632/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.5312 - val_loss: 116.9217\n",
      "Epoch 1633/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.0998 - val_loss: 116.6015\n",
      "Epoch 1634/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.5944 - val_loss: 126.1596\n",
      "Epoch 1635/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.9962 - val_loss: 125.0888\n",
      "Epoch 1636/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.5030 - val_loss: 114.5662\n",
      "Epoch 1637/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.8882 - val_loss: 122.3365\n",
      "Epoch 1638/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.2101 - val_loss: 123.0774\n",
      "Epoch 1639/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.5049 - val_loss: 112.4527\n",
      "Epoch 1640/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.4685 - val_loss: 117.9874\n",
      "Epoch 1641/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.1350 - val_loss: 119.5947\n",
      "Epoch 1642/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.8787 - val_loss: 145.3531\n",
      "Epoch 1643/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.7904 - val_loss: 177.0726\n",
      "Epoch 1644/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.4683 - val_loss: 130.5901\n",
      "Epoch 1645/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.1499 - val_loss: 115.3756\n",
      "Epoch 1646/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.9414 - val_loss: 125.1522\n",
      "Epoch 1647/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.0674 - val_loss: 124.6348\n",
      "Epoch 1648/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.4910 - val_loss: 149.8696\n",
      "Epoch 1649/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.4968 - val_loss: 131.1248\n",
      "Epoch 1650/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.8941 - val_loss: 125.4404\n",
      "Epoch 1651/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.6088 - val_loss: 117.4709\n",
      "Epoch 1652/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.6763 - val_loss: 121.1582\n",
      "Epoch 1653/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.8554 - val_loss: 131.2446\n",
      "Epoch 1654/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.7214 - val_loss: 126.9736\n",
      "Epoch 1655/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.9712 - val_loss: 119.5432\n",
      "Epoch 1656/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.1779 - val_loss: 126.7434\n",
      "Epoch 1657/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.1525 - val_loss: 124.9958\n",
      "Epoch 1658/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 162.3253 - val_loss: 118.2967\n",
      "Epoch 1659/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.5592 - val_loss: 114.0571\n",
      "Epoch 1660/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.9672 - val_loss: 117.6632\n",
      "Epoch 1661/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.3293 - val_loss: 120.9534\n",
      "Epoch 1662/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.9493 - val_loss: 126.0103\n",
      "Epoch 1663/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.4200 - val_loss: 148.6095\n",
      "Epoch 1664/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 154.6616 - val_loss: 116.4808\n",
      "Epoch 1665/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.7659 - val_loss: 132.2550\n",
      "Epoch 1666/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.0628 - val_loss: 132.1741\n",
      "Epoch 1667/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.6200 - val_loss: 186.8535\n",
      "Epoch 1668/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.9318 - val_loss: 117.5227\n",
      "Epoch 1669/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.8033 - val_loss: 127.0537\n",
      "Epoch 1670/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.4203 - val_loss: 167.8605\n",
      "Epoch 1671/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.3646 - val_loss: 144.5090\n",
      "Epoch 1672/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.2712 - val_loss: 130.1965\n",
      "Epoch 1673/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.1987 - val_loss: 117.7035\n",
      "Epoch 1674/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.1265 - val_loss: 117.0300\n",
      "Epoch 1675/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.3687 - val_loss: 112.4242\n",
      "Epoch 1676/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.0378 - val_loss: 126.0991\n",
      "Epoch 1677/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.1480 - val_loss: 112.4798\n",
      "Epoch 1678/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.3707 - val_loss: 126.8526\n",
      "Epoch 1679/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.8713 - val_loss: 121.1422\n",
      "Epoch 1680/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.3505 - val_loss: 118.6866\n",
      "Epoch 1681/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.8871 - val_loss: 126.3389\n",
      "Epoch 1682/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 151.2187 - val_loss: 148.0825\n",
      "Epoch 1683/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.9012 - val_loss: 128.8081\n",
      "Epoch 1684/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 155.6808 - val_loss: 133.0376\n",
      "Epoch 1685/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.3000 - val_loss: 130.1290\n",
      "Epoch 1686/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.3303 - val_loss: 136.2391\n",
      "Epoch 1687/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.4084 - val_loss: 117.5364\n",
      "Epoch 1688/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.4498 - val_loss: 130.6253\n",
      "Epoch 1689/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.4345 - val_loss: 126.6455\n",
      "Epoch 1690/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.2537 - val_loss: 136.4613\n",
      "Epoch 1691/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.1542 - val_loss: 117.0751\n",
      "Epoch 1692/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.5388 - val_loss: 116.2333\n",
      "Epoch 1693/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.9353 - val_loss: 114.8588\n",
      "Epoch 1694/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.1986 - val_loss: 117.5227\n",
      "Epoch 1695/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.7100 - val_loss: 162.9055\n",
      "Epoch 1696/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.1882 - val_loss: 123.6910\n",
      "Epoch 1697/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 132.2834 - val_loss: 194.4469\n",
      "Epoch 1698/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.7868 - val_loss: 115.6872\n",
      "Epoch 1699/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 131.0505 - val_loss: 119.6950\n",
      "Epoch 1700/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 136.5120 - val_loss: 118.2870\n",
      "Epoch 1701/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.7690 - val_loss: 128.6357\n",
      "Epoch 1702/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.0658 - val_loss: 119.6385\n",
      "Epoch 1703/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.3432 - val_loss: 122.7442\n",
      "Epoch 1704/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 137.5846 - val_loss: 123.0322\n",
      "Epoch 1705/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.1089 - val_loss: 123.7643\n",
      "Epoch 1706/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.0494 - val_loss: 138.5893\n",
      "Epoch 1707/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.0673 - val_loss: 117.7609\n",
      "Epoch 1708/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.1208 - val_loss: 135.4226\n",
      "Epoch 1709/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.9600 - val_loss: 128.2321\n",
      "Epoch 1710/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.0963 - val_loss: 111.9445\n",
      "Epoch 1711/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.0250 - val_loss: 122.7881\n",
      "Epoch 1712/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.8639 - val_loss: 129.1355\n",
      "Epoch 1713/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.7620 - val_loss: 130.0863\n",
      "Epoch 1714/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.8528 - val_loss: 116.6827\n",
      "Epoch 1715/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 154.8717 - val_loss: 282.5559\n",
      "Epoch 1716/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.6971 - val_loss: 120.9073\n",
      "Epoch 1717/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.9318 - val_loss: 142.1673\n",
      "Epoch 1718/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.5163 - val_loss: 166.2740\n",
      "Epoch 1719/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.0463 - val_loss: 125.5395\n",
      "Epoch 1720/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.5682 - val_loss: 131.4961\n",
      "Epoch 1721/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.0471 - val_loss: 150.3849\n",
      "Epoch 1722/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.4658 - val_loss: 124.5599\n",
      "Epoch 1723/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.6289 - val_loss: 122.5773\n",
      "Epoch 1724/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.9352 - val_loss: 117.5321\n",
      "Epoch 1725/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.5837 - val_loss: 114.9976\n",
      "Epoch 1726/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.0708 - val_loss: 134.2252\n",
      "Epoch 1727/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.3854 - val_loss: 118.4520\n",
      "Epoch 1728/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.3087 - val_loss: 126.3217\n",
      "Epoch 1729/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.0843 - val_loss: 147.3569\n",
      "Epoch 1730/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.0836 - val_loss: 159.4022\n",
      "Epoch 1731/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.1476 - val_loss: 155.4738\n",
      "Epoch 1732/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.2952 - val_loss: 159.7742\n",
      "Epoch 1733/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 157.3023 - val_loss: 211.8187\n",
      "Epoch 1734/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.5626 - val_loss: 121.2454\n",
      "Epoch 1735/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.4328 - val_loss: 138.0718\n",
      "Epoch 1736/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.7609 - val_loss: 145.1603\n",
      "Epoch 1737/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.3750 - val_loss: 115.9786\n",
      "Epoch 1738/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.2586 - val_loss: 119.1863\n",
      "Epoch 1739/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.5089 - val_loss: 120.7704\n",
      "Epoch 1740/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.6410 - val_loss: 133.3661\n",
      "Epoch 1741/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.1844 - val_loss: 175.2537\n",
      "Epoch 1742/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.8267 - val_loss: 131.1451\n",
      "Epoch 1743/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.4729 - val_loss: 124.8150\n",
      "Epoch 1744/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.7397 - val_loss: 133.4590\n",
      "Epoch 1745/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.8401 - val_loss: 122.4835\n",
      "Epoch 1746/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.7541 - val_loss: 115.5616\n",
      "Epoch 1747/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.9788 - val_loss: 117.5373\n",
      "Epoch 1748/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.3986 - val_loss: 116.6456\n",
      "Epoch 1749/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.3263 - val_loss: 148.6293\n",
      "Epoch 1750/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.6582 - val_loss: 123.7708\n",
      "Epoch 1751/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.8434 - val_loss: 134.2754\n",
      "Epoch 1752/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.7678 - val_loss: 160.2273\n",
      "Epoch 1753/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.2374 - val_loss: 116.1840\n",
      "Epoch 1754/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.2481 - val_loss: 207.9977\n",
      "Epoch 1755/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.5938 - val_loss: 145.3341\n",
      "Epoch 1756/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.4626 - val_loss: 148.1701\n",
      "Epoch 1757/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.4149 - val_loss: 135.9325\n",
      "Epoch 1758/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.6515 - val_loss: 119.3576\n",
      "Epoch 1759/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.0995 - val_loss: 124.9375\n",
      "Epoch 1760/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.1608 - val_loss: 160.2808\n",
      "Epoch 1761/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.0984 - val_loss: 130.7826\n",
      "Epoch 1762/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.3383 - val_loss: 154.2373\n",
      "Epoch 1763/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.8798 - val_loss: 128.5600\n",
      "Epoch 1764/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.1895 - val_loss: 165.5330\n",
      "Epoch 1765/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.8475 - val_loss: 144.2968\n",
      "Epoch 1766/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.8284 - val_loss: 137.8538\n",
      "Epoch 1767/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.5167 - val_loss: 121.7330\n",
      "Epoch 1768/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.8156 - val_loss: 134.3396\n",
      "Epoch 1769/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.3681 - val_loss: 120.5319\n",
      "Epoch 1770/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.9804 - val_loss: 114.8667\n",
      "Epoch 1771/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.5774 - val_loss: 115.2549\n",
      "Epoch 1772/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.4208 - val_loss: 120.9411\n",
      "Epoch 1773/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.8388 - val_loss: 117.1826\n",
      "Epoch 1774/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 128.7659 - val_loss: 120.0494\n",
      "Epoch 1775/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.4229 - val_loss: 116.7341\n",
      "Epoch 1776/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.1170 - val_loss: 120.3776\n",
      "Epoch 1777/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.9914 - val_loss: 131.9609\n",
      "Epoch 1778/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.0283 - val_loss: 112.1432\n",
      "Epoch 1779/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.3204 - val_loss: 123.6024\n",
      "Epoch 1780/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.7857 - val_loss: 142.7975\n",
      "Epoch 1781/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.2704 - val_loss: 158.8723\n",
      "Epoch 1782/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.6781 - val_loss: 122.7519\n",
      "Epoch 1783/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.2194 - val_loss: 119.0908\n",
      "Epoch 1784/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.2600 - val_loss: 118.7385\n",
      "Epoch 1785/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.4075 - val_loss: 137.2086\n",
      "Epoch 1786/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.8110 - val_loss: 150.3800\n",
      "Epoch 1787/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.0139 - val_loss: 163.5657\n",
      "Epoch 1788/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.8808 - val_loss: 137.5652\n",
      "Epoch 1789/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.3414 - val_loss: 146.7523\n",
      "Epoch 1790/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.8640 - val_loss: 120.6687\n",
      "Epoch 1791/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.4378 - val_loss: 120.1944\n",
      "Epoch 1792/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.2796 - val_loss: 143.4743\n",
      "Epoch 1793/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.5833 - val_loss: 131.1322\n",
      "Epoch 1794/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 219.6117 - val_loss: 162.9087\n",
      "Epoch 1795/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.8271 - val_loss: 114.2931\n",
      "Epoch 1796/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.0836 - val_loss: 130.0681\n",
      "Epoch 1797/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.1258 - val_loss: 136.9418\n",
      "Epoch 1798/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.9371 - val_loss: 141.3574\n",
      "Epoch 1799/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.4232 - val_loss: 115.6651\n",
      "Epoch 1800/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.9772 - val_loss: 121.0366\n",
      "Epoch 1801/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.1331 - val_loss: 137.1828\n",
      "Epoch 1802/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.0165 - val_loss: 125.7157\n",
      "Epoch 1803/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.1784 - val_loss: 154.4821\n",
      "Epoch 1804/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.7857 - val_loss: 115.9358\n",
      "Epoch 1805/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 243.2614 - val_loss: 122.1272\n",
      "Epoch 1806/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.5855 - val_loss: 129.3465\n",
      "Epoch 1807/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.7139 - val_loss: 128.1065\n",
      "Epoch 1808/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.3528 - val_loss: 152.3576\n",
      "Epoch 1809/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.9829 - val_loss: 135.2018\n",
      "Epoch 1810/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.0818 - val_loss: 164.5639\n",
      "Epoch 1811/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.8576 - val_loss: 128.8576\n",
      "Epoch 1812/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.8940 - val_loss: 123.3904\n",
      "Epoch 1813/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.0756 - val_loss: 129.6969\n",
      "Epoch 1814/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.3717 - val_loss: 120.3118\n",
      "Epoch 1815/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.1419 - val_loss: 114.0778\n",
      "Epoch 1816/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.9349 - val_loss: 129.1287\n",
      "Epoch 1817/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.1654 - val_loss: 153.4433\n",
      "Epoch 1818/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.6646 - val_loss: 124.6843\n",
      "Epoch 1819/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.3868 - val_loss: 124.5602\n",
      "Epoch 1820/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 133.3042 - val_loss: 126.8155\n",
      "Epoch 1821/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.9297 - val_loss: 149.7278\n",
      "Epoch 1822/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.6640 - val_loss: 138.6723\n",
      "Epoch 1823/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.8563 - val_loss: 139.6774\n",
      "Epoch 1824/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.1366 - val_loss: 121.7872\n",
      "Epoch 1825/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.6130 - val_loss: 124.7291\n",
      "Epoch 1826/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.8535 - val_loss: 118.8409\n",
      "Epoch 1827/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.2142 - val_loss: 195.6821\n",
      "Epoch 1828/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.5890 - val_loss: 127.5087\n",
      "Epoch 1829/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.4503 - val_loss: 125.8215\n",
      "Epoch 1830/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.5411 - val_loss: 154.7418\n",
      "Epoch 1831/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.0654 - val_loss: 137.8647\n",
      "Epoch 1832/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.6851 - val_loss: 115.6220\n",
      "Epoch 1833/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.5189 - val_loss: 112.6175\n",
      "Epoch 1834/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.1619 - val_loss: 129.2156\n",
      "Epoch 1835/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.8576 - val_loss: 119.1313\n",
      "Epoch 1836/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.2365 - val_loss: 111.2892\n",
      "Epoch 1837/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 160.5658 - val_loss: 142.4309\n",
      "Epoch 1838/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.1211 - val_loss: 128.4122\n",
      "Epoch 1839/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.0531 - val_loss: 112.2317\n",
      "Epoch 1840/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.2068 - val_loss: 128.5259\n",
      "Epoch 1841/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.2390 - val_loss: 160.4226\n",
      "Epoch 1842/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.0596 - val_loss: 113.9411\n",
      "Epoch 1843/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.7522 - val_loss: 124.0347\n",
      "Epoch 1844/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.0906 - val_loss: 118.9543\n",
      "Epoch 1845/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.0440 - val_loss: 139.8499\n",
      "Epoch 1846/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.5531 - val_loss: 160.9285\n",
      "Epoch 1847/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.2744 - val_loss: 130.2733\n",
      "Epoch 1848/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.8785 - val_loss: 121.1830\n",
      "Epoch 1849/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.1688 - val_loss: 123.5313\n",
      "Epoch 1850/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.6348 - val_loss: 128.4792\n",
      "Epoch 1851/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.3542 - val_loss: 124.6604\n",
      "Epoch 1852/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 219.0608 - val_loss: 243.2699\n",
      "Epoch 1853/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.6340 - val_loss: 115.6047\n",
      "Epoch 1854/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 126.8438 - val_loss: 116.7606\n",
      "Epoch 1855/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.9979 - val_loss: 116.2453\n",
      "Epoch 1856/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.5257 - val_loss: 131.3564\n",
      "Epoch 1857/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.1330 - val_loss: 117.5273\n",
      "Epoch 1858/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.4418 - val_loss: 127.0538\n",
      "Epoch 1859/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 142.3767 - val_loss: 117.2432\n",
      "Epoch 1860/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.4223 - val_loss: 168.5624\n",
      "Epoch 1861/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.3806 - val_loss: 143.5026\n",
      "Epoch 1862/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.5230 - val_loss: 112.7499\n",
      "Epoch 1863/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.7566 - val_loss: 138.2840\n",
      "Epoch 1864/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.0477 - val_loss: 111.9575\n",
      "Epoch 1865/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.8440 - val_loss: 126.0344\n",
      "Epoch 1866/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.2084 - val_loss: 118.2399\n",
      "Epoch 1867/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.3784 - val_loss: 134.8685\n",
      "Epoch 1868/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.6356 - val_loss: 131.6084\n",
      "Epoch 1869/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.4703 - val_loss: 117.6176\n",
      "Epoch 1870/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.6088 - val_loss: 129.2545\n",
      "Epoch 1871/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.3775 - val_loss: 126.3881\n",
      "Epoch 1872/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.7226 - val_loss: 126.7431\n",
      "Epoch 1873/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.0408 - val_loss: 110.8999\n",
      "Epoch 1874/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.5599 - val_loss: 113.0805\n",
      "Epoch 1875/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.6114 - val_loss: 129.6637\n",
      "Epoch 1876/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.1266 - val_loss: 123.7510\n",
      "Epoch 1877/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 160.1274 - val_loss: 114.4286\n",
      "Epoch 1878/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.4437 - val_loss: 121.9706\n",
      "Epoch 1879/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.7932 - val_loss: 117.7748\n",
      "Epoch 1880/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.5406 - val_loss: 154.6811\n",
      "Epoch 1881/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.1189 - val_loss: 117.0656\n",
      "Epoch 1882/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.7685 - val_loss: 112.2375\n",
      "Epoch 1883/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.5519 - val_loss: 127.4807\n",
      "Epoch 1884/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.2701 - val_loss: 116.4082\n",
      "Epoch 1885/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.7320 - val_loss: 123.2266\n",
      "Epoch 1886/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.2267 - val_loss: 117.9124\n",
      "Epoch 1887/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.1333 - val_loss: 116.0218\n",
      "Epoch 1888/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.9909 - val_loss: 114.1476\n",
      "Epoch 1889/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.8611 - val_loss: 297.6930\n",
      "Epoch 1890/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.6473 - val_loss: 136.0319\n",
      "Epoch 1891/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.9463 - val_loss: 118.9142\n",
      "Epoch 1892/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.8335 - val_loss: 117.5279\n",
      "Epoch 1893/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.3199 - val_loss: 120.5823\n",
      "Epoch 1894/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.4300 - val_loss: 123.3106\n",
      "Epoch 1895/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.1555 - val_loss: 142.9734\n",
      "Epoch 1896/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 203.4046 - val_loss: 116.2835\n",
      "Epoch 1897/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.7970 - val_loss: 117.3375\n",
      "Epoch 1898/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.4069 - val_loss: 127.2229\n",
      "Epoch 1899/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.9513 - val_loss: 133.0644\n",
      "Epoch 1900/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.3214 - val_loss: 118.5065\n",
      "Epoch 1901/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.2585 - val_loss: 116.7162\n",
      "Epoch 1902/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.8395 - val_loss: 124.7785\n",
      "Epoch 1903/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.6604 - val_loss: 135.6472\n",
      "Epoch 1904/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.2775 - val_loss: 120.3586\n",
      "Epoch 1905/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.7535 - val_loss: 212.5449\n",
      "Epoch 1906/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.6185 - val_loss: 121.9770\n",
      "Epoch 1907/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.3069 - val_loss: 137.2187\n",
      "Epoch 1908/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.1049 - val_loss: 119.9394\n",
      "Epoch 1909/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 156.4584 - val_loss: 212.0779\n",
      "Epoch 1910/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.8500 - val_loss: 166.8931\n",
      "Epoch 1911/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.0995 - val_loss: 146.7713\n",
      "Epoch 1912/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.2458 - val_loss: 133.3206\n",
      "Epoch 1913/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.6313 - val_loss: 120.3891\n",
      "Epoch 1914/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.3464 - val_loss: 122.6386\n",
      "Epoch 1915/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.0801 - val_loss: 112.5780\n",
      "Epoch 1916/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.3942 - val_loss: 122.9418\n",
      "Epoch 1917/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.0989 - val_loss: 136.5867\n",
      "Epoch 1918/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.8424 - val_loss: 118.5965\n",
      "Epoch 1919/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.0557 - val_loss: 125.5846\n",
      "Epoch 1920/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.2372 - val_loss: 115.6957\n",
      "Epoch 1921/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.3055 - val_loss: 119.1631\n",
      "Epoch 1922/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.3288 - val_loss: 121.7579\n",
      "Epoch 1923/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.0238 - val_loss: 115.3341\n",
      "Epoch 1924/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.6529 - val_loss: 119.6494\n",
      "Epoch 1925/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.5151 - val_loss: 145.2710\n",
      "Epoch 1926/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.3505 - val_loss: 116.3686\n",
      "Epoch 1927/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.5818 - val_loss: 145.8567\n",
      "Epoch 1928/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.0178 - val_loss: 132.1485\n",
      "Epoch 1929/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.8078 - val_loss: 127.8042\n",
      "Epoch 1930/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 129.0271 - val_loss: 128.8572\n",
      "Epoch 1931/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.3146 - val_loss: 117.3976\n",
      "Epoch 1932/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 126.8848 - val_loss: 123.5188\n",
      "Epoch 1933/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.6976 - val_loss: 114.9014\n",
      "Epoch 1934/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.5027 - val_loss: 123.1687\n",
      "Epoch 1935/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.9525 - val_loss: 123.8424\n",
      "Epoch 1936/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.4699 - val_loss: 118.4895\n",
      "Epoch 1937/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 137.0237 - val_loss: 189.2473\n",
      "Epoch 1938/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.6016 - val_loss: 157.7101\n",
      "Epoch 1939/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.3951 - val_loss: 132.9172\n",
      "Epoch 1940/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.3044 - val_loss: 116.2884\n",
      "Epoch 1941/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.5520 - val_loss: 124.0464\n",
      "Epoch 1942/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.1637 - val_loss: 155.5729\n",
      "Epoch 1943/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.6730 - val_loss: 122.7863\n",
      "Epoch 1944/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.7403 - val_loss: 124.2600\n",
      "Epoch 1945/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.0681 - val_loss: 118.4938\n",
      "Epoch 1946/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.2765 - val_loss: 112.6138\n",
      "Epoch 1947/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.6639 - val_loss: 156.9886\n",
      "Epoch 1948/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.0137 - val_loss: 152.8915\n",
      "Epoch 1949/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.9055 - val_loss: 230.8479\n",
      "Epoch 1950/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.5574 - val_loss: 114.9588\n",
      "Epoch 1951/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.0647 - val_loss: 138.1015\n",
      "Epoch 1952/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.7256 - val_loss: 129.9445\n",
      "Epoch 1953/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.3043 - val_loss: 138.6153\n",
      "Epoch 1954/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.1207 - val_loss: 127.9726\n",
      "Epoch 1955/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.4046 - val_loss: 173.6408\n",
      "Epoch 1956/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.3929 - val_loss: 118.9762\n",
      "Epoch 1957/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.9618 - val_loss: 115.9721\n",
      "Epoch 1958/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.4665 - val_loss: 130.1344\n",
      "Epoch 1959/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.8145 - val_loss: 123.8597\n",
      "Epoch 1960/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.3008 - val_loss: 141.6957\n",
      "Epoch 1961/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.9016 - val_loss: 117.2733\n",
      "Epoch 1962/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.8000 - val_loss: 116.7625\n",
      "Epoch 1963/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.0466 - val_loss: 141.6978\n",
      "Epoch 1964/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.0328 - val_loss: 117.7629\n",
      "Epoch 1965/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.4092 - val_loss: 118.1573\n",
      "Epoch 1966/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.2540 - val_loss: 142.0038\n",
      "Epoch 1967/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.7267 - val_loss: 145.7697\n",
      "Epoch 1968/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.1518 - val_loss: 116.4507\n",
      "Epoch 1969/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.8498 - val_loss: 150.9741\n",
      "Epoch 1970/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.3663 - val_loss: 127.2211\n",
      "Epoch 1971/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.8000 - val_loss: 121.1312\n",
      "Epoch 1972/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.8080 - val_loss: 150.6171\n",
      "Epoch 1973/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.0316 - val_loss: 119.5226\n",
      "Epoch 1974/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.1523 - val_loss: 114.1814\n",
      "Epoch 1975/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 141.4345 - val_loss: 118.0726\n",
      "Epoch 1976/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.0147 - val_loss: 131.9111\n",
      "Epoch 1977/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.2790 - val_loss: 121.9796\n",
      "Epoch 1978/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.8817 - val_loss: 137.6304\n",
      "Epoch 1979/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.5411 - val_loss: 118.9993\n",
      "Epoch 1980/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.3795 - val_loss: 131.5320\n",
      "Epoch 1981/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.9360 - val_loss: 122.3103\n",
      "Epoch 1982/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 206.1631 - val_loss: 129.4699\n",
      "Epoch 1983/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.5963 - val_loss: 114.4344\n",
      "Epoch 1984/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.5684 - val_loss: 119.9397\n",
      "Epoch 1985/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.1411 - val_loss: 128.0625\n",
      "Epoch 1986/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.8431 - val_loss: 116.9049\n",
      "Epoch 1987/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.7076 - val_loss: 114.1747\n",
      "Epoch 1988/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.5124 - val_loss: 151.4900\n",
      "Epoch 1989/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.2054 - val_loss: 115.1322\n",
      "Epoch 1990/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.0639 - val_loss: 128.0902\n",
      "Epoch 1991/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.0673 - val_loss: 147.1116\n",
      "Epoch 1992/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.5918 - val_loss: 201.4542\n",
      "Epoch 1993/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.0400 - val_loss: 152.2644\n",
      "Epoch 1994/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.9196 - val_loss: 150.1858\n",
      "Epoch 1995/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.8868 - val_loss: 126.9262\n",
      "Epoch 1996/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.5843 - val_loss: 120.8854\n",
      "Epoch 1997/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.6837 - val_loss: 112.2596\n",
      "Epoch 1998/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.7808 - val_loss: 126.0740\n",
      "Epoch 1999/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.5300 - val_loss: 127.0101\n",
      "Epoch 2000/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.8422 - val_loss: 120.8290\n",
      "Epoch 2001/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.2587 - val_loss: 131.6792\n",
      "Epoch 2002/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.6046 - val_loss: 137.4460\n",
      "Epoch 2003/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.8941 - val_loss: 117.3076\n",
      "Epoch 2004/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.7497 - val_loss: 123.9150\n",
      "Epoch 2005/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.2894 - val_loss: 128.9636\n",
      "Epoch 2006/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.9639 - val_loss: 124.1829\n",
      "Epoch 2007/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 136.7034 - val_loss: 128.6324\n",
      "Epoch 2008/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.2345 - val_loss: 123.9779\n",
      "Epoch 2009/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 128.8951 - val_loss: 120.8234\n",
      "Epoch 2010/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 134.5775 - val_loss: 121.7880\n",
      "Epoch 2011/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.4993 - val_loss: 118.3049\n",
      "Epoch 2012/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.7338 - val_loss: 150.3908\n",
      "Epoch 2013/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.6060 - val_loss: 134.0592\n",
      "Epoch 2014/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.8832 - val_loss: 113.6110\n",
      "Epoch 2015/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.6878 - val_loss: 114.9366\n",
      "Epoch 2016/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.7172 - val_loss: 149.9994\n",
      "Epoch 2017/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 122.7958 - val_loss: 122.3162\n",
      "Epoch 2018/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.7505 - val_loss: 134.1464\n",
      "Epoch 2019/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.9857 - val_loss: 127.9887\n",
      "Epoch 2020/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.2315 - val_loss: 108.0156\n",
      "Epoch 2021/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.8303 - val_loss: 116.7427\n",
      "Epoch 2022/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 209.4500 - val_loss: 178.1673\n",
      "Epoch 2023/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.8213 - val_loss: 151.1848\n",
      "Epoch 2024/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.7236 - val_loss: 113.6577\n",
      "Epoch 2025/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.0089 - val_loss: 119.2729\n",
      "Epoch 2026/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.2324 - val_loss: 116.0903\n",
      "Epoch 2027/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.6376 - val_loss: 133.0653\n",
      "Epoch 2028/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.2021 - val_loss: 123.5139\n",
      "Epoch 2029/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.4194 - val_loss: 137.2280\n",
      "Epoch 2030/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.7902 - val_loss: 126.7122\n",
      "Epoch 2031/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.0285 - val_loss: 125.9843\n",
      "Epoch 2032/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.0817 - val_loss: 129.8780\n",
      "Epoch 2033/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 190.9178 - val_loss: 126.9634\n",
      "Epoch 2034/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.3389 - val_loss: 116.0942\n",
      "Epoch 2035/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.1843 - val_loss: 116.5287\n",
      "Epoch 2036/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.0250 - val_loss: 125.9566\n",
      "Epoch 2037/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.2782 - val_loss: 122.0717\n",
      "Epoch 2038/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.6458 - val_loss: 140.3195\n",
      "Epoch 2039/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.9380 - val_loss: 135.2974\n",
      "Epoch 2040/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.8953 - val_loss: 119.1500\n",
      "Epoch 2041/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 129.5899 - val_loss: 146.5148\n",
      "Epoch 2042/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.3240 - val_loss: 120.2593\n",
      "Epoch 2043/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 154.5725 - val_loss: 120.4490\n",
      "Epoch 2044/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.6447 - val_loss: 115.6411\n",
      "Epoch 2045/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.4839 - val_loss: 122.3796\n",
      "Epoch 2046/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.6996 - val_loss: 124.5446\n",
      "Epoch 2047/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.4379 - val_loss: 216.7408\n",
      "Epoch 2048/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.9925 - val_loss: 116.6014\n",
      "Epoch 2049/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.0992 - val_loss: 129.8708\n",
      "Epoch 2050/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.7539 - val_loss: 120.4770\n",
      "Epoch 2051/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.4983 - val_loss: 126.2789\n",
      "Epoch 2052/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.1521 - val_loss: 123.6431\n",
      "Epoch 2053/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.8973 - val_loss: 120.6655\n",
      "Epoch 2054/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.6610 - val_loss: 126.5270\n",
      "Epoch 2055/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.6796 - val_loss: 137.0616\n",
      "Epoch 2056/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.8961 - val_loss: 117.9930\n",
      "Epoch 2057/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.7978 - val_loss: 128.5568\n",
      "Epoch 2058/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.7485 - val_loss: 142.5304\n",
      "Epoch 2059/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.9299 - val_loss: 124.3630\n",
      "Epoch 2060/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.5937 - val_loss: 119.9530\n",
      "Epoch 2061/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.3633 - val_loss: 137.9804\n",
      "Epoch 2062/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.5363 - val_loss: 133.4355\n",
      "Epoch 2063/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.4012 - val_loss: 124.4491\n",
      "Epoch 2064/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.7395 - val_loss: 119.5719\n",
      "Epoch 2065/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.0279 - val_loss: 138.3836\n",
      "Epoch 2066/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 204.2935 - val_loss: 132.0150\n",
      "Epoch 2067/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.2535 - val_loss: 113.6877\n",
      "Epoch 2068/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.9218 - val_loss: 117.1667\n",
      "Epoch 2069/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.8845 - val_loss: 122.0909\n",
      "Epoch 2070/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.1774 - val_loss: 126.2134\n",
      "Epoch 2071/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.0074 - val_loss: 111.6988\n",
      "Epoch 2072/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.9879 - val_loss: 167.4114\n",
      "Epoch 2073/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.7924 - val_loss: 123.3810\n",
      "Epoch 2074/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.8917 - val_loss: 131.4814\n",
      "Epoch 2075/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.0413 - val_loss: 114.4515\n",
      "Epoch 2076/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.8378 - val_loss: 126.3887\n",
      "Epoch 2077/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.1589 - val_loss: 123.3506\n",
      "Epoch 2078/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.7861 - val_loss: 117.7535\n",
      "Epoch 2079/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.3633 - val_loss: 142.0171\n",
      "Epoch 2080/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.3750 - val_loss: 130.9847\n",
      "Epoch 2081/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.1397 - val_loss: 126.2905\n",
      "Epoch 2082/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.1753 - val_loss: 120.0097\n",
      "Epoch 2083/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.3972 - val_loss: 119.8733\n",
      "Epoch 2084/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.4323 - val_loss: 149.6081\n",
      "Epoch 2085/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.7611 - val_loss: 181.1509\n",
      "Epoch 2086/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.1390 - val_loss: 131.0489\n",
      "Epoch 2087/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.0211 - val_loss: 113.4568\n",
      "Epoch 2088/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.1637 - val_loss: 119.3161\n",
      "Epoch 2089/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 151.4664 - val_loss: 233.5395\n",
      "Epoch 2090/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 181.8944 - val_loss: 122.0540\n",
      "Epoch 2091/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.7715 - val_loss: 132.2988\n",
      "Epoch 2092/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.8062 - val_loss: 128.9809\n",
      "Epoch 2093/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.7943 - val_loss: 113.9028\n",
      "Epoch 2094/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.8380 - val_loss: 119.0330\n",
      "Epoch 2095/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 127.2996 - val_loss: 127.0117\n",
      "Epoch 2096/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 127.7970 - val_loss: 114.9263\n",
      "Epoch 2097/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 128.8857 - val_loss: 129.3820\n",
      "Epoch 2098/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.1496 - val_loss: 123.9280\n",
      "Epoch 2099/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.5615 - val_loss: 139.7800\n",
      "Epoch 2100/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.8482 - val_loss: 123.0877\n",
      "Epoch 2101/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.4541 - val_loss: 148.6622\n",
      "Epoch 2102/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.8694 - val_loss: 117.7840\n",
      "Epoch 2103/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.6780 - val_loss: 167.8946\n",
      "Epoch 2104/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.2131 - val_loss: 123.4385\n",
      "Epoch 2105/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.9430 - val_loss: 110.8104\n",
      "Epoch 2106/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.5825 - val_loss: 121.0508\n",
      "Epoch 2107/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.4916 - val_loss: 132.5826\n",
      "Epoch 2108/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.8650 - val_loss: 136.3348\n",
      "Epoch 2109/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.1621 - val_loss: 164.9820\n",
      "Epoch 2110/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.0777 - val_loss: 132.1644\n",
      "Epoch 2111/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.9899 - val_loss: 123.5514\n",
      "Epoch 2112/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.8753 - val_loss: 125.3208\n",
      "Epoch 2113/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.5688 - val_loss: 176.3326\n",
      "Epoch 2114/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 315.1529 - val_loss: 126.9311\n",
      "Epoch 2115/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.1186 - val_loss: 123.2398\n",
      "Epoch 2116/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.1690 - val_loss: 118.6141\n",
      "Epoch 2117/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.1375 - val_loss: 133.2396\n",
      "Epoch 2118/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.8648 - val_loss: 121.1177\n",
      "Epoch 2119/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.6582 - val_loss: 123.2908\n",
      "Epoch 2120/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.5023 - val_loss: 133.7088\n",
      "Epoch 2121/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.6264 - val_loss: 161.9887\n",
      "Epoch 2122/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.3664 - val_loss: 138.2782\n",
      "Epoch 2123/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 150.7645 - val_loss: 132.9477\n",
      "Epoch 2124/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.6937 - val_loss: 169.3615\n",
      "Epoch 2125/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.0432 - val_loss: 133.9039\n",
      "Epoch 2126/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.9934 - val_loss: 116.7755\n",
      "Epoch 2127/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.5195 - val_loss: 118.9131\n",
      "Epoch 2128/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.5313 - val_loss: 166.6927\n",
      "Epoch 2129/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.0644 - val_loss: 127.4761\n",
      "Epoch 2130/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.3290 - val_loss: 123.4349\n",
      "Epoch 2131/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.1029 - val_loss: 117.3081\n",
      "Epoch 2132/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.0959 - val_loss: 118.3622\n",
      "Epoch 2133/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.1799 - val_loss: 138.9098\n",
      "Epoch 2134/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.3556 - val_loss: 148.9660\n",
      "Epoch 2135/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.2917 - val_loss: 131.5768\n",
      "Epoch 2136/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.8561 - val_loss: 123.3944\n",
      "Epoch 2137/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.7105 - val_loss: 120.9788\n",
      "Epoch 2138/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.7134 - val_loss: 197.5393\n",
      "Epoch 2139/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.8975 - val_loss: 130.0431\n",
      "Epoch 2140/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.0294 - val_loss: 127.8008\n",
      "Epoch 2141/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.3728 - val_loss: 191.8880\n",
      "Epoch 2142/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.5404 - val_loss: 125.3404\n",
      "Epoch 2143/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.7432 - val_loss: 122.2230\n",
      "Epoch 2144/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.2227 - val_loss: 120.6615\n",
      "Epoch 2145/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.8788 - val_loss: 150.5273\n",
      "Epoch 2146/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.4545 - val_loss: 134.6961\n",
      "Epoch 2147/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.3269 - val_loss: 135.4002\n",
      "Epoch 2148/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.0737 - val_loss: 124.4053\n",
      "Epoch 2149/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.2169 - val_loss: 123.1996\n",
      "Epoch 2150/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.3453 - val_loss: 123.3268\n",
      "Epoch 2151/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.3435 - val_loss: 118.8003\n",
      "Epoch 2152/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.4617 - val_loss: 123.6002\n",
      "Epoch 2153/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.9843 - val_loss: 124.7803\n",
      "Epoch 2154/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.8906 - val_loss: 125.7969\n",
      "Epoch 2155/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.6531 - val_loss: 129.5503\n",
      "Epoch 2156/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.8672 - val_loss: 193.3781\n",
      "Epoch 2157/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.2045 - val_loss: 120.0239\n",
      "Epoch 2158/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.5563 - val_loss: 148.8433\n",
      "Epoch 2159/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.5611 - val_loss: 138.6137\n",
      "Epoch 2160/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.4863 - val_loss: 122.1487\n",
      "Epoch 2161/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 149.6123 - val_loss: 154.4566\n",
      "Epoch 2162/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 127.1195 - val_loss: 125.9963\n",
      "Epoch 2163/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 128.9031 - val_loss: 135.0372\n",
      "Epoch 2164/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 131.3009 - val_loss: 132.9224\n",
      "Epoch 2165/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.1388 - val_loss: 167.1918\n",
      "Epoch 2166/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.3258 - val_loss: 124.6250\n",
      "Epoch 2167/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.6260 - val_loss: 121.0744\n",
      "Epoch 2168/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.0832 - val_loss: 113.5778\n",
      "Epoch 2169/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.3247 - val_loss: 123.9226\n",
      "Epoch 2170/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.3492 - val_loss: 122.8069\n",
      "Epoch 2171/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.7399 - val_loss: 130.0583\n",
      "Epoch 2172/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.3647 - val_loss: 131.7657\n",
      "Epoch 2173/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.7853 - val_loss: 138.5414\n",
      "Epoch 2174/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.2302 - val_loss: 118.7120\n",
      "Epoch 2175/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.7537 - val_loss: 136.2151\n",
      "Epoch 2176/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.9675 - val_loss: 124.6375\n",
      "Epoch 2177/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.1281 - val_loss: 143.5327\n",
      "Epoch 2178/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.5360 - val_loss: 123.1650\n",
      "Epoch 2179/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.8744 - val_loss: 215.5261\n",
      "Epoch 2180/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.2441 - val_loss: 140.2447\n",
      "Epoch 2181/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.2557 - val_loss: 130.6146\n",
      "Epoch 2182/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.2840 - val_loss: 116.9416\n",
      "Epoch 2183/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.1806 - val_loss: 124.3980\n",
      "Epoch 2184/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.9633 - val_loss: 129.7632\n",
      "Epoch 2185/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.1558 - val_loss: 114.9041\n",
      "Epoch 2186/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.1587 - val_loss: 125.2534\n",
      "Epoch 2187/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.9120 - val_loss: 132.6331\n",
      "Epoch 2188/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.1453 - val_loss: 134.6179\n",
      "Epoch 2189/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 154.1472 - val_loss: 128.6484\n",
      "Epoch 2190/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.7525 - val_loss: 133.8946\n",
      "Epoch 2191/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.8407 - val_loss: 121.4524\n",
      "Epoch 2192/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.5985 - val_loss: 122.2968\n",
      "Epoch 2193/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.9902 - val_loss: 125.4270\n",
      "Epoch 2194/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.1603 - val_loss: 128.3897\n",
      "Epoch 2195/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.8989 - val_loss: 128.6298\n",
      "Epoch 2196/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.2363 - val_loss: 124.4892\n",
      "Epoch 2197/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.4809 - val_loss: 123.6835\n",
      "Epoch 2198/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.3956 - val_loss: 127.0629\n",
      "Epoch 2199/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.8779 - val_loss: 114.5086\n",
      "Epoch 2200/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.6509 - val_loss: 116.2878\n",
      "Epoch 2201/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.0423 - val_loss: 120.1671\n",
      "Epoch 2202/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.2225 - val_loss: 118.1539\n",
      "Epoch 2203/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.2260 - val_loss: 151.5958\n",
      "Epoch 2204/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.7415 - val_loss: 131.8933\n",
      "Epoch 2205/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.1788 - val_loss: 138.4936\n",
      "Epoch 2206/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.0249 - val_loss: 116.2414\n",
      "Epoch 2207/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.4704 - val_loss: 149.0096\n",
      "Epoch 2208/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.7464 - val_loss: 144.4792\n",
      "Epoch 2209/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.8707 - val_loss: 131.1663\n",
      "Epoch 2210/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.0518 - val_loss: 124.7266\n",
      "Epoch 2211/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.3786 - val_loss: 117.5989\n",
      "Epoch 2212/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.2077 - val_loss: 125.3267\n",
      "Epoch 2213/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.1692 - val_loss: 118.7181\n",
      "Epoch 2214/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.4644 - val_loss: 132.3210\n",
      "Epoch 2215/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.2033 - val_loss: 136.6039\n",
      "Epoch 2216/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.2204 - val_loss: 124.6879\n",
      "Epoch 2217/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.9457 - val_loss: 129.5107\n",
      "Epoch 2218/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.2444 - val_loss: 116.9159\n",
      "Epoch 2219/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.6240 - val_loss: 125.7117\n",
      "Epoch 2220/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.7741 - val_loss: 136.5638\n",
      "Epoch 2221/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.0409 - val_loss: 129.7306\n",
      "Epoch 2222/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.6026 - val_loss: 138.4465\n",
      "Epoch 2223/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.5807 - val_loss: 129.2933\n",
      "Epoch 2224/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.1321 - val_loss: 128.1528\n",
      "Epoch 2225/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 121.7712 - val_loss: 121.3496\n",
      "Epoch 2226/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.6081 - val_loss: 142.1383\n",
      "Epoch 2227/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.1538 - val_loss: 137.2814\n",
      "Epoch 2228/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.7805 - val_loss: 160.0653\n",
      "Epoch 2229/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.6583 - val_loss: 142.9789\n",
      "Epoch 2230/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.9433 - val_loss: 114.2369\n",
      "Epoch 2231/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.1589 - val_loss: 149.1600\n",
      "Epoch 2232/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.6383 - val_loss: 121.3372\n",
      "Epoch 2233/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.4656 - val_loss: 124.5398\n",
      "Epoch 2234/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.0887 - val_loss: 119.5310\n",
      "Epoch 2235/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.5185 - val_loss: 173.7801\n",
      "Epoch 2236/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.7156 - val_loss: 148.2489\n",
      "Epoch 2237/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.7223 - val_loss: 123.0194\n",
      "Epoch 2238/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.4127 - val_loss: 126.0811\n",
      "Epoch 2239/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 147.8745 - val_loss: 118.1296\n",
      "Epoch 2240/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 127.5125 - val_loss: 120.1530\n",
      "Epoch 2241/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 131.7682 - val_loss: 130.2925\n",
      "Epoch 2242/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 125.6063 - val_loss: 121.0517\n",
      "Epoch 2243/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.1959 - val_loss: 118.2877\n",
      "Epoch 2244/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.6476 - val_loss: 140.3599\n",
      "Epoch 2245/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.2035 - val_loss: 123.2877\n",
      "Epoch 2246/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.3118 - val_loss: 142.6523\n",
      "Epoch 2247/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.4730 - val_loss: 131.7187\n",
      "Epoch 2248/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.0900 - val_loss: 126.1707\n",
      "Epoch 2249/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.0977 - val_loss: 128.5841\n",
      "Epoch 2250/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.5395 - val_loss: 122.1163\n",
      "Epoch 2251/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.6939 - val_loss: 212.1859\n",
      "Epoch 2252/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 170.4935 - val_loss: 133.7282\n",
      "Epoch 2253/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.4441 - val_loss: 125.9207\n",
      "Epoch 2254/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.0640 - val_loss: 122.7760\n",
      "Epoch 2255/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.3212 - val_loss: 116.3411\n",
      "Epoch 2256/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.0613 - val_loss: 122.3038\n",
      "Epoch 2257/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.7834 - val_loss: 147.9810\n",
      "Epoch 2258/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.5018 - val_loss: 119.5710\n",
      "Epoch 2259/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.7783 - val_loss: 125.1838\n",
      "Epoch 2260/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.2341 - val_loss: 121.3732\n",
      "Epoch 2261/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.8701 - val_loss: 121.5150\n",
      "Epoch 2262/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.8644 - val_loss: 119.4160\n",
      "Epoch 2263/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.4945 - val_loss: 119.6027\n",
      "Epoch 2264/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.5342 - val_loss: 127.3501\n",
      "Epoch 2265/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.4546 - val_loss: 136.2233\n",
      "Epoch 2266/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.9542 - val_loss: 131.1535\n",
      "Epoch 2267/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.6607 - val_loss: 119.6822\n",
      "Epoch 2268/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.1670 - val_loss: 127.5252\n",
      "Epoch 2269/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.8951 - val_loss: 119.6754\n",
      "Epoch 2270/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.5521 - val_loss: 126.2187\n",
      "Epoch 2271/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.7401 - val_loss: 133.6849\n",
      "Epoch 2272/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.5899 - val_loss: 119.3785\n",
      "Epoch 2273/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.8197 - val_loss: 120.7529\n",
      "Epoch 2274/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.5040 - val_loss: 132.9565\n",
      "Epoch 2275/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.4174 - val_loss: 117.4554\n",
      "Epoch 2276/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.0251 - val_loss: 179.4319\n",
      "Epoch 2277/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.9816 - val_loss: 176.8507\n",
      "Epoch 2278/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.7000 - val_loss: 188.2823\n",
      "Epoch 2279/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.8452 - val_loss: 132.7684\n",
      "Epoch 2280/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.9178 - val_loss: 126.9331\n",
      "Epoch 2281/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.1850 - val_loss: 118.4476\n",
      "Epoch 2282/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.2509 - val_loss: 138.6665\n",
      "Epoch 2283/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.9382 - val_loss: 136.3267\n",
      "Epoch 2284/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 157.9157 - val_loss: 117.9194\n",
      "Epoch 2285/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.4710 - val_loss: 126.1065\n",
      "Epoch 2286/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.5395 - val_loss: 138.5820\n",
      "Epoch 2287/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.8999 - val_loss: 220.0826\n",
      "Epoch 2288/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.6041 - val_loss: 123.6313\n",
      "Epoch 2289/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 124.0597 - val_loss: 114.8886\n",
      "Epoch 2290/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.5675 - val_loss: 144.7987\n",
      "Epoch 2291/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.2253 - val_loss: 124.2622\n",
      "Epoch 2292/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.2247 - val_loss: 121.3719\n",
      "Epoch 2293/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.0547 - val_loss: 149.2942\n",
      "Epoch 2294/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 137.4116 - val_loss: 157.5295\n",
      "Epoch 2295/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.0041 - val_loss: 126.8683\n",
      "Epoch 2296/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.3220 - val_loss: 116.1501\n",
      "Epoch 2297/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.7300 - val_loss: 131.9512\n",
      "Epoch 2298/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.0385 - val_loss: 143.2142\n",
      "Epoch 2299/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.1110 - val_loss: 147.4120\n",
      "Epoch 2300/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.5277 - val_loss: 155.1492\n",
      "Epoch 2301/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.7500 - val_loss: 117.5818\n",
      "Epoch 2302/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 204.2560 - val_loss: 135.8155\n",
      "Epoch 2303/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.8950 - val_loss: 128.6939\n",
      "Epoch 2304/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.7693 - val_loss: 122.7562\n",
      "Epoch 2305/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.4093 - val_loss: 123.8566\n",
      "Epoch 2306/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.7393 - val_loss: 128.2827\n",
      "Epoch 2307/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.1848 - val_loss: 125.6104\n",
      "Epoch 2308/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.7828 - val_loss: 138.3497\n",
      "Epoch 2309/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.4541 - val_loss: 127.9834\n",
      "Epoch 2310/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.3858 - val_loss: 126.3892\n",
      "Epoch 2311/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.8966 - val_loss: 123.0317\n",
      "Epoch 2312/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.7981 - val_loss: 124.7328\n",
      "Epoch 2313/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.2454 - val_loss: 124.3662\n",
      "Epoch 2314/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.8262 - val_loss: 151.1593\n",
      "Epoch 2315/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.5265 - val_loss: 154.0264\n",
      "Epoch 2316/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 129.7769 - val_loss: 136.3162\n",
      "Epoch 2317/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 56us/step - loss: 119.7886 - val_loss: 132.2957\n",
      "Epoch 2318/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 122.9425 - val_loss: 114.5535\n",
      "Epoch 2319/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 130.7002 - val_loss: 126.7879\n",
      "Epoch 2320/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 126.1420 - val_loss: 124.4728\n",
      "Epoch 2321/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.6373 - val_loss: 186.0677\n",
      "Epoch 2322/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.8486 - val_loss: 121.1463\n",
      "Epoch 2323/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.9256 - val_loss: 124.2900\n",
      "Epoch 2324/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.1884 - val_loss: 139.9464\n",
      "Epoch 2325/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.4705 - val_loss: 146.6995\n",
      "Epoch 2326/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.8432 - val_loss: 134.4321\n",
      "Epoch 2327/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.3668 - val_loss: 123.7390\n",
      "Epoch 2328/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.1818 - val_loss: 122.5846\n",
      "Epoch 2329/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.7540 - val_loss: 120.8991\n",
      "Epoch 2330/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 164.2578 - val_loss: 117.5380\n",
      "Epoch 2331/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.7846 - val_loss: 128.3270\n",
      "Epoch 2332/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.5042 - val_loss: 127.4131\n",
      "Epoch 2333/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.0052 - val_loss: 126.4359\n",
      "Epoch 2334/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.2834 - val_loss: 133.9521\n",
      "Epoch 2335/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.3452 - val_loss: 162.9990\n",
      "Epoch 2336/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.2317 - val_loss: 130.3387\n",
      "Epoch 2337/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.2623 - val_loss: 142.8795\n",
      "Epoch 2338/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.9284 - val_loss: 117.0055\n",
      "Epoch 2339/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.8022 - val_loss: 158.4389\n",
      "Epoch 2340/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.3114 - val_loss: 131.0488\n",
      "Epoch 2341/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.9146 - val_loss: 122.7536\n",
      "Epoch 2342/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.3667 - val_loss: 131.8055\n",
      "Epoch 2343/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.3047 - val_loss: 149.3546\n",
      "Epoch 2344/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.4507 - val_loss: 152.5939\n",
      "Epoch 2345/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.9508 - val_loss: 142.3790\n",
      "Epoch 2346/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.7002 - val_loss: 164.9144\n",
      "Epoch 2347/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.2932 - val_loss: 116.1440\n",
      "Epoch 2348/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.5267 - val_loss: 114.6334\n",
      "Epoch 2349/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.8135 - val_loss: 120.6973\n",
      "Epoch 2350/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.0806 - val_loss: 129.8789\n",
      "Epoch 2351/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.6810 - val_loss: 118.0576\n",
      "Epoch 2352/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.5286 - val_loss: 146.4474\n",
      "Epoch 2353/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.5188 - val_loss: 155.9186\n",
      "Epoch 2354/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.1368 - val_loss: 123.2915\n",
      "Epoch 2355/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.7075 - val_loss: 122.6200\n",
      "Epoch 2356/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.1301 - val_loss: 132.8041\n",
      "Epoch 2357/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.2708 - val_loss: 122.0153\n",
      "Epoch 2358/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.4181 - val_loss: 151.8402\n",
      "Epoch 2359/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.2446 - val_loss: 115.8623\n",
      "Epoch 2360/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.5163 - val_loss: 123.7771\n",
      "Epoch 2361/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.2204 - val_loss: 116.3284\n",
      "Epoch 2362/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.2261 - val_loss: 133.4357\n",
      "Epoch 2363/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.1065 - val_loss: 137.2390\n",
      "Epoch 2364/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.1624 - val_loss: 173.1739\n",
      "Epoch 2365/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.5640 - val_loss: 132.8350\n",
      "Epoch 2366/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.1292 - val_loss: 124.1690\n",
      "Epoch 2367/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.4743 - val_loss: 181.1225\n",
      "Epoch 2368/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.1490 - val_loss: 148.1021\n",
      "Epoch 2369/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.8095 - val_loss: 121.9641\n",
      "Epoch 2370/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.4090 - val_loss: 124.5328\n",
      "Epoch 2371/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.6474 - val_loss: 222.8914\n",
      "Epoch 2372/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.4050 - val_loss: 122.1211\n",
      "Epoch 2373/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.2994 - val_loss: 127.9694\n",
      "Epoch 2374/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.3631 - val_loss: 133.1814\n",
      "Epoch 2375/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.0944 - val_loss: 114.0001\n",
      "Epoch 2376/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 172.3312 - val_loss: 134.4788\n",
      "Epoch 2377/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.2942 - val_loss: 144.3058\n",
      "Epoch 2378/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.6117 - val_loss: 121.1738\n",
      "Epoch 2379/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.4695 - val_loss: 135.4427\n",
      "Epoch 2380/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.7581 - val_loss: 113.2295\n",
      "Epoch 2381/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.3308 - val_loss: 140.8180\n",
      "Epoch 2382/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.8672 - val_loss: 119.2419\n",
      "Epoch 2383/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.6533 - val_loss: 164.8211\n",
      "Epoch 2384/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.1195 - val_loss: 119.2739\n",
      "Epoch 2385/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.5109 - val_loss: 117.5794\n",
      "Epoch 2386/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.2389 - val_loss: 120.8312\n",
      "Epoch 2387/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.4356 - val_loss: 128.5781\n",
      "Epoch 2388/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.8281 - val_loss: 122.9962\n",
      "Epoch 2389/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.1661 - val_loss: 145.3873\n",
      "Epoch 2390/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.2639 - val_loss: 122.1910\n",
      "Epoch 2391/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.3166 - val_loss: 122.3802\n",
      "Epoch 2392/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.5944 - val_loss: 133.5034\n",
      "Epoch 2393/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.0405 - val_loss: 124.8558\n",
      "Epoch 2394/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.3612 - val_loss: 123.9250\n",
      "Epoch 2395/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.1013 - val_loss: 130.4592\n",
      "Epoch 2396/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 127.0655 - val_loss: 120.9815\n",
      "Epoch 2397/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 119.7779 - val_loss: 127.2796\n",
      "Epoch 2398/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.3253 - val_loss: 120.1364\n",
      "Epoch 2399/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.1960 - val_loss: 133.8414\n",
      "Epoch 2400/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.4051 - val_loss: 127.7884\n",
      "Epoch 2401/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.6944 - val_loss: 116.4230\n",
      "Epoch 2402/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.0826 - val_loss: 121.4762\n",
      "Epoch 2403/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.1322 - val_loss: 120.4360\n",
      "Epoch 2404/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.0979 - val_loss: 124.3097\n",
      "Epoch 2405/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.2797 - val_loss: 124.4258\n",
      "Epoch 2406/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.3132 - val_loss: 127.8165\n",
      "Epoch 2407/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.8357 - val_loss: 143.4681\n",
      "Epoch 2408/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.0000 - val_loss: 127.8780\n",
      "Epoch 2409/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.7914 - val_loss: 122.0460\n",
      "Epoch 2410/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 304.1915 - val_loss: 117.6426\n",
      "Epoch 2411/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.3276 - val_loss: 116.9047\n",
      "Epoch 2412/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.5296 - val_loss: 124.8346\n",
      "Epoch 2413/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.9334 - val_loss: 169.8006\n",
      "Epoch 2414/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.6893 - val_loss: 120.6462\n",
      "Epoch 2415/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.8496 - val_loss: 152.2773\n",
      "Epoch 2416/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.0733 - val_loss: 138.7848\n",
      "Epoch 2417/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.4626 - val_loss: 197.0425\n",
      "Epoch 2418/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.2117 - val_loss: 134.8489\n",
      "Epoch 2419/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.4353 - val_loss: 135.2554\n",
      "Epoch 2420/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.7095 - val_loss: 193.1517\n",
      "Epoch 2421/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.9560 - val_loss: 115.9835\n",
      "Epoch 2422/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.8436 - val_loss: 121.9148\n",
      "Epoch 2423/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.6574 - val_loss: 140.3550\n",
      "Epoch 2424/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.4441 - val_loss: 127.6333\n",
      "Epoch 2425/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.3962 - val_loss: 117.8916\n",
      "Epoch 2426/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.6891 - val_loss: 115.3107\n",
      "Epoch 2427/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.1023 - val_loss: 152.7174\n",
      "Epoch 2428/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.7358 - val_loss: 117.8819\n",
      "Epoch 2429/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.5028 - val_loss: 114.6629\n",
      "Epoch 2430/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.5479 - val_loss: 168.6164\n",
      "Epoch 2431/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.2025 - val_loss: 132.9545\n",
      "Epoch 2432/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.7921 - val_loss: 215.8285\n",
      "Epoch 2433/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.2079 - val_loss: 123.8321\n",
      "Epoch 2434/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.1556 - val_loss: 120.2905\n",
      "Epoch 2435/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.6790 - val_loss: 145.6098\n",
      "Epoch 2436/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 163.1957 - val_loss: 125.7125\n",
      "Epoch 2437/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.7930 - val_loss: 125.0154\n",
      "Epoch 2438/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.6395 - val_loss: 118.2487\n",
      "Epoch 2439/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.2514 - val_loss: 126.7997\n",
      "Epoch 2440/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.9240 - val_loss: 123.1411\n",
      "Epoch 2441/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.1041 - val_loss: 124.5560\n",
      "Epoch 2442/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.3626 - val_loss: 144.9763\n",
      "Epoch 2443/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.1453 - val_loss: 144.8086\n",
      "Epoch 2444/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 135.9950 - val_loss: 132.8598\n",
      "Epoch 2445/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.7105 - val_loss: 137.5182\n",
      "Epoch 2446/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.4332 - val_loss: 125.0080\n",
      "Epoch 2447/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.8164 - val_loss: 116.3245\n",
      "Epoch 2448/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.1479 - val_loss: 136.8684\n",
      "Epoch 2449/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.6821 - val_loss: 120.0912\n",
      "Epoch 2450/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.8363 - val_loss: 143.0763\n",
      "Epoch 2451/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.6691 - val_loss: 143.6539\n",
      "Epoch 2452/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.2357 - val_loss: 126.4045\n",
      "Epoch 2453/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.8825 - val_loss: 117.3679\n",
      "Epoch 2454/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.0876 - val_loss: 134.4178\n",
      "Epoch 2455/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.6099 - val_loss: 111.6119\n",
      "Epoch 2456/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.5283 - val_loss: 119.6867\n",
      "Epoch 2457/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 126.1806 - val_loss: 144.4177\n",
      "Epoch 2458/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.1657 - val_loss: 130.0089\n",
      "Epoch 2459/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.9575 - val_loss: 124.2422\n",
      "Epoch 2460/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.3954 - val_loss: 149.3953\n",
      "Epoch 2461/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.2800 - val_loss: 126.6353\n",
      "Epoch 2462/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.5542 - val_loss: 133.5112\n",
      "Epoch 2463/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.3430 - val_loss: 120.9151\n",
      "Epoch 2464/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.5172 - val_loss: 145.4056\n",
      "Epoch 2465/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.2140 - val_loss: 152.7473\n",
      "Epoch 2466/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.5563 - val_loss: 131.6880\n",
      "Epoch 2467/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.2642 - val_loss: 120.7046\n",
      "Epoch 2468/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.8792 - val_loss: 118.2550\n",
      "Epoch 2469/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.9550 - val_loss: 113.9949\n",
      "Epoch 2470/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.2708 - val_loss: 131.2906\n",
      "Epoch 2471/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.6514 - val_loss: 126.0696\n",
      "Epoch 2472/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 119.1872 - val_loss: 117.8837\n",
      "Epoch 2473/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.1690 - val_loss: 132.4972\n",
      "Epoch 2474/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 124.7052 - val_loss: 123.1843\n",
      "Epoch 2475/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.7734 - val_loss: 118.1976\n",
      "Epoch 2476/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 148.0678 - val_loss: 134.9939\n",
      "Epoch 2477/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.8600 - val_loss: 129.9444\n",
      "Epoch 2478/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.2762 - val_loss: 127.2148\n",
      "Epoch 2479/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.0934 - val_loss: 133.2249\n",
      "Epoch 2480/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.2027 - val_loss: 139.2339\n",
      "Epoch 2481/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.4825 - val_loss: 129.0144\n",
      "Epoch 2482/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.9392 - val_loss: 123.0463\n",
      "Epoch 2483/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.8952 - val_loss: 138.8964\n",
      "Epoch 2484/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.8679 - val_loss: 124.1643\n",
      "Epoch 2485/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 140.7145 - val_loss: 120.3261\n",
      "Epoch 2486/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.1112 - val_loss: 121.1192\n",
      "Epoch 2487/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.8835 - val_loss: 121.1476\n",
      "Epoch 2488/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.4853 - val_loss: 136.8898\n",
      "Epoch 2489/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.4987 - val_loss: 122.6745\n",
      "Epoch 2490/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.0857 - val_loss: 117.5190\n",
      "Epoch 2491/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.0923 - val_loss: 150.2231\n",
      "Epoch 2492/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.6925 - val_loss: 145.4006\n",
      "Epoch 2493/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.3860 - val_loss: 130.0963\n",
      "Epoch 2494/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.3270 - val_loss: 155.6341\n",
      "Epoch 2495/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.0471 - val_loss: 133.1446\n",
      "Epoch 2496/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.2432 - val_loss: 125.3117\n",
      "Epoch 2497/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.3845 - val_loss: 159.8794\n",
      "Epoch 2498/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 152.2835 - val_loss: 138.7135\n",
      "Epoch 2499/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.4320 - val_loss: 194.9754\n",
      "Epoch 2500/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.2372 - val_loss: 138.4751\n",
      "Epoch 2501/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.9107 - val_loss: 132.2298\n",
      "Epoch 2502/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.0589 - val_loss: 125.1238\n",
      "Epoch 2503/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.7593 - val_loss: 141.7327\n",
      "Epoch 2504/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.7913 - val_loss: 141.9606\n",
      "Epoch 2505/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.4080 - val_loss: 120.0627\n",
      "Epoch 2506/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 118.0646 - val_loss: 121.4748\n",
      "Epoch 2507/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.8563 - val_loss: 126.2726\n",
      "Epoch 2508/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.9488 - val_loss: 123.2099\n",
      "Epoch 2509/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.8845 - val_loss: 116.9462\n",
      "Epoch 2510/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.7804 - val_loss: 119.4946\n",
      "Epoch 2511/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.3012 - val_loss: 173.4716\n",
      "Epoch 2512/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.5961 - val_loss: 128.0752\n",
      "Epoch 2513/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.9333 - val_loss: 150.7699\n",
      "Epoch 2514/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.5227 - val_loss: 120.3190\n",
      "Epoch 2515/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 167.6165 - val_loss: 133.7133\n",
      "Epoch 2516/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.3761 - val_loss: 127.7176\n",
      "Epoch 2517/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.3670 - val_loss: 128.0992\n",
      "Epoch 2518/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.9674 - val_loss: 133.1251\n",
      "Epoch 2519/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 129.5300 - val_loss: 131.0384\n",
      "Epoch 2520/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.7153 - val_loss: 131.4557\n",
      "Epoch 2521/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.3879 - val_loss: 138.6346\n",
      "Epoch 2522/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.5053 - val_loss: 136.2717\n",
      "Epoch 2523/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.3295 - val_loss: 156.4351\n",
      "Epoch 2524/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.6938 - val_loss: 129.0091\n",
      "Epoch 2525/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.5173 - val_loss: 121.8409\n",
      "Epoch 2526/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.9728 - val_loss: 175.9207\n",
      "Epoch 2527/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.2786 - val_loss: 120.1829\n",
      "Epoch 2528/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.0495 - val_loss: 120.4815\n",
      "Epoch 2529/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.2957 - val_loss: 191.8843\n",
      "Epoch 2530/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.4765 - val_loss: 132.3616\n",
      "Epoch 2531/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.8813 - val_loss: 136.1568\n",
      "Epoch 2532/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.3537 - val_loss: 150.1309\n",
      "Epoch 2533/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.3610 - val_loss: 138.0009\n",
      "Epoch 2534/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.9727 - val_loss: 136.7543\n",
      "Epoch 2535/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.7689 - val_loss: 211.4461\n",
      "Epoch 2536/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.2309 - val_loss: 143.3807\n",
      "Epoch 2537/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.1473 - val_loss: 123.4773\n",
      "Epoch 2538/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.0551 - val_loss: 124.7294\n",
      "Epoch 2539/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.9225 - val_loss: 114.6730\n",
      "Epoch 2540/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.5173 - val_loss: 133.9787\n",
      "Epoch 2541/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.7211 - val_loss: 123.2052\n",
      "Epoch 2542/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.2323 - val_loss: 1699.9572\n",
      "Epoch 2543/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.7782 - val_loss: 123.6461\n",
      "Epoch 2544/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.6276 - val_loss: 158.9751\n",
      "Epoch 2545/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.6998 - val_loss: 120.2840\n",
      "Epoch 2546/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.6919 - val_loss: 129.9624\n",
      "Epoch 2547/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.7129 - val_loss: 116.8096\n",
      "Epoch 2548/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.7861 - val_loss: 123.7635\n",
      "Epoch 2549/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.7576 - val_loss: 120.0845\n",
      "Epoch 2550/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 124.6071 - val_loss: 134.1334\n",
      "Epoch 2551/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 124.6234 - val_loss: 140.5869\n",
      "Epoch 2552/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 126.0453 - val_loss: 122.2208\n",
      "Epoch 2553/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 126.9971 - val_loss: 130.7734\n",
      "Epoch 2554/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 121.7758 - val_loss: 170.6574\n",
      "Epoch 2555/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.3401 - val_loss: 146.4921\n",
      "Epoch 2556/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.1046 - val_loss: 131.6396\n",
      "Epoch 2557/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.8491 - val_loss: 115.5810\n",
      "Epoch 2558/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.5689 - val_loss: 116.0965\n",
      "Epoch 2559/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.3928 - val_loss: 137.1314\n",
      "Epoch 2560/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.1357 - val_loss: 138.0877\n",
      "Epoch 2561/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.7790 - val_loss: 142.0765\n",
      "Epoch 2562/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.1250 - val_loss: 123.0168\n",
      "Epoch 2563/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.2745 - val_loss: 162.1459\n",
      "Epoch 2564/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.1065 - val_loss: 138.8751\n",
      "Epoch 2565/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.9802 - val_loss: 130.6260\n",
      "Epoch 2566/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.7122 - val_loss: 120.9919\n",
      "Epoch 2567/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.5775 - val_loss: 140.5883\n",
      "Epoch 2568/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.1358 - val_loss: 122.2411\n",
      "Epoch 2569/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.8409 - val_loss: 126.7906\n",
      "Epoch 2570/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.9270 - val_loss: 152.5301\n",
      "Epoch 2571/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.8965 - val_loss: 136.1256\n",
      "Epoch 2572/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.0420 - val_loss: 124.1230\n",
      "Epoch 2573/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.7057 - val_loss: 139.0453\n",
      "Epoch 2574/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.1562 - val_loss: 123.7901\n",
      "Epoch 2575/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.6951 - val_loss: 118.1502\n",
      "Epoch 2576/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 121.7677 - val_loss: 127.3657\n",
      "Epoch 2577/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 121.3137 - val_loss: 118.8383\n",
      "Epoch 2578/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.8468 - val_loss: 122.0541\n",
      "Epoch 2579/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.3677 - val_loss: 122.7311\n",
      "Epoch 2580/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.3204 - val_loss: 128.2034\n",
      "Epoch 2581/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.0613 - val_loss: 118.6680\n",
      "Epoch 2582/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.0484 - val_loss: 146.5256\n",
      "Epoch 2583/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.8115 - val_loss: 124.8057\n",
      "Epoch 2584/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.1571 - val_loss: 129.2432\n",
      "Epoch 2585/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.5787 - val_loss: 123.6261\n",
      "Epoch 2586/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.2057 - val_loss: 122.5763\n",
      "Epoch 2587/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.7775 - val_loss: 169.6857\n",
      "Epoch 2588/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.9375 - val_loss: 128.3110\n",
      "Epoch 2589/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.3189 - val_loss: 116.4771\n",
      "Epoch 2590/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.9691 - val_loss: 134.8885\n",
      "Epoch 2591/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.6194 - val_loss: 119.6380\n",
      "Epoch 2592/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 123.2641 - val_loss: 130.1469\n",
      "Epoch 2593/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.4763 - val_loss: 118.0059\n",
      "Epoch 2594/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.4194 - val_loss: 306.9760\n",
      "Epoch 2595/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.3811 - val_loss: 116.1075\n",
      "Epoch 2596/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 128.7132 - val_loss: 133.7571\n",
      "Epoch 2597/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.4176 - val_loss: 146.5828\n",
      "Epoch 2598/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.5531 - val_loss: 132.5761\n",
      "Epoch 2599/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.4318 - val_loss: 130.5717\n",
      "Epoch 2600/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.9109 - val_loss: 126.9786\n",
      "Epoch 2601/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.7705 - val_loss: 118.1139\n",
      "Epoch 2602/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.0596 - val_loss: 142.1461\n",
      "Epoch 2603/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.5751 - val_loss: 128.7990\n",
      "Epoch 2604/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.3427 - val_loss: 131.3555\n",
      "Epoch 2605/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.3604 - val_loss: 134.0189\n",
      "Epoch 2606/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.8268 - val_loss: 118.3199\n",
      "Epoch 2607/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.0827 - val_loss: 120.8478\n",
      "Epoch 2608/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.6350 - val_loss: 122.2911\n",
      "Epoch 2609/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.0597 - val_loss: 129.0842\n",
      "Epoch 2610/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.5768 - val_loss: 113.3661\n",
      "Epoch 2611/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.4486 - val_loss: 123.7381\n",
      "Epoch 2612/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.2956 - val_loss: 127.8592\n",
      "Epoch 2613/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.6367 - val_loss: 151.6707\n",
      "Epoch 2614/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.9881 - val_loss: 128.1467\n",
      "Epoch 2615/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.0599 - val_loss: 121.9756\n",
      "Epoch 2616/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.3915 - val_loss: 124.0072\n",
      "Epoch 2617/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.9367 - val_loss: 124.8062\n",
      "Epoch 2618/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.8700 - val_loss: 148.6024\n",
      "Epoch 2619/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.0176 - val_loss: 126.7119\n",
      "Epoch 2620/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.0629 - val_loss: 123.3953\n",
      "Epoch 2621/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.6124 - val_loss: 168.3322\n",
      "Epoch 2622/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.0089 - val_loss: 163.6081\n",
      "Epoch 2623/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.9694 - val_loss: 122.2570\n",
      "Epoch 2624/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.2552 - val_loss: 143.1476\n",
      "Epoch 2625/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.5756 - val_loss: 155.5077\n",
      "Epoch 2626/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 126.7911 - val_loss: 124.5460\n",
      "Epoch 2627/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 129.2310 - val_loss: 184.7555\n",
      "Epoch 2628/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 124.9551 - val_loss: 125.4378\n",
      "Epoch 2629/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.0868 - val_loss: 147.2493\n",
      "Epoch 2630/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.0510 - val_loss: 125.2909\n",
      "Epoch 2631/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.3139 - val_loss: 123.2293\n",
      "Epoch 2632/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.4047 - val_loss: 207.0811\n",
      "Epoch 2633/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.7459 - val_loss: 233.3755\n",
      "Epoch 2634/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.1006 - val_loss: 122.4399\n",
      "Epoch 2635/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.0964 - val_loss: 138.7499\n",
      "Epoch 2636/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.0244 - val_loss: 195.8659\n",
      "Epoch 2637/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.7742 - val_loss: 133.5705\n",
      "Epoch 2638/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 123.7099 - val_loss: 124.4407\n",
      "Epoch 2639/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.5759 - val_loss: 130.2859\n",
      "Epoch 2640/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.4795 - val_loss: 123.7213\n",
      "Epoch 2641/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.2826 - val_loss: 125.6798\n",
      "Epoch 2642/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.8245 - val_loss: 122.2953\n",
      "Epoch 2643/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 130.4212 - val_loss: 139.6199\n",
      "Epoch 2644/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.8663 - val_loss: 146.6613\n",
      "Epoch 2645/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.2105 - val_loss: 131.0646\n",
      "Epoch 2646/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.5662 - val_loss: 138.8058\n",
      "Epoch 2647/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.5298 - val_loss: 122.8536\n",
      "Epoch 2648/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.1640 - val_loss: 143.8720\n",
      "Epoch 2649/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.9686 - val_loss: 122.1228\n",
      "Epoch 2650/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.3935 - val_loss: 120.7697\n",
      "Epoch 2651/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.5060 - val_loss: 137.9347\n",
      "Epoch 2652/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.1124 - val_loss: 135.1213\n",
      "Epoch 2653/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 127.2695 - val_loss: 116.2118\n",
      "Epoch 2654/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.7095 - val_loss: 126.8530\n",
      "Epoch 2655/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 118.0714 - val_loss: 122.8998\n",
      "Epoch 2656/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.2208 - val_loss: 124.5599\n",
      "Epoch 2657/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.5210 - val_loss: 122.0553\n",
      "Epoch 2658/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.0102 - val_loss: 119.4812\n",
      "Epoch 2659/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.7445 - val_loss: 121.2871\n",
      "Epoch 2660/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.6187 - val_loss: 156.5177\n",
      "Epoch 2661/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.5377 - val_loss: 120.8768\n",
      "Epoch 2662/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.5500 - val_loss: 191.9791\n",
      "Epoch 2663/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.0802 - val_loss: 141.3020\n",
      "Epoch 2664/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 119.5343 - val_loss: 132.1395\n",
      "Epoch 2665/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.2637 - val_loss: 115.0816\n",
      "Epoch 2666/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.3618 - val_loss: 150.3400\n",
      "Epoch 2667/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 173.3328 - val_loss: 132.1727\n",
      "Epoch 2668/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.6921 - val_loss: 129.6738\n",
      "Epoch 2669/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.4495 - val_loss: 146.4006\n",
      "Epoch 2670/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.8056 - val_loss: 124.0951\n",
      "Epoch 2671/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.0722 - val_loss: 176.4041\n",
      "Epoch 2672/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.8199 - val_loss: 122.3330\n",
      "Epoch 2673/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.5339 - val_loss: 131.8063\n",
      "Epoch 2674/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.8323 - val_loss: 142.5067\n",
      "Epoch 2675/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 115.9340 - val_loss: 130.0996\n",
      "Epoch 2676/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.7131 - val_loss: 137.5184\n",
      "Epoch 2677/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.1657 - val_loss: 145.6176\n",
      "Epoch 2678/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.9602 - val_loss: 152.5861\n",
      "Epoch 2679/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.4576 - val_loss: 130.9412\n",
      "Epoch 2680/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.5985 - val_loss: 126.2956\n",
      "Epoch 2681/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.9302 - val_loss: 140.4677\n",
      "Epoch 2682/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.1401 - val_loss: 119.2681\n",
      "Epoch 2683/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.3751 - val_loss: 131.0945\n",
      "Epoch 2684/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.3414 - val_loss: 116.8603\n",
      "Epoch 2685/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.2120 - val_loss: 122.0804\n",
      "Epoch 2686/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.0717 - val_loss: 144.9747\n",
      "Epoch 2687/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.9781 - val_loss: 584.1382\n",
      "Epoch 2688/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.4822 - val_loss: 124.4124\n",
      "Epoch 2689/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 122.5002 - val_loss: 123.3069\n",
      "Epoch 2690/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 118.7468 - val_loss: 119.9522\n",
      "Epoch 2691/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.6682 - val_loss: 125.3772\n",
      "Epoch 2692/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.7651 - val_loss: 177.9244\n",
      "Epoch 2693/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.2287 - val_loss: 132.6425\n",
      "Epoch 2694/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.9237 - val_loss: 149.6007\n",
      "Epoch 2695/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 126.4175 - val_loss: 129.4163\n",
      "Epoch 2696/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.8694 - val_loss: 122.6693\n",
      "Epoch 2697/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.0908 - val_loss: 126.0517\n",
      "Epoch 2698/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.9642 - val_loss: 123.7671\n",
      "Epoch 2699/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.1789 - val_loss: 120.1323\n",
      "Epoch 2700/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.9732 - val_loss: 122.0945\n",
      "Epoch 2701/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.6790 - val_loss: 134.6290\n",
      "Epoch 2702/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 130.6244 - val_loss: 144.4730\n",
      "Epoch 2703/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 126.1793 - val_loss: 128.0460\n",
      "Epoch 2704/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 128.3007 - val_loss: 135.9268\n",
      "Epoch 2705/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 133.4027 - val_loss: 123.6044\n",
      "Epoch 2706/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.3084 - val_loss: 125.6202\n",
      "Epoch 2707/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.6726 - val_loss: 127.7555\n",
      "Epoch 2708/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.9227 - val_loss: 122.2788\n",
      "Epoch 2709/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.0319 - val_loss: 142.7192\n",
      "Epoch 2710/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.0875 - val_loss: 127.1053\n",
      "Epoch 2711/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.7680 - val_loss: 122.5684\n",
      "Epoch 2712/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.6460 - val_loss: 122.7022\n",
      "Epoch 2713/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.5498 - val_loss: 127.9134\n",
      "Epoch 2714/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.9745 - val_loss: 130.3996\n",
      "Epoch 2715/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.8338 - val_loss: 125.6937\n",
      "Epoch 2716/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.2302 - val_loss: 127.7115\n",
      "Epoch 2717/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.8353 - val_loss: 118.3099\n",
      "Epoch 2718/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.0768 - val_loss: 129.7885\n",
      "Epoch 2719/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.1516 - val_loss: 152.8005\n",
      "Epoch 2720/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.9956 - val_loss: 145.4637\n",
      "Epoch 2721/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.1429 - val_loss: 115.5858\n",
      "Epoch 2722/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.9957 - val_loss: 130.8987\n",
      "Epoch 2723/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.1383 - val_loss: 127.8785\n",
      "Epoch 2724/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.2318 - val_loss: 183.6936\n",
      "Epoch 2725/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.8521 - val_loss: 117.1449\n",
      "Epoch 2726/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.4055 - val_loss: 124.2357\n",
      "Epoch 2727/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.0118 - val_loss: 127.2804\n",
      "Epoch 2728/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.2311 - val_loss: 126.6933\n",
      "Epoch 2729/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 120.9252 - val_loss: 141.4076\n",
      "Epoch 2730/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.0983 - val_loss: 122.6456\n",
      "Epoch 2731/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.2116 - val_loss: 198.4172\n",
      "Epoch 2732/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.5106 - val_loss: 126.5305\n",
      "Epoch 2733/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.9167 - val_loss: 183.2276\n",
      "Epoch 2734/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.0452 - val_loss: 118.3416\n",
      "Epoch 2735/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.9381 - val_loss: 127.7072\n",
      "Epoch 2736/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.6613 - val_loss: 174.6260\n",
      "Epoch 2737/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.4785 - val_loss: 130.4545\n",
      "Epoch 2738/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.8348 - val_loss: 128.4783\n",
      "Epoch 2739/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 149.7247 - val_loss: 122.1135\n",
      "Epoch 2740/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.9730 - val_loss: 119.4484\n",
      "Epoch 2741/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.7583 - val_loss: 130.1300\n",
      "Epoch 2742/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.4220 - val_loss: 120.7100\n",
      "Epoch 2743/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.4423 - val_loss: 118.8426\n",
      "Epoch 2744/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.3761 - val_loss: 147.4048\n",
      "Epoch 2745/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.8454 - val_loss: 119.0186\n",
      "Epoch 2746/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.9812 - val_loss: 121.5290\n",
      "Epoch 2747/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 125.1998 - val_loss: 136.8996\n",
      "Epoch 2748/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.8272 - val_loss: 142.3192\n",
      "Epoch 2749/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.5671 - val_loss: 116.0339\n",
      "Epoch 2750/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.5441 - val_loss: 122.0664\n",
      "Epoch 2751/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.3583 - val_loss: 116.6696\n",
      "Epoch 2752/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.6167 - val_loss: 126.2694\n",
      "Epoch 2753/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.7024 - val_loss: 120.4860\n",
      "Epoch 2754/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 167.0212 - val_loss: 158.4831\n",
      "Epoch 2755/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.3809 - val_loss: 123.4036\n",
      "Epoch 2756/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.7193 - val_loss: 119.8596\n",
      "Epoch 2757/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.5450 - val_loss: 135.2239\n",
      "Epoch 2758/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.8928 - val_loss: 117.1716\n",
      "Epoch 2759/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.1895 - val_loss: 127.7283\n",
      "Epoch 2760/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.1379 - val_loss: 124.1491\n",
      "Epoch 2761/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.2994 - val_loss: 138.4942\n",
      "Epoch 2762/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.3477 - val_loss: 129.0684\n",
      "Epoch 2763/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.2387 - val_loss: 116.7517\n",
      "Epoch 2764/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.4673 - val_loss: 127.6068\n",
      "Epoch 2765/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.8987 - val_loss: 132.8252\n",
      "Epoch 2766/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.1912 - val_loss: 126.0703\n",
      "Epoch 2767/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.2153 - val_loss: 151.1094\n",
      "Epoch 2768/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.6819 - val_loss: 116.3797\n",
      "Epoch 2769/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.9976 - val_loss: 118.5365\n",
      "Epoch 2770/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 130.0882 - val_loss: 120.0094\n",
      "Epoch 2771/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.7438 - val_loss: 133.1460\n",
      "Epoch 2772/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.9078 - val_loss: 122.9149\n",
      "Epoch 2773/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.1524 - val_loss: 127.3807\n",
      "Epoch 2774/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.0589 - val_loss: 122.3352\n",
      "Epoch 2775/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.2028 - val_loss: 122.6553\n",
      "Epoch 2776/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.3928 - val_loss: 126.4631\n",
      "Epoch 2777/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.1844 - val_loss: 110.0459\n",
      "Epoch 2778/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.9400 - val_loss: 165.9759\n",
      "Epoch 2779/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 129.5939 - val_loss: 128.7727\n",
      "Epoch 2780/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 125.3983 - val_loss: 121.3015\n",
      "Epoch 2781/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 125.4918 - val_loss: 130.1814\n",
      "Epoch 2782/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.2298 - val_loss: 120.5431\n",
      "Epoch 2783/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.2259 - val_loss: 124.6331\n",
      "Epoch 2784/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.2074 - val_loss: 122.6982\n",
      "Epoch 2785/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.0543 - val_loss: 124.6591\n",
      "Epoch 2786/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.6862 - val_loss: 128.9245\n",
      "Epoch 2787/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.6996 - val_loss: 122.4122\n",
      "Epoch 2788/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.4751 - val_loss: 116.5157\n",
      "Epoch 2789/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.8783 - val_loss: 123.2204\n",
      "Epoch 2790/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.0754 - val_loss: 124.5237\n",
      "Epoch 2791/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.7723 - val_loss: 165.7399\n",
      "Epoch 2792/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.3683 - val_loss: 137.0184\n",
      "Epoch 2793/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.8682 - val_loss: 123.4452\n",
      "Epoch 2794/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.3896 - val_loss: 186.9004\n",
      "Epoch 2795/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.1226 - val_loss: 169.1777\n",
      "Epoch 2796/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.0705 - val_loss: 125.9360\n",
      "Epoch 2797/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.5495 - val_loss: 144.5234\n",
      "Epoch 2798/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.0031 - val_loss: 123.6548\n",
      "Epoch 2799/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.5027 - val_loss: 140.1970\n",
      "Epoch 2800/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.2855 - val_loss: 125.5027\n",
      "Epoch 2801/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.4095 - val_loss: 117.3633\n",
      "Epoch 2802/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.8593 - val_loss: 121.6641\n",
      "Epoch 2803/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.4290 - val_loss: 153.6909\n",
      "Epoch 2804/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.1660 - val_loss: 130.1775\n",
      "Epoch 2805/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.1528 - val_loss: 125.5474\n",
      "Epoch 2806/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.2658 - val_loss: 125.7821\n",
      "Epoch 2807/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.2158 - val_loss: 132.5103\n",
      "Epoch 2808/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.9217 - val_loss: 131.0160\n",
      "Epoch 2809/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.4073 - val_loss: 141.8965\n",
      "Epoch 2810/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 120.0872 - val_loss: 117.4039\n",
      "Epoch 2811/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.7455 - val_loss: 123.1798\n",
      "Epoch 2812/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 183.5256 - val_loss: 128.5957\n",
      "Epoch 2813/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.8563 - val_loss: 134.1230\n",
      "Epoch 2814/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 116.0976 - val_loss: 118.5684\n",
      "Epoch 2815/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 120.3087 - val_loss: 118.3371\n",
      "Epoch 2816/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 118.4464 - val_loss: 124.9944\n",
      "Epoch 2817/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.8211 - val_loss: 144.6007\n",
      "Epoch 2818/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.9566 - val_loss: 122.2277\n",
      "Epoch 2819/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.0867 - val_loss: 119.5020\n",
      "Epoch 2820/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.9135 - val_loss: 113.9659\n",
      "Epoch 2821/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.7098 - val_loss: 128.6311\n",
      "Epoch 2822/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.9751 - val_loss: 142.4636\n",
      "Epoch 2823/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.2879 - val_loss: 117.7629\n",
      "Epoch 2824/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.9427 - val_loss: 115.3770\n",
      "Epoch 2825/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 125.9055 - val_loss: 123.5639\n",
      "Epoch 2826/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.7649 - val_loss: 154.8898\n",
      "Epoch 2827/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.0691 - val_loss: 129.7587\n",
      "Epoch 2828/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.8944 - val_loss: 171.2755\n",
      "Epoch 2829/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 164.6443 - val_loss: 123.8192\n",
      "Epoch 2830/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.1108 - val_loss: 113.3941\n",
      "Epoch 2831/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.0691 - val_loss: 124.8504\n",
      "Epoch 2832/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 117.0587 - val_loss: 112.5127\n",
      "Epoch 2833/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 120.7516 - val_loss: 125.5138\n",
      "Epoch 2834/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 123.2238 - val_loss: 141.4088\n",
      "Epoch 2835/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 126.0402 - val_loss: 140.7948\n",
      "Epoch 2836/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.4813 - val_loss: 125.0433\n",
      "Epoch 2837/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.4697 - val_loss: 125.9595\n",
      "Epoch 2838/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.0727 - val_loss: 137.8225\n",
      "Epoch 2839/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 119.0317 - val_loss: 134.8682\n",
      "Epoch 2840/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.0306 - val_loss: 135.5663\n",
      "Epoch 2841/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.0891 - val_loss: 125.7519\n",
      "Epoch 2842/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.7789 - val_loss: 122.8346\n",
      "Epoch 2843/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.9875 - val_loss: 180.3490\n",
      "Epoch 2844/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.2296 - val_loss: 127.4071\n",
      "Epoch 2845/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.4176 - val_loss: 124.3809\n",
      "Epoch 2846/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.6991 - val_loss: 127.2433\n",
      "Epoch 2847/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.0610 - val_loss: 115.0687\n",
      "Epoch 2848/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.6423 - val_loss: 130.2870\n",
      "Epoch 2849/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.2958 - val_loss: 126.1107\n",
      "Epoch 2850/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.4931 - val_loss: 121.8229\n",
      "Epoch 2851/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.2019 - val_loss: 120.6984\n",
      "Epoch 2852/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.0923 - val_loss: 121.8297\n",
      "Epoch 2853/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.5240 - val_loss: 122.5794\n",
      "Epoch 2854/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.5136 - val_loss: 124.7696\n",
      "Epoch 2855/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.2002 - val_loss: 124.5124\n",
      "Epoch 2856/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.5020 - val_loss: 121.5197\n",
      "Epoch 2857/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.6051 - val_loss: 120.8040\n",
      "Epoch 2858/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.0115 - val_loss: 127.1086\n",
      "Epoch 2859/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.6604 - val_loss: 153.9641\n",
      "Epoch 2860/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.5390 - val_loss: 121.3117\n",
      "Epoch 2861/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.5189 - val_loss: 129.3509\n",
      "Epoch 2862/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.2543 - val_loss: 127.6622\n",
      "Epoch 2863/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.5290 - val_loss: 121.7718\n",
      "Epoch 2864/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.9561 - val_loss: 160.1512\n",
      "Epoch 2865/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.2277 - val_loss: 119.3083\n",
      "Epoch 2866/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.4433 - val_loss: 126.2466\n",
      "Epoch 2867/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 126.9919 - val_loss: 141.7991\n",
      "Epoch 2868/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 119.2457 - val_loss: 137.4489\n",
      "Epoch 2869/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 124.4506 - val_loss: 123.4253\n",
      "Epoch 2870/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 131.2424 - val_loss: 126.2302\n",
      "Epoch 2871/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 248.5941 - val_loss: 126.0233\n",
      "Epoch 2872/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.5772 - val_loss: 137.0730\n",
      "Epoch 2873/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.8500 - val_loss: 131.2501\n",
      "Epoch 2874/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.9313 - val_loss: 122.6082\n",
      "Epoch 2875/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.4552 - val_loss: 172.7774\n",
      "Epoch 2876/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.1746 - val_loss: 117.0464\n",
      "Epoch 2877/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.5190 - val_loss: 155.4654\n",
      "Epoch 2878/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.4188 - val_loss: 129.7963\n",
      "Epoch 2879/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.1505 - val_loss: 132.6748\n",
      "Epoch 2880/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.9975 - val_loss: 114.9211\n",
      "Epoch 2881/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 179.4149 - val_loss: 127.9610\n",
      "Epoch 2882/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.9174 - val_loss: 122.5654\n",
      "Epoch 2883/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.3387 - val_loss: 116.4131\n",
      "Epoch 2884/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.9210 - val_loss: 114.7784\n",
      "Epoch 2885/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 118.3866 - val_loss: 132.1685\n",
      "Epoch 2886/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.7194 - val_loss: 132.0107\n",
      "Epoch 2887/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.9877 - val_loss: 157.2963\n",
      "Epoch 2888/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.8211 - val_loss: 129.5244\n",
      "Epoch 2889/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.0428 - val_loss: 122.4431\n",
      "Epoch 2890/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.5701 - val_loss: 128.2932\n",
      "Epoch 2891/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.9082 - val_loss: 129.1237\n",
      "Epoch 2892/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.8935 - val_loss: 125.1975\n",
      "Epoch 2893/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.7212 - val_loss: 118.6212\n",
      "Epoch 2894/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.6536 - val_loss: 117.0981\n",
      "Epoch 2895/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.3240 - val_loss: 115.7702\n",
      "Epoch 2896/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 124.3793 - val_loss: 116.0623\n",
      "Epoch 2897/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.4042 - val_loss: 119.6447\n",
      "Epoch 2898/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.4687 - val_loss: 198.8366\n",
      "Epoch 2899/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.1947 - val_loss: 137.5386\n",
      "Epoch 2900/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.8760 - val_loss: 153.7867\n",
      "Epoch 2901/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.7826 - val_loss: 129.6423\n",
      "Epoch 2902/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.6855 - val_loss: 134.7790\n",
      "Epoch 2903/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.0962 - val_loss: 127.3809\n",
      "Epoch 2904/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.6079 - val_loss: 122.2754\n",
      "Epoch 2905/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 117.6054 - val_loss: 130.3369\n",
      "Epoch 2906/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 120.4624 - val_loss: 141.7130\n",
      "Epoch 2907/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.4206 - val_loss: 119.2579\n",
      "Epoch 2908/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 127.5247 - val_loss: 114.0070\n",
      "Epoch 2909/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.4177 - val_loss: 121.9994\n",
      "Epoch 2910/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 122.0221 - val_loss: 130.9450\n",
      "Epoch 2911/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.3242 - val_loss: 138.1786\n",
      "Epoch 2912/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 180.6688 - val_loss: 134.7144\n",
      "Epoch 2913/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.2420 - val_loss: 120.6414\n",
      "Epoch 2914/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.9231 - val_loss: 124.1046\n",
      "Epoch 2915/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.0130 - val_loss: 125.0345\n",
      "Epoch 2916/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.4465 - val_loss: 129.3317\n",
      "Epoch 2917/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.9978 - val_loss: 136.5391\n",
      "Epoch 2918/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.3535 - val_loss: 136.1457\n",
      "Epoch 2919/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.9541 - val_loss: 179.3172\n",
      "Epoch 2920/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.5853 - val_loss: 126.9738\n",
      "Epoch 2921/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.8970 - val_loss: 118.6791\n",
      "Epoch 2922/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.4765 - val_loss: 128.2865\n",
      "Epoch 2923/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.7938 - val_loss: 120.4085\n",
      "Epoch 2924/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 117.1141 - val_loss: 128.7057\n",
      "Epoch 2925/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.1706 - val_loss: 364.6577\n",
      "Epoch 2926/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.9830 - val_loss: 146.5552\n",
      "Epoch 2927/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.0444 - val_loss: 126.6101\n",
      "Epoch 2928/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.1441 - val_loss: 142.3870\n",
      "Epoch 2929/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 118.7650 - val_loss: 129.5988\n",
      "Epoch 2930/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 119.0061 - val_loss: 118.4724\n",
      "Epoch 2931/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 125.1060 - val_loss: 124.4515\n",
      "Epoch 2932/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.5259 - val_loss: 123.7797\n",
      "Epoch 2933/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 169.0132 - val_loss: 114.7163\n",
      "Epoch 2934/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 121.8382 - val_loss: 132.2735\n",
      "Epoch 2935/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 126.8095 - val_loss: 113.9229\n",
      "Epoch 2936/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 126.0113 - val_loss: 123.9055\n",
      "Epoch 2937/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 125.1200 - val_loss: 128.5079\n",
      "Epoch 2938/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.5653 - val_loss: 113.6280\n",
      "Epoch 2939/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 121.4730 - val_loss: 122.5593\n",
      "Epoch 2940/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 120.6075 - val_loss: 140.7583\n",
      "Epoch 2941/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 121.8383 - val_loss: 123.5586\n",
      "Epoch 2942/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.1240 - val_loss: 123.1687\n",
      "Epoch 2943/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.5330 - val_loss: 164.7600\n",
      "Epoch 2944/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.0515 - val_loss: 147.3186\n",
      "Epoch 2945/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.3554 - val_loss: 147.7496\n",
      "Epoch 2946/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.9688 - val_loss: 139.9216\n",
      "Epoch 2947/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.8984 - val_loss: 121.5029\n",
      "Epoch 2948/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 124.0958 - val_loss: 129.9245\n",
      "Epoch 2949/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.4125 - val_loss: 120.3289\n",
      "Epoch 2950/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 119.7084 - val_loss: 121.4352\n",
      "Epoch 2951/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 123.4004 - val_loss: 140.0631\n",
      "Epoch 2952/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.4404 - val_loss: 130.9203\n",
      "Epoch 2953/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.2740 - val_loss: 124.8916\n",
      "Epoch 2954/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 120.8607 - val_loss: 116.0428\n",
      "Epoch 2955/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.4653 - val_loss: 119.7373\n",
      "Epoch 2956/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 126.6910 - val_loss: 129.1671\n",
      "Epoch 2957/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.4524 - val_loss: 128.4925\n",
      "Epoch 2958/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 121.3004 - val_loss: 120.6811\n",
      "Epoch 2959/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 122.0111 - val_loss: 118.9678\n",
      "Epoch 2960/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.2860 - val_loss: 141.9500\n",
      "Epoch 2961/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.6258 - val_loss: 140.1839\n",
      "Epoch 2962/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.6037 - val_loss: 118.8400\n",
      "Epoch 2963/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 124.3944 - val_loss: 131.2908\n",
      "Epoch 2964/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 126.0810 - val_loss: 123.1070\n",
      "Epoch 2965/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step - loss: 122.0705 - val_loss: 143.6173\n",
      "Epoch 2966/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.9932 - val_loss: 131.3067\n",
      "Epoch 2967/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.4046 - val_loss: 119.3722\n",
      "Epoch 2968/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 128.5085 - val_loss: 126.2029\n",
      "Epoch 2969/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.1731 - val_loss: 134.9181\n",
      "Epoch 2970/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.3254 - val_loss: 128.1038\n",
      "Epoch 2971/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 126.8227 - val_loss: 123.2929\n",
      "Epoch 2972/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 125.7603 - val_loss: 120.4019\n",
      "Epoch 2973/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 125.1000 - val_loss: 125.2093\n",
      "Epoch 2974/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 117.4828 - val_loss: 151.3916\n",
      "Epoch 2975/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.4645 - val_loss: 127.7560\n",
      "Epoch 2976/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.8459 - val_loss: 148.9308\n",
      "Epoch 2977/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.1217 - val_loss: 132.1064\n",
      "Epoch 2978/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.2626 - val_loss: 129.2429\n",
      "Epoch 2979/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.8597 - val_loss: 158.2269\n",
      "Epoch 2980/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.7836 - val_loss: 144.1314\n",
      "Epoch 2981/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 121.9307 - val_loss: 127.3306\n",
      "Epoch 2982/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 125.9781 - val_loss: 122.3360\n",
      "Epoch 2983/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 123.4223 - val_loss: 132.5096\n",
      "Epoch 2984/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.9325 - val_loss: 130.8994\n",
      "Epoch 2985/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.5693 - val_loss: 121.9437\n",
      "Epoch 2986/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 120.2374 - val_loss: 123.9573\n",
      "Epoch 2987/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.6459 - val_loss: 121.6639\n",
      "Epoch 2988/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 121.1924 - val_loss: 126.6299\n",
      "Epoch 2989/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.7658 - val_loss: 142.7612\n",
      "Epoch 2990/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.8476 - val_loss: 130.4128\n",
      "Epoch 2991/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.4369 - val_loss: 126.9643\n",
      "Epoch 2992/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.0012 - val_loss: 133.5160\n",
      "Epoch 2993/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.5770 - val_loss: 128.1436\n",
      "Epoch 2994/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.7043 - val_loss: 117.2970\n",
      "Epoch 2995/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 124.2331 - val_loss: 118.7728\n",
      "Epoch 2996/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.3481 - val_loss: 115.2566\n",
      "Epoch 2997/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.8027 - val_loss: 114.2402\n",
      "Epoch 2998/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.5038 - val_loss: 114.4457\n",
      "Epoch 2999/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.6709 - val_loss: 130.0275\n",
      "Epoch 3000/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.0139 - val_loss: 113.2406\n",
      "Epoch 3001/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 124.8132 - val_loss: 119.1269\n",
      "Epoch 3002/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.6532 - val_loss: 134.3619\n",
      "Epoch 3003/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 125.0120 - val_loss: 119.4584\n",
      "Epoch 3004/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 122.0912 - val_loss: 139.6138\n",
      "Epoch 3005/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.8321 - val_loss: 139.4016\n",
      "Epoch 3006/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.3056 - val_loss: 129.5709\n",
      "Epoch 3007/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 130.6430 - val_loss: 128.7358\n",
      "Epoch 3008/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 122.5466 - val_loss: 127.2435\n",
      "Epoch 3009/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 123.9147 - val_loss: 155.3212\n",
      "Epoch 3010/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.4180 - val_loss: 166.6465\n",
      "Epoch 3011/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 123.4456 - val_loss: 119.4601\n",
      "Epoch 3012/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 119.1227 - val_loss: 139.0508\n",
      "Epoch 3013/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 123.1401 - val_loss: 122.1302\n",
      "Epoch 3014/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 120.8268 - val_loss: 174.9733\n",
      "Epoch 3015/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.3436 - val_loss: 121.9293\n",
      "Epoch 3016/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 123.5774 - val_loss: 143.3549\n",
      "Epoch 3017/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.2506 - val_loss: 130.4109\n",
      "Epoch 3018/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 126.8161 - val_loss: 116.7180\n",
      "Epoch 3019/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 119.1760 - val_loss: 198.9049\n",
      "Epoch 3020/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.2813 - val_loss: 121.9292\n",
      "Epoch 03020: early stopping\n",
      "Fold score (RMSE): 10.817968368530273\n",
      "Fold #5\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 9671.5455 - val_loss: 4920.6224\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 5165.1916 - val_loss: 4647.2063\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 4829.4588 - val_loss: 4133.6059\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 4690.9399 - val_loss: 4067.4523\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 4431.9911 - val_loss: 3508.2523\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 4356.8248 - val_loss: 3762.0081\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 4354.2221 - val_loss: 3453.6714\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 4031.6840 - val_loss: 3383.5941\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 3912.1078 - val_loss: 3151.4164\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 3602.4490 - val_loss: 2960.1740\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 3400.9045 - val_loss: 2558.7994\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 3266.0888 - val_loss: 2289.5779\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 2716.6625 - val_loss: 2471.3500\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 2887.6600 - val_loss: 1832.7038\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 2316.7729 - val_loss: 1601.0236\n",
      "Epoch 16/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 49us/step - loss: 2001.0549 - val_loss: 1568.7146\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 1733.0481 - val_loss: 1089.9380\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 1225.0510 - val_loss: 1054.4100\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 1559.5431 - val_loss: 983.2263\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 1169.9696 - val_loss: 651.0045\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 949.4067 - val_loss: 637.7452\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 1138.9704 - val_loss: 736.6098\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 968.4575 - val_loss: 758.2180\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 759.9875 - val_loss: 701.6894\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 941.1960 - val_loss: 658.5941\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 599.2003 - val_loss: 445.7417\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 666.0895 - val_loss: 2554.1091\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 599.0841 - val_loss: 1380.8312\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 740.5728 - val_loss: 1614.3443\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 558.8114 - val_loss: 890.3110\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 783.2359 - val_loss: 944.8163\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 702.2256 - val_loss: 1562.7615\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 591.3657 - val_loss: 353.5992\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 543.0838 - val_loss: 338.1858\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 436.3252 - val_loss: 311.6923\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 450.6460 - val_loss: 1609.2337\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 638.7761 - val_loss: 365.7330\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 600.6014 - val_loss: 573.6835\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 470.0243 - val_loss: 322.0700\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 380.6891 - val_loss: 490.6014\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 495.2866 - val_loss: 468.7457\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 856.0226 - val_loss: 364.9507\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 398.1416 - val_loss: 303.5091\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 429.3910 - val_loss: 345.8692\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 414.9206 - val_loss: 485.5397\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 403.2095 - val_loss: 275.0692\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 450.4216 - val_loss: 637.1479\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 462.6720 - val_loss: 291.5107\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 512.0845 - val_loss: 518.1730\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 474.4977 - val_loss: 414.6817\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 377.4874 - val_loss: 281.0610\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 387.6181 - val_loss: 459.9827\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 417.1296 - val_loss: 385.9351\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 383.5096 - val_loss: 264.1743\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 395.1942 - val_loss: 352.5981\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 446.1304 - val_loss: 757.0466\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 455.0820 - val_loss: 267.2731\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 361.8677 - val_loss: 258.7395\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 460.5786 - val_loss: 630.4943\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 408.2605 - val_loss: 326.0296\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 359.0423 - val_loss: 388.4458\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 511.0947 - val_loss: 274.2246\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 622.7392 - val_loss: 412.3866\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 401.6930 - val_loss: 270.0698\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 349.5723 - val_loss: 329.0406\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 347.9984 - val_loss: 952.1930\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 467.1961 - val_loss: 243.6204\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 368.5095 - val_loss: 334.4521\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 340.8129 - val_loss: 226.5788\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 388.6653 - val_loss: 487.5714\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 458.1431 - val_loss: 387.7794\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 370.6021 - val_loss: 355.3125\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 362.5682 - val_loss: 231.3461\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 378.2993 - val_loss: 240.3520\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 535.9094 - val_loss: 307.7722\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 361.1055 - val_loss: 267.5473\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 323.3706 - val_loss: 263.6672\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 359.6019 - val_loss: 226.4296\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 304.8118 - val_loss: 273.2111\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 392.2373 - val_loss: 433.3151\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 315.4006 - val_loss: 269.8201\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 377.6247 - val_loss: 228.7588\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 285.5670 - val_loss: 656.1349\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 380.7600 - val_loss: 236.4626\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 295.5667 - val_loss: 334.2420\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 349.6650 - val_loss: 251.0051\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 318.9910 - val_loss: 438.2510\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 320.5554 - val_loss: 208.1562\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 567.1915 - val_loss: 881.0475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 601.3746 - val_loss: 474.8414\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 325.1652 - val_loss: 220.2144\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 323.1496 - val_loss: 245.3431\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 323.6391 - val_loss: 235.7081\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 289.0083 - val_loss: 351.8780\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 259.1271 - val_loss: 208.6838\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 289.2924 - val_loss: 233.1883\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 387.6565 - val_loss: 218.9530\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 319.7826 - val_loss: 374.3542\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 293.9909 - val_loss: 253.8706\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 359.7108 - val_loss: 1025.5845\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 455.5614 - val_loss: 232.4091\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 308.7949 - val_loss: 205.0711\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 280.2203 - val_loss: 245.1532\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 277.1805 - val_loss: 230.9624\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 497.4822 - val_loss: 271.8370\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 351.3134 - val_loss: 317.3475\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 391.3503 - val_loss: 458.3164\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 301.2201 - val_loss: 188.7177\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 273.6531 - val_loss: 278.5790\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 305.6493 - val_loss: 279.0003\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 262.4750 - val_loss: 204.0583\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 258.7696 - val_loss: 203.5162\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 291.2092 - val_loss: 227.7290\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 336.8281 - val_loss: 212.8409\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 322.2364 - val_loss: 208.8267\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 259.7860 - val_loss: 202.7476\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 288.7816 - val_loss: 197.6215\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 340.4655 - val_loss: 760.7240\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 284.9779 - val_loss: 389.5441\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 287.2363 - val_loss: 272.4391\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 271.7667 - val_loss: 185.1269\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 297.4894 - val_loss: 278.1209\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 271.8244 - val_loss: 179.1458\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 277.6597 - val_loss: 259.5252\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 318.5776 - val_loss: 262.8868\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 317.8484 - val_loss: 789.5546\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 461.2897 - val_loss: 201.8680\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 320.6832 - val_loss: 196.3843\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 342.9642 - val_loss: 199.6238\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 264.3697 - val_loss: 247.1630\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 250.8018 - val_loss: 341.0861\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 286.1786 - val_loss: 220.2860\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 270.7617 - val_loss: 186.0037\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 249.1944 - val_loss: 248.7188\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 259.1038 - val_loss: 183.8954\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 256.5720 - val_loss: 199.1535\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 369.7014 - val_loss: 249.0413\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 294.1394 - val_loss: 188.5902\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 239.7515 - val_loss: 204.3790\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 234.2180 - val_loss: 421.4017\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 309.4832 - val_loss: 268.1951\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 313.8048 - val_loss: 303.1759\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 327.9644 - val_loss: 283.6608\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 285.0193 - val_loss: 663.8989\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 242.9258 - val_loss: 188.3071\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 289.9262 - val_loss: 427.5097\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 227.9211 - val_loss: 270.8656\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 235.5205 - val_loss: 161.8259\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 270.0111 - val_loss: 185.7844\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 217.1590 - val_loss: 321.3908\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 229.2785 - val_loss: 222.9880\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 278.8633 - val_loss: 411.2044\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 316.0775 - val_loss: 190.6870\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 260.8794 - val_loss: 165.5277\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 260.2758 - val_loss: 668.2429\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 236.9209 - val_loss: 185.0216\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 237.6822 - val_loss: 200.6505\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 221.6564 - val_loss: 238.4825\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 239.6649 - val_loss: 164.4725\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 270.7292 - val_loss: 481.2447\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 287.5610 - val_loss: 163.3285\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 253.3308 - val_loss: 272.9229\n",
      "Epoch 163/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 240.2603 - val_loss: 161.4353\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 267.2624 - val_loss: 235.9382\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 227.6952 - val_loss: 197.8141\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 290.3658 - val_loss: 172.4180\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 268.8482 - val_loss: 195.5720\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 223.6170 - val_loss: 186.0021\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 243.1710 - val_loss: 317.5336\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 219.8799 - val_loss: 180.2305\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 231.5019 - val_loss: 169.0825\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 209.4517 - val_loss: 165.7349\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 207.6894 - val_loss: 182.3107\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 227.3771 - val_loss: 182.3471\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 263.7848 - val_loss: 164.5407\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 382.7206 - val_loss: 173.4528\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 210.2331 - val_loss: 181.0129\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 240.8047 - val_loss: 257.9901\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 217.4141 - val_loss: 288.1461\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 231.7055 - val_loss: 191.9911\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 255.4968 - val_loss: 148.5683\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 283.4221 - val_loss: 233.7874\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 239.5620 - val_loss: 166.1076\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 191.7933 - val_loss: 152.2147\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 204.7450 - val_loss: 173.2135\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 235.5352 - val_loss: 154.2802\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 217.0838 - val_loss: 246.2249\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 254.9551 - val_loss: 201.4089\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 279.6956 - val_loss: 220.6243\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 252.4761 - val_loss: 239.6671\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 217.6836 - val_loss: 241.3521\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 209.1142 - val_loss: 165.8076\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 209.1759 - val_loss: 168.8685\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 263.7400 - val_loss: 271.6555\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 214.0050 - val_loss: 167.0099\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 202.4300 - val_loss: 179.1904\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 263.8787 - val_loss: 229.9340\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 210.6899 - val_loss: 165.6595\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 221.6957 - val_loss: 505.9707\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 235.7268 - val_loss: 304.9594\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 225.1922 - val_loss: 151.8095\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 209.8507 - val_loss: 207.7839\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 240.9375 - val_loss: 165.6228\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 204.5749 - val_loss: 219.5279\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 231.5464 - val_loss: 165.2896\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 199.4296 - val_loss: 213.9048\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 232.2081 - val_loss: 506.5963\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 222.4648 - val_loss: 243.1049\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 202.7198 - val_loss: 219.8495\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 225.1820 - val_loss: 154.3685\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 207.4848 - val_loss: 230.6381\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 203.4773 - val_loss: 143.7983\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 200.9131 - val_loss: 264.4061\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 246.2760 - val_loss: 151.0732\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 186.7762 - val_loss: 150.3832\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 199.0485 - val_loss: 146.9623\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 259.9145 - val_loss: 148.0767\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 236.0446 - val_loss: 153.3534\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 357.0411 - val_loss: 152.8335\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 201.2123 - val_loss: 152.3973\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 197.8348 - val_loss: 157.3824\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 191.4475 - val_loss: 145.0495\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 191.9861 - val_loss: 169.1019\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 242.1451 - val_loss: 177.2055\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 369.8025 - val_loss: 287.6598\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 239.2461 - val_loss: 191.1849\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 206.9155 - val_loss: 251.1581\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 202.0874 - val_loss: 146.2799\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 277.2905 - val_loss: 156.9846\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 189.0316 - val_loss: 153.6184\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 189.5962 - val_loss: 145.2661\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 200.7409 - val_loss: 154.7498\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 181.4868 - val_loss: 232.5695\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 195.5192 - val_loss: 155.7548\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 217.5494 - val_loss: 386.0683\n",
      "Epoch 236/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 49us/step - loss: 349.5657 - val_loss: 150.8671\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 184.1941 - val_loss: 250.3346\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 211.4604 - val_loss: 171.4366\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 191.6478 - val_loss: 149.7359\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 247.2683 - val_loss: 165.2560\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 191.0458 - val_loss: 242.7866\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 213.7045 - val_loss: 183.5322\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 237.1712 - val_loss: 139.4115\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 204.0524 - val_loss: 236.8878\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 196.8403 - val_loss: 138.4087\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 183.9535 - val_loss: 186.9578\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 241.9729 - val_loss: 264.7057\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 232.1926 - val_loss: 170.1483\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 232.7835 - val_loss: 242.2193\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 205.8232 - val_loss: 204.6120\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 199.9816 - val_loss: 153.5754\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 201.6063 - val_loss: 154.2975\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 245.6851 - val_loss: 156.3107\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 183.4798 - val_loss: 311.3933\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 176.6528 - val_loss: 216.4357\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 182.6655 - val_loss: 143.8860\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 175.1084 - val_loss: 228.3416\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 221.1315 - val_loss: 209.7394\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 326.1271 - val_loss: 164.0713\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 201.5449 - val_loss: 138.6395\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 202.3882 - val_loss: 150.9558\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 177.9649 - val_loss: 140.4964\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 242.7409 - val_loss: 142.7622\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 213.5606 - val_loss: 145.7395\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 204.1876 - val_loss: 161.5465\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 312.6245 - val_loss: 181.9645\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 201.6490 - val_loss: 204.2963\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 236.4458 - val_loss: 139.6399\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 174.6253 - val_loss: 142.5908\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 201.9532 - val_loss: 138.9568\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 183.0606 - val_loss: 188.7834\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 259.6097 - val_loss: 148.8189\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 185.0200 - val_loss: 146.3676\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 186.0416 - val_loss: 165.1203\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 221.5176 - val_loss: 162.0254\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 197.4873 - val_loss: 148.5711\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 222.5416 - val_loss: 135.7567\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 175.8361 - val_loss: 132.5017\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 176.5703 - val_loss: 137.8641\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 239.5448 - val_loss: 193.8705\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 169.9596 - val_loss: 165.7551\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 175.5104 - val_loss: 156.1865\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 274.9761 - val_loss: 152.4346\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 251.6959 - val_loss: 139.3107\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 169.6317 - val_loss: 129.1756\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 203.9629 - val_loss: 330.4430\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 225.6782 - val_loss: 136.8959\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 176.4980 - val_loss: 133.1103\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 180.4174 - val_loss: 131.9967\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 190.4058 - val_loss: 211.8842\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 187.7058 - val_loss: 130.3832\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 178.3612 - val_loss: 133.4144\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 169.2800 - val_loss: 187.5368\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 218.4927 - val_loss: 177.5675\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 220.0148 - val_loss: 139.6551\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 181.5316 - val_loss: 241.9610\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 214.9164 - val_loss: 162.4683\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 184.9601 - val_loss: 232.3206\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 198.5310 - val_loss: 174.8862\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 193.2757 - val_loss: 186.0875\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 229.6292 - val_loss: 189.6404\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 197.3373 - val_loss: 211.1425\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 174.3306 - val_loss: 136.6275\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 179.1078 - val_loss: 191.7406\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 220.7468 - val_loss: 137.4418\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 198.7306 - val_loss: 362.2962\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 246.0702 - val_loss: 135.5368\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 173.7642 - val_loss: 144.5876\n",
      "Epoch 309/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 169.6160 - val_loss: 125.7047\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 172.6187 - val_loss: 139.2588\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 174.8899 - val_loss: 134.1115\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 271.4187 - val_loss: 151.8167\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 190.0146 - val_loss: 190.8888\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 203.3658 - val_loss: 170.8526\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 186.3020 - val_loss: 214.2894\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 166.4622 - val_loss: 319.8585\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 179.9805 - val_loss: 154.1869\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 325.2607 - val_loss: 141.0857\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 198.3834 - val_loss: 136.8072\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 178.9270 - val_loss: 155.2967\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 183.1349 - val_loss: 139.8056\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 168.0014 - val_loss: 129.9345\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 169.4178 - val_loss: 140.0292\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 162.5877 - val_loss: 135.0829\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 174.5197 - val_loss: 151.4995\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 174.8487 - val_loss: 265.6722\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 401.6923 - val_loss: 215.7554\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 171.0164 - val_loss: 164.8359\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 174.9284 - val_loss: 146.6421\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 170.2753 - val_loss: 147.0946\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 203.0656 - val_loss: 131.8516\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 178.5006 - val_loss: 138.8662\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 167.6492 - val_loss: 186.7670\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 187.9727 - val_loss: 127.6417\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 158.0046 - val_loss: 128.2268\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 166.1037 - val_loss: 132.7417\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 173.8413 - val_loss: 150.6868\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 158.6268 - val_loss: 203.0296\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 298.7721 - val_loss: 137.8528\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 164.4175 - val_loss: 124.6759\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 188.3099 - val_loss: 147.2757\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 179.6991 - val_loss: 135.2385- ETA: 0s - loss: 157\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 170.1530 - val_loss: 150.7140\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 170.4359 - val_loss: 137.7635\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 178.5578 - val_loss: 186.2859\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 250.9320 - val_loss: 197.6208\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 163.2550 - val_loss: 161.9515\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 190.3469 - val_loss: 156.0004\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 185.0040 - val_loss: 140.6696\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 167.9844 - val_loss: 128.5860\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 172.0553 - val_loss: 129.4991\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 208.7180 - val_loss: 161.2587\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 218.7199 - val_loss: 155.8458\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 189.9658 - val_loss: 167.1674\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 202.1431 - val_loss: 186.0816\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 166.7189 - val_loss: 130.1017\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 157.6749 - val_loss: 225.9255\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 167.8113 - val_loss: 141.2952\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 169.0761 - val_loss: 127.5770\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 198.7349 - val_loss: 244.0307\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 224.3818 - val_loss: 155.0400\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 172.6804 - val_loss: 127.9280\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 168.8955 - val_loss: 410.5068\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 241.0063 - val_loss: 134.9949\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.0343 - val_loss: 189.6891\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 167.5048 - val_loss: 197.2418\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 195.9120 - val_loss: 129.9450\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.9006 - val_loss: 148.7187\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 170.5087 - val_loss: 154.3092\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 162.7659 - val_loss: 237.0099\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 174.1226 - val_loss: 153.2207\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 187.5270 - val_loss: 208.1116\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 179.5796 - val_loss: 126.2235\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 194.9086 - val_loss: 299.7766\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 194.8684 - val_loss: 130.9858\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 166.7113 - val_loss: 154.5688\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 186.7608 - val_loss: 128.1045\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 171.8899 - val_loss: 166.1499\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 188.7020 - val_loss: 150.7765\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 250.1246 - val_loss: 123.5355\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 155.2674 - val_loss: 127.8211\n",
      "Epoch 382/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 154.6299 - val_loss: 134.3432\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.8189 - val_loss: 158.4562\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 167.1007 - val_loss: 240.7716\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 182.4229 - val_loss: 179.6535\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 160.4522 - val_loss: 130.6264\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 263.3539 - val_loss: 141.9466\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 170.4704 - val_loss: 147.9684\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 167.8025 - val_loss: 124.8362\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 174.0860 - val_loss: 126.9075\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 158.0675 - val_loss: 124.5836\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 164.6991 - val_loss: 151.7822\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.0386 - val_loss: 132.4379\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 177.9377 - val_loss: 138.0169\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 165.7274 - val_loss: 314.9611\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 212.2202 - val_loss: 415.2185\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 177.2515 - val_loss: 143.2764\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 208.7648 - val_loss: 295.9698\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 204.7514 - val_loss: 130.0711\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 153.9886 - val_loss: 154.3166\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 160.0377 - val_loss: 175.1730\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 180.9741 - val_loss: 147.7119\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 178.3719 - val_loss: 212.6452\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 179.4293 - val_loss: 135.5632\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 158.9117 - val_loss: 131.2316\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 241.2051 - val_loss: 160.1533\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 185.7761 - val_loss: 133.5926\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 156.2252 - val_loss: 191.3599\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 160.0108 - val_loss: 121.9058\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 169.2716 - val_loss: 126.0045\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 163.9019 - val_loss: 147.6060\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 175.1843 - val_loss: 154.8010\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 187.9672 - val_loss: 126.4983\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 160.9427 - val_loss: 147.7807\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 173.1898 - val_loss: 146.6158\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 168.0185 - val_loss: 167.3332\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 163.8424 - val_loss: 388.4338\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 183.1582 - val_loss: 149.8506\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 205.6816 - val_loss: 123.0051\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 165.1932 - val_loss: 146.5096\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 178.8421 - val_loss: 122.6403\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 165.8301 - val_loss: 130.5300\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 177.8361 - val_loss: 146.3435\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 160.9718 - val_loss: 294.5350\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 157.7594 - val_loss: 123.8280\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 191.0719 - val_loss: 183.8904\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 167.6023 - val_loss: 124.6725\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.9719 - val_loss: 122.9921\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 158.9492 - val_loss: 148.9849\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.0474 - val_loss: 124.5732\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 187.7114 - val_loss: 147.3037\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 163.5176 - val_loss: 127.8623\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.4434 - val_loss: 124.2371\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 191.6002 - val_loss: 129.8824\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 191.2271 - val_loss: 129.5001\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 179.3832 - val_loss: 131.3556\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 168.1572 - val_loss: 126.8560\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 175.0085 - val_loss: 126.6416\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 203.9689 - val_loss: 188.9354\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 160.1674 - val_loss: 163.6776\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 157.8748 - val_loss: 120.4374\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.9679 - val_loss: 141.7304\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 165.1128 - val_loss: 154.7401\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 159.7346 - val_loss: 120.9967\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 170.1838 - val_loss: 164.3643\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.0325 - val_loss: 135.0440\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 223.1491 - val_loss: 228.6939\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 181.5135 - val_loss: 158.4300\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 156.6978 - val_loss: 151.5169\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 160.9696 - val_loss: 277.6569\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 164.4525 - val_loss: 311.2165\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 267.0541 - val_loss: 126.5960\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.4477 - val_loss: 174.1026\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 163.3054 - val_loss: 147.7701\n",
      "Epoch 455/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 49us/step - loss: 160.7632 - val_loss: 122.0024\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 167.4410 - val_loss: 120.9912\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 147.9976 - val_loss: 229.2826\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 199.3198 - val_loss: 121.5807\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 156.6899 - val_loss: 127.5903\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 370.0376 - val_loss: 125.2295\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 154.1316 - val_loss: 123.0301\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.5340 - val_loss: 214.1720\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 171.1077 - val_loss: 191.6744\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.5636 - val_loss: 161.1614\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 175.5352 - val_loss: 123.2093\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 192.6370 - val_loss: 134.7492\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 169.5729 - val_loss: 125.1371\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 161.1359 - val_loss: 125.0283\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.4788 - val_loss: 130.9079\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.3029 - val_loss: 122.8670\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 178.5587 - val_loss: 129.0167\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 238.9370 - val_loss: 122.5268\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 171.6781 - val_loss: 247.7856\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 187.7800 - val_loss: 129.5161\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 154.5318 - val_loss: 134.1075\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.7966 - val_loss: 136.5252\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 152.8491 - val_loss: 150.8723\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.9814 - val_loss: 171.3838\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 173.2508 - val_loss: 136.4118\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 163.5681 - val_loss: 125.6396\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 173.4435 - val_loss: 126.7071\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.5257 - val_loss: 123.5766\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 182.6261 - val_loss: 133.1805\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.0015 - val_loss: 128.0408\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 195.9192 - val_loss: 127.5064\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 157.2097 - val_loss: 137.2211\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 165.0180 - val_loss: 139.1641\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 149.7744 - val_loss: 177.8119\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 177.9850 - val_loss: 124.4959\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 166.5611 - val_loss: 136.0607\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 298.3040 - val_loss: 156.1012\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 163.4432 - val_loss: 146.8456\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 156.3084 - val_loss: 127.9382\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 154.4261 - val_loss: 149.4629\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.1806 - val_loss: 187.1483\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 173.0645 - val_loss: 122.1658\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.2850 - val_loss: 138.8054\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 166.8688 - val_loss: 188.0616\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 208.5048 - val_loss: 138.2309\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.0550 - val_loss: 119.4598\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.3349 - val_loss: 125.3751\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 169.4764 - val_loss: 131.9869\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 198.7537 - val_loss: 131.0626\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.4533 - val_loss: 125.7648\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 156.1392 - val_loss: 162.4362\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 178.6222 - val_loss: 1884.1867\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 278.2063 - val_loss: 135.6314\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.2952 - val_loss: 116.9301\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 159.0207 - val_loss: 117.2176\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.5215 - val_loss: 125.3423\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 155.8220 - val_loss: 122.5146\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.3167 - val_loss: 118.9144\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.9080 - val_loss: 154.0340\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 156.4449 - val_loss: 130.2705\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 178.6238 - val_loss: 122.0821\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.0656 - val_loss: 160.2557\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.7741 - val_loss: 143.6599\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 255.1033 - val_loss: 228.6455\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 164.1192 - val_loss: 144.8423\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 176.8530 - val_loss: 128.1604\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.4733 - val_loss: 115.9329\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.7578 - val_loss: 122.3788\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 164.8350 - val_loss: 132.2681\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.9717 - val_loss: 138.1080\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 178.3894 - val_loss: 128.5196\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.4824 - val_loss: 127.3648\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.0940 - val_loss: 135.0868\n",
      "Epoch 528/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 164.4999 - val_loss: 122.6173\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 162.1763 - val_loss: 239.1584\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 229.2822 - val_loss: 142.2782\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 148.6515 - val_loss: 128.3106\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 159.8191 - val_loss: 122.6927\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 185.2568 - val_loss: 123.0086\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.9322 - val_loss: 126.3412\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.8101 - val_loss: 122.1568\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 197.8144 - val_loss: 128.1863\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 172.8940 - val_loss: 130.8508\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 163.7581 - val_loss: 122.9232\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 204.2932 - val_loss: 121.5366\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 154.0536 - val_loss: 121.7853\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.4821 - val_loss: 132.1805\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 209.3764 - val_loss: 865.1734\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 270.2937 - val_loss: 131.6287\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 164.7097 - val_loss: 126.7261\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 158.5324 - val_loss: 137.1933\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 191.5337 - val_loss: 124.9289\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 157.2297 - val_loss: 147.2034\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.4917 - val_loss: 121.2917\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 154.2836 - val_loss: 315.0949\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 180.4296 - val_loss: 223.0051\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 165.8446 - val_loss: 148.1231\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 158.9992 - val_loss: 127.5272\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.8593 - val_loss: 127.3060\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.1744 - val_loss: 114.3700\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 226.5008 - val_loss: 121.6182\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.0445 - val_loss: 117.8819\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 147.9031 - val_loss: 192.3250\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 167.1411 - val_loss: 192.6253\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 179.3278 - val_loss: 134.6570\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 148.4630 - val_loss: 126.6539\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 156.2238 - val_loss: 137.3191\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 162.9346 - val_loss: 129.3907\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 166.4596 - val_loss: 118.0297\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.5413 - val_loss: 229.3117\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.1133 - val_loss: 140.2040\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 168.5568 - val_loss: 159.1902\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 210.0425 - val_loss: 128.9783\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.8002 - val_loss: 123.6429\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.9737 - val_loss: 275.7003\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 158.5590 - val_loss: 129.1533\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.3107 - val_loss: 124.9898\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 157.5343 - val_loss: 120.3903\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.0458 - val_loss: 119.8745\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.0016 - val_loss: 125.1259\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 173.7774 - val_loss: 136.9313\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 169.7684 - val_loss: 381.1066\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.0363 - val_loss: 126.9932\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 163.7519 - val_loss: 133.7320\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.5501 - val_loss: 198.4109\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 182.4816 - val_loss: 466.3251\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 184.3854 - val_loss: 123.8946\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.3799 - val_loss: 125.9840\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.1292 - val_loss: 132.7846\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.8215 - val_loss: 126.9905\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 159.3777 - val_loss: 127.5726\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 285.3188 - val_loss: 130.6639\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.4359 - val_loss: 123.8621\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.5792 - val_loss: 154.9353\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 149.0400 - val_loss: 119.5866\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.6304 - val_loss: 133.1011\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 375.2285 - val_loss: 158.0972\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 145.3050 - val_loss: 124.0194\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.4882 - val_loss: 150.9333\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.7629 - val_loss: 129.1932\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.8436 - val_loss: 115.5588\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.9817 - val_loss: 150.5574\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 248.0887 - val_loss: 151.8687\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 186.1452 - val_loss: 176.1401\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 173.3124 - val_loss: 240.3708\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.7922 - val_loss: 146.8297\n",
      "Epoch 601/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 49us/step - loss: 166.8465 - val_loss: 191.4496\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 168.1693 - val_loss: 140.0083\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 158.8389 - val_loss: 118.8790\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.5736 - val_loss: 145.9831\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 150.6486 - val_loss: 135.9940\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.7923 - val_loss: 154.9605\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 161.9331 - val_loss: 249.6799\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 353.0383 - val_loss: 148.1911\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 168.5656 - val_loss: 124.3736\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 160.4555 - val_loss: 127.5799\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.7469 - val_loss: 287.3724\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.6402 - val_loss: 126.5600\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 154.7888 - val_loss: 120.0873\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 150.4042 - val_loss: 121.3268\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 162.0436 - val_loss: 146.0038\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 165.9940 - val_loss: 150.2421\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 169.2416 - val_loss: 126.0044\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 154.1080 - val_loss: 136.8156\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 164.5134 - val_loss: 133.9703\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.6150 - val_loss: 120.1220\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 165.6830 - val_loss: 122.1579\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 183.9100 - val_loss: 127.1825\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.0766 - val_loss: 283.3586\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 163.5469 - val_loss: 128.9050\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 168.9689 - val_loss: 153.2342\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.5533 - val_loss: 381.6395\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 233.0960 - val_loss: 120.0547\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 142.9080 - val_loss: 150.9677\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.9176 - val_loss: 148.3511\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.1814 - val_loss: 116.8436\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.4079 - val_loss: 116.7286\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 177.8981 - val_loss: 142.7518\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 162.8936 - val_loss: 366.9940\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 228.9997 - val_loss: 143.4976\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.1653 - val_loss: 123.3338\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.4764 - val_loss: 125.4187\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.8260 - val_loss: 116.9338\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.3852 - val_loss: 121.9500\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.8281 - val_loss: 201.6834\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 200.2221 - val_loss: 193.6786\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 569.1597 - val_loss: 128.5654\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.0129 - val_loss: 121.3967\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.0077 - val_loss: 119.4658\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.3180 - val_loss: 144.8214\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 169.0963 - val_loss: 173.5566\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.3835 - val_loss: 121.3531\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 153.4515 - val_loss: 127.2568\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.8999 - val_loss: 135.5316\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 153.0189 - val_loss: 125.7243\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 255.1089 - val_loss: 233.4073\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.7990 - val_loss: 119.4897\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.0188 - val_loss: 125.6430\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 156.5590 - val_loss: 170.0545\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.3263 - val_loss: 140.6967\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 183.1144 - val_loss: 116.9246\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.7663 - val_loss: 125.2472\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.8732 - val_loss: 128.7971\n",
      "Epoch 658/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 193.3081 - val_loss: 463.8202\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 172.1906 - val_loss: 133.2879\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 155.8805 - val_loss: 120.6307\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.6861 - val_loss: 146.5947\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 159.5372 - val_loss: 205.9462\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.0230 - val_loss: 119.9839\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.3210 - val_loss: 118.5040\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 164.6788 - val_loss: 172.3728\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 201.6706 - val_loss: 202.3548\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 155.4953 - val_loss: 160.9923\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.2525 - val_loss: 143.2614\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 158.0453 - val_loss: 126.0706\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 157.0608 - val_loss: 137.7201\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 173.1855 - val_loss: 286.3491\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 187.0294 - val_loss: 128.3612\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.5711 - val_loss: 125.1506\n",
      "Epoch 674/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.0304 - val_loss: 158.4645\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.4300 - val_loss: 158.9226\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 190.6237 - val_loss: 245.5166\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 288.9674 - val_loss: 127.7609\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.7247 - val_loss: 122.7992\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.3083 - val_loss: 131.1533\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.1494 - val_loss: 161.4186\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.6512 - val_loss: 137.2483\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.5773 - val_loss: 153.6224\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.6137 - val_loss: 122.4271\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.1314 - val_loss: 121.2644\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 150.8059 - val_loss: 161.0889\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 154.9965 - val_loss: 149.3435\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 219.5997 - val_loss: 149.4446\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.2629 - val_loss: 117.7253\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.7464 - val_loss: 117.9169\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.6542 - val_loss: 194.4592\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 157.4590 - val_loss: 121.2245\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 158.3297 - val_loss: 164.3030\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.9521 - val_loss: 136.8109\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.4851 - val_loss: 121.1593\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.2129 - val_loss: 127.3683\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.5711 - val_loss: 153.2877\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 172.6795 - val_loss: 116.6781\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 167.4432 - val_loss: 152.1916\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.7231 - val_loss: 118.9366\n",
      "Epoch 700/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 157.4379 - val_loss: 120.9767\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.1202 - val_loss: 421.9850\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 162.6177 - val_loss: 131.3073\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.6663 - val_loss: 115.2231\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.6658 - val_loss: 139.0100\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 395.5192 - val_loss: 135.6012\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.4828 - val_loss: 201.2766\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.2618 - val_loss: 114.5954\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.4415 - val_loss: 119.4442\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.6617 - val_loss: 120.0332\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 175.8309 - val_loss: 137.2830\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.0344 - val_loss: 117.6572\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.3322 - val_loss: 127.8005\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.3553 - val_loss: 124.5640\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.5008 - val_loss: 143.0388\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 157.6210 - val_loss: 144.2836\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.3749 - val_loss: 124.6582\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.1255 - val_loss: 120.0678\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.0180 - val_loss: 174.5677\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 158.8749 - val_loss: 116.6066\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.7258 - val_loss: 124.9070\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 191.0201 - val_loss: 150.3308\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.7947 - val_loss: 169.2559\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.8679 - val_loss: 116.6806\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 154.7598 - val_loss: 128.4213\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.6949 - val_loss: 116.5646\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.2980 - val_loss: 126.3280\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.5754 - val_loss: 129.3812\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 147.4468 - val_loss: 131.3622\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 161.4820 - val_loss: 211.3568\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.1888 - val_loss: 126.0821\n",
      "Epoch 731/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 291.5728 - val_loss: 141.0587\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 161.5630 - val_loss: 171.4478\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.1593 - val_loss: 117.0566\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.4996 - val_loss: 126.0263\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.6046 - val_loss: 117.7833\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.5162 - val_loss: 121.7988\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.2482 - val_loss: 304.8848\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 160.3013 - val_loss: 135.8400\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.6917 - val_loss: 114.5681\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.6235 - val_loss: 120.2827\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 206.4203 - val_loss: 294.5016\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 153.8127 - val_loss: 114.4887\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 146.7432 - val_loss: 121.5726\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 149.8100 - val_loss: 118.9500\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.1253 - val_loss: 129.7079\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.3916 - val_loss: 127.3601\n",
      "Epoch 747/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.7723 - val_loss: 143.8370\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.6798 - val_loss: 119.5865\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.4527 - val_loss: 146.0265\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.6164 - val_loss: 120.2438\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.8565 - val_loss: 140.2540\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.9533 - val_loss: 189.3991\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.4681 - val_loss: 135.1275\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 171.2562 - val_loss: 142.0459\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.7530 - val_loss: 144.2411\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 157.0224 - val_loss: 173.3409\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 155.6662 - val_loss: 114.9442\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 145.6662 - val_loss: 125.1964\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 237.6612 - val_loss: 112.9156\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.1049 - val_loss: 117.2358\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 161.2473 - val_loss: 150.7921\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 154.8459 - val_loss: 120.5863\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.1001 - val_loss: 132.1450\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 166.3817 - val_loss: 123.0993\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.7029 - val_loss: 114.8457\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.4687 - val_loss: 114.0941\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.6087 - val_loss: 119.6472\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 206.4323 - val_loss: 511.1175\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 310.9000 - val_loss: 116.3403\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.3136 - val_loss: 116.7642\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.1136 - val_loss: 113.1185\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.9395 - val_loss: 124.8296\n",
      "Epoch 773/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.6354 - val_loss: 143.7713\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.4176 - val_loss: 123.4529\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.2217 - val_loss: 114.8898\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.2365 - val_loss: 163.1802\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 167.4467 - val_loss: 118.8258\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 262.6001 - val_loss: 284.7973\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.0639 - val_loss: 116.1323\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.5480 - val_loss: 127.5841\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 148.6470 - val_loss: 125.0259\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.6473 - val_loss: 117.4452\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.9885 - val_loss: 114.8175\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.2281 - val_loss: 131.3434\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.7232 - val_loss: 120.7965\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.8003 - val_loss: 120.2551\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.3817 - val_loss: 128.7724\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 160.0699 - val_loss: 133.4346\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.9660 - val_loss: 134.7434\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.9927 - val_loss: 164.9668\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 167.4732 - val_loss: 115.7265\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 179.7879 - val_loss: 815.3767\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 407.5723 - val_loss: 133.1984\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.5041 - val_loss: 127.9542\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.0640 - val_loss: 120.3970\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.4466 - val_loss: 124.9301\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.3916 - val_loss: 121.6410\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.7258 - val_loss: 121.1481\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.6273 - val_loss: 122.4020\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.7168 - val_loss: 117.0296\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.6436 - val_loss: 115.9937\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.3984 - val_loss: 114.0139\n",
      "Epoch 803/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.5171 - val_loss: 296.6583\n",
      "Epoch 804/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 234.4293 - val_loss: 124.7325\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.4028 - val_loss: 119.0796\n",
      "Epoch 806/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.3945 - val_loss: 129.6892\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.9879 - val_loss: 114.6500\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 154.3192 - val_loss: 169.3224\n",
      "Epoch 809/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.1581 - val_loss: 137.1898\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.8271 - val_loss: 157.0721\n",
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.5340 - val_loss: 151.5040\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.7257 - val_loss: 123.7256\n",
      "Epoch 813/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.0725 - val_loss: 117.6589\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.4373 - val_loss: 177.7199\n",
      "Epoch 815/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 166.4611 - val_loss: 118.2056\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 155.5726 - val_loss: 144.4438\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 150.1667 - val_loss: 128.9805\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.1055 - val_loss: 129.4914\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 185.8315 - val_loss: 182.1775\n",
      "Epoch 820/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 49us/step - loss: 148.9546 - val_loss: 127.1576\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.9738 - val_loss: 120.2965\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.2646 - val_loss: 176.6053\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 331.0756 - val_loss: 409.2359\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 193.1689 - val_loss: 128.7273\n",
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.8888 - val_loss: 122.6979\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.9006 - val_loss: 134.1927\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.2037 - val_loss: 119.7300\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.6614 - val_loss: 139.5884\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.4548 - val_loss: 125.3781\n",
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.1518 - val_loss: 128.2566\n",
      "Epoch 831/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 142.7760 - val_loss: 136.4666\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 170.1766 - val_loss: 159.5054\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.5055 - val_loss: 120.5088\n",
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 140.7178 - val_loss: 120.4582\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 162.3456 - val_loss: 130.2370\n",
      "Epoch 836/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 141.3734 - val_loss: 130.8587\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 160.3844 - val_loss: 178.7718\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 159.3119 - val_loss: 115.5097\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.8857 - val_loss: 121.5408\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.7092 - val_loss: 152.1207\n",
      "Epoch 841/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 185.9617 - val_loss: 126.3002\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.1575 - val_loss: 136.1179\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.9521 - val_loss: 155.7298\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.2710 - val_loss: 135.6223\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.4023 - val_loss: 123.7885\n",
      "Epoch 846/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 157.7620 - val_loss: 122.5026\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.2389 - val_loss: 126.9730\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 151.4432 - val_loss: 122.8028\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.9455 - val_loss: 145.4405\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 182.5349 - val_loss: 115.4290\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.6751 - val_loss: 115.6927\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 155.7986 - val_loss: 126.0043\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.6597 - val_loss: 228.7203\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 150.8732 - val_loss: 129.5283\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 170.5252 - val_loss: 118.8434\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.9993 - val_loss: 114.9983\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.5400 - val_loss: 128.2463\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.2069 - val_loss: 117.3392\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.5223 - val_loss: 119.6924\n",
      "Epoch 860/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.5795 - val_loss: 120.8072\n",
      "Epoch 861/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.4199 - val_loss: 124.9495\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.9635 - val_loss: 209.9712\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 177.3156 - val_loss: 168.7387\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.8416 - val_loss: 113.3623\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.9520 - val_loss: 126.6969\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.7813 - val_loss: 123.4642\n",
      "Epoch 867/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 150.1963 - val_loss: 162.9062\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.9685 - val_loss: 137.2753\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 159.2437 - val_loss: 210.6900\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.4104 - val_loss: 118.1315\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 223.1820 - val_loss: 124.8158\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.2422 - val_loss: 120.5895\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.8532 - val_loss: 145.0410\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.2968 - val_loss: 115.9614\n",
      "Epoch 875/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.7212 - val_loss: 152.9626\n",
      "Epoch 876/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.7520 - val_loss: 129.4906\n",
      "Epoch 877/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.5710 - val_loss: 116.5606\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 164.0911 - val_loss: 131.0018\n",
      "Epoch 879/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.8713 - val_loss: 119.5860\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.9129 - val_loss: 150.0938\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.1288 - val_loss: 117.3413\n",
      "Epoch 882/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.0539 - val_loss: 124.8328\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.2896 - val_loss: 121.1159\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.1332 - val_loss: 118.2302\n",
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 306.5293 - val_loss: 533.5859\n",
      "Epoch 886/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 179.4496 - val_loss: 173.0128\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.9755 - val_loss: 162.4711\n",
      "Epoch 888/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.4720 - val_loss: 114.5786\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.6829 - val_loss: 115.3208\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.3080 - val_loss: 123.3361\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.1909 - val_loss: 119.6655\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.7288 - val_loss: 120.2754\n",
      "Epoch 893/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.8450 - val_loss: 122.3317\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 147.4748 - val_loss: 113.5042\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 156.1450 - val_loss: 128.1777\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 163.0238 - val_loss: 132.9867\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 196.0523 - val_loss: 197.5714\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 148.0443 - val_loss: 171.7496\n",
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.2458 - val_loss: 125.1959\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.9619 - val_loss: 149.9240\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.8020 - val_loss: 123.6521\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.9297 - val_loss: 113.4772\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.3827 - val_loss: 117.2597\n",
      "Epoch 904/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.0033 - val_loss: 141.0354\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.4635 - val_loss: 123.7593\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.8499 - val_loss: 117.0125\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.1615 - val_loss: 124.1221\n",
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 193.7901 - val_loss: 120.3234\n",
      "Epoch 909/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 143.8510 - val_loss: 134.8135\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 159.3834 - val_loss: 124.8004\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 227.8844 - val_loss: 154.2992\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 142.3941 - val_loss: 153.1138\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.3766 - val_loss: 148.9766\n",
      "Epoch 914/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.5516 - val_loss: 132.4289\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 142.2895 - val_loss: 139.9697\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 160.2038 - val_loss: 117.8817\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.9815 - val_loss: 123.1127\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 145.9129 - val_loss: 123.6605\n",
      "Epoch 919/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.4156 - val_loss: 137.5862\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.6533 - val_loss: 149.4298\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 173.2884 - val_loss: 288.0958\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 151.2561 - val_loss: 122.6352\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.3975 - val_loss: 117.2597\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.4942 - val_loss: 116.6489\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.4306 - val_loss: 119.2296\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.9309 - val_loss: 140.4676\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.8478 - val_loss: 169.3961\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 158.6746 - val_loss: 118.1107\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 348.8451 - val_loss: 122.4808\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.4330 - val_loss: 116.6073\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.2165 - val_loss: 146.4303\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.7647 - val_loss: 147.8483\n",
      "Epoch 933/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.5143 - val_loss: 115.0584\n",
      "Epoch 934/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 133.7878 - val_loss: 137.4355\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.2973 - val_loss: 118.3322\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.5678 - val_loss: 116.8914\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 142.7695 - val_loss: 121.3440\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.7662 - val_loss: 163.6233\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.4621 - val_loss: 115.1754\n",
      "Epoch 940/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.5531 - val_loss: 125.1068\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.2506 - val_loss: 142.1546\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.1133 - val_loss: 119.6559\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.6644 - val_loss: 242.8491\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.3732 - val_loss: 139.2179\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.4180 - val_loss: 115.2399\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.8459 - val_loss: 114.1873\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 503.9886 - val_loss: 179.0967\n",
      "Epoch 948/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.0814 - val_loss: 119.5227\n",
      "Epoch 949/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.5639 - val_loss: 117.0356\n",
      "Epoch 950/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.4658 - val_loss: 112.7403\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.6986 - val_loss: 118.0422\n",
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.7463 - val_loss: 147.9276\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.9221 - val_loss: 116.7379\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.8768 - val_loss: 129.5694\n",
      "Epoch 955/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.4011 - val_loss: 118.6990\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.0742 - val_loss: 118.6060\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.9374 - val_loss: 139.5078\n",
      "Epoch 958/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.3581 - val_loss: 152.5109\n",
      "Epoch 959/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.2749 - val_loss: 117.2297\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.0194 - val_loss: 121.3005\n",
      "Epoch 961/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.0799 - val_loss: 120.9043\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 197.8752 - val_loss: 117.9631\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.7552 - val_loss: 114.4074\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.8907 - val_loss: 137.3305\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.6998 - val_loss: 122.5757\n",
      "Epoch 966/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.6630 - val_loss: 116.4900\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 150.1195 - val_loss: 117.2841\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.9404 - val_loss: 132.6828\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.3306 - val_loss: 126.7185\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 157.1842 - val_loss: 114.3680\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 168.2923 - val_loss: 117.8374\n",
      "Epoch 972/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.6916 - val_loss: 127.6311\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 140.3619 - val_loss: 136.1728\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.4609 - val_loss: 120.6421\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 158.4939 - val_loss: 125.2569\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.0265 - val_loss: 161.7228\n",
      "Epoch 977/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 160.1700 - val_loss: 329.7454\n",
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 216.9535 - val_loss: 128.7081\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.8194 - val_loss: 126.7987\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.2342 - val_loss: 150.1094\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 150.3707 - val_loss: 126.7125\n",
      "Epoch 982/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.5867 - val_loss: 198.6633\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.7148 - val_loss: 123.0161\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 136.890 - 0s 50us/step - loss: 137.7519 - val_loss: 120.6931\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.5872 - val_loss: 122.9469\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.8477 - val_loss: 122.2480\n",
      "Epoch 987/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.5113 - val_loss: 231.3499\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 150.0786 - val_loss: 122.1319\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 143.7152 - val_loss: 116.7046\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.8561 - val_loss: 116.5419\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 203.0778 - val_loss: 131.7379\n",
      "Epoch 992/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.9383 - val_loss: 124.9769\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.5395 - val_loss: 140.1464\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.1045 - val_loss: 153.9038\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.1177 - val_loss: 151.2751\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.3236 - val_loss: 152.3614\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.7481 - val_loss: 169.5965\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.0465 - val_loss: 140.3600\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 145.9150 - val_loss: 113.0348\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.3982 - val_loss: 157.5575\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.8108 - val_loss: 122.2040\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 154.3147 - val_loss: 125.4468\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 161.3964 - val_loss: 115.5118\n",
      "Epoch 1004/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.4339 - val_loss: 137.5103\n",
      "Epoch 1005/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.7074 - val_loss: 126.0586\n",
      "Epoch 1006/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.0118 - val_loss: 194.9470\n",
      "Epoch 1007/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 373.3576 - val_loss: 114.8655\n",
      "Epoch 1008/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.8841 - val_loss: 113.4466\n",
      "Epoch 1009/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.8828 - val_loss: 120.9535\n",
      "Epoch 1010/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.1399 - val_loss: 132.0325\n",
      "Epoch 1011/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.6671 - val_loss: 129.8573\n",
      "Epoch 1012/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.5411 - val_loss: 133.7286\n",
      "Epoch 1013/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.0422 - val_loss: 127.5086\n",
      "Epoch 1014/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.8547 - val_loss: 120.9981\n",
      "Epoch 1015/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.5478 - val_loss: 127.3120\n",
      "Epoch 1016/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.8570 - val_loss: 126.8094\n",
      "Epoch 1017/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.9527 - val_loss: 137.8559\n",
      "Epoch 1018/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 162.1628 - val_loss: 199.3389\n",
      "Epoch 1019/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.5921 - val_loss: 120.3194\n",
      "Epoch 1020/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.7788 - val_loss: 171.8028\n",
      "Epoch 1021/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.8498 - val_loss: 149.4368\n",
      "Epoch 1022/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 174.7579 - val_loss: 146.9009\n",
      "Epoch 1023/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.3134 - val_loss: 115.2770\n",
      "Epoch 1024/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.6214 - val_loss: 119.9790\n",
      "Epoch 1025/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.1665 - val_loss: 126.1849\n",
      "Epoch 1026/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 254.9501 - val_loss: 128.3798\n",
      "Epoch 1027/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.5714 - val_loss: 120.2321\n",
      "Epoch 1028/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.1138 - val_loss: 117.1047\n",
      "Epoch 1029/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.5230 - val_loss: 153.7140\n",
      "Epoch 1030/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.0911 - val_loss: 112.0132\n",
      "Epoch 1031/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.0790 - val_loss: 133.1021\n",
      "Epoch 1032/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.8066 - val_loss: 121.5598\n",
      "Epoch 1033/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.4880 - val_loss: 116.8138\n",
      "Epoch 1034/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.0607 - val_loss: 127.8113\n",
      "Epoch 1035/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.0175 - val_loss: 117.1133\n",
      "Epoch 1036/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.4511 - val_loss: 133.9023\n",
      "Epoch 1037/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.7881 - val_loss: 121.0035\n",
      "Epoch 1038/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 146.7322 - val_loss: 149.0290\n",
      "Epoch 1039/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 177.2224 - val_loss: 355.7139\n",
      "Epoch 1040/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.8274 - val_loss: 143.8771\n",
      "Epoch 1041/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 142.4911 - val_loss: 161.3347\n",
      "Epoch 1042/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.8046 - val_loss: 122.1259\n",
      "Epoch 1043/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 162.1131 - val_loss: 159.3616\n",
      "Epoch 1044/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.3156 - val_loss: 128.1282\n",
      "Epoch 1045/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.0890 - val_loss: 114.6067\n",
      "Epoch 1046/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.0591 - val_loss: 121.6912\n",
      "Epoch 1047/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.0388 - val_loss: 124.2626\n",
      "Epoch 1048/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.5069 - val_loss: 111.3722\n",
      "Epoch 1049/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.0412 - val_loss: 156.4437\n",
      "Epoch 1050/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 182.1269 - val_loss: 116.9071\n",
      "Epoch 1051/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.0443 - val_loss: 121.7497\n",
      "Epoch 1052/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.1627 - val_loss: 135.2515\n",
      "Epoch 1053/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.7500 - val_loss: 129.1077\n",
      "Epoch 1054/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.5296 - val_loss: 148.7575\n",
      "Epoch 1055/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.1956 - val_loss: 126.2737\n",
      "Epoch 1056/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.2188 - val_loss: 115.3578\n",
      "Epoch 1057/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 157.7963 - val_loss: 125.9133\n",
      "Epoch 1058/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.2189 - val_loss: 126.3834\n",
      "Epoch 1059/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.4324 - val_loss: 160.3604\n",
      "Epoch 1060/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.0317 - val_loss: 128.3652\n",
      "Epoch 1061/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.9633 - val_loss: 130.7880\n",
      "Epoch 1062/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.9394 - val_loss: 173.6956\n",
      "Epoch 1063/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 153.6161 - val_loss: 142.2265\n",
      "Epoch 1064/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 160.0754 - val_loss: 167.8805\n",
      "Epoch 1065/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.9442 - val_loss: 125.5549\n",
      "Epoch 1066/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.7244 - val_loss: 120.4599\n",
      "Epoch 1067/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.7861 - val_loss: 125.5387\n",
      "Epoch 1068/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 152.8668 - val_loss: 121.5625\n",
      "Epoch 1069/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.1883 - val_loss: 118.8817\n",
      "Epoch 1070/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.3261 - val_loss: 155.0037\n",
      "Epoch 1071/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.2342 - val_loss: 113.5165\n",
      "Epoch 1072/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.7313 - val_loss: 142.3892\n",
      "Epoch 1073/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 155.0473 - val_loss: 114.0886\n",
      "Epoch 1074/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.4565 - val_loss: 134.3502\n",
      "Epoch 1075/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.2411 - val_loss: 133.4186\n",
      "Epoch 1076/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.2332 - val_loss: 121.2469\n",
      "Epoch 1077/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.6541 - val_loss: 116.5034\n",
      "Epoch 1078/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.4802 - val_loss: 129.1924\n",
      "Epoch 1079/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.0099 - val_loss: 160.6878\n",
      "Epoch 1080/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.9826 - val_loss: 115.8122\n",
      "Epoch 1081/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.5743 - val_loss: 129.5326\n",
      "Epoch 1082/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.1934 - val_loss: 120.8619\n",
      "Epoch 1083/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.4407 - val_loss: 121.8785\n",
      "Epoch 1084/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 153.4127 - val_loss: 147.2202\n",
      "Epoch 1085/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.4186 - val_loss: 141.6708\n",
      "Epoch 1086/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 190.9293 - val_loss: 139.2899\n",
      "Epoch 1087/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.8372 - val_loss: 118.9819\n",
      "Epoch 1088/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.0308 - val_loss: 187.8207\n",
      "Epoch 1089/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.5356 - val_loss: 128.7665\n",
      "Epoch 1090/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.9685 - val_loss: 130.1765\n",
      "Epoch 1091/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.2017 - val_loss: 190.8288\n",
      "Epoch 1092/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.0753 - val_loss: 124.7393\n",
      "Epoch 1093/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.7514 - val_loss: 123.5317\n",
      "Epoch 1094/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 155.0964 - val_loss: 209.3533\n",
      "Epoch 1095/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 172.8066 - val_loss: 124.8559\n",
      "Epoch 1096/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.7690 - val_loss: 125.3810\n",
      "Epoch 1097/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.7077 - val_loss: 125.0283\n",
      "Epoch 1098/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.6323 - val_loss: 123.5123\n",
      "Epoch 1099/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.9172 - val_loss: 119.4656\n",
      "Epoch 1100/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 168.8355 - val_loss: 124.7486\n",
      "Epoch 1101/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.4988 - val_loss: 120.6115\n",
      "Epoch 1102/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.3174 - val_loss: 118.2674\n",
      "Epoch 1103/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.9014 - val_loss: 124.8561\n",
      "Epoch 1104/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.6988 - val_loss: 127.8899\n",
      "Epoch 1105/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.5312 - val_loss: 122.5959\n",
      "Epoch 1106/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 143.7221 - val_loss: 119.7109\n",
      "Epoch 1107/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.5792 - val_loss: 135.6029\n",
      "Epoch 1108/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.6365 - val_loss: 117.7794\n",
      "Epoch 1109/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.1409 - val_loss: 121.8246\n",
      "Epoch 1110/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.3333 - val_loss: 116.0313\n",
      "Epoch 1111/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.6329 - val_loss: 119.1563\n",
      "Epoch 1112/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.3902 - val_loss: 121.0912\n",
      "Epoch 1113/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 138.0606 - val_loss: 118.1213\n",
      "Epoch 1114/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.9568 - val_loss: 146.2577\n",
      "Epoch 1115/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.6268 - val_loss: 178.2844\n",
      "Epoch 1116/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 148.0182 - val_loss: 164.2131\n",
      "Epoch 1117/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.5151 - val_loss: 132.1324\n",
      "Epoch 1118/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.0041 - val_loss: 118.9259\n",
      "Epoch 1119/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.2507 - val_loss: 141.4539\n",
      "Epoch 1120/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.5966 - val_loss: 147.7466\n",
      "Epoch 1121/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 164.6461 - val_loss: 186.1144\n",
      "Epoch 1122/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.6419 - val_loss: 114.6236\n",
      "Epoch 1123/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.7659 - val_loss: 143.5922\n",
      "Epoch 1124/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 219.3099 - val_loss: 127.1542\n",
      "Epoch 1125/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.7732 - val_loss: 160.7964\n",
      "Epoch 1126/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.6036 - val_loss: 115.1319\n",
      "Epoch 1127/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.1500 - val_loss: 119.5394\n",
      "Epoch 1128/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.6912 - val_loss: 113.1384\n",
      "Epoch 1129/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.8161 - val_loss: 114.9368\n",
      "Epoch 1130/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.0114 - val_loss: 149.5598\n",
      "Epoch 1131/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.4680 - val_loss: 142.5279\n",
      "Epoch 1132/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.1174 - val_loss: 113.5783\n",
      "Epoch 1133/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.3595 - val_loss: 148.6918\n",
      "Epoch 1134/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.5462 - val_loss: 119.8507\n",
      "Epoch 1135/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.4866 - val_loss: 111.1322\n",
      "Epoch 1136/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.2487 - val_loss: 121.3920\n",
      "Epoch 1137/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.6711 - val_loss: 169.3361\n",
      "Epoch 1138/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.0050 - val_loss: 128.7147\n",
      "Epoch 1139/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.1653 - val_loss: 123.1298\n",
      "Epoch 1140/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 138.8857 - val_loss: 112.9302\n",
      "Epoch 1141/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 155.4203 - val_loss: 127.6685\n",
      "Epoch 1142/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 167.4753 - val_loss: 144.4973\n",
      "Epoch 1143/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.6385 - val_loss: 115.5479\n",
      "Epoch 1144/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 140.0888 - val_loss: 123.8616\n",
      "Epoch 1145/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.2369 - val_loss: 125.3644\n",
      "Epoch 1146/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.3449 - val_loss: 122.4262\n",
      "Epoch 1147/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.9191 - val_loss: 139.5287\n",
      "Epoch 1148/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.6119 - val_loss: 222.4686\n",
      "Epoch 1149/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 157.6748 - val_loss: 122.8853\n",
      "Epoch 1150/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 238.6513 - val_loss: 792.0867\n",
      "Epoch 1151/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 220.3690 - val_loss: 112.1996\n",
      "Epoch 1152/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.0325 - val_loss: 113.7219\n",
      "Epoch 1153/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 129.5932 - val_loss: 112.7356\n",
      "Epoch 1154/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.5712 - val_loss: 121.7694\n",
      "Epoch 1155/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.6318 - val_loss: 110.5800\n",
      "Epoch 1156/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 135.4952 - val_loss: 115.2605\n",
      "Epoch 1157/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 135.5245 - val_loss: 111.9447\n",
      "Epoch 1158/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.4801 - val_loss: 116.2109\n",
      "Epoch 1159/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.4898 - val_loss: 113.0301\n",
      "Epoch 1160/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.4341 - val_loss: 113.3432\n",
      "Epoch 1161/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.4487 - val_loss: 115.3950\n",
      "Epoch 1162/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 134.6041 - val_loss: 127.4755\n",
      "Epoch 1163/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.4191 - val_loss: 160.0726\n",
      "Epoch 1164/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 142.5134 - val_loss: 117.1193\n",
      "Epoch 1165/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 155.8831 - val_loss: 121.1527\n",
      "Epoch 1166/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.6086 - val_loss: 116.4776\n",
      "Epoch 1167/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 140.9167 - val_loss: 125.1839\n",
      "Epoch 1168/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 163.5211 - val_loss: 165.7848\n",
      "Epoch 1169/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.1735 - val_loss: 121.1731\n",
      "Epoch 1170/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 314.4811 - val_loss: 121.4034\n",
      "Epoch 1171/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 130.0081 - val_loss: 139.3461\n",
      "Epoch 1172/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.1429 - val_loss: 115.8264\n",
      "Epoch 1173/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.3345 - val_loss: 117.9135\n",
      "Epoch 1174/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.3667 - val_loss: 127.6321\n",
      "Epoch 1175/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.6516 - val_loss: 119.7132\n",
      "Epoch 1176/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.7467 - val_loss: 115.8045\n",
      "Epoch 1177/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.4711 - val_loss: 135.4428\n",
      "Epoch 1178/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 145.5359 - val_loss: 113.1325\n",
      "Epoch 1179/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.6914 - val_loss: 153.2154\n",
      "Epoch 1180/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.7462 - val_loss: 153.8572\n",
      "Epoch 1181/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 133.8061 - val_loss: 120.2042\n",
      "Epoch 1182/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.9333 - val_loss: 122.3966\n",
      "Epoch 1183/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 212.2460 - val_loss: 115.3066\n",
      "Epoch 1184/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.5688 - val_loss: 156.2546\n",
      "Epoch 1185/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.5418 - val_loss: 117.4102\n",
      "Epoch 1186/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.6617 - val_loss: 121.0761\n",
      "Epoch 1187/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.2623 - val_loss: 122.4517\n",
      "Epoch 1188/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.4498 - val_loss: 115.7220\n",
      "Epoch 1189/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.2974 - val_loss: 114.9597\n",
      "Epoch 1190/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.9058 - val_loss: 110.5414\n",
      "Epoch 1191/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 188.1792 - val_loss: 115.7702\n",
      "Epoch 1192/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.8627 - val_loss: 127.5247\n",
      "Epoch 1193/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.1628 - val_loss: 143.5756\n",
      "Epoch 1194/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 142.6901 - val_loss: 121.9151\n",
      "Epoch 1195/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.7162 - val_loss: 120.4185\n",
      "Epoch 1196/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.7978 - val_loss: 128.9199\n",
      "Epoch 1197/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 145.4592 - val_loss: 126.3415\n",
      "Epoch 1198/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.3045 - val_loss: 121.3146\n",
      "Epoch 1199/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.5947 - val_loss: 117.2848\n",
      "Epoch 1200/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.6891 - val_loss: 112.0004\n",
      "Epoch 1201/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.3538 - val_loss: 115.8738\n",
      "Epoch 1202/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 139.6700 - val_loss: 174.3908\n",
      "Epoch 1203/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 159.6374 - val_loss: 114.0620\n",
      "Epoch 1204/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.4502 - val_loss: 112.5581\n",
      "Epoch 1205/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.4238 - val_loss: 145.3723\n",
      "Epoch 1206/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.5795 - val_loss: 118.1785\n",
      "Epoch 1207/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.3300 - val_loss: 120.2887\n",
      "Epoch 1208/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.0518 - val_loss: 273.1511\n",
      "Epoch 1209/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 150.6234 - val_loss: 168.5706\n",
      "Epoch 1210/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 210.9070 - val_loss: 120.1285\n",
      "Epoch 1211/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.2892 - val_loss: 117.1928\n",
      "Epoch 1212/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.6050 - val_loss: 123.7671\n",
      "Epoch 1213/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.2134 - val_loss: 114.1348\n",
      "Epoch 1214/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 138.1653 - val_loss: 120.4640\n",
      "Epoch 1215/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.3200 - val_loss: 213.5315\n",
      "Epoch 1216/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.3668 - val_loss: 146.7934\n",
      "Epoch 1217/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 196.9142 - val_loss: 115.5918\n",
      "Epoch 1218/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.3055 - val_loss: 125.8907\n",
      "Epoch 1219/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.5927 - val_loss: 176.7926\n",
      "Epoch 1220/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.5199 - val_loss: 118.3982\n",
      "Epoch 1221/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 150.3523 - val_loss: 123.0394\n",
      "Epoch 1222/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.5400 - val_loss: 117.2546\n",
      "Epoch 1223/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.8418 - val_loss: 137.4526\n",
      "Epoch 1224/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.9434 - val_loss: 139.3473\n",
      "Epoch 1225/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.2642 - val_loss: 113.3890\n",
      "Epoch 1226/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.8962 - val_loss: 117.0771\n",
      "Epoch 1227/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.7006 - val_loss: 111.0210\n",
      "Epoch 1228/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.6562 - val_loss: 152.8776\n",
      "Epoch 1229/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.0869 - val_loss: 118.8471\n",
      "Epoch 1230/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 148.9901 - val_loss: 116.9893\n",
      "Epoch 1231/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.3920 - val_loss: 115.8946\n",
      "Epoch 1232/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.9404 - val_loss: 114.2425\n",
      "Epoch 1233/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.7084 - val_loss: 172.4814\n",
      "Epoch 1234/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 148.0505 - val_loss: 181.5852\n",
      "Epoch 1235/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 154.8282 - val_loss: 135.9572\n",
      "Epoch 1236/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.8986 - val_loss: 117.9177\n",
      "Epoch 1237/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.3021 - val_loss: 119.6635\n",
      "Epoch 1238/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.4668 - val_loss: 114.0126\n",
      "Epoch 1239/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.1074 - val_loss: 121.9215\n",
      "Epoch 1240/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 158.3827 - val_loss: 123.1988\n",
      "Epoch 1241/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.2601 - val_loss: 119.6661\n",
      "Epoch 1242/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 148.3208 - val_loss: 113.4256\n",
      "Epoch 1243/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.6743 - val_loss: 124.5743\n",
      "Epoch 1244/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.4826 - val_loss: 135.0140\n",
      "Epoch 1245/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 287.5100 - val_loss: 129.2121\n",
      "Epoch 1246/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.6123 - val_loss: 114.9902\n",
      "Epoch 1247/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.0381 - val_loss: 112.8859\n",
      "Epoch 1248/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.8212 - val_loss: 121.0701\n",
      "Epoch 1249/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.9989 - val_loss: 147.7555\n",
      "Epoch 1250/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.5910 - val_loss: 163.0731\n",
      "Epoch 1251/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.2218 - val_loss: 112.5239\n",
      "Epoch 1252/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.6457 - val_loss: 121.4660\n",
      "Epoch 1253/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.9460 - val_loss: 163.7034\n",
      "Epoch 1254/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.7161 - val_loss: 115.3940\n",
      "Epoch 1255/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.0549 - val_loss: 127.1500\n",
      "Epoch 1256/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.2985 - val_loss: 136.2914\n",
      "Epoch 1257/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.8287 - val_loss: 117.6801ETA: 0s - loss: 129\n",
      "Epoch 1258/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.1310 - val_loss: 133.6761\n",
      "Epoch 1259/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.4197 - val_loss: 142.0147\n",
      "Epoch 1260/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.1495 - val_loss: 205.1656\n",
      "Epoch 1261/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.2614 - val_loss: 121.2896\n",
      "Epoch 1262/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.1926 - val_loss: 111.7230\n",
      "Epoch 1263/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.3568 - val_loss: 191.9471\n",
      "Epoch 1264/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 195.5560 - val_loss: 136.5840\n",
      "Epoch 1265/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.1620 - val_loss: 149.5548\n",
      "Epoch 1266/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 134.4798 - val_loss: 116.6895\n",
      "Epoch 1267/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.4635 - val_loss: 113.8502\n",
      "Epoch 1268/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.7157 - val_loss: 118.6472\n",
      "Epoch 1269/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.3743 - val_loss: 114.9375\n",
      "Epoch 1270/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.7731 - val_loss: 136.3256\n",
      "Epoch 1271/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.1782 - val_loss: 123.3319\n",
      "Epoch 1272/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.3939 - val_loss: 2994.4859\n",
      "Epoch 1273/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 399.5663 - val_loss: 120.6068\n",
      "Epoch 1274/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.1377 - val_loss: 130.0947\n",
      "Epoch 1275/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.1560 - val_loss: 122.7568\n",
      "Epoch 1276/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.5928 - val_loss: 116.3629\n",
      "Epoch 1277/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.5555 - val_loss: 117.3878\n",
      "Epoch 1278/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.0428 - val_loss: 118.8843\n",
      "Epoch 1279/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.0445 - val_loss: 113.0711\n",
      "Epoch 1280/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.1797 - val_loss: 133.7789\n",
      "Epoch 1281/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.3673 - val_loss: 127.3816\n",
      "Epoch 1282/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.1953 - val_loss: 117.7509\n",
      "Epoch 1283/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.2303 - val_loss: 134.1246\n",
      "Epoch 1284/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.0863 - val_loss: 128.7363\n",
      "Epoch 1285/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.9908 - val_loss: 113.4319\n",
      "Epoch 1286/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.3133 - val_loss: 130.9056\n",
      "Epoch 1287/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 159.6513 - val_loss: 114.3714\n",
      "Epoch 1288/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.3094 - val_loss: 113.3374\n",
      "Epoch 1289/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.0931 - val_loss: 112.3512\n",
      "Epoch 1290/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.6420 - val_loss: 110.8060\n",
      "Epoch 1291/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.6608 - val_loss: 115.7919\n",
      "Epoch 1292/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 140.2424 - val_loss: 118.9539\n",
      "Epoch 1293/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 135.4044 - val_loss: 113.7809\n",
      "Epoch 1294/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.7968 - val_loss: 119.2979\n",
      "Epoch 1295/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.5321 - val_loss: 137.0383\n",
      "Epoch 1296/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.5083 - val_loss: 127.8356\n",
      "Epoch 1297/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.1311 - val_loss: 121.3013\n",
      "Epoch 1298/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.8563 - val_loss: 153.6210\n",
      "Epoch 1299/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.6954 - val_loss: 125.6698\n",
      "Epoch 1300/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 321.7326 - val_loss: 118.7028\n",
      "Epoch 1301/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.8500 - val_loss: 135.8515\n",
      "Epoch 1302/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.0476 - val_loss: 113.9876\n",
      "Epoch 1303/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.3661 - val_loss: 115.0927\n",
      "Epoch 1304/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.9640 - val_loss: 116.6380\n",
      "Epoch 1305/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.6716 - val_loss: 158.7537\n",
      "Epoch 1306/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.2015 - val_loss: 121.1077\n",
      "Epoch 1307/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.9782 - val_loss: 161.1029\n",
      "Epoch 1308/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.5927 - val_loss: 121.8520\n",
      "Epoch 1309/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.6684 - val_loss: 272.9118\n",
      "Epoch 1310/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.2578 - val_loss: 113.4969\n",
      "Epoch 1311/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 228.4303 - val_loss: 116.5029\n",
      "Epoch 1312/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.8157 - val_loss: 120.5750\n",
      "Epoch 1313/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.6499 - val_loss: 115.0563\n",
      "Epoch 1314/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.5340 - val_loss: 116.8036\n",
      "Epoch 1315/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.7296 - val_loss: 134.4693\n",
      "Epoch 1316/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.9986 - val_loss: 112.6148\n",
      "Epoch 1317/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.5692 - val_loss: 140.9152\n",
      "Epoch 1318/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.5670 - val_loss: 114.9944\n",
      "Epoch 1319/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.6639 - val_loss: 139.7932\n",
      "Epoch 1320/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.3024 - val_loss: 111.4036\n",
      "Epoch 1321/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.5893 - val_loss: 157.6504\n",
      "Epoch 1322/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 142.2778 - val_loss: 125.3036\n",
      "Epoch 1323/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.1185 - val_loss: 129.6358\n",
      "Epoch 1324/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.2506 - val_loss: 127.1664\n",
      "Epoch 1325/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.5291 - val_loss: 133.9456\n",
      "Epoch 1326/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.5377 - val_loss: 185.3177\n",
      "Epoch 1327/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 164.0548 - val_loss: 114.3653\n",
      "Epoch 1328/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.5573 - val_loss: 124.1261\n",
      "Epoch 1329/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.7012 - val_loss: 121.6153\n",
      "Epoch 1330/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 133.1365 - val_loss: 149.2575\n",
      "Epoch 1331/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 336.3504 - val_loss: 171.8942\n",
      "Epoch 1332/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.2993 - val_loss: 114.4345\n",
      "Epoch 1333/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.4984 - val_loss: 122.9349\n",
      "Epoch 1334/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.9238 - val_loss: 113.0350\n",
      "Epoch 1335/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.3316 - val_loss: 122.3810\n",
      "Epoch 1336/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.8001 - val_loss: 110.2274\n",
      "Epoch 1337/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.2628 - val_loss: 157.7764\n",
      "Epoch 1338/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.5230 - val_loss: 116.0805\n",
      "Epoch 1339/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.4407 - val_loss: 117.7346\n",
      "Epoch 1340/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.8740 - val_loss: 158.4161\n",
      "Epoch 1341/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.2085 - val_loss: 121.2908\n",
      "Epoch 1342/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.8683 - val_loss: 114.5727\n",
      "Epoch 1343/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 142.6826 - val_loss: 120.8768\n",
      "Epoch 1344/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.1359 - val_loss: 154.3628\n",
      "Epoch 1345/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.7335 - val_loss: 120.0302\n",
      "Epoch 1346/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.2204 - val_loss: 117.8989\n",
      "Epoch 1347/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.0093 - val_loss: 125.5255\n",
      "Epoch 1348/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.9098 - val_loss: 177.5092\n",
      "Epoch 1349/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.1471 - val_loss: 118.6575\n",
      "Epoch 1350/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.3445 - val_loss: 123.1485\n",
      "Epoch 1351/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 171.0264 - val_loss: 115.9762\n",
      "Epoch 1352/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.5063 - val_loss: 115.8126\n",
      "Epoch 1353/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.1282 - val_loss: 122.6762\n",
      "Epoch 1354/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.2802 - val_loss: 137.7002\n",
      "Epoch 1355/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.6620 - val_loss: 164.7742\n",
      "Epoch 1356/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.8855 - val_loss: 110.6216\n",
      "Epoch 1357/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.3254 - val_loss: 127.5750\n",
      "Epoch 1358/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.4158 - val_loss: 176.6872\n",
      "Epoch 1359/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.1340 - val_loss: 144.9279\n",
      "Epoch 1360/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.8680 - val_loss: 126.0287\n",
      "Epoch 1361/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.2474 - val_loss: 154.9113\n",
      "Epoch 1362/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 191.1802 - val_loss: 122.4068\n",
      "Epoch 1363/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.0095 - val_loss: 120.2837\n",
      "Epoch 1364/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.6485 - val_loss: 133.0243\n",
      "Epoch 1365/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.6197 - val_loss: 137.2756\n",
      "Epoch 1366/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.6750 - val_loss: 125.0716\n",
      "Epoch 1367/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 136.9875 - val_loss: 122.2448\n",
      "Epoch 1368/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 147.3283 - val_loss: 115.5393\n",
      "Epoch 1369/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.0682 - val_loss: 120.1173\n",
      "Epoch 1370/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 153.4176 - val_loss: 113.3204\n",
      "Epoch 1371/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.1140 - val_loss: 121.3987\n",
      "Epoch 1372/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.4258 - val_loss: 111.2332\n",
      "Epoch 1373/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.9754 - val_loss: 114.7709\n",
      "Epoch 1374/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.6272 - val_loss: 121.9982\n",
      "Epoch 1375/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.3099 - val_loss: 158.9892\n",
      "Epoch 1376/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.4632 - val_loss: 132.2774\n",
      "Epoch 1377/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.2145 - val_loss: 114.9449\n",
      "Epoch 1378/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 135.7077 - val_loss: 122.5237\n",
      "Epoch 1379/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 166.2552 - val_loss: 130.6230\n",
      "Epoch 1380/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 135.0540 - val_loss: 123.2846\n",
      "Epoch 1381/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 135.5134 - val_loss: 117.1537\n",
      "Epoch 1382/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.4829 - val_loss: 111.2651\n",
      "Epoch 1383/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 193.0835 - val_loss: 140.3611\n",
      "Epoch 1384/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.8269 - val_loss: 111.6444\n",
      "Epoch 1385/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 127.8797 - val_loss: 109.6702\n",
      "Epoch 1386/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.2378 - val_loss: 125.9428\n",
      "Epoch 1387/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 133.5028 - val_loss: 125.0869\n",
      "Epoch 1388/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.3307 - val_loss: 127.6142\n",
      "Epoch 1389/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.7458 - val_loss: 176.8649\n",
      "Epoch 1390/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.9074 - val_loss: 115.4271\n",
      "Epoch 1391/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.8379 - val_loss: 119.1661\n",
      "Epoch 1392/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.4301 - val_loss: 126.6993\n",
      "Epoch 1393/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 153.2602 - val_loss: 118.8521\n",
      "Epoch 1394/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.8371 - val_loss: 114.1112\n",
      "Epoch 1395/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 133.9016 - val_loss: 124.6118\n",
      "Epoch 1396/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 147.4542 - val_loss: 114.6080\n",
      "Epoch 1397/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 139.9920 - val_loss: 192.3506\n",
      "Epoch 1398/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.2579 - val_loss: 143.2961\n",
      "Epoch 1399/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.6685 - val_loss: 111.4225\n",
      "Epoch 1400/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.3705 - val_loss: 116.5076\n",
      "Epoch 1401/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.8594 - val_loss: 117.1209\n",
      "Epoch 1402/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 182.0834 - val_loss: 116.4770\n",
      "Epoch 1403/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 128.6083 - val_loss: 117.7726\n",
      "Epoch 1404/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 132.7549 - val_loss: 125.0318\n",
      "Epoch 1405/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 144.2032 - val_loss: 150.6240\n",
      "Epoch 1406/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 136.8753 - val_loss: 120.8438\n",
      "Epoch 1407/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 134.9611 - val_loss: 141.4250\n",
      "Epoch 1408/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 173.4748 - val_loss: 117.1989\n",
      "Epoch 1409/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 137.4094 - val_loss: 141.5984\n",
      "Epoch 1410/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.9915 - val_loss: 119.5381\n",
      "Epoch 1411/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 134.8632 - val_loss: 117.1214\n",
      "Epoch 1412/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.1617 - val_loss: 119.1676\n",
      "Epoch 1413/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.7759 - val_loss: 114.7981\n",
      "Epoch 1414/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 143.8293 - val_loss: 150.0481\n",
      "Epoch 1415/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 141.5015 - val_loss: 118.5126\n",
      "Epoch 1416/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.3350 - val_loss: 122.3943\n",
      "Epoch 1417/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.2636 - val_loss: 116.8015\n",
      "Epoch 1418/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.0204 - val_loss: 147.9062\n",
      "Epoch 1419/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 139.5604 - val_loss: 112.9290\n",
      "Epoch 1420/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 138.4499 - val_loss: 115.5901\n",
      "Epoch 1421/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 159.4946 - val_loss: 131.9496\n",
      "Epoch 1422/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 131.7204 - val_loss: 113.4534\n",
      "Epoch 1423/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 132.0454 - val_loss: 126.4517\n",
      "Epoch 1424/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 142.0054 - val_loss: 120.0912\n",
      "Epoch 1425/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 283.6463 - val_loss: 132.1598\n",
      "Epoch 1426/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.3204 - val_loss: 114.6091\n",
      "Epoch 1427/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 131.2731 - val_loss: 110.1353\n",
      "Epoch 1428/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.7866 - val_loss: 116.5971\n",
      "Epoch 1429/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 132.2329 - val_loss: 118.0859\n",
      "Epoch 1430/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 127.4038 - val_loss: 112.6346\n",
      "Epoch 1431/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.9686 - val_loss: 118.5510\n",
      "Epoch 1432/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 139.8836 - val_loss: 149.8805\n",
      "Epoch 1433/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 136.4956 - val_loss: 160.3559\n",
      "Epoch 1434/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.4669 - val_loss: 115.7205\n",
      "Epoch 1435/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.6805 - val_loss: 122.7728\n",
      "Epoch 1436/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 131.4711 - val_loss: 112.9417\n",
      "Epoch 1437/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 175.2858 - val_loss: 280.4158\n",
      "Epoch 1438/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 193.0821 - val_loss: 119.7788\n",
      "Epoch 1439/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 131.5819 - val_loss: 117.1333\n",
      "Epoch 1440/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 133.1799 - val_loss: 113.1966\n",
      "Epoch 1441/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 136.3031 - val_loss: 112.4730\n",
      "Epoch 1442/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.6874 - val_loss: 116.5134\n",
      "Epoch 1443/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 134.9132 - val_loss: 115.7934\n",
      "Epoch 1444/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.1823 - val_loss: 112.5195\n",
      "Epoch 1445/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 131.7151 - val_loss: 140.5212\n",
      "Epoch 1446/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 133.2254 - val_loss: 112.4936\n",
      "Epoch 1447/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 135.1746 - val_loss: 140.0350\n",
      "Epoch 1448/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 140.2978 - val_loss: 119.6427\n",
      "Epoch 1449/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 134.8752 - val_loss: 116.4860\n",
      "Epoch 1450/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 145.0113 - val_loss: 111.8383\n",
      "Epoch 1451/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.4562 - val_loss: 117.4553\n",
      "Epoch 1452/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.2977 - val_loss: 117.8508\n",
      "Epoch 1453/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 143.0675 - val_loss: 123.5667\n",
      "Epoch 1454/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.6859 - val_loss: 114.4074\n",
      "Epoch 1455/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 135.5638 - val_loss: 125.7843\n",
      "Epoch 1456/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.4385 - val_loss: 128.1008\n",
      "Epoch 1457/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.9841 - val_loss: 169.6287\n",
      "Epoch 1458/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.6781 - val_loss: 119.2740\n",
      "Epoch 1459/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 232.0655 - val_loss: 118.9442\n",
      "Epoch 1460/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.8480 - val_loss: 113.6764\n",
      "Epoch 1461/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.8282 - val_loss: 119.1236\n",
      "Epoch 1462/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.2964 - val_loss: 123.9547\n",
      "Epoch 1463/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.1480 - val_loss: 137.1191\n",
      "Epoch 1464/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.1375 - val_loss: 113.1816\n",
      "Epoch 1465/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.3421 - val_loss: 112.0861\n",
      "Epoch 1466/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.5030 - val_loss: 135.2231\n",
      "Epoch 1467/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.7200 - val_loss: 125.8178\n",
      "Epoch 1468/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.8255 - val_loss: 120.0866\n",
      "Epoch 1469/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 130.5399 - val_loss: 114.8510\n",
      "Epoch 1470/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.7009 - val_loss: 112.4030\n",
      "Epoch 1471/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.8505 - val_loss: 118.6490\n",
      "Epoch 1472/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.3944 - val_loss: 133.8173\n",
      "Epoch 1473/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.0346 - val_loss: 126.3165\n",
      "Epoch 1474/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.1436 - val_loss: 121.9520\n",
      "Epoch 1475/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.6657 - val_loss: 112.8163\n",
      "Epoch 1476/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.5661 - val_loss: 116.3744\n",
      "Epoch 1477/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.7282 - val_loss: 116.1137\n",
      "Epoch 1478/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.4759 - val_loss: 117.0037\n",
      "Epoch 1479/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.9056 - val_loss: 112.4124\n",
      "Epoch 1480/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.1297 - val_loss: 114.1983\n",
      "Epoch 1481/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.4432 - val_loss: 142.7666\n",
      "Epoch 1482/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 320.6560 - val_loss: 135.3031\n",
      "Epoch 1483/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.3892 - val_loss: 123.4906\n",
      "Epoch 1484/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.4952 - val_loss: 124.0981\n",
      "Epoch 1485/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.0610 - val_loss: 119.8384\n",
      "Epoch 1486/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.1485 - val_loss: 153.6429\n",
      "Epoch 1487/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.3804 - val_loss: 113.0502\n",
      "Epoch 1488/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.8244 - val_loss: 116.9675\n",
      "Epoch 1489/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.9003 - val_loss: 127.6790\n",
      "Epoch 1490/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.8808 - val_loss: 115.9062\n",
      "Epoch 1491/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 142.1673 - val_loss: 167.2188\n",
      "Epoch 1492/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.4184 - val_loss: 117.6105\n",
      "Epoch 1493/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.9627 - val_loss: 114.7219\n",
      "Epoch 1494/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.8603 - val_loss: 117.2562\n",
      "Epoch 1495/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 133.937 - 0s 51us/step - loss: 134.7991 - val_loss: 112.4472\n",
      "Epoch 1496/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 177.2964 - val_loss: 125.9939\n",
      "Epoch 1497/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.9949 - val_loss: 117.2541\n",
      "Epoch 1498/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.7749 - val_loss: 114.6686\n",
      "Epoch 1499/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.6373 - val_loss: 152.7149\n",
      "Epoch 1500/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.6778 - val_loss: 120.8581\n",
      "Epoch 1501/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.3646 - val_loss: 120.2993\n",
      "Epoch 1502/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.7038 - val_loss: 133.0343\n",
      "Epoch 1503/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.6395 - val_loss: 112.4251\n",
      "Epoch 1504/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.1628 - val_loss: 126.5219\n",
      "Epoch 1505/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.0010 - val_loss: 126.2050\n",
      "Epoch 1506/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.0140 - val_loss: 124.9812\n",
      "Epoch 1507/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.7295 - val_loss: 115.1291\n",
      "Epoch 1508/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 131.9067 - val_loss: 119.5266\n",
      "Epoch 1509/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 144.7233 - val_loss: 115.2633\n",
      "Epoch 1510/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 137.3265 - val_loss: 128.5890\n",
      "Epoch 1511/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.5300 - val_loss: 113.0950\n",
      "Epoch 1512/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.7402 - val_loss: 179.7211\n",
      "Epoch 1513/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.7822 - val_loss: 134.4187\n",
      "Epoch 1514/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.4934 - val_loss: 116.3686\n",
      "Epoch 1515/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.6182 - val_loss: 113.2126\n",
      "Epoch 1516/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.4474 - val_loss: 118.6335\n",
      "Epoch 1517/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.9315 - val_loss: 116.1459\n",
      "Epoch 1518/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.4172 - val_loss: 160.5882\n",
      "Epoch 1519/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.5844 - val_loss: 114.2829\n",
      "Epoch 1520/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.2024 - val_loss: 173.5543\n",
      "Epoch 1521/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.1573 - val_loss: 125.3009\n",
      "Epoch 1522/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 154.6210 - val_loss: 181.7093\n",
      "Epoch 1523/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 244.9231 - val_loss: 115.6339\n",
      "Epoch 1524/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.4149 - val_loss: 120.1787\n",
      "Epoch 1525/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.4937 - val_loss: 131.2057\n",
      "Epoch 1526/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.1087 - val_loss: 115.2637\n",
      "Epoch 1527/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.4061 - val_loss: 117.9920\n",
      "Epoch 1528/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.0981 - val_loss: 113.6362\n",
      "Epoch 1529/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.3015 - val_loss: 125.3465\n",
      "Epoch 1530/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.3031 - val_loss: 157.1689\n",
      "Epoch 1531/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.8841 - val_loss: 144.9361\n",
      "Epoch 1532/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.1237 - val_loss: 127.0689\n",
      "Epoch 1533/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.1859 - val_loss: 124.4183\n",
      "Epoch 1534/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.6311 - val_loss: 163.2108\n",
      "Epoch 1535/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 169.4725 - val_loss: 248.5675\n",
      "Epoch 1536/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.3434 - val_loss: 127.0239\n",
      "Epoch 1537/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.6145 - val_loss: 111.4605\n",
      "Epoch 1538/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.4052 - val_loss: 111.4926\n",
      "Epoch 1539/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.9521 - val_loss: 120.0614\n",
      "Epoch 1540/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.2795 - val_loss: 149.1372\n",
      "Epoch 1541/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.3116 - val_loss: 112.2225\n",
      "Epoch 1542/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.0153 - val_loss: 142.0460\n",
      "Epoch 1543/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.5697 - val_loss: 121.5120\n",
      "Epoch 1544/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.0107 - val_loss: 126.9260\n",
      "Epoch 1545/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.1242 - val_loss: 114.0055\n",
      "Epoch 1546/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.1681 - val_loss: 111.9901\n",
      "Epoch 1547/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 161.1746 - val_loss: 118.8558\n",
      "Epoch 1548/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.7166 - val_loss: 127.8057\n",
      "Epoch 1549/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.7969 - val_loss: 113.0268\n",
      "Epoch 1550/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.9668 - val_loss: 122.0668\n",
      "Epoch 1551/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.6799 - val_loss: 145.6709\n",
      "Epoch 1552/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 154.1074 - val_loss: 130.7346\n",
      "Epoch 1553/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.0188 - val_loss: 113.2182\n",
      "Epoch 1554/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.3817 - val_loss: 156.6557\n",
      "Epoch 1555/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.2300 - val_loss: 153.0233\n",
      "Epoch 1556/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.3764 - val_loss: 2182.5961\n",
      "Epoch 1557/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 188.7759 - val_loss: 117.0034\n",
      "Epoch 1558/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.6385 - val_loss: 168.1239\n",
      "Epoch 1559/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.2147 - val_loss: 133.4499\n",
      "Epoch 1560/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 246.4362 - val_loss: 123.3371\n",
      "Epoch 1561/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.8831 - val_loss: 116.0281\n",
      "Epoch 1562/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 127.3844 - val_loss: 113.5927\n",
      "Epoch 1563/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.4515 - val_loss: 141.2717\n",
      "Epoch 1564/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.8570 - val_loss: 125.2873\n",
      "Epoch 1565/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.0399 - val_loss: 116.4413\n",
      "Epoch 1566/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.4116 - val_loss: 112.8836\n",
      "Epoch 1567/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.2718 - val_loss: 152.7232\n",
      "Epoch 1568/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.8310 - val_loss: 110.2245\n",
      "Epoch 1569/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 127.5919 - val_loss: 116.9765\n",
      "Epoch 1570/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.2089 - val_loss: 121.1771\n",
      "Epoch 1571/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.1447 - val_loss: 143.3761\n",
      "Epoch 1572/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.3736 - val_loss: 138.3561\n",
      "Epoch 1573/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.1717 - val_loss: 118.7305\n",
      "Epoch 1574/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.0420 - val_loss: 146.9149\n",
      "Epoch 1575/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.4240 - val_loss: 114.1031\n",
      "Epoch 1576/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 137.7886 - val_loss: 127.0272\n",
      "Epoch 1577/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.4289 - val_loss: 114.0727\n",
      "Epoch 1578/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 136.6629 - val_loss: 112.0631\n",
      "Epoch 1579/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.3047 - val_loss: 120.1504\n",
      "Epoch 1580/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.2818 - val_loss: 120.4665\n",
      "Epoch 1581/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 189.7644 - val_loss: 122.3573\n",
      "Epoch 1582/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.1437 - val_loss: 124.6051\n",
      "Epoch 1583/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 131.7974 - val_loss: 124.2290\n",
      "Epoch 1584/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 137.0884 - val_loss: 200.9725\n",
      "Epoch 1585/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 134.1624 - val_loss: 124.0939\n",
      "Epoch 1586/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 132.3854 - val_loss: 111.3028\n",
      "Epoch 1587/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.7712 - val_loss: 112.9028\n",
      "Epoch 1588/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.3053 - val_loss: 133.3761\n",
      "Epoch 1589/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.4134 - val_loss: 115.8441\n",
      "Epoch 1590/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.1972 - val_loss: 116.1724\n",
      "Epoch 1591/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.5243 - val_loss: 230.2107\n",
      "Epoch 1592/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.0412 - val_loss: 125.8811\n",
      "Epoch 1593/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.7723 - val_loss: 119.7680\n",
      "Epoch 1594/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 133.0881 - val_loss: 125.9410\n",
      "Epoch 1595/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 200.0465 - val_loss: 128.0757\n",
      "Epoch 1596/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.3443 - val_loss: 115.2444\n",
      "Epoch 1597/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.6416 - val_loss: 117.5826\n",
      "Epoch 1598/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.3692 - val_loss: 111.0505\n",
      "Epoch 1599/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.4989 - val_loss: 132.8718\n",
      "Epoch 1600/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.5862 - val_loss: 110.5189\n",
      "Epoch 1601/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.7075 - val_loss: 138.0656\n",
      "Epoch 1602/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.7446 - val_loss: 130.9525\n",
      "Epoch 1603/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.1993 - val_loss: 115.8651\n",
      "Epoch 1604/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.0580 - val_loss: 113.0245\n",
      "Epoch 1605/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.7634 - val_loss: 121.3915\n",
      "Epoch 1606/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.1705 - val_loss: 131.8938\n",
      "Epoch 1607/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.8274 - val_loss: 117.1133\n",
      "Epoch 1608/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.7045 - val_loss: 136.2991\n",
      "Epoch 1609/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.3905 - val_loss: 121.3775\n",
      "Epoch 1610/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.1314 - val_loss: 122.7316\n",
      "Epoch 1611/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.8249 - val_loss: 114.5287\n",
      "Epoch 1612/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 291.6537 - val_loss: 125.8066\n",
      "Epoch 1613/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.6560 - val_loss: 113.5943\n",
      "Epoch 1614/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 127.9288 - val_loss: 117.5024\n",
      "Epoch 1615/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.3088 - val_loss: 115.1587\n",
      "Epoch 1616/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.2592 - val_loss: 117.4282\n",
      "Epoch 1617/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.7808 - val_loss: 128.7819\n",
      "Epoch 1618/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.6239 - val_loss: 122.6962\n",
      "Epoch 1619/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.7538 - val_loss: 126.9052\n",
      "Epoch 1620/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 186.4948 - val_loss: 146.0543\n",
      "Epoch 1621/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.7217 - val_loss: 116.2780\n",
      "Epoch 1622/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.7763 - val_loss: 113.5675\n",
      "Epoch 1623/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.0755 - val_loss: 175.4106\n",
      "Epoch 1624/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.7313 - val_loss: 113.4835\n",
      "Epoch 1625/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.2506 - val_loss: 115.1122\n",
      "Epoch 1626/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.2033 - val_loss: 133.1242\n",
      "Epoch 1627/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.0406 - val_loss: 113.4983\n",
      "Epoch 1628/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.6860 - val_loss: 111.7970\n",
      "Epoch 1629/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.1069 - val_loss: 180.5908\n",
      "Epoch 1630/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.8267 - val_loss: 129.0594\n",
      "Epoch 1631/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.0888 - val_loss: 115.9707\n",
      "Epoch 1632/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.6984 - val_loss: 118.1409\n",
      "Epoch 1633/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 175.0784 - val_loss: 135.1408\n",
      "Epoch 1634/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.2892 - val_loss: 114.3698\n",
      "Epoch 1635/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.3884 - val_loss: 118.8603\n",
      "Epoch 1636/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.3699 - val_loss: 110.6405\n",
      "Epoch 1637/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.0352 - val_loss: 111.9613\n",
      "Epoch 1638/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.0245 - val_loss: 142.5963\n",
      "Epoch 1639/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.6159 - val_loss: 111.8145\n",
      "Epoch 1640/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.1691 - val_loss: 119.3403\n",
      "Epoch 1641/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 184.2323 - val_loss: 114.9697\n",
      "Epoch 1642/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.0956 - val_loss: 114.9579\n",
      "Epoch 1643/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.2207 - val_loss: 114.0789\n",
      "Epoch 1644/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.3861 - val_loss: 116.5519\n",
      "Epoch 1645/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.5406 - val_loss: 117.7465\n",
      "Epoch 1646/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.4109 - val_loss: 114.8375\n",
      "Epoch 1647/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 142.8065 - val_loss: 129.9776\n",
      "Epoch 1648/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 132.4638 - val_loss: 157.2302\n",
      "Epoch 1649/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.9391 - val_loss: 250.0982\n",
      "Epoch 1650/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.6397 - val_loss: 112.8482\n",
      "Epoch 1651/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 132.5549 - val_loss: 123.5590\n",
      "Epoch 1652/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.1130 - val_loss: 112.2403\n",
      "Epoch 1653/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.4000 - val_loss: 131.5705\n",
      "Epoch 1654/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.7656 - val_loss: 117.4302\n",
      "Epoch 1655/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.6349 - val_loss: 118.3723\n",
      "Epoch 1656/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.4730 - val_loss: 126.7206\n",
      "Epoch 1657/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.7162 - val_loss: 115.6072\n",
      "Epoch 1658/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.5586 - val_loss: 123.9498\n",
      "Epoch 1659/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.0410 - val_loss: 134.3946\n",
      "Epoch 1660/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 133.1420 - val_loss: 125.6774\n",
      "Epoch 1661/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 150.3499 - val_loss: 155.2884\n",
      "Epoch 1662/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 386.2650 - val_loss: 122.3407\n",
      "Epoch 1663/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.1569 - val_loss: 126.2200\n",
      "Epoch 1664/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.4204 - val_loss: 112.0796\n",
      "Epoch 1665/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.1944 - val_loss: 113.0536\n",
      "Epoch 1666/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.5918 - val_loss: 113.2519\n",
      "Epoch 1667/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.1679 - val_loss: 149.5129\n",
      "Epoch 1668/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.4836 - val_loss: 111.1882\n",
      "Epoch 1669/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.3293 - val_loss: 112.1214\n",
      "Epoch 1670/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.3064 - val_loss: 112.9855\n",
      "Epoch 1671/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 128.9210 - val_loss: 122.2106\n",
      "Epoch 1672/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.7964 - val_loss: 176.7355\n",
      "Epoch 1673/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.0110 - val_loss: 111.6479\n",
      "Epoch 1674/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.6671 - val_loss: 114.3567\n",
      "Epoch 1675/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.3808 - val_loss: 138.6867\n",
      "Epoch 1676/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.2617 - val_loss: 116.0481\n",
      "Epoch 1677/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.8376 - val_loss: 113.3876\n",
      "Epoch 1678/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.3064 - val_loss: 114.6255\n",
      "Epoch 1679/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.8875 - val_loss: 162.3537\n",
      "Epoch 1680/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.4944 - val_loss: 156.6589\n",
      "Epoch 1681/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 155.4007 - val_loss: 248.6152\n",
      "Epoch 1682/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.5820 - val_loss: 125.5491\n",
      "Epoch 1683/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.1229 - val_loss: 112.5763\n",
      "Epoch 1684/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.6332 - val_loss: 118.0894\n",
      "Epoch 1685/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.3443 - val_loss: 151.4689\n",
      "Epoch 1686/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.7150 - val_loss: 154.3591\n",
      "Epoch 1687/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.9695 - val_loss: 115.5940\n",
      "Epoch 1688/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.3511 - val_loss: 115.3423\n",
      "Epoch 1689/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.4711 - val_loss: 112.8372\n",
      "Epoch 1690/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.8086 - val_loss: 134.3483\n",
      "Epoch 1691/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.2603 - val_loss: 115.8220\n",
      "Epoch 1692/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.3952 - val_loss: 113.8479\n",
      "Epoch 1693/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 162.0635 - val_loss: 122.6151\n",
      "Epoch 1694/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.4972 - val_loss: 126.7483\n",
      "Epoch 1695/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.4405 - val_loss: 119.0926\n",
      "Epoch 1696/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.4777 - val_loss: 125.1534\n",
      "Epoch 1697/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.9422 - val_loss: 109.8915\n",
      "Epoch 1698/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.0285 - val_loss: 150.8834\n",
      "Epoch 1699/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 132.8555 - val_loss: 126.5686\n",
      "Epoch 1700/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 135.1093 - val_loss: 122.6259\n",
      "Epoch 1701/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.5823 - val_loss: 136.6735\n",
      "Epoch 1702/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.1073 - val_loss: 115.1131\n",
      "Epoch 1703/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 167.2893 - val_loss: 116.8089\n",
      "Epoch 1704/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.9998 - val_loss: 114.9948\n",
      "Epoch 1705/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.5145 - val_loss: 125.1464\n",
      "Epoch 1706/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 130.4228 - val_loss: 129.9714\n",
      "Epoch 1707/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.2271 - val_loss: 118.4687\n",
      "Epoch 1708/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.4158 - val_loss: 127.6001\n",
      "Epoch 1709/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.2308 - val_loss: 123.6738\n",
      "Epoch 1710/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 211.1417 - val_loss: 111.3869\n",
      "Epoch 1711/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 129.7845 - val_loss: 135.3188\n",
      "Epoch 1712/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.1524 - val_loss: 158.1555\n",
      "Epoch 1713/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.5755 - val_loss: 146.6908\n",
      "Epoch 1714/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.8785 - val_loss: 116.6156\n",
      "Epoch 1715/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.0178 - val_loss: 119.4654\n",
      "Epoch 1716/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.6300 - val_loss: 173.8008\n",
      "Epoch 1717/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.8688 - val_loss: 134.8964\n",
      "Epoch 1718/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.5587 - val_loss: 134.1081\n",
      "Epoch 1719/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.5551 - val_loss: 118.7478\n",
      "Epoch 1720/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 602.1223 - val_loss: 462.1831\n",
      "Epoch 1721/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 455.0065 - val_loss: 330.0097\n",
      "Epoch 1722/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 238.9887 - val_loss: 188.0778\n",
      "Epoch 1723/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 202.2448 - val_loss: 163.1888\n",
      "Epoch 1724/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 208.6956 - val_loss: 170.4001\n",
      "Epoch 1725/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 189.6877 - val_loss: 153.1228\n",
      "Epoch 1726/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 194.0228 - val_loss: 151.2074\n",
      "Epoch 1727/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 190.7356 - val_loss: 139.7215\n",
      "Epoch 1728/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 176.9275 - val_loss: 180.9564\n",
      "Epoch 1729/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 175.1403 - val_loss: 140.1455\n",
      "Epoch 1730/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 168.9349 - val_loss: 153.2772\n",
      "Epoch 1731/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 161.6781 - val_loss: 141.3492\n",
      "Epoch 1732/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 179.2159 - val_loss: 141.2381\n",
      "Epoch 1733/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 184.2074 - val_loss: 256.2929\n",
      "Epoch 1734/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 174.5045 - val_loss: 153.7538\n",
      "Epoch 1735/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 159.8135 - val_loss: 130.9293\n",
      "Epoch 1736/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.1831 - val_loss: 129.0745\n",
      "Epoch 1737/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.1844 - val_loss: 131.6967\n",
      "Epoch 1738/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 158.7220 - val_loss: 143.3528\n",
      "Epoch 1739/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.0935 - val_loss: 131.5049\n",
      "Epoch 1740/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 151.2033 - val_loss: 232.1577\n",
      "Epoch 1741/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 172.1729 - val_loss: 169.0205\n",
      "Epoch 1742/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 161.1004 - val_loss: 124.4059\n",
      "Epoch 1743/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 163.1602 - val_loss: 131.5144\n",
      "Epoch 1744/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.4879 - val_loss: 227.5695\n",
      "Epoch 1745/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 157.2200 - val_loss: 135.2495\n",
      "Epoch 1746/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 160.8051 - val_loss: 146.5380\n",
      "Epoch 1747/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 173.4060 - val_loss: 128.4231\n",
      "Epoch 1748/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.9974 - val_loss: 173.6357\n",
      "Epoch 1749/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.7907 - val_loss: 124.3071\n",
      "Epoch 1750/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 163.2840 - val_loss: 146.0764\n",
      "Epoch 1751/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 150.7421 - val_loss: 131.7363\n",
      "Epoch 1752/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.2953 - val_loss: 151.8498\n",
      "Epoch 1753/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.1308 - val_loss: 137.0321\n",
      "Epoch 1754/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.4427 - val_loss: 191.2170\n",
      "Epoch 1755/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 160.9378 - val_loss: 140.7403\n",
      "Epoch 1756/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.0826 - val_loss: 126.8084\n",
      "Epoch 1757/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 149.3600 - val_loss: 131.2360\n",
      "Epoch 1758/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 49us/step - loss: 165.8043 - val_loss: 127.0436\n",
      "Epoch 1759/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.3430 - val_loss: 136.9951\n",
      "Epoch 1760/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.5827 - val_loss: 163.4476\n",
      "Epoch 1761/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 164.3798 - val_loss: 143.2301\n",
      "Epoch 1762/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 163.1256 - val_loss: 120.8243\n",
      "Epoch 1763/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 156.3626 - val_loss: 245.3169\n",
      "Epoch 1764/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.3107 - val_loss: 169.4382\n",
      "Epoch 1765/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 153.1568 - val_loss: 124.4996\n",
      "Epoch 1766/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.6983 - val_loss: 144.2185\n",
      "Epoch 1767/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.6415 - val_loss: 140.7048\n",
      "Epoch 1768/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 153.5980 - val_loss: 126.4519\n",
      "Epoch 1769/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.4895 - val_loss: 148.4389\n",
      "Epoch 1770/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 154.5228 - val_loss: 136.1138\n",
      "Epoch 1771/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.1944 - val_loss: 131.0032\n",
      "Epoch 1772/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.7558 - val_loss: 120.5629\n",
      "Epoch 1773/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.3672 - val_loss: 134.6982\n",
      "Epoch 1774/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 152.4918 - val_loss: 139.4147\n",
      "Epoch 1775/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.3823 - val_loss: 141.1494\n",
      "Epoch 1776/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 167.8984 - val_loss: 152.4002\n",
      "Epoch 1777/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 152.3324 - val_loss: 119.8767\n",
      "Epoch 1778/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.3685 - val_loss: 139.7157\n",
      "Epoch 1779/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 153.4090 - val_loss: 118.5592\n",
      "Epoch 1780/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 165.5826 - val_loss: 118.2224\n",
      "Epoch 1781/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.8261 - val_loss: 123.5956\n",
      "Epoch 1782/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 143.7568 - val_loss: 128.2731\n",
      "Epoch 1783/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 142.7972 - val_loss: 124.6815\n",
      "Epoch 1784/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 216.2695 - val_loss: 127.6045\n",
      "Epoch 1785/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 137.6265 - val_loss: 154.1082\n",
      "Epoch 1786/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.3260 - val_loss: 141.7706\n",
      "Epoch 1787/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.2778 - val_loss: 126.4512\n",
      "Epoch 1788/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.3142 - val_loss: 118.2738\n",
      "Epoch 1789/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.2890 - val_loss: 126.1508\n",
      "Epoch 1790/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.6736 - val_loss: 142.2817\n",
      "Epoch 1791/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.8276 - val_loss: 115.8017\n",
      "Epoch 1792/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.2830 - val_loss: 121.4416\n",
      "Epoch 1793/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 146.7308 - val_loss: 130.7659\n",
      "Epoch 1794/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.6584 - val_loss: 127.5649\n",
      "Epoch 1795/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 155.9651 - val_loss: 128.8318\n",
      "Epoch 1796/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.6462 - val_loss: 128.2322\n",
      "Epoch 1797/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.4077 - val_loss: 122.7179\n",
      "Epoch 1798/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 200.1590 - val_loss: 270.8069\n",
      "Epoch 1799/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.1478 - val_loss: 135.2703\n",
      "Epoch 1800/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.1196 - val_loss: 127.2539\n",
      "Epoch 1801/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.8417 - val_loss: 134.2588\n",
      "Epoch 1802/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.8763 - val_loss: 121.2797\n",
      "Epoch 1803/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 152.4326 - val_loss: 140.5023\n",
      "Epoch 1804/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.3491 - val_loss: 140.4557\n",
      "Epoch 1805/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.4303 - val_loss: 132.7614\n",
      "Epoch 1806/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.2952 - val_loss: 119.1300\n",
      "Epoch 1807/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.4250 - val_loss: 124.2094\n",
      "Epoch 1808/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.4402 - val_loss: 168.7857\n",
      "Epoch 1809/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.9166 - val_loss: 159.1990\n",
      "Epoch 1810/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.3990 - val_loss: 127.1815\n",
      "Epoch 1811/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.2953 - val_loss: 122.0813\n",
      "Epoch 1812/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.9302 - val_loss: 127.9523\n",
      "Epoch 1813/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.4875 - val_loss: 115.0133\n",
      "Epoch 1814/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 175.6180 - val_loss: 116.0488\n",
      "Epoch 1815/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 139.8678 - val_loss: 142.0047\n",
      "Epoch 1816/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 142.2497 - val_loss: 162.4634\n",
      "Epoch 1817/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.2159 - val_loss: 128.0081\n",
      "Epoch 1818/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 149.7348 - val_loss: 123.9964\n",
      "Epoch 1819/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.4106 - val_loss: 119.0153\n",
      "Epoch 1820/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 219.7256 - val_loss: 1371.0887\n",
      "Epoch 1821/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 165.5498 - val_loss: 121.8829\n",
      "Epoch 1822/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.5943 - val_loss: 124.1216\n",
      "Epoch 1823/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.0799 - val_loss: 128.4661\n",
      "Epoch 1824/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.0639 - val_loss: 151.5822\n",
      "Epoch 1825/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.4906 - val_loss: 121.2658\n",
      "Epoch 1826/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.1569 - val_loss: 115.2233\n",
      "Epoch 1827/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.7803 - val_loss: 120.3407\n",
      "Epoch 1828/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.3376 - val_loss: 117.5222\n",
      "Epoch 1829/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.5547 - val_loss: 130.0614\n",
      "Epoch 1830/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.8200 - val_loss: 136.7453\n",
      "Epoch 1831/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 219.0402 - val_loss: 122.0107\n",
      "Epoch 1832/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 156.8148 - val_loss: 124.3448\n",
      "Epoch 1833/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.2387 - val_loss: 148.9089\n",
      "Epoch 1834/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.3782 - val_loss: 117.2090\n",
      "Epoch 1835/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.6812 - val_loss: 137.6918\n",
      "Epoch 1836/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.2529 - val_loss: 149.2606\n",
      "Epoch 1837/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.8088 - val_loss: 151.0314\n",
      "Epoch 1838/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.3380 - val_loss: 122.9485\n",
      "Epoch 1839/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 153.1004 - val_loss: 120.5821\n",
      "Epoch 1840/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.7659 - val_loss: 119.8743\n",
      "Epoch 1841/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.4067 - val_loss: 121.6421\n",
      "Epoch 1842/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.5711 - val_loss: 122.6507\n",
      "Epoch 1843/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.7055 - val_loss: 131.7423\n",
      "Epoch 1844/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.8613 - val_loss: 123.3839\n",
      "Epoch 1845/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 142.6456 - val_loss: 120.6044\n",
      "Epoch 1846/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.8474 - val_loss: 153.9367\n",
      "Epoch 1847/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.1426 - val_loss: 135.1229\n",
      "Epoch 1848/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.6543 - val_loss: 123.5526\n",
      "Epoch 1849/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 247.1403 - val_loss: 181.5922\n",
      "Epoch 1850/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.7066 - val_loss: 140.9579\n",
      "Epoch 1851/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.5275 - val_loss: 154.2141\n",
      "Epoch 1852/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 140.0780 - val_loss: 119.9191\n",
      "Epoch 1853/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.1096 - val_loss: 129.6754\n",
      "Epoch 1854/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.0630 - val_loss: 114.8683\n",
      "Epoch 1855/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 142.6046 - val_loss: 142.3753\n",
      "Epoch 1856/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.8054 - val_loss: 165.4091\n",
      "Epoch 1857/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 145.3392 - val_loss: 117.9602\n",
      "Epoch 1858/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.3333 - val_loss: 145.7819\n",
      "Epoch 1859/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.4995 - val_loss: 200.4044\n",
      "Epoch 1860/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.0249 - val_loss: 130.0410\n",
      "Epoch 1861/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.5652 - val_loss: 141.5473\n",
      "Epoch 1862/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.5365 - val_loss: 119.1406\n",
      "Epoch 1863/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.2746 - val_loss: 141.2508\n",
      "Epoch 1864/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 156.8636 - val_loss: 155.0936\n",
      "Epoch 1865/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 150.9082 - val_loss: 119.6919\n",
      "Epoch 1866/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 146.3980 - val_loss: 124.4125\n",
      "Epoch 1867/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 163.3779 - val_loss: 121.1213\n",
      "Epoch 1868/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.2424 - val_loss: 127.9198\n",
      "Epoch 1869/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.9379 - val_loss: 157.5348\n",
      "Epoch 1870/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.2148 - val_loss: 122.3650\n",
      "Epoch 1871/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.4848 - val_loss: 148.3794\n",
      "Epoch 1872/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.2636 - val_loss: 130.5271\n",
      "Epoch 1873/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.2834 - val_loss: 118.6969\n",
      "Epoch 1874/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 142.4131 - val_loss: 118.5476\n",
      "Epoch 1875/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.3633 - val_loss: 131.3201\n",
      "Epoch 1876/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.8544 - val_loss: 117.0355\n",
      "Epoch 1877/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 154.3565 - val_loss: 118.6627\n",
      "Epoch 1878/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 151.1090 - val_loss: 118.0388\n",
      "Epoch 1879/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.4122 - val_loss: 133.9833\n",
      "Epoch 1880/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 150.8958 - val_loss: 121.3990\n",
      "Epoch 1881/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.2668 - val_loss: 115.3636\n",
      "Epoch 1882/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.5627 - val_loss: 166.9822\n",
      "Epoch 1883/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.2879 - val_loss: 125.6017\n",
      "Epoch 1884/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.2575 - val_loss: 185.0042\n",
      "Epoch 1885/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.6737 - val_loss: 118.3441\n",
      "Epoch 1886/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.6785 - val_loss: 112.9614\n",
      "Epoch 1887/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.3389 - val_loss: 137.8004\n",
      "Epoch 1888/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.9553 - val_loss: 131.6283\n",
      "Epoch 1889/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 165.3628 - val_loss: 118.3340\n",
      "Epoch 1890/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 157.9104 - val_loss: 131.4711\n",
      "Epoch 1891/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 135.1975 - val_loss: 122.7513\n",
      "Epoch 1892/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.9961 - val_loss: 130.7164\n",
      "Epoch 1893/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 134.0017 - val_loss: 116.4272\n",
      "Epoch 1894/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.6423 - val_loss: 170.3906\n",
      "Epoch 1895/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 152.7148 - val_loss: 153.9570\n",
      "Epoch 1896/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.1235 - val_loss: 142.8783\n",
      "Epoch 1897/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 146.9852 - val_loss: 137.2733\n",
      "Epoch 1898/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.2169 - val_loss: 129.4296\n",
      "Epoch 1899/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.2254 - val_loss: 126.6552\n",
      "Epoch 1900/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.6217 - val_loss: 125.5817\n",
      "Epoch 1901/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.4559 - val_loss: 124.6402\n",
      "Epoch 1902/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.1448 - val_loss: 140.2751\n",
      "Epoch 1903/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 143.1708 - val_loss: 118.2705\n",
      "Epoch 1904/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.1741 - val_loss: 120.2955\n",
      "Epoch 1905/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.3835 - val_loss: 124.1320\n",
      "Epoch 1906/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.3647 - val_loss: 126.8164\n",
      "Epoch 1907/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.7862 - val_loss: 120.4088\n",
      "Epoch 1908/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.3781 - val_loss: 132.5549\n",
      "Epoch 1909/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 152.3813 - val_loss: 127.5841\n",
      "Epoch 1910/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 143.3467 - val_loss: 132.1202\n",
      "Epoch 1911/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.4597 - val_loss: 121.0269\n",
      "Epoch 1912/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.9857 - val_loss: 116.3360\n",
      "Epoch 1913/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.8486 - val_loss: 120.5952\n",
      "Epoch 1914/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 196.7004 - val_loss: 781.6061\n",
      "Epoch 1915/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.1041 - val_loss: 116.6790\n",
      "Epoch 1916/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.9673 - val_loss: 113.3055\n",
      "Epoch 1917/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.6954 - val_loss: 122.5324\n",
      "Epoch 1918/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.0460 - val_loss: 142.9038\n",
      "Epoch 1919/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.6446 - val_loss: 123.7833\n",
      "Epoch 1920/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.2101 - val_loss: 124.1485\n",
      "Epoch 1921/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.8076 - val_loss: 115.8905\n",
      "Epoch 1922/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.3094 - val_loss: 119.8115\n",
      "Epoch 1923/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.6063 - val_loss: 113.5105\n",
      "Epoch 1924/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.5157 - val_loss: 115.1427\n",
      "Epoch 1925/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.1544 - val_loss: 121.3453\n",
      "Epoch 1926/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 162.4865 - val_loss: 193.1426\n",
      "Epoch 1927/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.0326 - val_loss: 121.6746\n",
      "Epoch 1928/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.4258 - val_loss: 123.3899\n",
      "Epoch 1929/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.5658 - val_loss: 131.5324\n",
      "Epoch 1930/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.8951 - val_loss: 128.2649\n",
      "Epoch 1931/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.7701 - val_loss: 115.7583\n",
      "Epoch 1932/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.4137 - val_loss: 128.2889\n",
      "Epoch 1933/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.1871 - val_loss: 176.8898\n",
      "Epoch 1934/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 185.8470 - val_loss: 169.0712\n",
      "Epoch 1935/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.8341 - val_loss: 114.7413\n",
      "Epoch 1936/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.1053 - val_loss: 126.1232\n",
      "Epoch 1937/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.1143 - val_loss: 140.4380\n",
      "Epoch 1938/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.1926 - val_loss: 111.1055\n",
      "Epoch 1939/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.4331 - val_loss: 139.8448\n",
      "Epoch 1940/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.7816 - val_loss: 128.7574\n",
      "Epoch 1941/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 144.7051 - val_loss: 191.2671\n",
      "Epoch 1942/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.5531 - val_loss: 119.3541\n",
      "Epoch 1943/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.8570 - val_loss: 151.1066\n",
      "Epoch 1944/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 212.8545 - val_loss: 175.5901\n",
      "Epoch 1945/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.9101 - val_loss: 122.8950\n",
      "Epoch 1946/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.5478 - val_loss: 121.9756\n",
      "Epoch 1947/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.9907 - val_loss: 115.4258\n",
      "Epoch 1948/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.5386 - val_loss: 128.8628\n",
      "Epoch 1949/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.5483 - val_loss: 114.3307\n",
      "Epoch 1950/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.8282 - val_loss: 141.9823\n",
      "Epoch 1951/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.6001 - val_loss: 115.1594\n",
      "Epoch 1952/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.1704 - val_loss: 113.1156\n",
      "Epoch 1953/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.8064 - val_loss: 149.2595\n",
      "Epoch 1954/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.2805 - val_loss: 125.7245\n",
      "Epoch 1955/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 151.2948 - val_loss: 184.7349\n",
      "Epoch 1956/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.7566 - val_loss: 119.9634\n",
      "Epoch 1957/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.3555 - val_loss: 129.4918\n",
      "Epoch 1958/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 197.1536 - val_loss: 164.0866\n",
      "Epoch 1959/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.3153 - val_loss: 122.4913\n",
      "Epoch 1960/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.1307 - val_loss: 143.8039\n",
      "Epoch 1961/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.0909 - val_loss: 121.2977\n",
      "Epoch 1962/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.2351 - val_loss: 120.3867\n",
      "Epoch 1963/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.3111 - val_loss: 118.1592\n",
      "Epoch 1964/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.4028 - val_loss: 117.5668\n",
      "Epoch 1965/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.1175 - val_loss: 116.7332\n",
      "Epoch 1966/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 143.3956 - val_loss: 139.0826\n",
      "Epoch 1967/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 141.5433 - val_loss: 132.9574\n",
      "Epoch 1968/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 141.1163 - val_loss: 141.9713\n",
      "Epoch 1969/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.6709 - val_loss: 115.5979\n",
      "Epoch 1970/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.3572 - val_loss: 143.8612\n",
      "Epoch 1971/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.7982 - val_loss: 123.9160\n",
      "Epoch 1972/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.8445 - val_loss: 132.9426\n",
      "Epoch 1973/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 256.9099 - val_loss: 124.2829\n",
      "Epoch 1974/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.8254 - val_loss: 121.0933\n",
      "Epoch 1975/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.1184 - val_loss: 157.9431\n",
      "Epoch 1976/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.9799 - val_loss: 120.6359\n",
      "Epoch 1977/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.8910 - val_loss: 211.9841\n",
      "Epoch 1978/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.5488 - val_loss: 128.3230\n",
      "Epoch 1979/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.2661 - val_loss: 119.3139\n",
      "Epoch 1980/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.1891 - val_loss: 119.8234\n",
      "Epoch 1981/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 144.7206 - val_loss: 118.3392\n",
      "Epoch 1982/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.3752 - val_loss: 136.2801\n",
      "Epoch 1983/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.8308 - val_loss: 128.3545\n",
      "Epoch 1984/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.3896 - val_loss: 116.2564\n",
      "Epoch 1985/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.3734 - val_loss: 146.9624\n",
      "Epoch 1986/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.1452 - val_loss: 165.6039\n",
      "Epoch 1987/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.6421 - val_loss: 115.1395\n",
      "Epoch 1988/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.6582 - val_loss: 126.2828\n",
      "Epoch 1989/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.7154 - val_loss: 133.3736\n",
      "Epoch 1990/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.3770 - val_loss: 116.0220\n",
      "Epoch 1991/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 156.3747 - val_loss: 239.6699\n",
      "Epoch 1992/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 173.9870 - val_loss: 122.5026\n",
      "Epoch 1993/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.0271 - val_loss: 112.7808\n",
      "Epoch 1994/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.1855 - val_loss: 115.1714\n",
      "Epoch 1995/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.7234 - val_loss: 147.0151\n",
      "Epoch 1996/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.7307 - val_loss: 126.8846\n",
      "Epoch 1997/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 144.0168 - val_loss: 126.4310\n",
      "Epoch 1998/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.3883 - val_loss: 126.2322\n",
      "Epoch 1999/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.7240 - val_loss: 122.3616\n",
      "Epoch 2000/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.0154 - val_loss: 118.3487\n",
      "Epoch 2001/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.3996 - val_loss: 119.2883\n",
      "Epoch 2002/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 184.0038 - val_loss: 138.1091\n",
      "Epoch 2003/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 158.5033 - val_loss: 133.5295\n",
      "Epoch 2004/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.4574 - val_loss: 197.8812\n",
      "Epoch 2005/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 140.5551 - val_loss: 117.9066\n",
      "Epoch 2006/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.9005 - val_loss: 132.9129\n",
      "Epoch 2007/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.5821 - val_loss: 121.3237\n",
      "Epoch 2008/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.1988 - val_loss: 117.2479\n",
      "Epoch 2009/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.7941 - val_loss: 121.4605\n",
      "Epoch 2010/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.5302 - val_loss: 125.6912\n",
      "Epoch 2011/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.6807 - val_loss: 120.7902\n",
      "Epoch 2012/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.1604 - val_loss: 133.4844\n",
      "Epoch 2013/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.0579 - val_loss: 140.3552\n",
      "Epoch 2014/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.5636 - val_loss: 120.0842\n",
      "Epoch 2015/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 144.1158 - val_loss: 120.1006\n",
      "Epoch 2016/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.5075 - val_loss: 119.8861\n",
      "Epoch 2017/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.4494 - val_loss: 118.2858\n",
      "Epoch 2018/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 159.4075 - val_loss: 144.6608\n",
      "Epoch 2019/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.7738 - val_loss: 142.7974\n",
      "Epoch 2020/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 161.1749 - val_loss: 210.0537\n",
      "Epoch 2021/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 151.4763 - val_loss: 138.2460\n",
      "Epoch 2022/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.0627 - val_loss: 204.7133\n",
      "Epoch 2023/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.1634 - val_loss: 116.6746\n",
      "Epoch 2024/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.4519 - val_loss: 122.2677\n",
      "Epoch 2025/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 179.1139 - val_loss: 115.4798\n",
      "Epoch 2026/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.6221 - val_loss: 112.6431\n",
      "Epoch 2027/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.1408 - val_loss: 194.4280\n",
      "Epoch 2028/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.0616 - val_loss: 118.1188\n",
      "Epoch 2029/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.4445 - val_loss: 120.0472\n",
      "Epoch 2030/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.4151 - val_loss: 123.7376\n",
      "Epoch 2031/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.7637 - val_loss: 116.2463\n",
      "Epoch 2032/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.8497 - val_loss: 116.0190\n",
      "Epoch 2033/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.0486 - val_loss: 121.5298\n",
      "Epoch 2034/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.3692 - val_loss: 126.0741\n",
      "Epoch 2035/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.5239 - val_loss: 117.2390\n",
      "Epoch 2036/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.6500 - val_loss: 125.5170\n",
      "Epoch 2037/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.0036 - val_loss: 151.6517\n",
      "Epoch 2038/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.6493 - val_loss: 116.2382\n",
      "Epoch 2039/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.0191 - val_loss: 135.8792\n",
      "Epoch 2040/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 192.2892 - val_loss: 124.8714\n",
      "Epoch 2041/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.4809 - val_loss: 128.0869\n",
      "Epoch 2042/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.5115 - val_loss: 114.1805\n",
      "Epoch 2043/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 136.3722 - val_loss: 125.5356\n",
      "Epoch 2044/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 146.2734 - val_loss: 127.3116\n",
      "Epoch 2045/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 134.8660 - val_loss: 112.9737\n",
      "Epoch 2046/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 56us/step - loss: 133.3280 - val_loss: 154.8526\n",
      "Epoch 2047/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.5468 - val_loss: 126.2780\n",
      "Epoch 2048/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.4357 - val_loss: 117.9713\n",
      "Epoch 2049/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.5211 - val_loss: 137.1735\n",
      "Epoch 2050/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.6126 - val_loss: 148.3946\n",
      "Epoch 2051/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 153.3601 - val_loss: 120.8323\n",
      "Epoch 2052/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.7283 - val_loss: 117.9939\n",
      "Epoch 2053/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.4555 - val_loss: 119.8497\n",
      "Epoch 2054/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.2464 - val_loss: 178.0125\n",
      "Epoch 2055/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.5050 - val_loss: 118.5911\n",
      "Epoch 2056/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.5572 - val_loss: 117.0682\n",
      "Epoch 2057/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.9801 - val_loss: 128.3394\n",
      "Epoch 2058/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.4185 - val_loss: 129.6914\n",
      "Epoch 2059/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 147.2250 - val_loss: 131.1246\n",
      "Epoch 2060/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 173.1101 - val_loss: 130.8853\n",
      "Epoch 2061/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.5479 - val_loss: 115.6938\n",
      "Epoch 2062/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 137.0154 - val_loss: 111.9093\n",
      "Epoch 2063/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 145.7837 - val_loss: 116.2859\n",
      "Epoch 2064/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 158.4717 - val_loss: 115.9209\n",
      "Epoch 2065/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 140.2804 - val_loss: 137.5301\n",
      "Epoch 2066/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 138.4298 - val_loss: 137.4002\n",
      "Epoch 2067/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.1584 - val_loss: 151.1877\n",
      "Epoch 2068/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 176.0985 - val_loss: 152.6076\n",
      "Epoch 2069/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.4989 - val_loss: 156.3998\n",
      "Epoch 2070/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.4119 - val_loss: 117.9288\n",
      "Epoch 2071/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.3786 - val_loss: 119.4159\n",
      "Epoch 2072/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.1306 - val_loss: 123.7545\n",
      "Epoch 2073/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.5407 - val_loss: 177.6578\n",
      "Epoch 2074/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 146.2011 - val_loss: 111.3906\n",
      "Epoch 2075/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.6489 - val_loss: 167.4417\n",
      "Epoch 2076/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 147.8922 - val_loss: 135.4138\n",
      "Epoch 2077/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.2601 - val_loss: 118.8843\n",
      "Epoch 2078/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.7359 - val_loss: 139.3723\n",
      "Epoch 2079/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 144.3202 - val_loss: 122.1645\n",
      "Epoch 2080/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.7894 - val_loss: 124.1090\n",
      "Epoch 2081/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.2211 - val_loss: 154.0642\n",
      "Epoch 2082/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 147.3054 - val_loss: 117.3007\n",
      "Epoch 2083/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.7764 - val_loss: 134.7421\n",
      "Epoch 2084/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.8594 - val_loss: 115.5268\n",
      "Epoch 2085/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.6865 - val_loss: 121.5971\n",
      "Epoch 2086/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.5655 - val_loss: 118.0231\n",
      "Epoch 2087/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.1204 - val_loss: 130.8393\n",
      "Epoch 2088/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.4536 - val_loss: 116.0703\n",
      "Epoch 2089/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 138.5984 - val_loss: 129.1455\n",
      "Epoch 2090/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.4704 - val_loss: 127.4709\n",
      "Epoch 2091/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.6560 - val_loss: 151.3510\n",
      "Epoch 2092/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.6962 - val_loss: 145.3451\n",
      "Epoch 2093/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.3213 - val_loss: 155.8298\n",
      "Epoch 2094/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 147.6732 - val_loss: 113.8083\n",
      "Epoch 2095/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.6496 - val_loss: 258.2686\n",
      "Epoch 2096/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 159.0517 - val_loss: 128.3784\n",
      "Epoch 2097/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.1190 - val_loss: 111.5757\n",
      "Epoch 2098/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.7340 - val_loss: 124.3744\n",
      "Epoch 2099/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.1232 - val_loss: 136.8490\n",
      "Epoch 2100/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.1084 - val_loss: 117.5107\n",
      "Epoch 2101/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.0137 - val_loss: 125.8306\n",
      "Epoch 2102/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.4640 - val_loss: 176.9076\n",
      "Epoch 2103/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 145.0698 - val_loss: 127.5775\n",
      "Epoch 2104/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.8267 - val_loss: 121.5755\n",
      "Epoch 2105/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.8601 - val_loss: 118.7516\n",
      "Epoch 2106/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 151.3256 - val_loss: 147.3666\n",
      "Epoch 2107/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 134.4185 - val_loss: 120.5458\n",
      "Epoch 2108/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.1704 - val_loss: 135.5686\n",
      "Epoch 2109/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.3714 - val_loss: 141.7996\n",
      "Epoch 2110/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 144.3049 - val_loss: 122.2972\n",
      "Epoch 2111/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 166.5979 - val_loss: 377.8380\n",
      "Epoch 2112/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 157.2080 - val_loss: 113.3630\n",
      "Epoch 2113/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.3791 - val_loss: 160.3350\n",
      "Epoch 2114/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.2618 - val_loss: 134.9715\n",
      "Epoch 2115/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.2634 - val_loss: 114.3141\n",
      "Epoch 2116/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.5564 - val_loss: 150.1837\n",
      "Epoch 2117/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 147.8269 - val_loss: 367.2498\n",
      "Epoch 2118/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 63us/step - loss: 141.3254 - val_loss: 127.3386\n",
      "Epoch 2119/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 143.2184 - val_loss: 112.4493\n",
      "Epoch 2120/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 140.3172 - val_loss: 124.8921\n",
      "Epoch 2121/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 131.4885 - val_loss: 128.7073\n",
      "Epoch 2122/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 137.8376 - val_loss: 133.7722\n",
      "Epoch 2123/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.3801 - val_loss: 112.3164\n",
      "Epoch 2124/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.2226 - val_loss: 117.4905\n",
      "Epoch 2125/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.1846 - val_loss: 125.3473\n",
      "Epoch 2126/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 149.7338 - val_loss: 129.4756\n",
      "Epoch 2127/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 142.6949 - val_loss: 146.9690\n",
      "Epoch 2128/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 136.0710 - val_loss: 113.7562\n",
      "Epoch 2129/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 133.9034 - val_loss: 120.4065\n",
      "Epoch 2130/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 137.3808 - val_loss: 118.1494\n",
      "Epoch 2131/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 136.4599 - val_loss: 115.7555\n",
      "Epoch 2132/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 135.8398 - val_loss: 122.5452\n",
      "Epoch 2133/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 160.1850 - val_loss: 161.6659\n",
      "Epoch 2134/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 137.1950 - val_loss: 115.9547\n",
      "Epoch 2135/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 137.9284 - val_loss: 117.8349\n",
      "Epoch 2136/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.8634 - val_loss: 120.5345\n",
      "Epoch 2137/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.9107 - val_loss: 135.6836\n",
      "Epoch 2138/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.6251 - val_loss: 180.5503\n",
      "Epoch 2139/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 143.8464 - val_loss: 113.1476\n",
      "Epoch 2140/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.4371 - val_loss: 128.5342\n",
      "Epoch 2141/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 144.5059 - val_loss: 128.3595\n",
      "Epoch 2142/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 145.4201 - val_loss: 164.7830\n",
      "Epoch 2143/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 139.5199 - val_loss: 116.8031\n",
      "Epoch 2144/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 138.5159 - val_loss: 129.8749\n",
      "Epoch 2145/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 132.9878 - val_loss: 184.2030\n",
      "Epoch 2146/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 143.0897 - val_loss: 152.1668\n",
      "Epoch 2147/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 140.6229 - val_loss: 115.2850\n",
      "Epoch 2148/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 137.3142 - val_loss: 116.4736\n",
      "Epoch 2149/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 134.0427 - val_loss: 112.8213\n",
      "Epoch 2150/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 180.9787 - val_loss: 113.5330\n",
      "Epoch 2151/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 141.5703 - val_loss: 120.2113\n",
      "Epoch 2152/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 134.2941 - val_loss: 120.3016\n",
      "Epoch 2153/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.3713 - val_loss: 116.0165\n",
      "Epoch 2154/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 130.6913 - val_loss: 128.5369\n",
      "Epoch 2155/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 138.7856 - val_loss: 118.0131\n",
      "Epoch 2156/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 136.0208 - val_loss: 147.3482\n",
      "Epoch 2157/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 139.2597 - val_loss: 135.7980\n",
      "Epoch 2158/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.3785 - val_loss: 120.1486\n",
      "Epoch 2159/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 141.0684 - val_loss: 165.1330\n",
      "Epoch 2160/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 134.1538 - val_loss: 114.3977\n",
      "Epoch 2161/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 145.8425 - val_loss: 139.5093\n",
      "Epoch 2162/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 149.9448 - val_loss: 140.5057\n",
      "Epoch 2163/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 146.7536 - val_loss: 126.9776\n",
      "Epoch 2164/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.5507 - val_loss: 129.5627\n",
      "Epoch 2165/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 133.3387 - val_loss: 128.5381\n",
      "Epoch 2166/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 136.5331 - val_loss: 115.8421\n",
      "Epoch 2167/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.4666 - val_loss: 114.6670\n",
      "Epoch 2168/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 140.5602 - val_loss: 139.2259\n",
      "Epoch 2169/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 150.6852 - val_loss: 131.8403\n",
      "Epoch 2170/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.4684 - val_loss: 114.2589\n",
      "Epoch 2171/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 139.0298 - val_loss: 145.5436\n",
      "Epoch 2172/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.6962 - val_loss: 116.7947\n",
      "Epoch 2173/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.5181 - val_loss: 114.9160\n",
      "Epoch 2174/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.8588 - val_loss: 114.1617\n",
      "Epoch 2175/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.6896 - val_loss: 116.4117\n",
      "Epoch 2176/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.9090 - val_loss: 113.5666\n",
      "Epoch 2177/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.5601 - val_loss: 125.8809\n",
      "Epoch 2178/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.0204 - val_loss: 112.4673\n",
      "Epoch 2179/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.0262 - val_loss: 118.1759\n",
      "Epoch 2180/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.3636 - val_loss: 110.9195\n",
      "Epoch 2181/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.5315 - val_loss: 134.8960\n",
      "Epoch 2182/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.5696 - val_loss: 131.0730\n",
      "Epoch 2183/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.3913 - val_loss: 129.0878\n",
      "Epoch 2184/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 134.5065 - val_loss: 117.4134\n",
      "Epoch 2185/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 147.2148 - val_loss: 113.7760\n",
      "Epoch 2186/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 139.550 - 1s 63us/step - loss: 137.8566 - val_loss: 118.0651\n",
      "Epoch 2187/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 143.9577 - val_loss: 116.6721\n",
      "Epoch 2188/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.1251 - val_loss: 197.9897\n",
      "Epoch 2189/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.0619 - val_loss: 115.5334\n",
      "Epoch 2190/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.3767 - val_loss: 120.6815\n",
      "Epoch 2191/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 202.4064 - val_loss: 116.3754\n",
      "Epoch 2192/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.4317 - val_loss: 127.7427\n",
      "Epoch 2193/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.5682 - val_loss: 136.2425\n",
      "Epoch 2194/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.5909 - val_loss: 124.6359\n",
      "Epoch 2195/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.6106 - val_loss: 134.2118\n",
      "Epoch 2196/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.5519 - val_loss: 117.4670\n",
      "Epoch 2197/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 130.7648 - val_loss: 132.9668\n",
      "Epoch 2198/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.2545 - val_loss: 130.9856\n",
      "Epoch 2199/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 143.1904 - val_loss: 117.4998\n",
      "Epoch 2200/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.5739 - val_loss: 115.1114\n",
      "Epoch 2201/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.5484 - val_loss: 132.0186\n",
      "Epoch 2202/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.4547 - val_loss: 123.0203\n",
      "Epoch 2203/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 135.9217 - val_loss: 142.5638\n",
      "Epoch 2204/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.4181 - val_loss: 172.0122\n",
      "Epoch 2205/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 154.4467 - val_loss: 131.6774\n",
      "Epoch 2206/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 151.2980 - val_loss: 127.2542\n",
      "Epoch 2207/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.4535 - val_loss: 117.7351\n",
      "Epoch 2208/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.9204 - val_loss: 116.7703\n",
      "Epoch 2209/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.8251 - val_loss: 116.9401\n",
      "Epoch 2210/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 141.3906 - val_loss: 115.0972\n",
      "Epoch 2211/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.2530 - val_loss: 119.4432\n",
      "Epoch 2212/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.1397 - val_loss: 146.7605\n",
      "Epoch 2213/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.4986 - val_loss: 128.5215\n",
      "Epoch 2214/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.2754 - val_loss: 126.8717\n",
      "Epoch 2215/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.5082 - val_loss: 117.5162\n",
      "Epoch 2216/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.7144 - val_loss: 148.8546\n",
      "Epoch 2217/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.4047 - val_loss: 113.8908\n",
      "Epoch 2218/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.8663 - val_loss: 147.8090\n",
      "Epoch 2219/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.2241 - val_loss: 128.8417\n",
      "Epoch 2220/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.4779 - val_loss: 156.6696\n",
      "Epoch 2221/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 149.3184 - val_loss: 116.5473\n",
      "Epoch 2222/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.7384 - val_loss: 114.7162\n",
      "Epoch 2223/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.7382 - val_loss: 121.7997\n",
      "Epoch 2224/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.3699 - val_loss: 115.1759\n",
      "Epoch 2225/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 143.5682 - val_loss: 151.8331\n",
      "Epoch 2226/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.5720 - val_loss: 114.7358\n",
      "Epoch 2227/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 162.5145 - val_loss: 118.5772\n",
      "Epoch 2228/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.4428 - val_loss: 135.1929\n",
      "Epoch 2229/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.3628 - val_loss: 117.6661\n",
      "Epoch 2230/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.9718 - val_loss: 122.3732\n",
      "Epoch 2231/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.3630 - val_loss: 174.5246\n",
      "Epoch 2232/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.9149 - val_loss: 123.7615\n",
      "Epoch 2233/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 149.8708 - val_loss: 110.5659\n",
      "Epoch 2234/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.9711 - val_loss: 125.8279\n",
      "Epoch 2235/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.9908 - val_loss: 115.9733\n",
      "Epoch 2236/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 142.1217 - val_loss: 129.1551\n",
      "Epoch 2237/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.9724 - val_loss: 122.3634\n",
      "Epoch 2238/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 135.0432 - val_loss: 126.8024\n",
      "Epoch 2239/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.1303 - val_loss: 122.7983\n",
      "Epoch 2240/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 149.1675 - val_loss: 118.2483\n",
      "Epoch 2241/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 140.8802 - val_loss: 120.9537\n",
      "Epoch 2242/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 133.0167 - val_loss: 116.0121\n",
      "Epoch 2243/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.1065 - val_loss: 154.1314\n",
      "Epoch 2244/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 155.0587 - val_loss: 115.3124\n",
      "Epoch 2245/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.7882 - val_loss: 113.9483\n",
      "Epoch 2246/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.6865 - val_loss: 131.5831\n",
      "Epoch 2247/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 133.9998 - val_loss: 115.8725\n",
      "Epoch 2248/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 133.6967 - val_loss: 116.1151\n",
      "Epoch 2249/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.0749 - val_loss: 120.8527\n",
      "Epoch 2250/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.5334 - val_loss: 119.4517\n",
      "Epoch 2251/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.7277 - val_loss: 151.4575\n",
      "Epoch 2252/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 148.8044 - val_loss: 148.6723\n",
      "Epoch 2253/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.5862 - val_loss: 134.9144\n",
      "Epoch 2254/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.9395 - val_loss: 131.1293\n",
      "Epoch 2255/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.0641 - val_loss: 117.2165\n",
      "Epoch 2256/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 128.4984 - val_loss: 121.4666\n",
      "Epoch 2257/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.0631 - val_loss: 115.3095\n",
      "Epoch 2258/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 320.0242 - val_loss: 131.7530\n",
      "Epoch 2259/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 151.1966 - val_loss: 146.2164\n",
      "Epoch 2260/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 141.0387 - val_loss: 122.4460\n",
      "Epoch 2261/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 144.3936 - val_loss: 115.8410\n",
      "Epoch 2262/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 151.1035 - val_loss: 115.2685\n",
      "Epoch 2263/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.5394 - val_loss: 144.9483\n",
      "Epoch 2264/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 146.1486 - val_loss: 111.3871\n",
      "Epoch 2265/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.2785 - val_loss: 137.2931\n",
      "Epoch 2266/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 133.3389 - val_loss: 116.7584\n",
      "Epoch 2267/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.8386 - val_loss: 136.6534\n",
      "Epoch 2268/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.5170 - val_loss: 112.2706\n",
      "Epoch 2269/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.1939 - val_loss: 113.0184\n",
      "Epoch 2270/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.8637 - val_loss: 126.6609\n",
      "Epoch 2271/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.7215 - val_loss: 142.9663\n",
      "Epoch 2272/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.1902 - val_loss: 124.8193\n",
      "Epoch 2273/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 129.3627 - val_loss: 114.6823\n",
      "Epoch 2274/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.2428 - val_loss: 145.5355\n",
      "Epoch 2275/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.2474 - val_loss: 123.5395\n",
      "Epoch 2276/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 170.6950 - val_loss: 169.4497\n",
      "Epoch 2277/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.2190 - val_loss: 121.6505\n",
      "Epoch 2278/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.3972 - val_loss: 123.0204\n",
      "Epoch 2279/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 128.7117 - val_loss: 123.6868\n",
      "Epoch 2280/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.1837 - val_loss: 124.5146\n",
      "Epoch 2281/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.5825 - val_loss: 122.2440\n",
      "Epoch 2282/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.0711 - val_loss: 116.2076\n",
      "Epoch 2283/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.5329 - val_loss: 135.4661\n",
      "Epoch 2284/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 134.9047 - val_loss: 112.2125\n",
      "Epoch 2285/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 153.8892 - val_loss: 132.8181\n",
      "Epoch 2286/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.3656 - val_loss: 120.6289\n",
      "Epoch 2287/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.8304 - val_loss: 112.9704\n",
      "Epoch 2288/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 170.2591 - val_loss: 126.3718\n",
      "Epoch 2289/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 142.1580 - val_loss: 112.9672\n",
      "Epoch 2290/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.2530 - val_loss: 157.4824\n",
      "Epoch 2291/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 134.9991 - val_loss: 131.0533\n",
      "Epoch 2292/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.8049 - val_loss: 114.0869\n",
      "Epoch 2293/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.1832 - val_loss: 143.9757\n",
      "Epoch 2294/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.2356 - val_loss: 127.6264\n",
      "Epoch 2295/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 148.2061 - val_loss: 143.2037\n",
      "Epoch 2296/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.9408 - val_loss: 123.6313\n",
      "Epoch 2297/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 136.8722 - val_loss: 119.3215\n",
      "Epoch 2298/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.3090 - val_loss: 150.6426\n",
      "Epoch 2299/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.3074 - val_loss: 373.9147\n",
      "Epoch 2300/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.8002 - val_loss: 115.6837\n",
      "Epoch 2301/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.3441 - val_loss: 115.7022\n",
      "Epoch 2302/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.9392 - val_loss: 115.5490\n",
      "Epoch 2303/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.7461 - val_loss: 119.7002\n",
      "Epoch 2304/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 138.6105 - val_loss: 122.3566\n",
      "Epoch 2305/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.4505 - val_loss: 116.7747\n",
      "Epoch 2306/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 136.8260 - val_loss: 133.5941\n",
      "Epoch 2307/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.7540 - val_loss: 118.9075\n",
      "Epoch 2308/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.9429 - val_loss: 112.7462\n",
      "Epoch 2309/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.3487 - val_loss: 135.5640\n",
      "Epoch 2310/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 156.5429 - val_loss: 128.1827\n",
      "Epoch 2311/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.0097 - val_loss: 141.7620\n",
      "Epoch 2312/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 140.8569 - val_loss: 114.5665\n",
      "Epoch 2313/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 131.3518 - val_loss: 120.4638\n",
      "Epoch 2314/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 130.4602 - val_loss: 137.2547\n",
      "Epoch 2315/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.6236 - val_loss: 117.6409\n",
      "Epoch 2316/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 130.7056 - val_loss: 127.3425\n",
      "Epoch 2317/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 136.2166 - val_loss: 114.2017\n",
      "Epoch 2318/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.9025 - val_loss: 165.7070\n",
      "Epoch 2319/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.6997 - val_loss: 121.4293\n",
      "Epoch 2320/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 142.3471 - val_loss: 136.9287\n",
      "Epoch 2321/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.4815 - val_loss: 132.9343\n",
      "Epoch 2322/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.9623 - val_loss: 137.7066\n",
      "Epoch 2323/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.1606 - val_loss: 127.8317\n",
      "Epoch 2324/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.6224 - val_loss: 118.6845\n",
      "Epoch 2325/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.1046 - val_loss: 123.5412\n",
      "Epoch 2326/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.7722 - val_loss: 113.9948\n",
      "Epoch 2327/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.1363 - val_loss: 120.3480\n",
      "Epoch 2328/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 155.9565 - val_loss: 133.7985\n",
      "Epoch 2329/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 132.7473 - val_loss: 130.3754\n",
      "Epoch 2330/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 141.9233 - val_loss: 120.5660\n",
      "Epoch 2331/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.0601 - val_loss: 116.2511\n",
      "Epoch 2332/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.8528 - val_loss: 147.4288\n",
      "Epoch 2333/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 129.8429 - val_loss: 119.0658\n",
      "Epoch 2334/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.5048 - val_loss: 118.5946\n",
      "Epoch 2335/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 163.6421 - val_loss: 243.7796\n",
      "Epoch 2336/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.0653 - val_loss: 117.1700\n",
      "Epoch 2337/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 138.2348 - val_loss: 117.7964\n",
      "Epoch 2338/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 134.2914 - val_loss: 135.3829\n",
      "Epoch 2339/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 131.8588 - val_loss: 136.2833\n",
      "Epoch 2340/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 134.7518 - val_loss: 118.0355\n",
      "Epoch 2341/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 134.2628 - val_loss: 117.5776\n",
      "Epoch 2342/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 141.8487 - val_loss: 125.5658\n",
      "Epoch 2343/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.0009 - val_loss: 238.9279\n",
      "Epoch 2344/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 154.4160 - val_loss: 113.5871\n",
      "Epoch 2345/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.5697 - val_loss: 129.7161\n",
      "Epoch 2346/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.7756 - val_loss: 116.6318\n",
      "Epoch 2347/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 140.6391 - val_loss: 117.0880\n",
      "Epoch 2348/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 140.7914 - val_loss: 116.2893\n",
      "Epoch 2349/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.3552 - val_loss: 113.0137\n",
      "Epoch 2350/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 132.6435 - val_loss: 141.9229\n",
      "Epoch 2351/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 135.4873 - val_loss: 123.1592\n",
      "Epoch 2352/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.9753 - val_loss: 140.6178\n",
      "Epoch 2353/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 138.4078 - val_loss: 125.0018\n",
      "Epoch 2354/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 143.3281 - val_loss: 119.2820\n",
      "Epoch 2355/10000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 131.8559 - val_loss: 122.1815\n",
      "Epoch 2356/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.2132 - val_loss: 117.3101\n",
      "Epoch 2357/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 134.0812 - val_loss: 116.2718\n",
      "Epoch 2358/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 139.5999 - val_loss: 124.7384\n",
      "Epoch 2359/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.3919 - val_loss: 112.4663\n",
      "Epoch 2360/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.5794 - val_loss: 122.5625\n",
      "Epoch 2361/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 140.5063 - val_loss: 126.4745\n",
      "Epoch 2362/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 157.1430 - val_loss: 116.8876\n",
      "Epoch 2363/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 131.3885 - val_loss: 115.9617\n",
      "Epoch 2364/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 152.9758 - val_loss: 121.8417\n",
      "Epoch 2365/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.0305 - val_loss: 130.9033\n",
      "Epoch 2366/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 136.9160 - val_loss: 146.1503\n",
      "Epoch 2367/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 133.3691 - val_loss: 138.5354\n",
      "Epoch 2368/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.6168 - val_loss: 172.4943\n",
      "Epoch 2369/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 162.9591 - val_loss: 157.2683\n",
      "Epoch 2370/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 135.3875 - val_loss: 113.3292\n",
      "Epoch 2371/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.3000 - val_loss: 115.8786\n",
      "Epoch 2372/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 139.1805 - val_loss: 199.9848\n",
      "Epoch 2373/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 134.6475 - val_loss: 125.6003\n",
      "Epoch 2374/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 132.8949 - val_loss: 153.6072\n",
      "Epoch 2375/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 145.3028 - val_loss: 112.3568\n",
      "Epoch 2376/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 133.0616 - val_loss: 113.6000\n",
      "Epoch 2377/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 133.7513 - val_loss: 115.5567\n",
      "Epoch 2378/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 137.3613 - val_loss: 134.8466\n",
      "Epoch 2379/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.4987 - val_loss: 120.2030\n",
      "Epoch 2380/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 137.5416 - val_loss: 144.0116\n",
      "Epoch 2381/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 131.5113 - val_loss: 125.0390\n",
      "Epoch 2382/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 134.1838 - val_loss: 120.2376\n",
      "Epoch 2383/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.5608 - val_loss: 121.5422\n",
      "Epoch 2384/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 138.4522 - val_loss: 116.1215\n",
      "Epoch 2385/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 153.4248 - val_loss: 131.9671\n",
      "Epoch 02385: early stopping\n",
      "Fold score (RMSE): 11.199807167053223\n"
     ]
    }
   ],
   "source": [
    "# Cross-Validate\n",
    "kf = KFold(5)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=filename_checkpoint, verbose=0, save_best_only=True)\n",
    "\n",
    "# Turn off KFold\n",
    "#if (0):\n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "    \n",
    "for train, test in kf.split(x):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "    #model.add(Dropout(0.01)) # Dropout Layer\n",
    "    model.add(Dense(50, activation='relu')) # Hidden 2\n",
    "    model.add(Dense(25, \n",
    "                    kernel_regularizer=regularizers.l2(0.01), #L2 regularization\n",
    "                    activity_regularizer=regularizers.l1(0.01), #L1 Lasso regularization\n",
    "                    activation='relu')) # Hidden 3 \n",
    "    model.add(Dense(10, activation='relu')) # Hidden 4\n",
    "    model.add(Dense(1)) # Output\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=1000, verbose=1, mode='auto')\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpoint],verbose=1,epochs=10000)\n",
    "    \n",
    "    pred = model.predict(x_test)\n",
    "    \n",
    "    oos_y.append(y_test)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure this fold's RMSE\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "    print(\"Fold score (RMSE): {}\".format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))\n",
    "print(\"Final, out of sample score (RMSE): {}\".format(score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#pred = model.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "#score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "#print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "#chart_regression(pred.flatten(),y_test)\n",
    "#chart_regression(pred.flatten(),y_test,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(filename_checkpoint)\n",
    "\n",
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "extract_and_encode_features(df_test)\n",
    "\n",
    "ids_test = df_test['id']\n",
    "df_test.drop('id',1,inplace=True)\n",
    "\n",
    "names_test = df_test['name']\n",
    "df_test.drop('name',1,inplace=True)\n",
    "\n",
    "x_submit = df_test.as_matrix().astype(np.float32)\n",
    "pred_submit = model.predict(x_submit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = [n if n > 0 else n * -1 for n in pred_submit[:,0]]\n",
    "df_submit = pd.DataFrame({'id': ids_test,'cost': cost})\n",
    "df_submit = df_submit[['id', 'cost']]\n",
    "df_submit.to_csv(filename_submit, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
