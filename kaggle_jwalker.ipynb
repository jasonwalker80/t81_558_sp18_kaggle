{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n",
    "\n",
    "**Kaggle Assignment: **\n",
    "\n",
    "**Student Name: Jason Walker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## Description\n",
    "This is one of the projects from the course T81-855: Applications of Deep Learning at Washington University in St. Louis. All students must create a Kaggle account and submit a solution. Once you have submitted your solution entry log into Blackboard (at WUSTL) and submit a single file telling me your Kaggle name on the leaderboard (you do not need to register to Kaggle with your real name). This competition will be visible to the public, so there may be non-student submissions as well as student.\n",
    "\n",
    "The data set for this competition consists of a number of input columns that should be used to predict a stores sales. This is a regression problem. The inputs are a mixture of discrete and category values. The data set is from a simulation.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The evaluation pages describes how submissions will be scored and how students should format their submissions. The scores are in RMSE.\n",
    "Submission Format\n",
    "\n",
    "For every store in the dataset, submission files should contain a sales volume.\n",
    "\n",
    "The file should contain a header and have the following format:\n",
    "\n",
    "```\n",
    "100000,1.23\n",
    "100001,1.123\n",
    "100002,3.332\n",
    "100003,1.53\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The data contains data and costs for various office supplies. The data came from a simulation and do not directly correspond to any real-world items. See how well you can predict the cost of an item using the provided data. Feature engineering will likely help you. The *name* column may seem useless at first glance; however, it contains information that you can parse to help your predictions.\n",
    "File descriptions\n",
    "```\n",
    "    id - The identifier/primary key.\n",
    "    name - The name of this item.\n",
    "    manufacturer - The manufacturer.\n",
    "    pack - The number of items in this pack.\n",
    "    weight - The weight of a pack of these items.\n",
    "    height - The height of a pack of these items.\n",
    "    width - The width of a pack of these items.\n",
    "    length - The length of a pack of these items.\n",
    "    cost - The cost for this item pack. This is what you are to predict (the target). \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions\n",
    "\n",
    "You will see these at the top of every module and assignment.  These are simply a set of reusable functions that we will make use of.  Each of them will be explained as the semester progresses.  They are explained in greater detail as the course progresses.  Class 4 contains a complete overview of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n",
    "        \n",
    "# This function submits an assignment.  You can submit an assignment as much as you like, only the final\n",
    "# submission counts.  The paramaters are as follows:\n",
    "# data - Pandas dataframe output.\n",
    "# key - Your student key that was emailed to you.\n",
    "# no - The assignment class number, should be 1 through 1.\n",
    "# source_file - The full path to your Python or IPYNB file.  This must have \"_class1\" as part of its name.  \n",
    "# .             The number must match your assignment number.  For example \"_class2\" for class assignment #2.\n",
    "def submit(data,key,no,source_file=None):\n",
    "    if source_file is None and '__file__' not in globals(): raise Exception('Must specify a filename when a Jupyter notebook.')\n",
    "    if source_file is None: source_file = __file__\n",
    "    suffix = '_class{}'.format(no)\n",
    "    if suffix not in source_file: raise Exception('{} must be part of the filename.'.format(suffix))\n",
    "    with open(source_file, \"rb\") as image_file:\n",
    "        encoded_python = base64.b64encode(image_file.read()).decode('ascii')\n",
    "    ext = os.path.splitext(source_file)[-1].lower()\n",
    "    if ext not in ['.ipynb','.py']: raise Exception(\"Source file is {} must be .py or .ipynb\".format(ext))\n",
    "    r = requests.post(\"https://api.heatonresearch.com/assignment-submit\",\n",
    "        headers={'x-api-key':key}, json={'csv':base64.b64encode(data.to_csv(index=False).encode('ascii')).decode(\"ascii\"),\n",
    "        'assignment': no, 'ext':ext, 'py':encoded_python})\n",
    "    if r.status_code == 200:\n",
    "        print(\"Success: {}\".format(r.text))\n",
    "    else: print(\"Failure: {}\".format(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kaggle Code\n",
    "\n",
    "## Load Data and Encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers\n",
    "\n",
    "path = './data'\n",
    "\n",
    "filename_test = os.path.join(path,\"test.csv\")\n",
    "filename_train = os.path.join(path,\"train.csv\")\n",
    "filename_sample = os.path.join(path,\"sample.csv\")\n",
    "filename_submit = os.path.join(path,\"submit.csv\")\n",
    "filename_checkpoint = os.path.join(path,\"checkpoint.hdf5\")\n",
    "\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "\n",
    "np.random.seed(42) # Uncomment this line to get the same shuffle each time\n",
    "df_train = df_train.reindex(np.random.permutation(df_train.index))\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Encode Features\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def extract_and_encode_features(df):\n",
    "    color_regex='(?P<color>red|blue|green|yellow|orange|pink|black|brown|white)'\n",
    "    df['color'] = df.name.str.extract(color_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    quality_regex='(?P<quality>generic|high\\squality)'\n",
    "    df['quality'] = df.name.str.extract(quality_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    size_regex='(?P<size>tiny|small|medium|large)'\n",
    "    df['size'] = df.name.str.extract(size_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    item_regex='(?P<item>paperclips|paperweights|ink\\spens|pencils|stapler|tablets|thumbtacks|post\\sit\\snotes)'\n",
    "    df['item'] = df.name.str.extract(item_regex, flags=re.IGNORECASE, expand=False)\n",
    "    \n",
    "    for column in ['pack','weight','height','width','length']:\n",
    "        missing_median(df,column)\n",
    "    \n",
    "    df.insert(1,'surface_area',(df['height']*df['width']*df['length']).astype(np.float32))\n",
    "    \n",
    "    for column in ['height','width','length']:\n",
    "        df.drop(column,1,inplace=True)\n",
    "    \n",
    "    ## encode numeric features\n",
    "    for column in ['pack','weight','surface_area']:\n",
    "        encode_numeric_zscore(df,column)\n",
    "\n",
    "    # encode text/categorical features\n",
    "    for column in ['manufacturer','color','quality','size','item']:\n",
    "        encode_text_dummy(df,column)\n",
    "  \n",
    "extract_and_encode_features(df_train)\n",
    "\n",
    "ids_train = df_train['id']\n",
    "df_train.drop('id',1,inplace=True)\n",
    "\n",
    "names_train = df_train['name']\n",
    "df_train.drop('name',1,inplace=True)\n",
    "\n",
    "x,y = to_xy(df_train,'cost')\n",
    "\n",
    "# Used before KFold\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   surface_area      pack    weight    cost  manufacturer-6% Solution  \\\n",
      "0     -0.391150 -0.580980 -0.442755   76.74                         0   \n",
      "1      1.148394 -1.042627 -0.870281   65.21                         0   \n",
      "2     -0.781008  1.138882  0.355983  192.47                         1   \n",
      "3      0.476087 -0.490461 -0.138201  161.12                         0   \n",
      "4      2.011652 -1.057713 -0.892117   21.34                         0   \n",
      "5      0.476087 -1.057713 -0.894416   21.34                         1   \n",
      "6     -0.781008  1.681997  0.666284  224.41                         1   \n",
      "7     -0.781008  0.716460  0.114637  176.66                         1   \n",
      "8     -0.884440 -0.930987 -0.877177    8.48                         0   \n",
      "9     -0.781008 -0.864606 -0.788684   19.56                         0   \n",
      "\n",
      "   manufacturer-Deep Office Supplies  manufacturer-Duck Lake  \\\n",
      "0                                  1                       0   \n",
      "1                                  0                       0   \n",
      "2                                  0                       0   \n",
      "3                                  1                       0   \n",
      "4                                  0                       0   \n",
      "5                                  0                       0   \n",
      "6                                  0                       0   \n",
      "7                                  0                       0   \n",
      "8                                  0                       1   \n",
      "9                                  0                       0   \n",
      "\n",
      "   manufacturer-Offices-R-Us  manufacturer-WizBang  color-Black  \\\n",
      "0                          0                     0            0   \n",
      "1                          0                     1            1   \n",
      "2                          0                     0            0   \n",
      "3                          0                     0            0   \n",
      "4                          1                     0            0   \n",
      "5                          0                     0            1   \n",
      "6                          0                     0            0   \n",
      "7                          0                     0            0   \n",
      "8                          0                     0            0   \n",
      "9                          1                     0            0   \n",
      "\n",
      "        ...         size-Small  size-Tiny  item-Ink Pens  item-Paperclips  \\\n",
      "0       ...                  0          0              0                0   \n",
      "1       ...                  1          0              0                0   \n",
      "2       ...                  1          0              0                0   \n",
      "3       ...                  0          0              0                0   \n",
      "4       ...                  0          0              0                0   \n",
      "5       ...                  0          1              0                0   \n",
      "6       ...                  1          0              0                0   \n",
      "7       ...                  0          0              0                1   \n",
      "8       ...                  1          0              0                0   \n",
      "9       ...                  0          0              0                0   \n",
      "\n",
      "   item-Paperweights  item-Pencils  item-Post It Notes  item-Stapler  \\\n",
      "0                  0             0                   0             0   \n",
      "1                  0             0                   0             1   \n",
      "2                  0             0                   0             0   \n",
      "3                  0             0                   0             0   \n",
      "4                  0             0                   0             1   \n",
      "5                  0             0                   0             1   \n",
      "6                  0             0                   0             0   \n",
      "7                  0             0                   0             0   \n",
      "8                  0             0                   1             0   \n",
      "9                  0             0                   1             0   \n",
      "\n",
      "   item-Tablets  item-Thumbtacks  \n",
      "0             0                1  \n",
      "1             0                0  \n",
      "2             0                1  \n",
      "3             1                0  \n",
      "4             0                0  \n",
      "5             0                0  \n",
      "6             0                1  \n",
      "7             0                0  \n",
      "8             0                0  \n",
      "9             0                0  \n",
      "\n",
      "[10 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_train[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression (Coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 32.602088928222656\n",
      "['surface_area', 'pack', 'weight', 'manufacturer-6% Solution', 'manufacturer-Deep Office Supplies', 'manufacturer-Duck Lake', 'manufacturer-Offices-R-Us', 'manufacturer-WizBang', 'color-Black', 'color-Blue', 'color-Brown', 'color-Green', 'color-Pink', 'color-Red', 'color-White', 'quality-Generic', 'quality-High Quality', 'size-Large', 'size-Medium', 'size-Small', 'size-Tiny', 'item-Ink Pens', 'item-Paperclips', 'item-Paperweights', 'item-Pencils', 'item-Post It Notes', 'item-Stapler', 'item-Tablets', 'item-Thumbtacks']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item-Pencils</th>\n",
       "      <td>-118.101379</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Thumbtacks</th>\n",
       "      <td>-114.043709</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperweights</th>\n",
       "      <td>-101.276184</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Post It Notes</th>\n",
       "      <td>-97.399124</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperclips</th>\n",
       "      <td>-91.027069</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Ink Pens</th>\n",
       "      <td>-83.828979</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Stapler</th>\n",
       "      <td>-75.198425</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Red</th>\n",
       "      <td>-38.204388</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Green</th>\n",
       "      <td>-26.037094</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Large</th>\n",
       "      <td>-12.952026</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Blue</th>\n",
       "      <td>-11.875504</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Generic</th>\n",
       "      <td>-4.663696</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Medium</th>\n",
       "      <td>-2.992279</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Brown</th>\n",
       "      <td>-0.438875</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Small</th>\n",
       "      <td>6.113098</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surface_area</th>\n",
       "      <td>7.105158</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-High Quality</th>\n",
       "      <td>7.522461</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack</th>\n",
       "      <td>8.680387</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Black</th>\n",
       "      <td>13.945839</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Tiny</th>\n",
       "      <td>14.376572</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-White</th>\n",
       "      <td>26.836090</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Pink</th>\n",
       "      <td>38.613495</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Tablets</th>\n",
       "      <td>66.889893</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>94.858047</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Offices-R-Us</th>\n",
       "      <td>957.106812</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Deep Office Supplies</th>\n",
       "      <td>957.278320</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Duck Lake</th>\n",
       "      <td>958.228088</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-6% Solution</th>\n",
       "      <td>958.267578</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-WizBang</th>\n",
       "      <td>959.393433</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         coef  positive\n",
       "item-Pencils                      -118.101379     False\n",
       "item-Thumbtacks                   -114.043709     False\n",
       "item-Paperweights                 -101.276184     False\n",
       "item-Post It Notes                 -97.399124     False\n",
       "item-Paperclips                    -91.027069     False\n",
       "item-Ink Pens                      -83.828979     False\n",
       "item-Stapler                       -75.198425     False\n",
       "color-Red                          -38.204388     False\n",
       "color-Green                        -26.037094     False\n",
       "size-Large                         -12.952026     False\n",
       "color-Blue                         -11.875504     False\n",
       "quality-Generic                     -4.663696     False\n",
       "size-Medium                         -2.992279     False\n",
       "color-Brown                         -0.438875     False\n",
       "size-Small                           6.113098      True\n",
       "surface_area                         7.105158      True\n",
       "quality-High Quality                 7.522461      True\n",
       "pack                                 8.680387      True\n",
       "color-Black                         13.945839      True\n",
       "size-Tiny                           14.376572      True\n",
       "color-White                         26.836090      True\n",
       "color-Pink                          38.613495      True\n",
       "item-Tablets                        66.889893      True\n",
       "weight                              94.858047      True\n",
       "manufacturer-Offices-R-Us          957.106812      True\n",
       "manufacturer-Deep Office Supplies  957.278320      True\n",
       "manufacturer-Duck Lake             958.228088      True\n",
       "manufacturer-6% Solution           958.267578      True\n",
       "manufacturer-WizBang               959.393433      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [-754.47357178]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh0AAAD8CAYAAADT5xbpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzMzMWsKdDjMzM2sJdzrMzMysJf4fZlWfU9Fa9C0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a18ddcb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple function to evaluate the coefficients of a regression\n",
    "%matplotlib inline    \n",
    "from IPython.display import display, HTML    \n",
    "\n",
    "def report_coef(names,coef,intercept):\n",
    "    r = pd.DataFrame( { 'coef': coef, 'positive': coef>=0  }, index = names )\n",
    "    r = r.sort_values(by=['coef'])\n",
    "    display(r)\n",
    "    print(\"Intercept: {}\".format(intercept))\n",
    "    r['coef'].plot(kind='barh', color=r['positive'].map({True: 'b', False: 'r'}))\n",
    "    \n",
    "# Create linear regression\n",
    "regressor = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# Fit/train linear regression\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "print(names)\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_[0,:],\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization (Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 32.6033821105957\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>color-Red</th>\n",
       "      <td>-38.011356</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Pencils</th>\n",
       "      <td>-26.881954</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Green</th>\n",
       "      <td>-25.850136</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Thumbtacks</th>\n",
       "      <td>-22.845032</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Large</th>\n",
       "      <td>-12.851401</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Blue</th>\n",
       "      <td>-11.689812</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperweights</th>\n",
       "      <td>-9.976925</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Post It Notes</th>\n",
       "      <td>-6.170481</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Generic</th>\n",
       "      <td>-4.635770</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Medium</th>\n",
       "      <td>-2.924804</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Offices-R-Us</th>\n",
       "      <td>-1.074398</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Deep Office Supplies</th>\n",
       "      <td>-0.895361</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Brown</th>\n",
       "      <td>-0.249793</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Duck Lake</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-6% Solution</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperclips</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-WizBang</th>\n",
       "      <td>1.121335</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Small</th>\n",
       "      <td>6.067370</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surface_area</th>\n",
       "      <td>7.065131</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Ink Pens</th>\n",
       "      <td>7.255776</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-High Quality</th>\n",
       "      <td>7.510667</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack</th>\n",
       "      <td>8.728130</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Black</th>\n",
       "      <td>13.977563</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Tiny</th>\n",
       "      <td>14.321182</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Stapler</th>\n",
       "      <td>15.971769</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-White</th>\n",
       "      <td>26.883341</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Pink</th>\n",
       "      <td>38.648758</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>94.818710</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Tablets</th>\n",
       "      <td>158.003677</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         coef  positive\n",
       "color-Red                          -38.011356     False\n",
       "item-Pencils                       -26.881954     False\n",
       "color-Green                        -25.850136     False\n",
       "item-Thumbtacks                    -22.845032     False\n",
       "size-Large                         -12.851401     False\n",
       "color-Blue                         -11.689812     False\n",
       "item-Paperweights                   -9.976925     False\n",
       "item-Post It Notes                  -6.170481     False\n",
       "quality-Generic                     -4.635770     False\n",
       "size-Medium                         -2.924804     False\n",
       "manufacturer-Offices-R-Us           -1.074398     False\n",
       "manufacturer-Deep Office Supplies   -0.895361     False\n",
       "color-Brown                         -0.249793     False\n",
       "manufacturer-Duck Lake               0.000000      True\n",
       "manufacturer-6% Solution             0.000000      True\n",
       "item-Paperclips                      0.000000      True\n",
       "manufacturer-WizBang                 1.121335      True\n",
       "size-Small                           6.067370      True\n",
       "surface_area                         7.065131      True\n",
       "item-Ink Pens                        7.255776      True\n",
       "quality-High Quality                 7.510667      True\n",
       "pack                                 8.728130      True\n",
       "color-Black                         13.977563      True\n",
       "size-Tiny                           14.321182      True\n",
       "item-Stapler                        15.971769      True\n",
       "color-White                         26.883341      True\n",
       "color-Pink                          38.648758      True\n",
       "weight                              94.818710      True\n",
       "item-Tablets                       158.003677      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [ 112.45285034]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAD8CAYAAADExYYgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyYWZmZl1lCcXZmZm1lGeXJiZmVlH/T8uhfCn3yuvyAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a170e84e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Create linear regression\n",
    "regressor = Lasso(random_state=0,alpha=0.01)\n",
    "\n",
    "# Fit/train LASSO\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_,\n",
    "  regressor.intercept_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization (Plot LassoCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/jwalker/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1e-08, 100000000.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAF6CAYAAAAeZ/GvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a18c939b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lasso = Lasso(random_state=42)\n",
    "alphas = np.logspace(-8, 8, 10)\n",
    "\n",
    "scores = list()\n",
    "scores_std = list()\n",
    "\n",
    "n_folds = 3\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso.alpha = alpha\n",
    "    this_scores = cross_val_score(lasso, x, y, cv=n_folds, n_jobs=1)\n",
    "    scores.append(np.mean(this_scores))\n",
    "    scores_std.append(np.std(this_scores))\n",
    "\n",
    "scores, scores_std = np.array(scores), np.array(scores_std)\n",
    "\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "plt.semilogx(alphas, scores)\n",
    "\n",
    "# plot error lines showing +/- std. errors of the scores\n",
    "std_error = scores_std / np.sqrt(n_folds)\n",
    "\n",
    "plt.semilogx(alphas, scores + std_error, 'b--')\n",
    "plt.semilogx(alphas, scores - std_error, 'b--')\n",
    "\n",
    "# alpha=0.2 controls the translucency of the fill color\n",
    "plt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2)\n",
    "\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(scores), linestyle='--', color='.5')\n",
    "plt.xlim([alphas[0], alphas[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization (Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 32.60321044921875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item-Pencils</th>\n",
       "      <td>-41.270035</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Red</th>\n",
       "      <td>-38.166615</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Thumbtacks</th>\n",
       "      <td>-37.185127</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Green</th>\n",
       "      <td>-26.017801</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperweights</th>\n",
       "      <td>-24.549904</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Post It Notes</th>\n",
       "      <td>-20.518717</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperclips</th>\n",
       "      <td>-14.165739</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Large</th>\n",
       "      <td>-13.026008</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Blue</th>\n",
       "      <td>-11.871177</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Ink Pens</th>\n",
       "      <td>-7.076108</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Generic</th>\n",
       "      <td>-4.664358</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Medium</th>\n",
       "      <td>-3.014799</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Offices-R-Us</th>\n",
       "      <td>-0.951910</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Deep Office Supplies</th>\n",
       "      <td>-0.776552</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Brown</th>\n",
       "      <td>-0.438518</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Duck Lake</th>\n",
       "      <td>0.173204</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-6% Solution</th>\n",
       "      <td>0.210797</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-WizBang</th>\n",
       "      <td>1.338292</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Stapler</th>\n",
       "      <td>1.430109</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Small</th>\n",
       "      <td>6.124971</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surface_area</th>\n",
       "      <td>7.196461</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-High Quality</th>\n",
       "      <td>7.518651</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack</th>\n",
       "      <td>8.664386</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Black</th>\n",
       "      <td>13.926390</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Tiny</th>\n",
       "      <td>14.413168</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-White</th>\n",
       "      <td>26.806087</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Pink</th>\n",
       "      <td>38.568333</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>94.902184</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Tablets</th>\n",
       "      <td>143.429184</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         coef  positive\n",
       "item-Pencils                       -41.270035     False\n",
       "color-Red                          -38.166615     False\n",
       "item-Thumbtacks                    -37.185127     False\n",
       "color-Green                        -26.017801     False\n",
       "item-Paperweights                  -24.549904     False\n",
       "item-Post It Notes                 -20.518717     False\n",
       "item-Paperclips                    -14.165739     False\n",
       "size-Large                         -13.026008     False\n",
       "color-Blue                         -11.871177     False\n",
       "item-Ink Pens                       -7.076108     False\n",
       "quality-Generic                     -4.664358     False\n",
       "size-Medium                         -3.014799     False\n",
       "manufacturer-Offices-R-Us           -0.951910     False\n",
       "manufacturer-Deep Office Supplies   -0.776552     False\n",
       "color-Brown                         -0.438518     False\n",
       "manufacturer-Duck Lake               0.173204      True\n",
       "manufacturer-6% Solution             0.210797      True\n",
       "manufacturer-WizBang                 1.338292      True\n",
       "item-Stapler                         1.430109      True\n",
       "size-Small                           6.124971      True\n",
       "surface_area                         7.196461      True\n",
       "quality-High Quality                 7.518651      True\n",
       "pack                                 8.664386      True\n",
       "color-Black                         13.926390      True\n",
       "size-Tiny                           14.413168      True\n",
       "color-White                         26.806087      True\n",
       "color-Pink                          38.568333      True\n",
       "weight                              94.902184      True\n",
       "item-Tablets                       143.429184      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [ 126.84092712]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhkAAAD8CAYAAADaDLaTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a18d123c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Create linear regression\n",
    "regressor = Ridge(alpha=1)\n",
    "\n",
    "# Fit/train Ridge\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_[0,:],\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 32.942909240722656\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item-Pencils</th>\n",
       "      <td>-37.769466</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Red</th>\n",
       "      <td>-35.675865</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Thumbtacks</th>\n",
       "      <td>-33.047462</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Green</th>\n",
       "      <td>-24.636555</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperweights</th>\n",
       "      <td>-24.466547</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Large</th>\n",
       "      <td>-15.412159</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Post It Notes</th>\n",
       "      <td>-15.068984</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Blue</th>\n",
       "      <td>-11.372995</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperclips</th>\n",
       "      <td>-10.843545</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Ink Pens</th>\n",
       "      <td>-7.129179</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Generic</th>\n",
       "      <td>-4.688676</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Medium</th>\n",
       "      <td>-3.724350</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Stapler</th>\n",
       "      <td>-2.753412</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Offices-R-Us</th>\n",
       "      <td>-1.125373</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Deep Office Supplies</th>\n",
       "      <td>-0.700468</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Brown</th>\n",
       "      <td>-0.333706</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-6% Solution</th>\n",
       "      <td>0.157696</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Duck Lake</th>\n",
       "      <td>0.222704</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-WizBang</th>\n",
       "      <td>1.335581</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Small</th>\n",
       "      <td>6.350097</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-High Quality</th>\n",
       "      <td>7.270034</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack</th>\n",
       "      <td>9.273295</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surface_area</th>\n",
       "      <td>10.759686</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Black</th>\n",
       "      <td>12.855309</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Tiny</th>\n",
       "      <td>15.230996</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-White</th>\n",
       "      <td>25.104185</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Pink</th>\n",
       "      <td>35.910923</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>95.819649</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Tablets</th>\n",
       "      <td>131.751419</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         coef  positive\n",
       "item-Pencils                       -37.769466     False\n",
       "color-Red                          -35.675865     False\n",
       "item-Thumbtacks                    -33.047462     False\n",
       "color-Green                        -24.636555     False\n",
       "item-Paperweights                  -24.466547     False\n",
       "size-Large                         -15.412159     False\n",
       "item-Post It Notes                 -15.068984     False\n",
       "color-Blue                         -11.372995     False\n",
       "item-Paperclips                    -10.843545     False\n",
       "item-Ink Pens                       -7.129179     False\n",
       "quality-Generic                     -4.688676     False\n",
       "size-Medium                         -3.724350     False\n",
       "item-Stapler                        -2.753412     False\n",
       "manufacturer-Offices-R-Us           -1.125373     False\n",
       "manufacturer-Deep Office Supplies   -0.700468     False\n",
       "color-Brown                         -0.333706     False\n",
       "manufacturer-6% Solution             0.157696      True\n",
       "manufacturer-Duck Lake               0.222704      True\n",
       "manufacturer-WizBang                 1.335581      True\n",
       "size-Small                           6.350097      True\n",
       "quality-High Quality                 7.270034      True\n",
       "pack                                 9.273295      True\n",
       "surface_area                        10.759686      True\n",
       "color-Black                         12.855309      True\n",
       "size-Tiny                           15.230996      True\n",
       "color-White                         25.104185      True\n",
       "color-Pink                          35.910923      True\n",
       "weight                              95.819649      True\n",
       "item-Tablets                       131.751419      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [ 127.57927704]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAD8CAYAAADExYYgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuTAzM7O2cufCzMzM2ur/MASAXYZ57NoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a18e7b668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Create linear regression\n",
    "regressor = ElasticNet(alpha=0.01, l1_ratio=0.1)\n",
    "\n",
    "# Fit/train LASSO\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_,\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 12627.2007 - val_loss: 1079.7828\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 733.0681 - val_loss: 555.3366\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 440.4139 - val_loss: 389.7372\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 344.7600 - val_loss: 340.7027\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 316.0309 - val_loss: 331.2599\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 300.7299 - val_loss: 316.1182\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 293.1848 - val_loss: 303.3080\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 283.3550 - val_loss: 291.2142\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 280.1645 - val_loss: 296.4967\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 276.2918 - val_loss: 283.3096\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 269.9801 - val_loss: 286.4035\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 269.8594 - val_loss: 277.7233\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 268.2041 - val_loss: 274.1029\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 256.3715 - val_loss: 275.8049\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 255.1936 - val_loss: 270.7948\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 257.0674 - val_loss: 278.8307\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 255.9366 - val_loss: 261.3623\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 246.3272 - val_loss: 268.2641\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 249.1883 - val_loss: 302.2538\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 247.4444 - val_loss: 260.9452\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 241.6086 - val_loss: 285.1713\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 238.0734 - val_loss: 255.3434\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 240.7589 - val_loss: 260.1059\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 239.9172 - val_loss: 258.2690\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 230.4194 - val_loss: 255.8561\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 228.7436 - val_loss: 257.3197\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 230.4972 - val_loss: 249.9443\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 233.4780 - val_loss: 258.7701\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 228.1221 - val_loss: 267.9542\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 226.8773 - val_loss: 253.4042\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 223.1469 - val_loss: 249.5365\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 224.5318 - val_loss: 248.8351\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 219.2523 - val_loss: 249.8926\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 223.0783 - val_loss: 258.2565\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 217.2571 - val_loss: 246.0875\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 215.8448 - val_loss: 244.3873\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 211.8039 - val_loss: 252.0683\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 211.9170 - val_loss: 254.0272\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 210.3296 - val_loss: 244.8690\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 216.7364 - val_loss: 249.1859\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 210.2224 - val_loss: 253.2556\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 207.2924 - val_loss: 246.9474\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 207.3361 - val_loss: 263.7409\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 204.4171 - val_loss: 245.8799\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 204.5748 - val_loss: 244.4158\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 202.4144 - val_loss: 265.7074\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 203.4032 - val_loss: 242.4058\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 204.8335 - val_loss: 250.3383\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 195.1362 - val_loss: 248.6468\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 196.4146 - val_loss: 251.9194\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 198.4665 - val_loss: 240.4821\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 195.0802 - val_loss: 239.9818\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 193.3791 - val_loss: 243.8936\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 194.4113 - val_loss: 240.9583\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 192.6329 - val_loss: 245.9526\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 185.4400 - val_loss: 237.3585\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 195.3261 - val_loss: 326.0301\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 196.8122 - val_loss: 253.2162\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 188.2550 - val_loss: 237.3800\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 1s 144us/step - loss: 186.4344 - val_loss: 240.8736\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 186.4218 - val_loss: 238.9639\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 184.9138 - val_loss: 240.9212\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 185.1272 - val_loss: 239.1666\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 184.1721 - val_loss: 245.0739\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 1s 145us/step - loss: 189.1573 - val_loss: 237.9386\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 177.1763 - val_loss: 249.1961\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 184.7854 - val_loss: 233.7815\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 179.5122 - val_loss: 238.0746\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.8631 - val_loss: 245.1340\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 169.9299 - val_loss: 253.5751\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 176.3957 - val_loss: 259.0145\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 171.3381 - val_loss: 244.9234\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 173.0881 - val_loss: 245.3993\n",
      "Epoch 74/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 111us/step - loss: 168.9218 - val_loss: 237.3352\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 170.6176 - val_loss: 261.1483\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 173.0507 - val_loss: 240.0261\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 168.9815 - val_loss: 238.2473\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 160.7179 - val_loss: 230.4188\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 167.9766 - val_loss: 240.6996\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 1s 129us/step - loss: 163.8039 - val_loss: 234.3336\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 165.6244 - val_loss: 232.1388\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 161.0219 - val_loss: 244.6321\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 157.6101 - val_loss: 238.1138\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 152.3071 - val_loss: 226.0817\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 158.3938 - val_loss: 233.8594\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 158.3407 - val_loss: 224.8111\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 153.6753 - val_loss: 225.8886\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 160.1091 - val_loss: 229.5644\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 158.4075 - val_loss: 224.2937\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 151.6585 - val_loss: 242.3017\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 149.9689 - val_loss: 228.6521\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 151.8489 - val_loss: 237.6191\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 153.3053 - val_loss: 230.7591\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 148.2829 - val_loss: 212.9799\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 146.1186 - val_loss: 235.6831\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 146.3574 - val_loss: 218.9270\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 151.5476 - val_loss: 259.0868\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 144.4420 - val_loss: 236.2229\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 142.2954 - val_loss: 234.7960\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 144.6018 - val_loss: 240.6845\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 136.8433 - val_loss: 245.7016\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 146.6514 - val_loss: 227.7561\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 142.4527 - val_loss: 240.7115\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 146.0308 - val_loss: 234.3510\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 137.8029 - val_loss: 230.2348\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 138.5524 - val_loss: 243.6575\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 136.8060 - val_loss: 227.2194\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 140.8951 - val_loss: 238.2047\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 134.2031 - val_loss: 216.2854\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 134.9121 - val_loss: 202.2550\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 133.3374 - val_loss: 214.1143\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 137.8753 - val_loss: 224.8967\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.8246 - val_loss: 231.5496\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 134.1349 - val_loss: 225.4039\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 130.0663 - val_loss: 235.3062\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 135.1893 - val_loss: 212.7639\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 124.9076 - val_loss: 210.4749\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.4832 - val_loss: 208.0203\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 126.9804 - val_loss: 226.1501\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 122.6337 - val_loss: 232.2586\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 120.2459 - val_loss: 214.4953\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 128.8465 - val_loss: 226.9001\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 120.9047 - val_loss: 210.1867\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 122.9100 - val_loss: 219.2725\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 125.1965 - val_loss: 230.8719\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 121.2375 - val_loss: 226.5596\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 127.7000 - val_loss: 254.0802\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 124.6277 - val_loss: 200.6613\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 122.9358 - val_loss: 212.8179\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 116.8828 - val_loss: 203.7757\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 110.8055 - val_loss: 218.7091\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 112.4916 - val_loss: 211.6061\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 117.0881 - val_loss: 259.8955\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 118.0414 - val_loss: 216.9998\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 115.6338 - val_loss: 183.7971\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 113.1411 - val_loss: 198.2926\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 122.7388 - val_loss: 207.4110\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 112.1073 - val_loss: 192.1808\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 113.1718 - val_loss: 220.4042\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 113.7664 - val_loss: 234.2827\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 112.3466 - val_loss: 206.3960\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 108.3583 - val_loss: 197.8588\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 103.3831 - val_loss: 179.5547\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 111.8509 - val_loss: 201.2120\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 105.4751 - val_loss: 195.7707\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 108.1683 - val_loss: 191.3947\n",
      "Epoch 147/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 106.8520 - val_loss: 187.4934\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 103.7740 - val_loss: 202.3160\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 104.7920 - val_loss: 200.5417\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 103.3179 - val_loss: 179.9276\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 101.0710 - val_loss: 216.6587\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 104.4964 - val_loss: 208.3902\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 104.2245 - val_loss: 202.0151\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 97.8174 - val_loss: 255.6452\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 103.9478 - val_loss: 203.1953\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 97.5472 - val_loss: 186.8621\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 103.2007 - val_loss: 198.6124\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 99.5898 - val_loss: 192.7683\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 97.1961 - val_loss: 216.6014\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 100.8428 - val_loss: 185.6924\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 96.9714 - val_loss: 198.0877\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 97.3426 - val_loss: 184.6335\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 98.8312 - val_loss: 195.8026\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 95.3935 - val_loss: 174.3963\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 97.4083 - val_loss: 231.1031\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 91.7166 - val_loss: 185.3150\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 92.1246 - val_loss: 183.0175\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 94.7826 - val_loss: 194.5764\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 92.6306 - val_loss: 217.7762\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 93.2266 - val_loss: 191.0810\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 88.1602 - val_loss: 222.8852\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 86.9216 - val_loss: 175.2916\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 83.8844 - val_loss: 182.1279\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 93.1687 - val_loss: 191.4963\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 90.1805 - val_loss: 194.2195\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 95.7592 - val_loss: 187.8259\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 87.8639 - val_loss: 198.9507\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 89.8023 - val_loss: 228.5090\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 84.4227 - val_loss: 175.4141\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 86.8101 - val_loss: 204.7685\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 82.9580 - val_loss: 189.1387\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 85.6669 - val_loss: 179.1318\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 88.6075 - val_loss: 209.3136\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 85.6166 - val_loss: 203.1953\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 85.9980 - val_loss: 196.7185\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 84.2173 - val_loss: 203.6128\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 83.2730 - val_loss: 194.9633\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 90.2179 - val_loss: 184.7062\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 84.1775 - val_loss: 173.6081\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 82.1178 - val_loss: 189.3142\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 81.7806 - val_loss: 197.8057\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 84.9812 - val_loss: 195.4258\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 82.2976 - val_loss: 179.1123\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 82.1077 - val_loss: 199.4149\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 83.6262 - val_loss: 176.8326\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 79.8375 - val_loss: 185.9938\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 80.2074 - val_loss: 190.6628\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 79.9099 - val_loss: 184.8338\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 82.8210 - val_loss: 178.8728\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 77.2440 - val_loss: 186.9659\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 79.7179 - val_loss: 195.9497\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 78.3316 - val_loss: 183.4101\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 80.4591 - val_loss: 178.6113\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 78.6677 - val_loss: 189.4449\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 82.5639 - val_loss: 177.9456\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 75.2426 - val_loss: 179.1577\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 78.6571 - val_loss: 177.8796\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 78.3157 - val_loss: 218.6602\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 75.3961 - val_loss: 183.3572\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 75.4619 - val_loss: 198.0552\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 71.0668 - val_loss: 191.2146\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 76.8675 - val_loss: 194.3494\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 72.8969 - val_loss: 186.9044\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 71.8463 - val_loss: 187.1607\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 73.1525 - val_loss: 173.5110\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 74.4821 - val_loss: 178.6378\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 74.8617 - val_loss: 188.8944\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 73.0850 - val_loss: 207.5923\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 74.7914 - val_loss: 188.7971\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 73.3893 - val_loss: 194.7172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 74.2813 - val_loss: 198.3582\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 74.0317 - val_loss: 201.0242\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 72.4776 - val_loss: 199.3984\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 74.6160 - val_loss: 202.3717\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 74.3866 - val_loss: 186.3611\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 70.1977 - val_loss: 197.5994\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 67.9853 - val_loss: 181.8167\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 70.1706 - val_loss: 181.6167\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 69.5560 - val_loss: 187.6711\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 70.2546 - val_loss: 196.4838\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 70.6200 - val_loss: 194.9449\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 73.3281 - val_loss: 197.6568\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 74.9119 - val_loss: 184.7494\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 70.4636 - val_loss: 183.3657\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 71.1691 - val_loss: 178.0489\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 71.4381 - val_loss: 178.8983\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 68.9524 - val_loss: 191.9394\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 67.7068 - val_loss: 184.5403\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 66.7132 - val_loss: 184.5720\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 67.4278 - val_loss: 191.0145\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 68.8154 - val_loss: 187.8565\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 71.4669 - val_loss: 202.4565\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 68.3213 - val_loss: 194.3335\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 64.4329 - val_loss: 197.2829\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 67.0659 - val_loss: 195.6315\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 67.9694 - val_loss: 192.7295\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 65.9826 - val_loss: 181.9094\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 69.2796 - val_loss: 182.2470\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 64.8197 - val_loss: 177.6040\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 66.9725 - val_loss: 186.3427\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 71.1916 - val_loss: 198.4858\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 69.3815 - val_loss: 183.5025\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 66.9337 - val_loss: 191.9817\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 63.5076 - val_loss: 198.4436\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 66.1197 - val_loss: 200.9561\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 63.6286 - val_loss: 196.2452\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 64.7489 - val_loss: 198.9670\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 67.5330 - val_loss: 176.8955\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 64.7968 - val_loss: 200.5039\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 63.6504 - val_loss: 182.8758\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 70.5387 - val_loss: 179.0604\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 64.3287 - val_loss: 214.5613\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 61.9890 - val_loss: 195.8650\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 62.0281 - val_loss: 186.4209\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 60.1060 - val_loss: 179.6808\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 60.0448 - val_loss: 183.3321\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 60.3179 - val_loss: 202.1258\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 60.5989 - val_loss: 187.3627\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 65.9688 - val_loss: 174.0954\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 64.6248 - val_loss: 185.2592\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 63.7129 - val_loss: 191.4856\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 58.7759 - val_loss: 208.4456\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 60.7746 - val_loss: 205.4223\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 64.2302 - val_loss: 192.2155\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 62.0347 - val_loss: 212.3635\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 61.3927 - val_loss: 195.5404\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 66.2601 - val_loss: 197.1677\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 68.9846 - val_loss: 228.5844\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 58.6564 - val_loss: 196.4592\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 58.4740 - val_loss: 185.7357\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 60.4146 - val_loss: 172.8678\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 61.5229 - val_loss: 207.5484\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 60.7468 - val_loss: 181.3871\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 56.8562 - val_loss: 188.6582\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 59.3886 - val_loss: 185.6788\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 62.7030 - val_loss: 178.2122\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 56.8633 - val_loss: 195.3431\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 56.2483 - val_loss: 172.2302\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 60.3360 - val_loss: 190.6717\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 62.0024 - val_loss: 183.0211\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 59.1796 - val_loss: 192.5816\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 57.3414 - val_loss: 202.8639\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 59.0566 - val_loss: 176.2451\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 59.6990 - val_loss: 213.4350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 59.1973 - val_loss: 188.5092\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 57.2581 - val_loss: 185.1170\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 61.3577 - val_loss: 207.0631\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 59.9653 - val_loss: 190.6641\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 57.1786 - val_loss: 180.0150\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 55.6184 - val_loss: 184.9666\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 53.7501 - val_loss: 201.0857\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 56.8071 - val_loss: 211.8916\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 58.2411 - val_loss: 185.2928\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 56.2253 - val_loss: 208.5606\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 57.1820 - val_loss: 190.6227\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 55.0544 - val_loss: 186.4507\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 56.5436 - val_loss: 184.9685\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 56.2628 - val_loss: 194.9542\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 56.2127 - val_loss: 187.7385\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 57.6457 - val_loss: 187.6329\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 54.7935 - val_loss: 203.7243\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 59.1141 - val_loss: 191.0889\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 55.5714 - val_loss: 178.2306\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 53.6888 - val_loss: 198.7082\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 54.1533 - val_loss: 188.0072\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 62.3228 - val_loss: 179.3879\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 52.7060 - val_loss: 195.7212\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 60.8452 - val_loss: 177.3771\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 58.1465 - val_loss: 186.9155\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 52.0949 - val_loss: 196.1437\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 56.1296 - val_loss: 182.6130\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 57.6918 - val_loss: 193.3297\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 56.6136 - val_loss: 186.6761\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 53.6594 - val_loss: 210.0182\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 59.6403 - val_loss: 183.0863\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 58.7343 - val_loss: 192.7541\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 50.7080 - val_loss: 183.5835\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 56.8829 - val_loss: 199.4692\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 54.6374 - val_loss: 192.5094\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 50.9547 - val_loss: 192.2005\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 56.2188 - val_loss: 195.9748\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 51.5604 - val_loss: 192.5419\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 54.2890 - val_loss: 190.6620\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 51.1700 - val_loss: 195.2888\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 54.8253 - val_loss: 194.0091\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 50.2651 - val_loss: 198.6726\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 50.8202 - val_loss: 185.0672\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 50.9366 - val_loss: 178.7594\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 53.4618 - val_loss: 191.1934\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 54.9929 - val_loss: 175.0993\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 51.8505 - val_loss: 199.9672\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 53.5841 - val_loss: 195.9079\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 51.4890 - val_loss: 188.7786\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 56.7559 - val_loss: 174.9574\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 49.9918 - val_loss: 187.4125\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 52.2154 - val_loss: 177.9469\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 52.4159 - val_loss: 179.7013\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 53.6207 - val_loss: 192.6701\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 52.1612 - val_loss: 180.5856\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 49.5885 - val_loss: 185.0667\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 51.0074 - val_loss: 177.7805\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 54.2136 - val_loss: 187.4730\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 52.3308 - val_loss: 187.3569\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 50.3538 - val_loss: 202.9307\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 47.9721 - val_loss: 186.2546\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 55.4359 - val_loss: 202.6283\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 55.1337 - val_loss: 195.9494\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 50.1768 - val_loss: 179.8537\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 47.8851 - val_loss: 187.1398\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 49.9095 - val_loss: 170.5527\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 49.9223 - val_loss: 183.5833\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 45.9947 - val_loss: 197.2507\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 51.7032 - val_loss: 183.5220\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 57.5885 - val_loss: 187.0412\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 50.6405 - val_loss: 194.6280\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 49.3708 - val_loss: 180.2522\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 50.7458 - val_loss: 178.0779\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 50.7861 - val_loss: 180.0931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 48.8860 - val_loss: 196.8473\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 58.4882 - val_loss: 181.0124\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 50.0434 - val_loss: 175.5257\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 52.4537 - val_loss: 180.8307\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 49.9693 - val_loss: 174.4912\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 49.5423 - val_loss: 194.2439\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 48.0779 - val_loss: 174.8052\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 47.0760 - val_loss: 186.6531\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 50.8070 - val_loss: 206.2346\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 52.0643 - val_loss: 195.5326\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 48.3408 - val_loss: 187.6158\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 49.3844 - val_loss: 179.7355\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 47.7442 - val_loss: 179.1007\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 51.0965 - val_loss: 182.4825\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 50.1280 - val_loss: 183.0356\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 48.0839 - val_loss: 177.4728\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 46.9799 - val_loss: 232.3144\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 46.0072 - val_loss: 180.3983\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 48.5328 - val_loss: 205.1547\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 49.7237 - val_loss: 173.8795\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 51.7121 - val_loss: 179.0171\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 48.7894 - val_loss: 208.8363\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 49.7051 - val_loss: 192.6068\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 51.4663 - val_loss: 198.8124\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 48.2341 - val_loss: 183.8977\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 46.3508 - val_loss: 180.6198\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 47.7648 - val_loss: 183.1992\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 48.8114 - val_loss: 185.0632\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 51.4291 - val_loss: 201.3997\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 44.8111 - val_loss: 183.5869\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 52.9647 - val_loss: 185.0005\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 46.8219 - val_loss: 176.6090\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 52.7536 - val_loss: 190.6374\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 47.6796 - val_loss: 172.0719\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 46.2448 - val_loss: 188.7358\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 46.2665 - val_loss: 178.9951\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 47.1672 - val_loss: 188.6385\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 51.5897 - val_loss: 171.4698\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 46.3845 - val_loss: 204.8100\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 44.9898 - val_loss: 178.1862\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 48.0293 - val_loss: 191.9650\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 49.5357 - val_loss: 183.7286\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 48.1807 - val_loss: 183.1559\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 45.0370 - val_loss: 190.9982\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 1s 127us/step - loss: 51.3943 - val_loss: 182.2868\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 46.9126 - val_loss: 190.0853\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 1s 140us/step - loss: 53.5112 - val_loss: 188.6142\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 1s 147us/step - loss: 46.1900 - val_loss: 189.5747\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 45.3442 - val_loss: 171.9573\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 45.6259 - val_loss: 183.3287\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 44.6664 - val_loss: 172.4443\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 47.0803 - val_loss: 181.5292\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 48.0481 - val_loss: 185.4309\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 48.4896 - val_loss: 177.8335\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 46.5298 - val_loss: 189.5592\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 44.4399 - val_loss: 202.6177\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 48.8028 - val_loss: 190.1731\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 49.8118 - val_loss: 184.0821\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 46.0870 - val_loss: 189.4222\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 44.6884 - val_loss: 191.8778\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 43.6550 - val_loss: 184.2548\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 45.9699 - val_loss: 202.0049\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 46.6126 - val_loss: 169.7923\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 44.2534 - val_loss: 167.3988\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 45.3811 - val_loss: 175.4957\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 44.7380 - val_loss: 175.2165\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 46.5939 - val_loss: 196.8027\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 43.3594 - val_loss: 181.5330\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 45.0337 - val_loss: 178.7770\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 43.6141 - val_loss: 189.1901\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 47.5928 - val_loss: 177.8656\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 43.1903 - val_loss: 179.1377\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 50.1356 - val_loss: 174.3185\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 43.7968 - val_loss: 175.4332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 47.8289 - val_loss: 200.6794\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 45.8431 - val_loss: 173.3500\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 44.2541 - val_loss: 167.1894\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 41.0314 - val_loss: 175.4190\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 45.3691 - val_loss: 189.8141\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 52.3873 - val_loss: 193.1510\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 43.9409 - val_loss: 177.4492\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 40.6407 - val_loss: 180.1035\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 42.6382 - val_loss: 175.0186\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 45.9376 - val_loss: 177.7670\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 41.6265 - val_loss: 180.8943\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 42.6525 - val_loss: 178.7443\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 45.3668 - val_loss: 175.3196\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 44.6930 - val_loss: 211.6446\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 47.8111 - val_loss: 178.3775\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 40.6196 - val_loss: 189.5211\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 43.7117 - val_loss: 181.7622\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 44.2485 - val_loss: 184.9444\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 48.1279 - val_loss: 197.6910\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 44.6844 - val_loss: 185.1895\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 42.3748 - val_loss: 171.6401\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 44.8445 - val_loss: 183.0117\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 44.2675 - val_loss: 188.8590\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 40.6062 - val_loss: 187.4338\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 41.6431 - val_loss: 187.7445\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 42.6513 - val_loss: 198.9116\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 40.9077 - val_loss: 187.9485\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 43.7607 - val_loss: 188.1416\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 41.7996 - val_loss: 173.5513\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 41.1264 - val_loss: 183.8505\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 40.9571 - val_loss: 179.6486\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 43.7872 - val_loss: 208.5441\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 47.0717 - val_loss: 189.7316\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 43.8463 - val_loss: 173.7220\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 42.2883 - val_loss: 184.1881\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 41.4714 - val_loss: 188.9320\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 43.4307 - val_loss: 188.9324\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 46.2472 - val_loss: 179.4354\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 41.4717 - val_loss: 177.3120\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 41.3875 - val_loss: 180.2479\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 41.4153 - val_loss: 195.8051\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 42.6012 - val_loss: 191.1568\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 45.1689 - val_loss: 178.5581\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 41.6388 - val_loss: 189.5276\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 40.9987 - val_loss: 176.6306\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 42.1519 - val_loss: 172.8967\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 42.1464 - val_loss: 185.5387\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 46.5136 - val_loss: 189.6650\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 42.0397 - val_loss: 183.9271\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 39.9982 - val_loss: 194.9590\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 42.4443 - val_loss: 182.8279\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 41.6374 - val_loss: 178.2652\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 38.9084 - val_loss: 177.4776\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 42.6417 - val_loss: 175.4650\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 39.8534 - val_loss: 186.2300\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 39.6099 - val_loss: 197.3685\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 41.5496 - val_loss: 182.1743\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 42.1267 - val_loss: 186.1810\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 42.8467 - val_loss: 206.8478\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 45.8855 - val_loss: 184.0681\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 40.7463 - val_loss: 179.0628\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 38.8239 - val_loss: 183.0646\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 42.6477 - val_loss: 175.0940\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 41.3840 - val_loss: 195.3518\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 37.6759 - val_loss: 183.7648\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 44.7933 - val_loss: 193.5513\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 41.5644 - val_loss: 185.6781\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 41.3915 - val_loss: 185.8475\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 38.8234 - val_loss: 187.9558\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 41.9390 - val_loss: 177.4011\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 43.5768 - val_loss: 172.7360\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 41.9304 - val_loss: 187.5413\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 38.4172 - val_loss: 182.3247\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 41.3263 - val_loss: 191.4400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 41.3715 - val_loss: 180.7538\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 38.3833 - val_loss: 176.0811\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 36.8836 - val_loss: 181.3620\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 49.3199 - val_loss: 176.4944\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 39.2989 - val_loss: 186.0292\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 40.1389 - val_loss: 178.0488\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.2865 - val_loss: 183.3328\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 42.6366 - val_loss: 186.9338\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 39.9761 - val_loss: 189.8753\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 38.4492 - val_loss: 189.7723\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 40.0663 - val_loss: 165.3314\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 41.4554 - val_loss: 175.5311\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 40.7711 - val_loss: 184.7823\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 40.7266 - val_loss: 187.5248\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 43.3861 - val_loss: 181.3706\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.6135 - val_loss: 185.2562\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 43.3177 - val_loss: 186.9057\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 38.3041 - val_loss: 193.5751\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 39.7191 - val_loss: 177.5082\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 39.5025 - val_loss: 186.1600\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 42.4552 - val_loss: 179.6053\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 38.8255 - val_loss: 181.8691\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 43.2892 - val_loss: 176.0499\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 40.8988 - val_loss: 179.3962\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 37.6990 - val_loss: 183.2026\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 37.4461 - val_loss: 179.1164\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 37.6330 - val_loss: 201.0148\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 39.7331 - val_loss: 172.2171\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 38.2480 - val_loss: 178.5346\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.7171 - val_loss: 181.6809\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 41.0606 - val_loss: 176.6989\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 41.0739 - val_loss: 184.5806\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 39.0656 - val_loss: 184.7577\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.1169 - val_loss: 185.9040\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 43.5245 - val_loss: 183.1388\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 37.1008 - val_loss: 178.9331\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 38.1863 - val_loss: 176.5735\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 37.1844 - val_loss: 174.2596\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 38.5153 - val_loss: 178.9559\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 38.5878 - val_loss: 181.6402\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 37.3889 - val_loss: 187.9290\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 40.5185 - val_loss: 178.2044\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 42.6797 - val_loss: 189.7502\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 39.4763 - val_loss: 181.5815\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 38.2136 - val_loss: 179.1886\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 39.7063 - val_loss: 179.9101\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 39.1368 - val_loss: 170.6189\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 37.9113 - val_loss: 171.1724\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 38.6507 - val_loss: 181.9795\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 40.2510 - val_loss: 172.4516\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 37.1794 - val_loss: 174.9698\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 36.3406 - val_loss: 176.9235\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 38.0210 - val_loss: 179.5009\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 39.9567 - val_loss: 181.7094\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 38.0710 - val_loss: 179.5791\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 35.9620 - val_loss: 172.4541\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 42.1925 - val_loss: 186.0209\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 35.9843 - val_loss: 189.6776\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 39.2094 - val_loss: 177.7813\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 40.4479 - val_loss: 187.8872\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 37.4534 - val_loss: 168.8003\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 37.5320 - val_loss: 190.4673\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 39.2898 - val_loss: 181.8687\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 35.6013 - val_loss: 170.0430\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 36.7443 - val_loss: 172.3917\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 37.3642 - val_loss: 175.7316\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 36.6198 - val_loss: 185.8966\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 36.9155 - val_loss: 179.5179\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 40.5014 - val_loss: 183.7276\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 35.8407 - val_loss: 178.1520\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 36.0055 - val_loss: 177.1098\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 35.4466 - val_loss: 174.2032\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 40.92 - 1s 64us/step - loss: 41.3301 - val_loss: 186.7759\n",
      "Epoch 590/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 81us/step - loss: 36.7529 - val_loss: 177.2741\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 37.6566 - val_loss: 174.8288\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 40.4923 - val_loss: 173.7862\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 38.0743 - val_loss: 169.3288\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 36.3452 - val_loss: 182.4244\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 35.6313 - val_loss: 177.4043\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 34.7717 - val_loss: 176.5626\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 36.9023 - val_loss: 185.4315\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 37.6952 - val_loss: 177.4087\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 39.4827 - val_loss: 169.9658\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 40.1995 - val_loss: 201.3387\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 39.9489 - val_loss: 176.9497\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 35.4778 - val_loss: 181.1668\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 38.1599 - val_loss: 179.9153\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 37.9017 - val_loss: 178.6992\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 37.5069 - val_loss: 169.0749\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 33.2822 - val_loss: 177.5049\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 37.8194 - val_loss: 181.1245\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 36.3139 - val_loss: 179.3701\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 38.4461 - val_loss: 178.3138\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 38.6810 - val_loss: 189.2656\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 35.3359 - val_loss: 183.0458\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 37.7823 - val_loss: 190.4540\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 41.6944 - val_loss: 186.2663\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 37.9631 - val_loss: 175.2308\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 36.8406 - val_loss: 191.2626\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 38.5192 - val_loss: 189.1357\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 33.3847 - val_loss: 189.7669\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 35.4225 - val_loss: 174.7732\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 35.6425 - val_loss: 185.8379\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 38.4697 - val_loss: 174.2911\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 37.7059 - val_loss: 173.5479\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 34.9230 - val_loss: 171.8347\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 35.2865 - val_loss: 173.7970\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 36.3780 - val_loss: 187.8874\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 39.5429 - val_loss: 192.9740\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 39.2035 - val_loss: 217.3790\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 35.8040 - val_loss: 188.8156\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 33.1926 - val_loss: 181.0551\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 35.8443 - val_loss: 171.6259\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 35.7324 - val_loss: 183.4923\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 35.2362 - val_loss: 197.0652\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 36.2394 - val_loss: 171.6225\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 35.6464 - val_loss: 175.2886\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 37.7550 - val_loss: 178.3024\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 37.2659 - val_loss: 182.6695\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 37.1076 - val_loss: 184.1823\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 34.8722 - val_loss: 186.4766\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 36.8510 - val_loss: 175.4224\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 34.8391 - val_loss: 186.9916\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 34.4214 - val_loss: 183.1029\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 39.0732 - val_loss: 170.4225\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 35.0142 - val_loss: 167.3939\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 35.8228 - val_loss: 165.8566\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 36.3329 - val_loss: 182.8818\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 36.7444 - val_loss: 161.4068\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 37.2578 - val_loss: 176.2312\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 34.8413 - val_loss: 187.4343\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 35.3231 - val_loss: 177.2764\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 34.9437 - val_loss: 178.6945\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 35.9281 - val_loss: 180.0692\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 35.4402 - val_loss: 169.2307\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 35.1272 - val_loss: 190.3101\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 37.2270 - val_loss: 168.8531\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 35.8523 - val_loss: 176.6897\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 32.7376 - val_loss: 167.8843\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 34.3772 - val_loss: 185.9475\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 32.8604 - val_loss: 185.4226\n",
      "Epoch 658/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 36.7348 - val_loss: 177.6291\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 34.4565 - val_loss: 182.7829\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 38.1900 - val_loss: 175.5264\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 33.6995 - val_loss: 164.5390\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 33.3818 - val_loss: 178.8716\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 34.2151 - val_loss: 181.9639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 33.8288 - val_loss: 185.9444\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 39.1686 - val_loss: 178.6730\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 33.6657 - val_loss: 176.2262\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 36.0276 - val_loss: 174.0564\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 35.1812 - val_loss: 178.0019\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 32.8655 - val_loss: 180.2300\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 32.1214 - val_loss: 168.2715\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 34.1427 - val_loss: 178.6555\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 32.5019 - val_loss: 175.1049\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 33.6329 - val_loss: 178.0095\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 1s 140us/step - loss: 38.2184 - val_loss: 164.1501\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 1s 147us/step - loss: 33.0211 - val_loss: 186.1089\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 1s 126us/step - loss: 34.8820 - val_loss: 182.3692\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 36.9968 - val_loss: 181.4322\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 34.9096 - val_loss: 177.0162\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 34.8918 - val_loss: 172.7735\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 32.8912 - val_loss: 161.5802\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 34.4501 - val_loss: 170.7968\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 32.1969 - val_loss: 162.2976\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 34.9494 - val_loss: 160.8922\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 31.8947 - val_loss: 168.9027\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 35.1862 - val_loss: 178.3835\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 32.7185 - val_loss: 179.1809\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 31.5474 - val_loss: 179.6440\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 32.1913 - val_loss: 170.0985\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 29.2941 - val_loss: 193.0322\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 34.5922 - val_loss: 180.7812\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 36.4152 - val_loss: 181.6970\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 33.0485 - val_loss: 167.8728\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 33.7135 - val_loss: 172.9162\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 32.4923 - val_loss: 182.0789\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 35.7731 - val_loss: 168.4936\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 36.4181 - val_loss: 185.5969\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 29.5607 - val_loss: 165.7361\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 34.1397 - val_loss: 182.7171\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 33.0168 - val_loss: 172.2935\n",
      "Epoch 700/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 32.5509 - val_loss: 181.8156\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 33.1890 - val_loss: 184.5368\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 32.0835 - val_loss: 170.6027\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 33.1500 - val_loss: 169.0508\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 33.8709 - val_loss: 195.1026\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 34.1642 - val_loss: 191.3569\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 32.8571 - val_loss: 167.4877\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 34.3218 - val_loss: 174.5058\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 36.5912 - val_loss: 162.6856\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 31.4546 - val_loss: 171.9410\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 30.7093 - val_loss: 178.2048\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 36.6099 - val_loss: 182.1374\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 30.3606 - val_loss: 184.2747\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 30.0963 - val_loss: 179.5601\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 32.4525 - val_loss: 177.4926\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 35.3603 - val_loss: 188.4640\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 32.7529 - val_loss: 164.8299\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 31.3279 - val_loss: 179.6116\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 32.3294 - val_loss: 170.5555\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 31.8292 - val_loss: 159.0440\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 30.9975 - val_loss: 176.9492\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 32.0679 - val_loss: 186.6050\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 32.1457 - val_loss: 171.3108\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 33.8885 - val_loss: 183.4167\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 34.9957 - val_loss: 187.2359\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 36.4834 - val_loss: 179.7878\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 31.6120 - val_loss: 183.6099\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 35.3800 - val_loss: 188.5390\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.7046 - val_loss: 173.0312\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 30.9326 - val_loss: 165.9989\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 32.2669 - val_loss: 173.8045\n",
      "Epoch 731/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 31.8431 - val_loss: 182.2722\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 31.6386 - val_loss: 180.0476\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.3125 - val_loss: 166.6341\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.2095 - val_loss: 188.0947\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 33.6224 - val_loss: 181.1446\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 32.9033 - val_loss: 174.8843\n",
      "Epoch 737/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 32.9083 - val_loss: 172.9202\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.3246 - val_loss: 179.2284\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 31.9551 - val_loss: 174.7016\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 33.6621 - val_loss: 175.9029\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.0429 - val_loss: 162.7271\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 33.2709 - val_loss: 181.2246\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.8020 - val_loss: 177.5405\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.2033 - val_loss: 173.5247\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 32.9498 - val_loss: 181.3262\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.1175 - val_loss: 166.9864\n",
      "Epoch 747/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 33.1337 - val_loss: 184.5548\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.3263 - val_loss: 171.8347\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 32.5160 - val_loss: 169.6985\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 32.0277 - val_loss: 162.9803\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.2866 - val_loss: 174.3194\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.2021 - val_loss: 168.3427\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 36.5305 - val_loss: 170.5903\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 29.6604 - val_loss: 164.2117\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.1291 - val_loss: 170.1891\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.0168 - val_loss: 178.9398\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 30.3676 - val_loss: 171.9526\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 38.4906 - val_loss: 169.3106\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.0171 - val_loss: 168.6085\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.6545 - val_loss: 169.3106\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.5456 - val_loss: 182.6821\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 30.3103 - val_loss: 186.3375\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.3584 - val_loss: 167.7826\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.3025 - val_loss: 174.4403\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 29.0226 - val_loss: 169.5928\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 35.0050 - val_loss: 181.9564\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 31.8366 - val_loss: 181.6489\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 32.2742 - val_loss: 174.4991\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.3829 - val_loss: 171.2238\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 30.9453 - val_loss: 177.7524\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 30.4982 - val_loss: 179.2069\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.4412 - val_loss: 174.4877\n",
      "Epoch 773/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 36.7727 - val_loss: 170.7977\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.7152 - val_loss: 181.3506\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 31.4463 - val_loss: 185.0763\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.1221 - val_loss: 172.8902\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 31.3291 - val_loss: 173.1575\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.8398 - val_loss: 183.7967\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.8142 - val_loss: 170.2965\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 28.4561 - val_loss: 172.9249\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.0521 - val_loss: 166.4260\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.0278 - val_loss: 187.3826\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 31.0638 - val_loss: 172.8232\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 31.0878 - val_loss: 172.8285\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 29.7043 - val_loss: 173.2675\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 29.2857 - val_loss: 180.1635\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 32.5052 - val_loss: 170.1066\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 30.4482 - val_loss: 171.4987\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 29.2838 - val_loss: 178.6751\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 29.4619 - val_loss: 184.2326\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 30.5429 - val_loss: 180.7940\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 32.5464 - val_loss: 177.5408\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 29.6679 - val_loss: 173.0198\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 29.5211 - val_loss: 180.0802\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 30.8892 - val_loss: 173.3832\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.5548 - val_loss: 179.3336\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.4123 - val_loss: 183.9342\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 33.3363 - val_loss: 182.8897\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 33.7273 - val_loss: 169.7450\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 30.1726 - val_loss: 180.9196\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 30.4990 - val_loss: 179.4190\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 30.8796 - val_loss: 173.1496\n",
      "Epoch 803/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 30.4155 - val_loss: 170.5547\n",
      "Epoch 804/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 31.1911 - val_loss: 187.2549\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 29.8693 - val_loss: 171.1046\n",
      "Epoch 806/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 29.3494 - val_loss: 185.1432\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.5343 - val_loss: 175.4615\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 29.5840 - val_loss: 189.2415\n",
      "Epoch 809/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 32.8015 - val_loss: 201.7179\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.8035 - val_loss: 179.2153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 30.2332 - val_loss: 181.7656\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 32.1119 - val_loss: 195.8243\n",
      "Epoch 813/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.9188 - val_loss: 176.0051\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.5236 - val_loss: 178.5186\n",
      "Epoch 815/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 28.5999 - val_loss: 174.8517\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 31.6482 - val_loss: 193.7436\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.4442 - val_loss: 186.2563\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 31.3913 - val_loss: 182.9997\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 29.6233 - val_loss: 183.4332\n",
      "Epoch 820/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 30.3039 - val_loss: 176.8372\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 30.4630 - val_loss: 189.4332\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 31.7566 - val_loss: 182.2972\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.6184 - val_loss: 182.5032\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 28.8795 - val_loss: 177.1583\n",
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 30.1663 - val_loss: 168.3094\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.6058 - val_loss: 172.2733\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 27.5497 - val_loss: 184.6433\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 29.2542 - val_loss: 185.9497\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 29.6985 - val_loss: 178.4992\n",
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 29.4548 - val_loss: 183.0077\n",
      "Epoch 831/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 31.4596 - val_loss: 204.7993\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 28.0237 - val_loss: 171.3675\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 26.5279 - val_loss: 186.2629\n",
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 28.5534 - val_loss: 181.7223\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 31.7596 - val_loss: 181.4391\n",
      "Epoch 836/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 28.7563 - val_loss: 175.3231\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 27.1985 - val_loss: 176.2163\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 30.2260 - val_loss: 170.1658\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.6649 - val_loss: 169.9338\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 30.7861 - val_loss: 181.5147\n",
      "Epoch 841/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.8344 - val_loss: 180.5612\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.9753 - val_loss: 175.2474\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.4333 - val_loss: 186.7256\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.3522 - val_loss: 174.4140\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 28.8031 - val_loss: 182.3627\n",
      "Epoch 846/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 29.7486 - val_loss: 176.4031\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 29.9090 - val_loss: 175.9131\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 30.4879 - val_loss: 187.3751\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 28.3852 - val_loss: 191.7175\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.2188 - val_loss: 177.1511\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.4145 - val_loss: 189.1043\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 28.5006 - val_loss: 182.3661\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 28.7933 - val_loss: 180.1356\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.7949 - val_loss: 182.8821\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 27.5170 - val_loss: 170.0906\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 29.3759 - val_loss: 185.5756\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 29.5816 - val_loss: 180.2979\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 32.9505 - val_loss: 185.0309\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 32.4531 - val_loss: 164.0107\n",
      "Epoch 860/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 31.5142 - val_loss: 175.1003\n",
      "Epoch 861/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 28.2478 - val_loss: 167.5192\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 28.0214 - val_loss: 173.6939\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 28.5613 - val_loss: 184.4364\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.0498 - val_loss: 182.2992\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.5858 - val_loss: 189.2560\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 28.4378 - val_loss: 181.7002\n",
      "Epoch 867/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 26.3797 - val_loss: 187.4770\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 32.3632 - val_loss: 208.9618\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 33.2395 - val_loss: 177.3108\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 29.8769 - val_loss: 183.7517\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.9423 - val_loss: 169.4248\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 27.1886 - val_loss: 173.8618\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 26.6598 - val_loss: 171.8841\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 27.9979 - val_loss: 176.3123\n",
      "Epoch 875/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 27.8673 - val_loss: 167.2883\n",
      "Epoch 876/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 26.5002 - val_loss: 184.4770\n",
      "Epoch 877/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.1402 - val_loss: 186.4671\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 27.8858 - val_loss: 170.8780\n",
      "Epoch 879/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 29.7926 - val_loss: 171.8754\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.3839 - val_loss: 188.8799\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.4605 - val_loss: 186.6873\n",
      "Epoch 882/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 30.8156 - val_loss: 185.8872\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.1937 - val_loss: 177.3583\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.8311 - val_loss: 184.3447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 29.9224 - val_loss: 181.7542\n",
      "Epoch 886/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.2503 - val_loss: 170.1251\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.6215 - val_loss: 183.9006\n",
      "Epoch 888/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 26.2171 - val_loss: 176.0893\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 30.7162 - val_loss: 175.5282\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.2897 - val_loss: 183.5859\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.6601 - val_loss: 171.8708\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 27.8447 - val_loss: 176.5968\n",
      "Epoch 893/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.5581 - val_loss: 177.0536\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 26.9731 - val_loss: 185.4056\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 27.9109 - val_loss: 170.8502\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.2829 - val_loss: 187.9108\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 25.4383 - val_loss: 177.5347\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 28.9555 - val_loss: 171.4549\n",
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 28.6975 - val_loss: 184.4167\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 28.1232 - val_loss: 172.7839\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 27.5264 - val_loss: 171.2660\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 29.2802 - val_loss: 175.8917\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.8084 - val_loss: 175.3942\n",
      "Epoch 904/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 27.4377 - val_loss: 187.7586\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 25.9978 - val_loss: 176.8608\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 28.9473 - val_loss: 182.9904\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 29.0725 - val_loss: 178.2900\n",
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 27.3159 - val_loss: 178.1477\n",
      "Epoch 909/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 27.5683 - val_loss: 176.3448\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 27.5977 - val_loss: 180.7006\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 27.8128 - val_loss: 178.7769\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 28.0342 - val_loss: 184.3968\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 25.9240 - val_loss: 181.9632\n",
      "Epoch 914/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 27.3720 - val_loss: 178.2091\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 26.9413 - val_loss: 176.5974\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 1s 126us/step - loss: 30.4166 - val_loss: 179.2861\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 25.2893 - val_loss: 175.9772\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 27.3128 - val_loss: 187.2894\n",
      "Epoch 919/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 29.0167 - val_loss: 173.2545\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 27.9493 - val_loss: 185.8490\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 27.8669 - val_loss: 178.2431\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 27.7530 - val_loss: 183.3844\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 28.5960 - val_loss: 183.4147\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 25.1925 - val_loss: 170.2452\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 30.2329 - val_loss: 182.3403\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 28.5300 - val_loss: 187.6288\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 28.0106 - val_loss: 182.7009\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 28.4424 - val_loss: 188.2669\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 28.5832 - val_loss: 185.9797\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 26.5191 - val_loss: 183.9148\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 28.1023 - val_loss: 176.0969\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 29.7985 - val_loss: 179.9615\n",
      "Epoch 933/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 25.5072 - val_loss: 181.9337\n",
      "Epoch 934/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 28.9009 - val_loss: 187.4218\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 27.1933 - val_loss: 189.2904\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 28.0010 - val_loss: 182.9441\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 28.1393 - val_loss: 186.9021\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 1s 124us/step - loss: 26.6540 - val_loss: 170.3202\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 27.6821 - val_loss: 171.2662\n",
      "Epoch 940/10000\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 26.3136 - val_loss: 185.5977\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 27.0787 - val_loss: 177.5763\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 26.0558 - val_loss: 180.5980\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 27.2034 - val_loss: 183.2107\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 27.4728 - val_loss: 194.1421\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 27.7407 - val_loss: 191.4302\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 27.3778 - val_loss: 180.3564\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 28.5189 - val_loss: 180.6040\n",
      "Epoch 948/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 26.4367 - val_loss: 164.9212\n",
      "Epoch 949/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 27.5388 - val_loss: 176.8455\n",
      "Epoch 950/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 25.9302 - val_loss: 173.8213\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 27.5887 - val_loss: 184.2654\n",
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 32.3888 - val_loss: 175.4297\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 27.8708 - val_loss: 180.8797\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 24.7690 - val_loss: 168.1626\n",
      "Epoch 955/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 28.2425 - val_loss: 170.7300\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 26.5577 - val_loss: 182.2713\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 28.1115 - val_loss: 167.8449\n",
      "Epoch 958/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 92us/step - loss: 26.7424 - val_loss: 176.1353\n",
      "Epoch 959/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 25.0139 - val_loss: 172.7437\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 24.9374 - val_loss: 190.3782\n",
      "Epoch 961/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 26.1749 - val_loss: 172.9393\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 27.7831 - val_loss: 174.2149\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 27.6097 - val_loss: 184.9502\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 27.0084 - val_loss: 166.8564\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 25.1339 - val_loss: 194.7766\n",
      "Epoch 966/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 25.5968 - val_loss: 179.1591\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 27.0365 - val_loss: 181.8752\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 25.4538 - val_loss: 186.4372\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 24.8476 - val_loss: 189.4158\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 28.1725 - val_loss: 182.9098\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 27.7028 - val_loss: 189.4152\n",
      "Epoch 972/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 27.4135 - val_loss: 176.4191\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 25.7736 - val_loss: 171.7004\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 27.4549 - val_loss: 183.5082\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 26.6992 - val_loss: 174.3833\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 26.2320 - val_loss: 171.6441\n",
      "Epoch 977/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 27.1820 - val_loss: 167.5128\n",
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 25.6129 - val_loss: 177.0242\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 26.8929 - val_loss: 170.2220\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 26.7686 - val_loss: 173.1626\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 29.3994 - val_loss: 182.3169\n",
      "Epoch 982/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 24.9688 - val_loss: 170.7286\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 25.0681 - val_loss: 177.7669\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 24.2841 - val_loss: 168.8588\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 26.9900 - val_loss: 182.5060\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 25.8450 - val_loss: 174.1806\n",
      "Epoch 987/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 27.0146 - val_loss: 169.7146\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 28.2888 - val_loss: 179.2186\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 29.0292 - val_loss: 184.1159\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 26.2970 - val_loss: 183.5919\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 24.8349 - val_loss: 180.1529\n",
      "Epoch 992/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 25.1631 - val_loss: 179.8616\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 27.6511 - val_loss: 179.0016\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 25.9351 - val_loss: 180.1371\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 25.8030 - val_loss: 177.5804\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 25.1090 - val_loss: 184.3242\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 30.0672 - val_loss: 183.3412\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 26.0313 - val_loss: 181.4542\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 26.4736 - val_loss: 189.5721\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 25.3720 - val_loss: 185.0257\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 27.0347 - val_loss: 172.8214\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 24.8081 - val_loss: 175.2003\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 26.5782 - val_loss: 182.4567\n",
      "Epoch 1004/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 25.4953 - val_loss: 181.7108\n",
      "Epoch 1005/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 30.4791 - val_loss: 197.1828\n",
      "Epoch 1006/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 25.7747 - val_loss: 179.6525\n",
      "Epoch 1007/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 25.0154 - val_loss: 186.1381\n",
      "Epoch 1008/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 26.3879 - val_loss: 179.7264\n",
      "Epoch 1009/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 24.4747 - val_loss: 168.0057\n",
      "Epoch 1010/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 27.5285 - val_loss: 187.9582\n",
      "Epoch 1011/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 24.7501 - val_loss: 177.5912\n",
      "Epoch 1012/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 25.0302 - val_loss: 177.5253\n",
      "Epoch 1013/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 25.7419 - val_loss: 182.9011\n",
      "Epoch 1014/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 25.7537 - val_loss: 192.7409\n",
      "Epoch 1015/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 25.7341 - val_loss: 186.2667\n",
      "Epoch 1016/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 26.1829 - val_loss: 186.3406\n",
      "Epoch 1017/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 25.7783 - val_loss: 184.2660\n",
      "Epoch 1018/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 25.4046 - val_loss: 171.0669\n",
      "Epoch 1019/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 25.4199 - val_loss: 188.0564\n",
      "Epoch 1020/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 27.3704 - val_loss: 179.7530\n",
      "Epoch 1021/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 25.1448 - val_loss: 184.6030\n",
      "Epoch 1022/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 29.3152 - val_loss: 175.6775\n",
      "Epoch 1023/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 25.0422 - val_loss: 181.9194\n",
      "Epoch 1024/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 25.7075 - val_loss: 174.4934\n",
      "Epoch 1025/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 27.6458 - val_loss: 181.4785\n",
      "Epoch 1026/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 25.4039 - val_loss: 183.2540\n",
      "Epoch 1027/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 24.6751 - val_loss: 178.2634\n",
      "Epoch 1028/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 25.4497 - val_loss: 178.7610\n",
      "Epoch 1029/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 26.5913 - val_loss: 197.9019\n",
      "Epoch 1030/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 26.2372 - val_loss: 197.0974\n",
      "Epoch 1031/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 24.2144 - val_loss: 194.0802\n",
      "Epoch 1032/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 25.9518 - val_loss: 178.7175\n",
      "Epoch 1033/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 26.3696 - val_loss: 180.6691\n",
      "Epoch 1034/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 28.1133 - val_loss: 174.1120\n",
      "Epoch 1035/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 24.3453 - val_loss: 180.4313\n",
      "Epoch 1036/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 25.2122 - val_loss: 188.0274\n",
      "Epoch 1037/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 26.1894 - val_loss: 178.9834\n",
      "Epoch 1038/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 25.0083 - val_loss: 184.1558\n",
      "Epoch 1039/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 28.5545 - val_loss: 174.9804\n",
      "Epoch 1040/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.3681 - val_loss: 176.3271\n",
      "Epoch 1041/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 23.2745 - val_loss: 184.2961\n",
      "Epoch 1042/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 22.4690 - val_loss: 173.4280\n",
      "Epoch 1043/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 24.4397 - val_loss: 173.6159\n",
      "Epoch 1044/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 26.7179 - val_loss: 186.0553\n",
      "Epoch 1045/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 24.0305 - val_loss: 177.7169\n",
      "Epoch 1046/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 24.8231 - val_loss: 174.7801\n",
      "Epoch 1047/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.3028 - val_loss: 178.1118\n",
      "Epoch 1048/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 24.5280 - val_loss: 180.0432\n",
      "Epoch 1049/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 27.8707 - val_loss: 176.6257\n",
      "Epoch 1050/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 26.7433 - val_loss: 167.8190\n",
      "Epoch 1051/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 27.0508 - val_loss: 186.5477\n",
      "Epoch 1052/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 25.0488 - val_loss: 180.3516\n",
      "Epoch 1053/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 24.7889 - val_loss: 177.4010\n",
      "Epoch 1054/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.7613 - val_loss: 167.5667\n",
      "Epoch 1055/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.0072 - val_loss: 183.4690\n",
      "Epoch 1056/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.5915 - val_loss: 186.0145\n",
      "Epoch 1057/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 27.1090 - val_loss: 182.2652\n",
      "Epoch 1058/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 23.5765 - val_loss: 181.4473\n",
      "Epoch 1059/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 24.4277 - val_loss: 187.0420\n",
      "Epoch 1060/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 24.0609 - val_loss: 205.9203\n",
      "Epoch 1061/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 24.2433 - val_loss: 181.5931\n",
      "Epoch 1062/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 24.8382 - val_loss: 170.0649\n",
      "Epoch 1063/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.6185 - val_loss: 183.2084\n",
      "Epoch 1064/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 25.0569 - val_loss: 180.1886\n",
      "Epoch 1065/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 24.7890 - val_loss: 180.2050\n",
      "Epoch 1066/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 27.6995 - val_loss: 170.7621\n",
      "Epoch 1067/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 28.1958 - val_loss: 183.2047\n",
      "Epoch 1068/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 27.5682 - val_loss: 182.0247\n",
      "Epoch 1069/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 26.6135 - val_loss: 183.7527\n",
      "Epoch 1070/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.2018 - val_loss: 177.7993\n",
      "Epoch 1071/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 25.9579 - val_loss: 172.8916\n",
      "Epoch 1072/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 27.4749 - val_loss: 179.3673\n",
      "Epoch 1073/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.4674 - val_loss: 173.1700\n",
      "Epoch 1074/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 24.6789 - val_loss: 186.2717\n",
      "Epoch 1075/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 26.5763 - val_loss: 177.1835\n",
      "Epoch 1076/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 23.2394 - val_loss: 182.4869\n",
      "Epoch 1077/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 24.7712 - val_loss: 184.9732\n",
      "Epoch 1078/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 23.4703 - val_loss: 177.1568\n",
      "Epoch 1079/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 24.9132 - val_loss: 179.5253\n",
      "Epoch 1080/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 25.3088 - val_loss: 173.4379\n",
      "Epoch 1081/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 23.6820 - val_loss: 176.2895\n",
      "Epoch 1082/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.7452 - val_loss: 164.4198\n",
      "Epoch 1083/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.1662 - val_loss: 169.2034\n",
      "Epoch 1084/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 24.8318 - val_loss: 177.9628\n",
      "Epoch 1085/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 25.4223 - val_loss: 184.9008\n",
      "Epoch 1086/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.6412 - val_loss: 180.4623\n",
      "Epoch 1087/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 24.8574 - val_loss: 178.9670\n",
      "Epoch 1088/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 24.0240 - val_loss: 180.5171\n",
      "Epoch 1089/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.0772 - val_loss: 175.2815\n",
      "Epoch 1090/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 24.7219 - val_loss: 175.2366\n",
      "Epoch 1091/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 27.3266 - val_loss: 177.4984\n",
      "Epoch 1092/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 24.5961 - val_loss: 176.4727\n",
      "Epoch 1093/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 23.2895 - val_loss: 189.3384\n",
      "Epoch 1094/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 26.6326 - val_loss: 187.7574\n",
      "Epoch 1095/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 23.9812 - val_loss: 172.4538\n",
      "Epoch 1096/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 23.2334 - val_loss: 175.7753\n",
      "Epoch 1097/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 23.7972 - val_loss: 175.5082\n",
      "Epoch 1098/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 25.7831 - val_loss: 186.1988\n",
      "Epoch 1099/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 23.5394 - val_loss: 183.8861\n",
      "Epoch 1100/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 24.2491 - val_loss: 184.5440\n",
      "Epoch 1101/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 25.9830 - val_loss: 186.0809\n",
      "Epoch 1102/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 24.9499 - val_loss: 174.7886\n",
      "Epoch 1103/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 24.8666 - val_loss: 181.3777\n",
      "Epoch 1104/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 23.1363 - val_loss: 180.3923\n",
      "Epoch 1105/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 24.0299 - val_loss: 183.4514\n",
      "Epoch 1106/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 23.6875 - val_loss: 180.6414\n",
      "Epoch 1107/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 25.6566 - val_loss: 170.6096\n",
      "Epoch 1108/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 24.1154 - val_loss: 171.8672\n",
      "Epoch 1109/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 25.2603 - val_loss: 181.3729\n",
      "Epoch 1110/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 24.5929 - val_loss: 178.4818\n",
      "Epoch 1111/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 23.6319 - val_loss: 175.9205\n",
      "Epoch 1112/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 24.0439 - val_loss: 179.8492\n",
      "Epoch 1113/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 23.3024 - val_loss: 171.5142\n",
      "Epoch 1114/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 23.1665 - val_loss: 173.0408\n",
      "Epoch 1115/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 22.9206 - val_loss: 174.3081\n",
      "Epoch 1116/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 24.5748 - val_loss: 170.7965\n",
      "Epoch 1117/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 24.0922 - val_loss: 176.8955\n",
      "Epoch 1118/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 23.7024 - val_loss: 172.8422\n",
      "Epoch 1119/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 25.1414 - val_loss: 173.4063\n",
      "Epoch 1120/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 26.7028 - val_loss: 171.2316\n",
      "Epoch 1121/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 22.6490 - val_loss: 176.6731\n",
      "Epoch 1122/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 23.4166 - val_loss: 179.8175\n",
      "Epoch 1123/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 22.7597 - val_loss: 184.6014\n",
      "Epoch 1124/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 23.5032 - val_loss: 182.6304\n",
      "Epoch 1125/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 25.6677 - val_loss: 175.5339\n",
      "Epoch 1126/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 24.4872 - val_loss: 169.2254\n",
      "Epoch 1127/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 24.8276 - val_loss: 180.4281\n",
      "Epoch 1128/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 24.7934 - val_loss: 178.7840\n",
      "Epoch 1129/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 24.0897 - val_loss: 186.4015\n",
      "Epoch 1130/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 24.9558 - val_loss: 187.5489\n",
      "Epoch 1131/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 24.1120 - val_loss: 187.7820\n",
      "Epoch 1132/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 24.5105 - val_loss: 175.3174\n",
      "Epoch 1133/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 22.0493 - val_loss: 179.6188\n",
      "Epoch 1134/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 24.4850 - val_loss: 189.2903\n",
      "Epoch 1135/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 23.92 - 1s 64us/step - loss: 23.2159 - val_loss: 175.6794\n",
      "Epoch 1136/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 24.1722 - val_loss: 169.7089\n",
      "Epoch 1137/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 24.8383 - val_loss: 183.9763\n",
      "Epoch 1138/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 24.8013 - val_loss: 178.6654\n",
      "Epoch 1139/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 25.3776 - val_loss: 176.3589\n",
      "Epoch 1140/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 23.4209 - val_loss: 184.9531\n",
      "Epoch 1141/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 23.2366 - val_loss: 181.2763\n",
      "Epoch 1142/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 22.4609 - val_loss: 174.0913\n",
      "Epoch 1143/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 22.4994 - val_loss: 182.1062\n",
      "Epoch 1144/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 22.9444 - val_loss: 175.7016\n",
      "Epoch 1145/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 24.3824 - val_loss: 177.2471\n",
      "Epoch 1146/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 26.2619 - val_loss: 170.8504\n",
      "Epoch 1147/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 22.4519 - val_loss: 179.3257\n",
      "Epoch 1148/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 23.0926 - val_loss: 182.9614\n",
      "Epoch 1149/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 24.3670 - val_loss: 186.2240\n",
      "Epoch 1150/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 26.5055 - val_loss: 172.9696\n",
      "Epoch 1151/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 25.1202 - val_loss: 183.3608\n",
      "Epoch 1152/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 24.6766 - val_loss: 174.2753\n",
      "Epoch 1153/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 22.8499 - val_loss: 181.4152\n",
      "Epoch 1154/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 23.6354 - val_loss: 180.9044\n",
      "Epoch 1155/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 23.9123 - val_loss: 183.6182\n",
      "Epoch 1156/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 20.7066 - val_loss: 173.5619\n",
      "Epoch 1157/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 23.4395 - val_loss: 180.3323\n",
      "Epoch 1158/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 21.3950 - val_loss: 184.7869\n",
      "Epoch 1159/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 24.2938 - val_loss: 169.7574\n",
      "Epoch 1160/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 24.4808 - val_loss: 176.8555\n",
      "Epoch 1161/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 23.0909 - val_loss: 178.9055\n",
      "Epoch 1162/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 23.3393 - val_loss: 179.5705\n",
      "Epoch 1163/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 26.3330 - val_loss: 174.3737\n",
      "Epoch 1164/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 24.2749 - val_loss: 188.5306\n",
      "Epoch 1165/10000\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 22.2926 - val_loss: 179.0379\n",
      "Epoch 1166/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 24.0902 - val_loss: 176.7821\n",
      "Epoch 1167/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 23.5868 - val_loss: 176.7866\n",
      "Epoch 1168/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 22.2266 - val_loss: 174.9743\n",
      "Epoch 1169/10000\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 25.4992 - val_loss: 184.3539\n",
      "Epoch 1170/10000\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 26.8448 - val_loss: 180.3455\n",
      "Epoch 1171/10000\n",
      "8000/8000 [==============================] - 1s 130us/step - loss: 26.5472 - val_loss: 187.5875\n",
      "Epoch 1172/10000\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 23.5260 - val_loss: 186.8583\n",
      "Epoch 1173/10000\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 22.3136 - val_loss: 176.1967\n",
      "Epoch 1174/10000\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 23.9206 - val_loss: 181.7257\n",
      "Epoch 1175/10000\n",
      "8000/8000 [==============================] - 1s 130us/step - loss: 23.1487 - val_loss: 176.6837\n",
      "Epoch 1176/10000\n",
      "8000/8000 [==============================] - 1s 130us/step - loss: 21.8146 - val_loss: 182.8124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1177/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 23.9123 - val_loss: 183.2993\n",
      "Epoch 1178/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 23.7336 - val_loss: 182.3908\n",
      "Epoch 1179/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 22.2071 - val_loss: 180.8271\n",
      "Epoch 1180/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 23.7985 - val_loss: 180.9782\n",
      "Epoch 1181/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 22.8474 - val_loss: 174.5483\n",
      "Epoch 1182/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 25.1494 - val_loss: 182.6350\n",
      "Epoch 1183/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 23.5152 - val_loss: 183.3585\n",
      "Epoch 1184/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 24.6436 - val_loss: 185.5578\n",
      "Epoch 1185/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 22.0867 - val_loss: 176.2273\n",
      "Epoch 1186/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 22.4694 - val_loss: 173.9094\n",
      "Epoch 1187/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 23.5217 - val_loss: 188.0878\n",
      "Epoch 1188/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 23.9015 - val_loss: 178.5301\n",
      "Epoch 1189/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 23.1482 - val_loss: 177.4617\n",
      "Epoch 1190/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 22.2641 - val_loss: 180.1786\n",
      "Epoch 1191/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 21.7095 - val_loss: 179.3721\n",
      "Epoch 1192/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 20.3139 - val_loss: 182.2322\n",
      "Epoch 1193/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 25.7930 - val_loss: 177.1383\n",
      "Epoch 1194/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 24.0426 - val_loss: 179.8736\n",
      "Epoch 1195/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 23.0238 - val_loss: 180.6990\n",
      "Epoch 1196/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 22.1957 - val_loss: 176.2686\n",
      "Epoch 1197/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 21.5823 - val_loss: 177.1593\n",
      "Epoch 1198/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.7192 - val_loss: 188.5945\n",
      "Epoch 1199/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 23.7120 - val_loss: 184.5130\n",
      "Epoch 1200/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 24.0307 - val_loss: 171.4257\n",
      "Epoch 1201/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 25.4443 - val_loss: 181.2011\n",
      "Epoch 1202/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.9286 - val_loss: 178.1415\n",
      "Epoch 1203/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 26.3622 - val_loss: 189.3813\n",
      "Epoch 1204/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.2324 - val_loss: 180.3862\n",
      "Epoch 1205/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 23.9602 - val_loss: 185.0868\n",
      "Epoch 1206/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 23.3798 - val_loss: 179.9473\n",
      "Epoch 1207/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 21.7762 - val_loss: 187.9577\n",
      "Epoch 1208/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 22.7904 - val_loss: 182.5716\n",
      "Epoch 1209/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 22.3169 - val_loss: 197.8956\n",
      "Epoch 1210/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.3246 - val_loss: 179.8842\n",
      "Epoch 1211/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 23.2979 - val_loss: 185.0418\n",
      "Epoch 1212/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 24.5278 - val_loss: 181.2183\n",
      "Epoch 1213/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 23.38 - 0s 50us/step - loss: 23.4342 - val_loss: 173.3615\n",
      "Epoch 1214/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 22.6513 - val_loss: 174.6608\n",
      "Epoch 1215/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 21.2994 - val_loss: 169.0946\n",
      "Epoch 1216/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.1423 - val_loss: 185.8769\n",
      "Epoch 1217/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 21.8220 - val_loss: 174.5912\n",
      "Epoch 1218/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 21.6706 - val_loss: 169.6527\n",
      "Epoch 1219/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.1824 - val_loss: 173.3737\n",
      "Epoch 1220/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 20.9201 - val_loss: 181.6556\n",
      "Epoch 1221/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 21.9100 - val_loss: 178.5763\n",
      "Epoch 1222/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 25.3323 - val_loss: 172.0694\n",
      "Epoch 1223/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 24.3346 - val_loss: 186.1609\n",
      "Epoch 1224/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 25.9577 - val_loss: 179.4305\n",
      "Epoch 1225/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.8722 - val_loss: 185.1663\n",
      "Epoch 1226/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.5617 - val_loss: 170.4829\n",
      "Epoch 1227/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 20.3465 - val_loss: 171.6960\n",
      "Epoch 1228/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.2232 - val_loss: 177.2759\n",
      "Epoch 1229/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 25.0018 - val_loss: 175.3609\n",
      "Epoch 1230/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.7194 - val_loss: 180.1401\n",
      "Epoch 1231/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 21.5030 - val_loss: 184.2954\n",
      "Epoch 1232/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.0427 - val_loss: 169.4725\n",
      "Epoch 1233/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.1376 - val_loss: 179.1965\n",
      "Epoch 1234/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.9842 - val_loss: 164.4177\n",
      "Epoch 1235/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 22.6256 - val_loss: 173.6922\n",
      "Epoch 1236/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 22.9457 - val_loss: 177.8768\n",
      "Epoch 1237/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 22.9999 - val_loss: 177.5595\n",
      "Epoch 1238/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 22.1682 - val_loss: 178.3384\n",
      "Epoch 1239/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 20.7467 - val_loss: 185.6960\n",
      "Epoch 1240/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 23.7552 - val_loss: 185.4040\n",
      "Epoch 1241/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 22.1597 - val_loss: 182.4943\n",
      "Epoch 1242/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 22.1386 - val_loss: 169.9651\n",
      "Epoch 1243/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 20.8094 - val_loss: 183.7527\n",
      "Epoch 1244/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 23.0713 - val_loss: 178.5592\n",
      "Epoch 1245/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 23.5571 - val_loss: 182.6736\n",
      "Epoch 1246/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 21.7856 - val_loss: 180.4850\n",
      "Epoch 1247/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 22.8532 - val_loss: 181.5008\n",
      "Epoch 1248/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 24.0513 - val_loss: 181.0040\n",
      "Epoch 1249/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 22.1891 - val_loss: 173.2630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1250/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 22.6721 - val_loss: 178.0042\n",
      "Epoch 1251/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 20.7437 - val_loss: 180.2286\n",
      "Epoch 1252/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 23.7714 - val_loss: 190.2523\n",
      "Epoch 1253/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 22.3052 - val_loss: 187.1937\n",
      "Epoch 1254/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 22.2218 - val_loss: 178.2458\n",
      "Epoch 1255/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 22.8112 - val_loss: 172.0971\n",
      "Epoch 1256/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 19.7207 - val_loss: 175.7498\n",
      "Epoch 1257/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 20.2895 - val_loss: 184.9989\n",
      "Epoch 1258/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.1064 - val_loss: 179.2939\n",
      "Epoch 1259/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 23.0825 - val_loss: 171.2202\n",
      "Epoch 1260/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 23.8469 - val_loss: 175.6733\n",
      "Epoch 1261/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.0572 - val_loss: 178.2178\n",
      "Epoch 1262/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.5716 - val_loss: 193.2684\n",
      "Epoch 1263/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.2958 - val_loss: 183.9500\n",
      "Epoch 1264/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 23.3391 - val_loss: 178.5975\n",
      "Epoch 1265/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 22.8260 - val_loss: 187.3804\n",
      "Epoch 1266/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 23.33 - 0s 50us/step - loss: 23.3928 - val_loss: 184.9101\n",
      "Epoch 1267/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 23.8954 - val_loss: 180.8444\n",
      "Epoch 1268/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.4920 - val_loss: 186.6426\n",
      "Epoch 1269/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.8872 - val_loss: 176.3489\n",
      "Epoch 1270/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.9555 - val_loss: 185.6730\n",
      "Epoch 1271/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 21.9618 - val_loss: 179.3528\n",
      "Epoch 1272/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 21.9361 - val_loss: 178.1674\n",
      "Epoch 1273/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.8751 - val_loss: 177.9248\n",
      "Epoch 1274/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.6091 - val_loss: 174.4531\n",
      "Epoch 1275/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.0392 - val_loss: 171.6664\n",
      "Epoch 1276/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 20.1675 - val_loss: 168.7618\n",
      "Epoch 1277/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 24.8241 - val_loss: 168.9445\n",
      "Epoch 1278/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 24.3943 - val_loss: 188.6709\n",
      "Epoch 1279/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.8766 - val_loss: 171.7912\n",
      "Epoch 1280/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 23.1083 - val_loss: 174.5979\n",
      "Epoch 1281/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 20.4659 - val_loss: 180.1432\n",
      "Epoch 1282/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.5826 - val_loss: 176.8779\n",
      "Epoch 1283/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.2212 - val_loss: 176.6396\n",
      "Epoch 1284/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 23.1430 - val_loss: 183.1404\n",
      "Epoch 1285/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 21.9997 - val_loss: 170.9423\n",
      "Epoch 1286/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 22.5899 - val_loss: 171.0305\n",
      "Epoch 1287/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 23.4431 - val_loss: 176.3913\n",
      "Epoch 1288/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.8738 - val_loss: 171.1286\n",
      "Epoch 1289/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 20.7294 - val_loss: 180.0443\n",
      "Epoch 1290/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 23.0446 - val_loss: 173.4997\n",
      "Epoch 1291/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 20.9580 - val_loss: 178.4494\n",
      "Epoch 1292/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 21.2571 - val_loss: 182.8942\n",
      "Epoch 1293/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 22.0223 - val_loss: 177.7785\n",
      "Epoch 1294/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 21.5717 - val_loss: 176.5503\n",
      "Epoch 1295/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.5238 - val_loss: 178.3579\n",
      "Epoch 1296/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.0496 - val_loss: 182.6755\n",
      "Epoch 1297/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 20.6517 - val_loss: 180.1345\n",
      "Epoch 1298/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 20.8744 - val_loss: 176.7986\n",
      "Epoch 1299/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 20.3626 - val_loss: 175.9162\n",
      "Epoch 1300/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 20.8679 - val_loss: 183.6718\n",
      "Epoch 1301/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 25.6372 - val_loss: 187.9066\n",
      "Epoch 1302/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 24.1949 - val_loss: 176.2788\n",
      "Epoch 1303/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.4799 - val_loss: 174.9529\n",
      "Epoch 1304/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 21.6314 - val_loss: 180.2114\n",
      "Epoch 1305/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 23.0440 - val_loss: 178.8316\n",
      "Epoch 1306/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.8464 - val_loss: 183.0436\n",
      "Epoch 1307/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 21.2449 - val_loss: 177.9346\n",
      "Epoch 1308/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 22.0936 - val_loss: 183.5304\n",
      "Epoch 1309/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.7403 - val_loss: 181.1042\n",
      "Epoch 1310/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 22.2637 - val_loss: 180.0412\n",
      "Epoch 1311/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 22.3129 - val_loss: 175.1016\n",
      "Epoch 1312/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.2585 - val_loss: 175.2255\n",
      "Epoch 1313/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.5944 - val_loss: 175.1263\n",
      "Epoch 1314/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 20.5171 - val_loss: 172.7596\n",
      "Epoch 1315/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 21.89 - 0s 51us/step - loss: 21.7519 - val_loss: 171.8185\n",
      "Epoch 1316/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 19.0591 - val_loss: 174.5577\n",
      "Epoch 1317/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 23.3435 - val_loss: 177.4038\n",
      "Epoch 1318/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.2123 - val_loss: 170.8280\n",
      "Epoch 1319/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 19.7707 - val_loss: 177.3673\n",
      "Epoch 1320/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 23.5920 - val_loss: 184.4539\n",
      "Epoch 1321/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 22.7253 - val_loss: 172.2978\n",
      "Epoch 1322/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 64us/step - loss: 21.7889 - val_loss: 168.4125\n",
      "Epoch 1323/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 20.4236 - val_loss: 193.4172\n",
      "Epoch 1324/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 20.2897 - val_loss: 178.6115\n",
      "Epoch 1325/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 21.1612 - val_loss: 181.1879\n",
      "Epoch 1326/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.8823 - val_loss: 172.5643\n",
      "Epoch 1327/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.9494 - val_loss: 174.4587\n",
      "Epoch 1328/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 23.9823 - val_loss: 169.4415\n",
      "Epoch 1329/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 21.5795 - val_loss: 190.5652\n",
      "Epoch 1330/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 22.5169 - val_loss: 186.6699\n",
      "Epoch 1331/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.2685 - val_loss: 168.0620\n",
      "Epoch 1332/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 21.1098 - val_loss: 172.0119\n",
      "Epoch 1333/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 21.0637 - val_loss: 178.5937\n",
      "Epoch 1334/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.1999 - val_loss: 171.1543\n",
      "Epoch 1335/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 21.6882 - val_loss: 179.0975\n",
      "Epoch 1336/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 21.0770 - val_loss: 170.5227\n",
      "Epoch 1337/10000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 21.4194 - val_loss: 170.0630\n",
      "Epoch 1338/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 20.6530 - val_loss: 179.9961\n",
      "Epoch 1339/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 21.4376 - val_loss: 175.6333\n",
      "Epoch 1340/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 21.4407 - val_loss: 177.5701\n",
      "Epoch 1341/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 24.2707 - val_loss: 178.7963\n",
      "Epoch 1342/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.3504 - val_loss: 178.7959\n",
      "Epoch 1343/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 21.4623 - val_loss: 192.4234\n",
      "Epoch 1344/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 20.9734 - val_loss: 187.0998\n",
      "Epoch 1345/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.9660 - val_loss: 187.6950\n",
      "Epoch 1346/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.2828 - val_loss: 178.5033\n",
      "Epoch 1347/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.2690 - val_loss: 172.7442\n",
      "Epoch 1348/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 21.0027 - val_loss: 176.4385\n",
      "Epoch 1349/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.5147 - val_loss: 190.6497\n",
      "Epoch 1350/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 19.3418 - val_loss: 178.5527\n",
      "Epoch 1351/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 21.9284 - val_loss: 181.2107\n",
      "Epoch 1352/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 20.8546 - val_loss: 179.3474\n",
      "Epoch 1353/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 20.9779 - val_loss: 167.8639\n",
      "Epoch 1354/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.8886 - val_loss: 167.5527\n",
      "Epoch 1355/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 21.1344 - val_loss: 169.3180\n",
      "Epoch 1356/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 20.2856 - val_loss: 167.9885\n",
      "Epoch 1357/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.8594 - val_loss: 180.6579\n",
      "Epoch 1358/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.4572 - val_loss: 176.3856\n",
      "Epoch 1359/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.0606 - val_loss: 174.8178\n",
      "Epoch 1360/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 21.2698 - val_loss: 186.9092\n",
      "Epoch 1361/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.3517 - val_loss: 174.7991\n",
      "Epoch 1362/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.3863 - val_loss: 174.3843\n",
      "Epoch 1363/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 20.1380 - val_loss: 178.6731\n",
      "Epoch 1364/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 23.3258 - val_loss: 167.7463\n",
      "Epoch 1365/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.7288 - val_loss: 175.0436\n",
      "Epoch 1366/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.0584 - val_loss: 176.1249\n",
      "Epoch 1367/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.7723 - val_loss: 174.9379\n",
      "Epoch 1368/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.0002 - val_loss: 179.8553\n",
      "Epoch 1369/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 20.6863 - val_loss: 182.5689\n",
      "Epoch 1370/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 21.0399 - val_loss: 173.2370\n",
      "Epoch 1371/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 20.4689 - val_loss: 170.8402\n",
      "Epoch 1372/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.3517 - val_loss: 180.6174\n",
      "Epoch 1373/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.3842 - val_loss: 182.9423\n",
      "Epoch 1374/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.0831 - val_loss: 181.7493\n",
      "Epoch 1375/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 22.0798 - val_loss: 173.4971\n",
      "Epoch 1376/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 22.8047 - val_loss: 170.1589\n",
      "Epoch 1377/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 22.2051 - val_loss: 168.8251\n",
      "Epoch 1378/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.4789 - val_loss: 177.8532\n",
      "Epoch 1379/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.3091 - val_loss: 172.4521\n",
      "Epoch 1380/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 19.9309 - val_loss: 176.1018\n",
      "Epoch 1381/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 22.8400 - val_loss: 174.5530\n",
      "Epoch 1382/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.3914 - val_loss: 174.8420\n",
      "Epoch 1383/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 19.7141 - val_loss: 174.9764\n",
      "Epoch 1384/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 20.8163 - val_loss: 173.3616\n",
      "Epoch 1385/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 21.9415 - val_loss: 172.4971\n",
      "Epoch 1386/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.3066 - val_loss: 177.3965\n",
      "Epoch 1387/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.1570 - val_loss: 181.4039\n",
      "Epoch 1388/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 20.7880 - val_loss: 184.0508\n",
      "Epoch 1389/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 21.1923 - val_loss: 176.7748\n",
      "Epoch 1390/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.7066 - val_loss: 162.2334\n",
      "Epoch 1391/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 22.9408 - val_loss: 178.8391\n",
      "Epoch 1392/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.2957 - val_loss: 164.1551\n",
      "Epoch 1393/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 20.2606 - val_loss: 180.2226\n",
      "Epoch 1394/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 22.6588 - val_loss: 174.6325\n",
      "Epoch 1395/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 66us/step - loss: 22.2588 - val_loss: 186.4091\n",
      "Epoch 1396/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 21.7093 - val_loss: 175.7745\n",
      "Epoch 1397/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 20.2482 - val_loss: 177.6885\n",
      "Epoch 1398/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 20.3558 - val_loss: 168.2184\n",
      "Epoch 1399/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.3177 - val_loss: 180.0861\n",
      "Epoch 1400/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.4691 - val_loss: 168.0416\n",
      "Epoch 1401/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 22.5013 - val_loss: 174.2368\n",
      "Epoch 1402/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 22.1592 - val_loss: 177.2233\n",
      "Epoch 1403/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 19.0475 - val_loss: 184.3789\n",
      "Epoch 1404/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 20.0389 - val_loss: 182.1446\n",
      "Epoch 1405/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 19.4772 - val_loss: 179.3620\n",
      "Epoch 1406/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.1115 - val_loss: 168.5868\n",
      "Epoch 1407/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 20.2332 - val_loss: 172.0121\n",
      "Epoch 1408/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.5477 - val_loss: 173.3776\n",
      "Epoch 1409/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 22.1389 - val_loss: 171.7524\n",
      "Epoch 1410/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 20.8614 - val_loss: 172.0373\n",
      "Epoch 1411/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 23.9539 - val_loss: 174.7667\n",
      "Epoch 1412/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 21.3735 - val_loss: 168.6051\n",
      "Epoch 1413/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 20.1625 - val_loss: 177.7292\n",
      "Epoch 1414/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.2439 - val_loss: 170.6727\n",
      "Epoch 1415/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 19.2132 - val_loss: 167.8560\n",
      "Epoch 1416/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 19.5595 - val_loss: 173.0812\n",
      "Epoch 1417/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 19.9533 - val_loss: 169.1455\n",
      "Epoch 1418/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 20.0457 - val_loss: 164.5395\n",
      "Epoch 1419/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 22.1632 - val_loss: 174.8237\n",
      "Epoch 1420/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 19.7656 - val_loss: 175.0379\n",
      "Epoch 1421/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.2500 - val_loss: 171.3273\n",
      "Epoch 1422/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 20.1946 - val_loss: 176.5647\n",
      "Epoch 1423/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 18.5892 - val_loss: 176.1352\n",
      "Epoch 1424/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 22.0777 - val_loss: 179.6657\n",
      "Epoch 1425/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 19.3189 - val_loss: 175.6037\n",
      "Epoch 1426/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.0287 - val_loss: 178.3754\n",
      "Epoch 1427/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 22.9749 - val_loss: 174.9285\n",
      "Epoch 1428/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.5205 - val_loss: 168.9940\n",
      "Epoch 1429/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.1288 - val_loss: 180.6492\n",
      "Epoch 1430/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 20.4816 - val_loss: 174.1670\n",
      "Epoch 1431/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.4821 - val_loss: 164.4823\n",
      "Epoch 1432/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 19.7043 - val_loss: 171.8822\n",
      "Epoch 1433/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.5785 - val_loss: 173.5288\n",
      "Epoch 1434/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 20.5655 - val_loss: 166.8694\n",
      "Epoch 1435/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 19.1905 - val_loss: 183.1168\n",
      "Epoch 1436/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 17.8727 - val_loss: 176.7999\n",
      "Epoch 1437/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 19.2350 - val_loss: 167.2744\n",
      "Epoch 1438/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.4809 - val_loss: 167.9010\n",
      "Epoch 1439/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 22.0734 - val_loss: 180.7728\n",
      "Epoch 1440/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.4327 - val_loss: 174.3599\n",
      "Epoch 1441/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.9496 - val_loss: 169.9557\n",
      "Epoch 1442/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.0439 - val_loss: 174.1432\n",
      "Epoch 1443/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.5081 - val_loss: 173.1120\n",
      "Epoch 1444/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.2379 - val_loss: 165.4040\n",
      "Epoch 1445/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 21.2599 - val_loss: 168.7807\n",
      "Epoch 1446/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.9081 - val_loss: 169.4454\n",
      "Epoch 1447/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 22.7998 - val_loss: 193.0648\n",
      "Epoch 1448/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 20.8381 - val_loss: 175.6208\n",
      "Epoch 1449/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 19.9089 - val_loss: 182.0514\n",
      "Epoch 1450/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 20.7019 - val_loss: 176.7713\n",
      "Epoch 1451/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 21.4943 - val_loss: 173.4139\n",
      "Epoch 1452/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 20.4130 - val_loss: 173.4442\n",
      "Epoch 1453/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 21.2412 - val_loss: 182.6172\n",
      "Epoch 1454/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 21.4075 - val_loss: 172.8199\n",
      "Epoch 1455/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 18.3190 - val_loss: 177.6906\n",
      "Epoch 1456/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.1397 - val_loss: 172.8317\n",
      "Epoch 1457/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 20.1096 - val_loss: 175.1802\n",
      "Epoch 1458/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 21.8394 - val_loss: 178.6431\n",
      "Epoch 1459/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 24.3532 - val_loss: 173.6830\n",
      "Epoch 1460/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 18.9864 - val_loss: 168.9676\n",
      "Epoch 1461/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 19.4722 - val_loss: 166.2164\n",
      "Epoch 1462/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 20.1726 - val_loss: 177.5803\n",
      "Epoch 1463/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.1682 - val_loss: 176.8211\n",
      "Epoch 1464/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 23.1409 - val_loss: 178.3696\n",
      "Epoch 1465/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 17.8515 - val_loss: 168.7253\n",
      "Epoch 1466/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 19.3969 - val_loss: 161.5909\n",
      "Epoch 1467/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 18.8469 - val_loss: 167.0138\n",
      "Epoch 1468/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 55us/step - loss: 19.7161 - val_loss: 184.0464\n",
      "Epoch 1469/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 19.4862 - val_loss: 173.0888\n",
      "Epoch 1470/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 20.5956 - val_loss: 178.1148\n",
      "Epoch 1471/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 20.2332 - val_loss: 169.0124\n",
      "Epoch 1472/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.8543 - val_loss: 170.1173\n",
      "Epoch 1473/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 19.2407 - val_loss: 167.9615\n",
      "Epoch 1474/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 19.2709 - val_loss: 176.5165\n",
      "Epoch 1475/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 21.1101 - val_loss: 177.3750\n",
      "Epoch 1476/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.2934 - val_loss: 178.4664\n",
      "Epoch 1477/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 20.5019 - val_loss: 178.2523\n",
      "Epoch 1478/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 23.4405 - val_loss: 170.8139\n",
      "Epoch 1479/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 19.8648 - val_loss: 171.6308\n",
      "Epoch 1480/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 20.7545 - val_loss: 177.6640\n",
      "Epoch 1481/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 20.3993 - val_loss: 166.3157\n",
      "Epoch 1482/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 20.0364 - val_loss: 175.6690\n",
      "Epoch 1483/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.1380 - val_loss: 174.8175\n",
      "Epoch 1484/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 18.9095 - val_loss: 176.5467\n",
      "Epoch 1485/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.3891 - val_loss: 173.6180\n",
      "Epoch 1486/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 18.8943 - val_loss: 175.3787\n",
      "Epoch 1487/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.7791 - val_loss: 173.4744\n",
      "Epoch 1488/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 20.0421 - val_loss: 180.8354\n",
      "Epoch 1489/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 18.6366 - val_loss: 174.0135\n",
      "Epoch 1490/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 18.4637 - val_loss: 177.9490\n",
      "Epoch 1491/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 19.5839 - val_loss: 176.4452\n",
      "Epoch 1492/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 24.3688 - val_loss: 176.2006\n",
      "Epoch 1493/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 20.36 - 0s 50us/step - loss: 20.1639 - val_loss: 172.3945\n",
      "Epoch 1494/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 18.6321 - val_loss: 183.4459\n",
      "Epoch 1495/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.2761 - val_loss: 178.2462\n",
      "Epoch 1496/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 26.0829 - val_loss: 168.3880\n",
      "Epoch 1497/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 21.7651 - val_loss: 181.1157\n",
      "Epoch 1498/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 19.4534 - val_loss: 176.9391\n",
      "Epoch 1499/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 18.4708 - val_loss: 171.8157\n",
      "Epoch 1500/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 18.4475 - val_loss: 171.1929\n",
      "Epoch 1501/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 19.0124 - val_loss: 174.1400\n",
      "Epoch 1502/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 18.8867 - val_loss: 180.5706\n",
      "Epoch 1503/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 20.2904 - val_loss: 178.7127\n",
      "Epoch 1504/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 21.2011 - val_loss: 168.0793\n",
      "Epoch 1505/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 19.8501 - val_loss: 174.3899\n",
      "Epoch 1506/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 21.4617 - val_loss: 178.6263\n",
      "Epoch 1507/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 20.1159 - val_loss: 171.2104\n",
      "Epoch 1508/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 19.6429 - val_loss: 173.7494\n",
      "Epoch 1509/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 21.1933 - val_loss: 175.7858\n",
      "Epoch 1510/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 20.2311 - val_loss: 178.9958\n",
      "Epoch 1511/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.8230 - val_loss: 184.4070\n",
      "Epoch 1512/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 19.4418 - val_loss: 179.0346\n",
      "Epoch 1513/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 18.7908 - val_loss: 179.4908\n",
      "Epoch 1514/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 18.6953 - val_loss: 176.3632\n",
      "Epoch 1515/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 22.0587 - val_loss: 178.5177\n",
      "Epoch 1516/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 19.4480 - val_loss: 176.8384\n",
      "Epoch 1517/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 20.2705 - val_loss: 176.0767\n",
      "Epoch 1518/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 19.4704 - val_loss: 177.0423\n",
      "Epoch 1519/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 18.9294 - val_loss: 178.2921\n",
      "Epoch 1520/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 18.7475 - val_loss: 181.9939\n",
      "Epoch 1521/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 25.4430 - val_loss: 185.8072\n",
      "Epoch 1522/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 20.2049 - val_loss: 173.2533\n",
      "Epoch 1523/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 19.3670 - val_loss: 175.5269\n",
      "Epoch 1524/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 18.0475 - val_loss: 173.2405\n",
      "Epoch 1525/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 20.6937 - val_loss: 173.9677\n",
      "Epoch 1526/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 20.0995 - val_loss: 187.2540\n",
      "Epoch 1527/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 18.8383 - val_loss: 165.7651\n",
      "Epoch 1528/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 21.1899 - val_loss: 194.2275\n",
      "Epoch 1529/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 24.0033 - val_loss: 170.3814\n",
      "Epoch 1530/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 18.1131 - val_loss: 168.8480\n",
      "Epoch 1531/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 19.0130 - val_loss: 169.4678\n",
      "Epoch 1532/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 19.9861 - val_loss: 176.3222\n",
      "Epoch 1533/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 21.1070 - val_loss: 181.0050\n",
      "Epoch 1534/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 18.8358 - val_loss: 174.1976\n",
      "Epoch 1535/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 20.9982 - val_loss: 161.1332\n",
      "Epoch 1536/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 19.0117 - val_loss: 163.9484\n",
      "Epoch 1537/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 19.8371 - val_loss: 177.5741\n",
      "Epoch 1538/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 18.7867 - val_loss: 161.7559\n",
      "Epoch 1539/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 20.5361 - val_loss: 168.3027\n",
      "Epoch 1540/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 22.2860 - val_loss: 189.3321\n",
      "Epoch 1541/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 56us/step - loss: 18.7956 - val_loss: 172.3080\n",
      "Epoch 1542/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 20.9911 - val_loss: 180.8649\n",
      "Epoch 1543/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 19.0883 - val_loss: 175.6952\n",
      "Epoch 1544/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 18.0647 - val_loss: 170.6312\n",
      "Epoch 1545/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 21.3368 - val_loss: 166.9482\n",
      "Epoch 1546/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 20.8024 - val_loss: 167.3780\n",
      "Epoch 1547/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 21.7759 - val_loss: 173.5306\n",
      "Epoch 1548/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 17.8129 - val_loss: 167.7528\n",
      "Epoch 1549/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 19.9129 - val_loss: 161.3875\n",
      "Epoch 1550/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 21.5777 - val_loss: 161.3715\n",
      "Epoch 1551/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 19.7224 - val_loss: 166.9419\n",
      "Epoch 1552/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 19.4255 - val_loss: 163.0151\n",
      "Epoch 1553/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 20.0852 - val_loss: 168.1681\n",
      "Epoch 1554/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 19.8939 - val_loss: 175.1851\n",
      "Epoch 1555/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 18.6509 - val_loss: 172.4458\n",
      "Epoch 1556/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 23.8182 - val_loss: 172.8171\n",
      "Epoch 1557/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 19.8577 - val_loss: 166.5627\n",
      "Epoch 1558/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 21.0594 - val_loss: 175.5783\n",
      "Epoch 1559/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 20.7971 - val_loss: 168.0509\n",
      "Epoch 1560/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 19.2495 - val_loss: 171.7805\n",
      "Epoch 1561/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 17.61 - 0s 51us/step - loss: 18.6424 - val_loss: 162.0848\n",
      "Epoch 1562/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 17.6154 - val_loss: 168.1966\n",
      "Epoch 1563/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 18.7164 - val_loss: 177.6506\n",
      "Epoch 1564/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 19.7728 - val_loss: 160.4168\n",
      "Epoch 1565/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 23.7967 - val_loss: 166.3085\n",
      "Epoch 1566/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 22.1332 - val_loss: 169.8687\n",
      "Epoch 1567/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 18.8314 - val_loss: 162.8495\n",
      "Epoch 1568/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 19.7265 - val_loss: 166.8422\n",
      "Epoch 1569/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 21.4314 - val_loss: 170.9586\n",
      "Epoch 1570/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 20.2609 - val_loss: 165.3799\n",
      "Epoch 1571/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 19.5454 - val_loss: 169.1252\n",
      "Epoch 1572/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 21.3801 - val_loss: 164.7789\n",
      "Epoch 1573/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 20.3066 - val_loss: 177.0887\n",
      "Epoch 1574/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 20.9132 - val_loss: 167.7267\n",
      "Epoch 1575/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 16.9899 - val_loss: 163.8965\n",
      "Epoch 1576/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 18.0627 - val_loss: 169.9636\n",
      "Epoch 1577/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 21.5401 - val_loss: 163.8456\n",
      "Epoch 1578/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 20.3132 - val_loss: 172.9699\n",
      "Epoch 1579/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 19.3782 - val_loss: 173.1524\n",
      "Epoch 1580/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 20.6502 - val_loss: 171.7616\n",
      "Epoch 1581/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 19.8042 - val_loss: 175.5339\n",
      "Epoch 1582/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 19.2961 - val_loss: 161.8572\n",
      "Epoch 1583/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 19.9193 - val_loss: 166.0524\n",
      "Epoch 1584/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 19.5027 - val_loss: 169.9503\n",
      "Epoch 1585/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 19.6213 - val_loss: 171.0141\n",
      "Epoch 1586/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 18.4762 - val_loss: 160.4780\n",
      "Epoch 1587/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 17.7243 - val_loss: 177.5486\n",
      "Epoch 1588/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 18.2042 - val_loss: 171.0972\n",
      "Epoch 1589/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 20.6589 - val_loss: 179.4177\n",
      "Epoch 1590/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 22.5559 - val_loss: 166.1548\n",
      "Epoch 1591/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 20.5745 - val_loss: 168.4181\n",
      "Epoch 1592/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 20.8588 - val_loss: 175.3552\n",
      "Epoch 1593/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 17.3236 - val_loss: 172.6513\n",
      "Epoch 1594/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 18.9494 - val_loss: 171.6433\n",
      "Epoch 1595/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 17.9809 - val_loss: 171.4339\n",
      "Epoch 1596/10000\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 18.5170 - val_loss: 167.8762\n",
      "Epoch 1597/10000\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 19.4555 - val_loss: 171.7225\n",
      "Epoch 1598/10000\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 23.8925 - val_loss: 173.3574\n",
      "Epoch 1599/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 18.9567 - val_loss: 176.3128\n",
      "Epoch 1600/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 17.6743 - val_loss: 174.0557\n",
      "Epoch 1601/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 20.3126 - val_loss: 176.1467\n",
      "Epoch 1602/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 19.1047 - val_loss: 161.2923\n",
      "Epoch 1603/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 19.7557 - val_loss: 163.1660\n",
      "Epoch 1604/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 18.1852 - val_loss: 170.4962\n",
      "Epoch 1605/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 18.2527 - val_loss: 164.9927\n",
      "Epoch 1606/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 19.9450 - val_loss: 179.2305\n",
      "Epoch 1607/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 18.6309 - val_loss: 161.1702\n",
      "Epoch 1608/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 20.5639 - val_loss: 176.5524\n",
      "Epoch 1609/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 21.2220 - val_loss: 179.7348\n",
      "Epoch 1610/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 19.3747 - val_loss: 167.0228\n",
      "Epoch 1611/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 21.2332 - val_loss: 165.3771\n",
      "Epoch 1612/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 18.3509 - val_loss: 165.6555\n",
      "Epoch 1613/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 18.8779 - val_loss: 176.3076\n",
      "Epoch 1614/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 18.4989 - val_loss: 178.6415\n",
      "Epoch 1615/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 19.0965 - val_loss: 167.7977\n",
      "Epoch 1616/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 17.1963 - val_loss: 175.1268\n",
      "Epoch 1617/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 18.9640 - val_loss: 170.9048\n",
      "Epoch 1618/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 19.5019 - val_loss: 172.4920\n",
      "Epoch 1619/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 19.4819 - val_loss: 180.3354\n",
      "Epoch 1620/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 19.7507 - val_loss: 168.9657\n",
      "Epoch 1621/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 20.1022 - val_loss: 173.4686\n",
      "Epoch 1622/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 20.6608 - val_loss: 171.5872\n",
      "Epoch 1623/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 20.5229 - val_loss: 171.0821\n",
      "Epoch 1624/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 19.4974 - val_loss: 179.9547\n",
      "Epoch 1625/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 17.0035 - val_loss: 178.3984\n",
      "Epoch 1626/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 17.9419 - val_loss: 167.3164\n",
      "Epoch 1627/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 19.2004 - val_loss: 168.7193\n",
      "Epoch 1628/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 17.6753 - val_loss: 169.9465\n",
      "Epoch 1629/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 19.9880 - val_loss: 182.4768\n",
      "Epoch 1630/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 23.2417 - val_loss: 174.9855\n",
      "Epoch 1631/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 18.5711 - val_loss: 174.3173\n",
      "Epoch 1632/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 17.4267 - val_loss: 178.7609\n",
      "Epoch 1633/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 19.1440 - val_loss: 171.1679\n",
      "Epoch 1634/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 20.7170 - val_loss: 163.6387\n",
      "Epoch 1635/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 20.9634 - val_loss: 184.3066\n",
      "Epoch 1636/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 22.0070 - val_loss: 173.0547\n",
      "Epoch 1637/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 20.0247 - val_loss: 172.0833\n",
      "Epoch 1638/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 19.0076 - val_loss: 165.1126\n",
      "Epoch 1639/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 19.7220 - val_loss: 183.0870\n",
      "Epoch 1640/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 19.0516 - val_loss: 174.7858\n",
      "Epoch 1641/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 18.6813 - val_loss: 169.1157\n",
      "Epoch 1642/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 17.7876 - val_loss: 170.5166\n",
      "Epoch 1643/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 19.2664 - val_loss: 169.2901\n",
      "Epoch 1644/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 17.7801 - val_loss: 168.4262\n",
      "Epoch 1645/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 18.4907 - val_loss: 171.9234\n",
      "Epoch 1646/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 18.9835 - val_loss: 170.8481\n",
      "Epoch 1647/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 19.0527 - val_loss: 171.1356\n",
      "Epoch 1648/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 19.8836 - val_loss: 171.5898\n",
      "Epoch 1649/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 21.8783 - val_loss: 170.2040\n",
      "Epoch 1650/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 19.3403 - val_loss: 172.1733\n",
      "Epoch 1651/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 19.1228 - val_loss: 166.1566\n",
      "Epoch 1652/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 17.2883 - val_loss: 166.5625\n",
      "Epoch 1653/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 19.0087 - val_loss: 159.3547\n",
      "Epoch 1654/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 21.3216 - val_loss: 169.7778\n",
      "Epoch 1655/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 19.8716 - val_loss: 172.4539\n",
      "Epoch 1656/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 20.9797 - val_loss: 162.4843\n",
      "Epoch 1657/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 18.6763 - val_loss: 177.7800\n",
      "Epoch 1658/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 17.8020 - val_loss: 165.1956\n",
      "Epoch 1659/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 20.9969 - val_loss: 161.5655\n",
      "Epoch 1660/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 19.6598 - val_loss: 168.3295\n",
      "Epoch 1661/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 21.2966 - val_loss: 172.4788\n",
      "Epoch 1662/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 21.8788 - val_loss: 167.5904\n",
      "Epoch 1663/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 18.6017 - val_loss: 174.0155\n",
      "Epoch 1664/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 21.5254 - val_loss: 172.4981\n",
      "Epoch 1665/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 19.5938 - val_loss: 176.7625\n",
      "Epoch 1666/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 18.5495 - val_loss: 168.6722\n",
      "Epoch 1667/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 18.4730 - val_loss: 165.5230\n",
      "Epoch 1668/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 18.3996 - val_loss: 172.6962\n",
      "Epoch 1669/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 19.9032 - val_loss: 162.3795\n",
      "Epoch 1670/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 21.9996 - val_loss: 169.1463\n",
      "Epoch 1671/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 19.7302 - val_loss: 168.9874\n",
      "Epoch 1672/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 19.8264 - val_loss: 172.8331\n",
      "Epoch 1673/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 19.2102 - val_loss: 173.8835\n",
      "Epoch 1674/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 17.1161 - val_loss: 165.5665\n",
      "Epoch 1675/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 20.4353 - val_loss: 166.5884\n",
      "Epoch 1676/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 18.1259 - val_loss: 170.1011\n",
      "Epoch 1677/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 17.4231 - val_loss: 162.9427\n",
      "Epoch 1678/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 19.9222 - val_loss: 165.2864\n",
      "Epoch 1679/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 18.7414 - val_loss: 176.5884\n",
      "Epoch 1680/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 19.7178 - val_loss: 157.2425\n",
      "Epoch 1681/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 18.6814 - val_loss: 160.7923\n",
      "Epoch 1682/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 17.8394 - val_loss: 177.0132\n",
      "Epoch 1683/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 21.5989 - val_loss: 167.1409\n",
      "Epoch 1684/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 18.1509 - val_loss: 177.2374\n",
      "Epoch 1685/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 18.3868 - val_loss: 163.4152\n",
      "Epoch 1686/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 18.5510 - val_loss: 165.8012\n",
      "Epoch 1687/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 19.2544 - val_loss: 158.3105\n",
      "Epoch 1688/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 18.1556 - val_loss: 164.2070\n",
      "Epoch 1689/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 18.8984 - val_loss: 170.8908\n",
      "Epoch 1690/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 20.9315 - val_loss: 169.2653\n",
      "Epoch 1691/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 19.3803 - val_loss: 161.6705\n",
      "Epoch 1692/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 18.8578 - val_loss: 163.5760\n",
      "Epoch 1693/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 18.3963 - val_loss: 167.3096\n",
      "Epoch 1694/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 19.1245 - val_loss: 179.0443\n",
      "Epoch 1695/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 22.5683 - val_loss: 161.6369\n",
      "Epoch 1696/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 20.2083 - val_loss: 176.4030\n",
      "Epoch 1697/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 17.4389 - val_loss: 158.9581\n",
      "Epoch 1698/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 19.4504 - val_loss: 170.3132\n",
      "Epoch 1699/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 19.5524 - val_loss: 173.0304\n",
      "Epoch 1700/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 18.3647 - val_loss: 167.1106\n",
      "Epoch 1701/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 17.0944 - val_loss: 174.4534\n",
      "Epoch 1702/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 17.9814 - val_loss: 167.3018\n",
      "Epoch 1703/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 18.1196 - val_loss: 171.0609\n",
      "Epoch 1704/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 18.2425 - val_loss: 166.9068\n",
      "Epoch 1705/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 20.5388 - val_loss: 164.9951\n",
      "Epoch 1706/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 19.1006 - val_loss: 175.1741\n",
      "Epoch 1707/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 20.5566 - val_loss: 172.8532\n",
      "Epoch 1708/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 19.5035 - val_loss: 163.9772\n",
      "Epoch 1709/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 18.1562 - val_loss: 174.2974\n",
      "Epoch 1710/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 20.5416 - val_loss: 173.1234\n",
      "Epoch 1711/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 19.0887 - val_loss: 173.1704\n",
      "Epoch 1712/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 18.2074 - val_loss: 171.6119\n",
      "Epoch 1713/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 19.1941 - val_loss: 165.0703\n",
      "Epoch 1714/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 16.7318 - val_loss: 175.9751\n",
      "Epoch 1715/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 18.4684 - val_loss: 172.4895\n",
      "Epoch 1716/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 20.7364 - val_loss: 170.7032\n",
      "Epoch 1717/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 18.2096 - val_loss: 175.0923\n",
      "Epoch 1718/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 16.2776 - val_loss: 174.3264\n",
      "Epoch 1719/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 18.2282 - val_loss: 166.5272\n",
      "Epoch 1720/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 19.4331 - val_loss: 177.0383\n",
      "Epoch 1721/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 17.9107 - val_loss: 172.3266\n",
      "Epoch 1722/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 20.5043 - val_loss: 169.0298\n",
      "Epoch 1723/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 20.9492 - val_loss: 172.1791\n",
      "Epoch 1724/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 18.0651 - val_loss: 174.8255\n",
      "Epoch 1725/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 17.7064 - val_loss: 172.6325\n",
      "Epoch 1726/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 18.2139 - val_loss: 170.9975\n",
      "Epoch 1727/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 19.0045 - val_loss: 174.5549\n",
      "Epoch 1728/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 18.5192 - val_loss: 166.0605\n",
      "Epoch 1729/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 17.4663 - val_loss: 178.6477\n",
      "Epoch 1730/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 17.8157 - val_loss: 179.0149\n",
      "Epoch 1731/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 19.0054 - val_loss: 162.1373\n",
      "Epoch 1732/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 20.0922 - val_loss: 167.6612\n",
      "Epoch 1733/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 19.7787 - val_loss: 177.0649\n",
      "Epoch 1734/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 20.7584 - val_loss: 170.9335\n",
      "Epoch 1735/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 21.0847 - val_loss: 173.1663\n",
      "Epoch 1736/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 18.7344 - val_loss: 165.2010\n",
      "Epoch 1737/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 19.0139 - val_loss: 176.9511\n",
      "Epoch 1738/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 19.3982 - val_loss: 176.9650\n",
      "Epoch 1739/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 19.2655 - val_loss: 178.5316\n",
      "Epoch 1740/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 20.2303 - val_loss: 184.0125\n",
      "Epoch 1741/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 20.4407 - val_loss: 165.6919\n",
      "Epoch 1742/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 18.4893 - val_loss: 168.1747\n",
      "Epoch 1743/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 17.5407 - val_loss: 167.3658\n",
      "Epoch 1744/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 18.2437 - val_loss: 174.1519\n",
      "Epoch 1745/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 18.9660 - val_loss: 166.7785\n",
      "Epoch 1746/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 16.4670 - val_loss: 173.5642\n",
      "Epoch 1747/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 18.2669 - val_loss: 168.0263\n",
      "Epoch 1748/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 19.8185 - val_loss: 181.5343\n",
      "Epoch 1749/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 18.3044 - val_loss: 179.0584\n",
      "Epoch 1750/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 18.0020 - val_loss: 171.2838\n",
      "Epoch 1751/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 17.9965 - val_loss: 163.5102\n",
      "Epoch 1752/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 18.8393 - val_loss: 165.5344\n",
      "Epoch 1753/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 20.5053 - val_loss: 171.2411\n",
      "Epoch 1754/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 22.6539 - val_loss: 175.9960\n",
      "Epoch 1755/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 19.3394 - val_loss: 166.3482\n",
      "Epoch 1756/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 18.4078 - val_loss: 177.4817\n",
      "Epoch 1757/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 17.3142 - val_loss: 166.0589\n",
      "Epoch 1758/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 17.5702 - val_loss: 167.2204\n",
      "Epoch 1759/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 17.6584 - val_loss: 164.7420\n",
      "Epoch 1760/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 19.2777 - val_loss: 170.1273\n",
      "Epoch 1761/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 18.4069 - val_loss: 167.1502\n",
      "Epoch 1762/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 18.4570 - val_loss: 172.6831\n",
      "Epoch 1763/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 19.4523 - val_loss: 164.7834\n",
      "Epoch 1764/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 19.7106 - val_loss: 164.0309\n",
      "Epoch 1765/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 17.5774 - val_loss: 169.8008\n",
      "Epoch 1766/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 17.1768 - val_loss: 161.7446\n",
      "Epoch 1767/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 17.9287 - val_loss: 165.8517\n",
      "Epoch 1768/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 19.9862 - val_loss: 166.8483\n",
      "Epoch 1769/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 16.5740 - val_loss: 164.5400\n",
      "Epoch 1770/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 19.0021 - val_loss: 165.6378\n",
      "Epoch 1771/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 18.1928 - val_loss: 168.6948\n",
      "Epoch 1772/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 21.1406 - val_loss: 165.9300\n",
      "Epoch 1773/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 21.2757 - val_loss: 165.8768\n",
      "Epoch 1774/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 20.1082 - val_loss: 165.1830\n",
      "Epoch 1775/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 18.7054 - val_loss: 179.9219\n",
      "Epoch 1776/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 19.7315 - val_loss: 162.3769\n",
      "Epoch 1777/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 17.0603 - val_loss: 174.9291\n",
      "Epoch 1778/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 17.4195 - val_loss: 164.3083\n",
      "Epoch 1779/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 19.5668 - val_loss: 164.0357\n",
      "Epoch 1780/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 17.9272 - val_loss: 166.8738\n",
      "Epoch 1781/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 20.5576 - val_loss: 166.9492\n",
      "Epoch 1782/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 18.8684 - val_loss: 160.2248\n",
      "Epoch 1783/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 18.8627 - val_loss: 164.0530\n",
      "Epoch 1784/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 16.7131 - val_loss: 171.4868\n",
      "Epoch 1785/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 18.0670 - val_loss: 166.6295\n",
      "Epoch 1786/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 18.2210 - val_loss: 169.9100\n",
      "Epoch 1787/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 17.9560 - val_loss: 183.8822\n",
      "Epoch 1788/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 19.7611 - val_loss: 163.0958\n",
      "Epoch 1789/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 17.9313 - val_loss: 179.2440\n",
      "Epoch 1790/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 18.2599 - val_loss: 169.2307\n",
      "Epoch 1791/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 18.0843 - val_loss: 168.6245\n",
      "Epoch 1792/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 19.5690 - val_loss: 165.9772\n",
      "Epoch 1793/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 17.8199 - val_loss: 168.1418\n",
      "Epoch 1794/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 19.4955 - val_loss: 164.4509\n",
      "Epoch 1795/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 20.5716 - val_loss: 166.5172\n",
      "Epoch 1796/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 19.5055 - val_loss: 171.6241\n",
      "Epoch 1797/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 18.8665 - val_loss: 166.8593\n",
      "Epoch 1798/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 18.3714 - val_loss: 167.9652\n",
      "Epoch 1799/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 15.7662 - val_loss: 172.5668\n",
      "Epoch 1800/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 19.0939 - val_loss: 170.6961\n",
      "Epoch 1801/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 17.7881 - val_loss: 169.9199\n",
      "Epoch 1802/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 18.8109 - val_loss: 179.0971\n",
      "Epoch 1803/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 19.1612 - val_loss: 166.7189\n",
      "Epoch 1804/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 18.8116 - val_loss: 179.3099\n",
      "Epoch 1805/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 20.5912 - val_loss: 177.5739\n",
      "Epoch 1806/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 18.6086 - val_loss: 180.5990\n",
      "Epoch 1807/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 16.5742 - val_loss: 174.1691\n",
      "Epoch 1808/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 15.6434 - val_loss: 176.3984\n",
      "Epoch 1809/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 18.9308 - val_loss: 172.3365\n",
      "Epoch 1810/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 18.0670 - val_loss: 172.0207\n",
      "Epoch 1811/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 17.4244 - val_loss: 172.8781\n",
      "Epoch 1812/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 17.6321 - val_loss: 177.0030\n",
      "Epoch 1813/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 18.2047 - val_loss: 175.6663\n",
      "Epoch 1814/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 19.4436 - val_loss: 174.6984\n",
      "Epoch 1815/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 18.2884 - val_loss: 167.9807\n",
      "Epoch 1816/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 19.4665 - val_loss: 169.3255\n",
      "Epoch 1817/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 17.6353 - val_loss: 163.6979\n",
      "Epoch 1818/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 20.9050 - val_loss: 170.1458\n",
      "Epoch 1819/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 17.4086 - val_loss: 172.9326\n",
      "Epoch 1820/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 16.9710 - val_loss: 168.5431\n",
      "Epoch 1821/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 17.7690 - val_loss: 166.2627\n",
      "Epoch 1822/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 20.2947 - val_loss: 175.2371\n",
      "Epoch 1823/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.0172 - val_loss: 174.5936\n",
      "Epoch 1824/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 19.4948 - val_loss: 171.5098\n",
      "Epoch 1825/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 20.6875 - val_loss: 162.9838\n",
      "Epoch 1826/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 19.2303 - val_loss: 168.5016\n",
      "Epoch 1827/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 16.4912 - val_loss: 168.1459\n",
      "Epoch 1828/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 17.1386 - val_loss: 167.5230\n",
      "Epoch 1829/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 16.1152 - val_loss: 177.2946\n",
      "Epoch 1830/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 16.4537 - val_loss: 173.3432\n",
      "Epoch 1831/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 17.1771 - val_loss: 171.3499\n",
      "Epoch 1832/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 18.7803 - val_loss: 173.6207\n",
      "Epoch 1833/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 18.9727 - val_loss: 178.6189\n",
      "Epoch 1834/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 18.3764 - val_loss: 173.7447\n",
      "Epoch 1835/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 18.1562 - val_loss: 168.5323\n",
      "Epoch 1836/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 17.0318 - val_loss: 169.3261\n",
      "Epoch 1837/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 19.6453 - val_loss: 170.9573\n",
      "Epoch 1838/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 17.1432 - val_loss: 177.5951\n",
      "Epoch 1839/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 17.0741 - val_loss: 170.9132\n",
      "Epoch 1840/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 17.5882 - val_loss: 168.4737\n",
      "Epoch 1841/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 15.5277 - val_loss: 169.2433\n",
      "Epoch 1842/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 18.0649 - val_loss: 180.4859\n",
      "Epoch 1843/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 18.1011 - val_loss: 172.3107\n",
      "Epoch 1844/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 19.8279 - val_loss: 172.2856\n",
      "Epoch 1845/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 18.9910 - val_loss: 176.6302\n",
      "Epoch 1846/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 17.8033 - val_loss: 171.2623\n",
      "Epoch 1847/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 16.7443 - val_loss: 170.7527\n",
      "Epoch 1848/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 18.4149 - val_loss: 166.3833\n",
      "Epoch 1849/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 19.3799 - val_loss: 171.3831\n",
      "Epoch 1850/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 21.7714 - val_loss: 171.3944\n",
      "Epoch 1851/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 18.0284 - val_loss: 175.4954\n",
      "Epoch 1852/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 18.4925 - val_loss: 167.0621\n",
      "Epoch 1853/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 19.7301 - val_loss: 165.9203\n",
      "Epoch 1854/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 18.1913 - val_loss: 169.6844\n",
      "Epoch 1855/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 18.4281 - val_loss: 177.0462\n",
      "Epoch 1856/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 19.2514 - val_loss: 183.8754\n",
      "Epoch 1857/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 18.2618 - val_loss: 178.5159\n",
      "Epoch 1858/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 17.4290 - val_loss: 170.7747\n",
      "Epoch 1859/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 17.1583 - val_loss: 172.6792\n",
      "Epoch 1860/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 16.6731 - val_loss: 172.1512\n",
      "Epoch 1861/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 16.4689 - val_loss: 168.9997\n",
      "Epoch 1862/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 18.7874 - val_loss: 169.5547\n",
      "Epoch 1863/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 19.5101 - val_loss: 175.2004\n",
      "Epoch 1864/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 16.8691 - val_loss: 168.7232\n",
      "Epoch 1865/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 17.0870 - val_loss: 167.0759\n",
      "Epoch 1866/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 17.4096 - val_loss: 167.3581\n",
      "Epoch 1867/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 17.8846 - val_loss: 175.3249\n",
      "Epoch 1868/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 20.6025 - val_loss: 169.7514\n",
      "Epoch 1869/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 17.4756 - val_loss: 170.0191\n",
      "Epoch 1870/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 16.0721 - val_loss: 176.2460\n",
      "Epoch 1871/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 18.4222 - val_loss: 171.7294\n",
      "Epoch 1872/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 17.2679 - val_loss: 170.6343\n",
      "Epoch 1873/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 19.9259 - val_loss: 175.1154\n",
      "Epoch 1874/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 18.7607 - val_loss: 178.2969\n",
      "Epoch 1875/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 16.9971 - val_loss: 170.1326\n",
      "Epoch 1876/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 15.9964 - val_loss: 167.7026\n",
      "Epoch 1877/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 16.8973 - val_loss: 183.9057\n",
      "Epoch 1878/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 20.6812 - val_loss: 173.5102\n",
      "Epoch 1879/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 16.5773 - val_loss: 179.9266\n",
      "Epoch 1880/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 19.0871 - val_loss: 179.7000\n",
      "Epoch 1881/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 18.4144 - val_loss: 172.5489\n",
      "Epoch 1882/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 18.0103 - val_loss: 180.2449\n",
      "Epoch 1883/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 17.2030 - val_loss: 170.8047\n",
      "Epoch 1884/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 18.3700 - val_loss: 183.7643\n",
      "Epoch 1885/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 18.2251 - val_loss: 177.6279\n",
      "Epoch 1886/10000\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 15.6594 - val_loss: 178.1580\n",
      "Epoch 1887/10000\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 18.1860 - val_loss: 174.9983\n",
      "Epoch 1888/10000\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 21.0375 - val_loss: 166.2875\n",
      "Epoch 1889/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 17.9740 - val_loss: 170.4137\n",
      "Epoch 1890/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 17.6227 - val_loss: 158.4347\n",
      "Epoch 1891/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 15.2640 - val_loss: 173.1372\n",
      "Epoch 1892/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 18.9382 - val_loss: 166.3574\n",
      "Epoch 1893/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 17.2406 - val_loss: 167.7776\n",
      "Epoch 1894/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 19.0660 - val_loss: 170.9025\n",
      "Epoch 1895/10000\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 17.2763 - val_loss: 165.6543\n",
      "Epoch 1896/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 18.3235 - val_loss: 177.0688\n",
      "Epoch 1897/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 18.8851 - val_loss: 173.0016\n",
      "Epoch 1898/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 20.8132 - val_loss: 157.8902\n",
      "Epoch 1899/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 17.0055 - val_loss: 166.6086\n",
      "Epoch 1900/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 19.4471 - val_loss: 175.6737\n",
      "Epoch 1901/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 20.7923 - val_loss: 158.7857\n",
      "Epoch 1902/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 19.4431 - val_loss: 179.2271\n",
      "Epoch 1903/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 16.8963 - val_loss: 169.7556\n",
      "Epoch 1904/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 14.8009 - val_loss: 170.5497\n",
      "Epoch 1905/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 15.3026 - val_loss: 166.4575\n",
      "Epoch 1906/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 60us/step - loss: 17.0762 - val_loss: 163.2610\n",
      "Epoch 1907/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 16.8917 - val_loss: 172.0604\n",
      "Epoch 1908/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 19.0364 - val_loss: 172.7750\n",
      "Epoch 1909/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 15.8503 - val_loss: 161.8375\n",
      "Epoch 1910/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 18.1954 - val_loss: 177.0452\n",
      "Epoch 1911/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 17.6163 - val_loss: 166.1367\n",
      "Epoch 1912/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 17.7809 - val_loss: 161.6923\n",
      "Epoch 1913/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 18.8074 - val_loss: 176.1724\n",
      "Epoch 1914/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 18.4573 - val_loss: 162.5202\n",
      "Epoch 1915/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 17.7655 - val_loss: 167.7613\n",
      "Epoch 1916/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 18.2483 - val_loss: 164.1055\n",
      "Epoch 1917/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 18.3774 - val_loss: 165.7004\n",
      "Epoch 1918/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 18.1928 - val_loss: 178.5375\n",
      "Epoch 1919/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 16.6954 - val_loss: 167.5095\n",
      "Epoch 1920/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 18.0359 - val_loss: 171.2938\n",
      "Epoch 1921/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 17.2422 - val_loss: 164.7730\n",
      "Epoch 1922/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 19.7455 - val_loss: 170.6812\n",
      "Epoch 1923/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 17.3998 - val_loss: 176.9134\n",
      "Epoch 1924/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 17.5583 - val_loss: 179.4529\n",
      "Epoch 1925/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 17.8525 - val_loss: 168.4862\n",
      "Epoch 1926/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 18.1169 - val_loss: 180.4817\n",
      "Epoch 1927/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 21.7606 - val_loss: 167.5827\n",
      "Epoch 1928/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 15.2684 - val_loss: 185.8803\n",
      "Epoch 1929/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 16.9926 - val_loss: 166.3883\n",
      "Epoch 1930/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 19.3753 - val_loss: 165.6663\n",
      "Epoch 1931/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 19.1227 - val_loss: 174.2481\n",
      "Epoch 1932/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 18.0158 - val_loss: 166.4205\n",
      "Epoch 1933/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 16.3242 - val_loss: 179.6843\n",
      "Epoch 1934/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 18.1484 - val_loss: 176.0092\n",
      "Epoch 1935/10000\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 17.8275 - val_loss: 175.6072\n",
      "Epoch 1936/10000\n",
      "8000/8000 [==============================] - 1s 134us/step - loss: 16.0878 - val_loss: 170.9249\n",
      "Epoch 1937/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 19.6790 - val_loss: 176.1319\n",
      "Epoch 1938/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 16.7145 - val_loss: 174.4890\n",
      "Epoch 1939/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 18.8805 - val_loss: 169.7962\n",
      "Epoch 1940/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 15.9338 - val_loss: 165.3375\n",
      "Epoch 1941/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 15.9294 - val_loss: 178.5256\n",
      "Epoch 1942/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 16.6053 - val_loss: 168.4209\n",
      "Epoch 1943/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 18.5206 - val_loss: 177.8781\n",
      "Epoch 1944/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 19.4362 - val_loss: 176.9428\n",
      "Epoch 1945/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 17.7628 - val_loss: 167.5238\n",
      "Epoch 1946/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 17.0059 - val_loss: 163.5919\n",
      "Epoch 1947/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 15.6538 - val_loss: 167.5196\n",
      "Epoch 1948/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 16.6937 - val_loss: 171.6581\n",
      "Epoch 1949/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 18.0652 - val_loss: 175.0488\n",
      "Epoch 1950/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 19.5835 - val_loss: 170.4794\n",
      "Epoch 1951/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 16.8974 - val_loss: 169.7277\n",
      "Epoch 1952/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 18.4333 - val_loss: 161.9423\n",
      "Epoch 1953/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 16.1588 - val_loss: 179.5534\n",
      "Epoch 1954/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 19.2226 - val_loss: 176.2091\n",
      "Epoch 1955/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 19.8733 - val_loss: 180.7805\n",
      "Epoch 1956/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 17.3765 - val_loss: 172.7370\n",
      "Epoch 1957/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 19.6205 - val_loss: 167.1780\n",
      "Epoch 1958/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 18.0767 - val_loss: 169.7297\n",
      "Epoch 1959/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 17.0419 - val_loss: 169.3813\n",
      "Epoch 1960/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 18.3749 - val_loss: 183.7612\n",
      "Epoch 1961/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 16.5852 - val_loss: 181.5583\n",
      "Epoch 1962/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 17.3173 - val_loss: 169.9643\n",
      "Epoch 1963/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 18.0448 - val_loss: 170.8004\n",
      "Epoch 1964/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 15.0520 - val_loss: 178.9577\n",
      "Epoch 1965/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 18.0581 - val_loss: 174.7883\n",
      "Epoch 1966/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 16.8305 - val_loss: 172.9609\n",
      "Epoch 1967/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 17.5180 - val_loss: 177.0311\n",
      "Epoch 1968/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 16.3704 - val_loss: 165.8019\n",
      "Epoch 1969/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 17.2323 - val_loss: 177.6852\n",
      "Epoch 1970/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 18.2987 - val_loss: 166.8398\n",
      "Epoch 1971/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 18.3237 - val_loss: 181.3987\n",
      "Epoch 1972/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 16.9397 - val_loss: 157.4364\n",
      "Epoch 1973/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 16.8277 - val_loss: 174.1952\n",
      "Epoch 1974/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 16.7665 - val_loss: 173.4771\n",
      "Epoch 1975/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 19.9210 - val_loss: 171.2495\n",
      "Epoch 1976/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 17.4070 - val_loss: 170.0633\n",
      "Epoch 1977/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 18.4123 - val_loss: 165.0079\n",
      "Epoch 1978/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 16.7561 - val_loss: 167.4865\n",
      "Epoch 1979/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 69us/step - loss: 17.5959 - val_loss: 166.9347\n",
      "Epoch 1980/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 17.6736 - val_loss: 174.5589\n",
      "Epoch 1981/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 16.0542 - val_loss: 176.0440\n",
      "Epoch 1982/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 18.8059 - val_loss: 169.1685\n",
      "Epoch 1983/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 16.6263 - val_loss: 173.5193\n",
      "Epoch 1984/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 17.6732 - val_loss: 167.4846\n",
      "Epoch 1985/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 18.0909 - val_loss: 181.8466\n",
      "Epoch 1986/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 18.8925 - val_loss: 174.4881\n",
      "Epoch 1987/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 17.3711 - val_loss: 174.2392\n",
      "Epoch 1988/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 18.1210 - val_loss: 171.2588\n",
      "Epoch 1989/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 17.4859 - val_loss: 168.7899\n",
      "Epoch 1990/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 17.6653 - val_loss: 172.8100\n",
      "Epoch 1991/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 18.8386 - val_loss: 175.3432\n",
      "Epoch 1992/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 15.7173 - val_loss: 167.8392\n",
      "Epoch 1993/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 18.3817 - val_loss: 173.5389\n",
      "Epoch 1994/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 17.6420 - val_loss: 176.1664\n",
      "Epoch 1995/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 17.3308 - val_loss: 170.7074\n",
      "Epoch 1996/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 17.2506 - val_loss: 164.3034\n",
      "Epoch 1997/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 18.4780 - val_loss: 169.1508\n",
      "Epoch 1998/10000\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 16.2052 - val_loss: 167.7152\n",
      "Epoch 1999/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 15.9421 - val_loss: 176.4656\n",
      "Epoch 2000/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 16.9376 - val_loss: 178.6360\n",
      "Epoch 2001/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 18.5235 - val_loss: 177.6857\n",
      "Epoch 2002/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 16.9351 - val_loss: 170.3707\n",
      "Epoch 2003/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 16.4860 - val_loss: 169.0282\n",
      "Epoch 2004/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 16.1618 - val_loss: 167.0858\n",
      "Epoch 2005/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 18.1223 - val_loss: 176.0686\n",
      "Epoch 2006/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 16.4569 - val_loss: 175.2943\n",
      "Epoch 2007/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 16.4341 - val_loss: 172.6991\n",
      "Epoch 2008/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 18.2854 - val_loss: 167.1062\n",
      "Epoch 2009/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 18.1123 - val_loss: 172.0026\n",
      "Epoch 2010/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 18.1550 - val_loss: 180.4925\n",
      "Epoch 2011/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 16.1695 - val_loss: 176.6275\n",
      "Epoch 2012/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 17.9979 - val_loss: 171.1258\n",
      "Epoch 2013/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 19.9425 - val_loss: 166.6386\n",
      "Epoch 2014/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 14.6378 - val_loss: 173.2769\n",
      "Epoch 2015/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 15.8930 - val_loss: 170.3209\n",
      "Epoch 2016/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 20.8277 - val_loss: 169.4327\n",
      "Epoch 2017/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 17.7563 - val_loss: 164.9791\n",
      "Epoch 2018/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 18.3164 - val_loss: 166.9220\n",
      "Epoch 2019/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 15.5103 - val_loss: 164.1694\n",
      "Epoch 2020/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 16.2751 - val_loss: 163.0510\n",
      "Epoch 2021/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 17.1343 - val_loss: 171.0874\n",
      "Epoch 2022/10000\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 17.7128 - val_loss: 175.9593\n",
      "Epoch 2023/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 17.5668 - val_loss: 167.3382\n",
      "Epoch 2024/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 15.4550 - val_loss: 161.7637\n",
      "Epoch 2025/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 16.8401 - val_loss: 169.2731\n",
      "Epoch 2026/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 17.0926 - val_loss: 175.2555\n",
      "Epoch 2027/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 18.7378 - val_loss: 167.2045\n",
      "Epoch 2028/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 17.0043 - val_loss: 166.9521\n",
      "Epoch 2029/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 15.8099 - val_loss: 171.6203\n",
      "Epoch 2030/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 19.8626 - val_loss: 170.2033\n",
      "Epoch 2031/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 17.4235 - val_loss: 173.4085\n",
      "Epoch 2032/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 17.0899 - val_loss: 181.1137\n",
      "Epoch 2033/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 16.0565 - val_loss: 169.5068\n",
      "Epoch 2034/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 16.7272 - val_loss: 170.7666\n",
      "Epoch 2035/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 17.1016 - val_loss: 165.0866\n",
      "Epoch 2036/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 16.9320 - val_loss: 171.8732\n",
      "Epoch 2037/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 18.3964 - val_loss: 172.1280\n",
      "Epoch 2038/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 18.6645 - val_loss: 169.3801\n",
      "Epoch 2039/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 16.2104 - val_loss: 165.4560\n",
      "Epoch 2040/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 14.8955 - val_loss: 178.2951\n",
      "Epoch 2041/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 16.3458 - val_loss: 177.6285\n",
      "Epoch 2042/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 16.5146 - val_loss: 164.6608\n",
      "Epoch 2043/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 17.6193 - val_loss: 175.8032\n",
      "Epoch 2044/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 17.1045 - val_loss: 168.6424\n",
      "Epoch 2045/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 19.2278 - val_loss: 168.5768\n",
      "Epoch 2046/10000\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 16.0234 - val_loss: 171.0911\n",
      "Epoch 2047/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 18.1495 - val_loss: 166.8801\n",
      "Epoch 2048/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 18.0532 - val_loss: 176.6370\n",
      "Epoch 2049/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 16.6659 - val_loss: 169.7126\n",
      "Epoch 2050/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 15.6451 - val_loss: 170.1414\n",
      "Epoch 2051/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 20.0980 - val_loss: 170.8683\n",
      "Epoch 2052/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 75us/step - loss: 15.8582 - val_loss: 169.2499\n",
      "Epoch 2053/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 17.4788 - val_loss: 159.8885\n",
      "Epoch 2054/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 15.3757 - val_loss: 163.3181\n",
      "Epoch 2055/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 17.0541 - val_loss: 172.1478\n",
      "Epoch 2056/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 16.5663 - val_loss: 163.4628\n",
      "Epoch 2057/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 16.1050 - val_loss: 165.6173\n",
      "Epoch 2058/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 15.5553 - val_loss: 158.4865\n",
      "Epoch 2059/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 15.6202 - val_loss: 177.5776\n",
      "Epoch 2060/10000\n",
      "8000/8000 [==============================] - 1s 142us/step - loss: 20.2502 - val_loss: 168.7851\n",
      "Epoch 2061/10000\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 16.7587 - val_loss: 174.9767\n",
      "Epoch 2062/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 17.2981 - val_loss: 171.5308\n",
      "Epoch 2063/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 19.1846 - val_loss: 166.0493\n",
      "Epoch 2064/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 20.1550 - val_loss: 173.6585\n",
      "Epoch 2065/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 17.9144 - val_loss: 170.5135\n",
      "Epoch 2066/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 17.6296 - val_loss: 170.7012\n",
      "Epoch 2067/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 16.7493 - val_loss: 178.9213\n",
      "Epoch 2068/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 16.4052 - val_loss: 170.7895\n",
      "Epoch 2069/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 16.0079 - val_loss: 165.7780\n",
      "Epoch 2070/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 17.2058 - val_loss: 162.6115\n",
      "Epoch 2071/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 17.1446 - val_loss: 163.6459\n",
      "Epoch 2072/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 17.2240 - val_loss: 167.0973\n",
      "Epoch 2073/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 15.6708 - val_loss: 174.0576\n",
      "Epoch 2074/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 17.1525 - val_loss: 177.5265\n",
      "Epoch 2075/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 18.3102 - val_loss: 166.8274\n",
      "Epoch 2076/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 19.4310 - val_loss: 167.2030\n",
      "Epoch 2077/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 17.1720 - val_loss: 170.3057\n",
      "Epoch 2078/10000\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 19.5738 - val_loss: 173.7201\n",
      "Epoch 2079/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 16.3881 - val_loss: 170.4774\n",
      "Epoch 2080/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 15.8407 - val_loss: 168.2739\n",
      "Epoch 2081/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 18.2107 - val_loss: 203.6508\n",
      "Epoch 2082/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 19.8921 - val_loss: 167.9470\n",
      "Epoch 2083/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 17.4984 - val_loss: 164.6740\n",
      "Epoch 2084/10000\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 16.4054 - val_loss: 172.0283\n",
      "Epoch 2085/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 18.7335 - val_loss: 164.2144\n",
      "Epoch 2086/10000\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 16.6024 - val_loss: 171.3334\n",
      "Epoch 2087/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 14.9720 - val_loss: 167.8264\n",
      "Epoch 2088/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 17.0490 - val_loss: 169.4034\n",
      "Epoch 2089/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 15.9637 - val_loss: 168.2524\n",
      "Epoch 2090/10000\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 15.5350 - val_loss: 157.4909\n",
      "Epoch 2091/10000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 17.1387 - val_loss: 167.4821\n",
      "Epoch 2092/10000\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 18.2097 - val_loss: 170.7673\n",
      "Epoch 2093/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 15.6426 - val_loss: 163.4259\n",
      "Epoch 2094/10000\n",
      "8000/8000 [==============================] - 1s 134us/step - loss: 16.4769 - val_loss: 167.1693\n",
      "Epoch 2095/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 16.4245 - val_loss: 162.7361\n",
      "Epoch 2096/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 15.9322 - val_loss: 168.7794\n",
      "Epoch 2097/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 17.1090 - val_loss: 173.1155\n",
      "Epoch 2098/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 15.6393 - val_loss: 175.2153\n",
      "Epoch 2099/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 15.5517 - val_loss: 172.6486\n",
      "Epoch 2100/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 16.7066 - val_loss: 169.8935\n",
      "Epoch 2101/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 15.8689 - val_loss: 165.9893\n",
      "Epoch 2102/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 17.8395 - val_loss: 165.9281\n",
      "Epoch 2103/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 16.3579 - val_loss: 173.1642\n",
      "Epoch 2104/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.9518 - val_loss: 172.1006\n",
      "Epoch 2105/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 15.2755 - val_loss: 171.4060\n",
      "Epoch 2106/10000\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 16.5487 - val_loss: 169.3908\n",
      "Epoch 2107/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 14.5121 - val_loss: 175.7254\n",
      "Epoch 2108/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 15.6785 - val_loss: 167.6198\n",
      "Epoch 2109/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 16.0704 - val_loss: 178.6171\n",
      "Epoch 2110/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 17.3583 - val_loss: 170.6391\n",
      "Epoch 2111/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 17.8449 - val_loss: 177.0302\n",
      "Epoch 2112/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 16.3851 - val_loss: 178.0316\n",
      "Epoch 2113/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 15.8827 - val_loss: 167.8420\n",
      "Epoch 2114/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 18.9004 - val_loss: 172.3935\n",
      "Epoch 2115/10000\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 18.0821 - val_loss: 171.4365\n",
      "Epoch 2116/10000\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 15.7432 - val_loss: 163.5238\n",
      "Epoch 2117/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 16.3835 - val_loss: 170.9810\n",
      "Epoch 2118/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 17.3305 - val_loss: 180.5555\n",
      "Epoch 2119/10000\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 17.9940 - val_loss: 175.3880\n",
      "Epoch 2120/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 15.6408 - val_loss: 170.4808\n",
      "Epoch 2121/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 14.9396 - val_loss: 165.4736\n",
      "Epoch 2122/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 15.7845 - val_loss: 167.9565\n",
      "Epoch 2123/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 15.5293 - val_loss: 167.3890\n",
      "Epoch 2124/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 18.8817 - val_loss: 171.9653\n",
      "Epoch 2125/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 96us/step - loss: 16.2058 - val_loss: 169.9715\n",
      "Epoch 2126/10000\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 14.3041 - val_loss: 179.5786\n",
      "Epoch 2127/10000\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 19.2286 - val_loss: 177.7203\n",
      "Epoch 2128/10000\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 17.8900 - val_loss: 172.1263\n",
      "Epoch 2129/10000\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 19.6615 - val_loss: 163.3150\n",
      "Epoch 2130/10000\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 16.3519 - val_loss: 171.2696\n",
      "Epoch 2131/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 14.3532 - val_loss: 175.0411\n",
      "Epoch 2132/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 17.2638 - val_loss: 165.1358\n",
      "Epoch 2133/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 17.0387 - val_loss: 166.6484\n",
      "Epoch 2134/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 18.5352 - val_loss: 164.0208\n",
      "Epoch 2135/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 16.3547 - val_loss: 172.2158\n",
      "Epoch 2136/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 17.3793 - val_loss: 164.5190\n",
      "Epoch 2137/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 18.7773 - val_loss: 179.2893\n",
      "Epoch 2138/10000\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 17.2254 - val_loss: 162.8540\n",
      "Epoch 2139/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 15.4141 - val_loss: 170.6739\n",
      "Epoch 2140/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 15.5841 - val_loss: 170.8488\n",
      "Epoch 2141/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 19.1882 - val_loss: 170.7488\n",
      "Epoch 2142/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 17.1672 - val_loss: 168.5536\n",
      "Epoch 2143/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 18.1156 - val_loss: 169.9320\n",
      "Epoch 2144/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 15.6918 - val_loss: 167.3938\n",
      "Epoch 2145/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 17.6142 - val_loss: 176.9563\n",
      "Epoch 2146/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 17.2531 - val_loss: 177.3100\n",
      "Epoch 2147/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 16.6918 - val_loss: 163.5447\n",
      "Epoch 2148/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 16.4428 - val_loss: 167.2096\n",
      "Epoch 2149/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.6336 - val_loss: 167.6443\n",
      "Epoch 2150/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 17.2635 - val_loss: 172.2207\n",
      "Epoch 2151/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 19.6527 - val_loss: 162.4436\n",
      "Epoch 2152/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 16.3293 - val_loss: 177.6028\n",
      "Epoch 2153/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 15.2837 - val_loss: 167.0429\n",
      "Epoch 2154/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 15.7315 - val_loss: 176.3890\n",
      "Epoch 2155/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 17.4373 - val_loss: 167.6316\n",
      "Epoch 2156/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 17.6435 - val_loss: 165.1013\n",
      "Epoch 2157/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 16.0691 - val_loss: 165.2684\n",
      "Epoch 2158/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.2382 - val_loss: 172.0607\n",
      "Epoch 2159/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 17.0130 - val_loss: 170.9676\n",
      "Epoch 2160/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 18.8352 - val_loss: 178.5133\n",
      "Epoch 2161/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.7510 - val_loss: 173.7343\n",
      "Epoch 2162/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.7589 - val_loss: 166.2074\n",
      "Epoch 2163/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.4140 - val_loss: 165.1010\n",
      "Epoch 2164/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 15.9957 - val_loss: 172.0754\n",
      "Epoch 2165/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 18.1113 - val_loss: 176.1171\n",
      "Epoch 2166/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 18.2032 - val_loss: 164.1157\n",
      "Epoch 2167/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.4362 - val_loss: 173.0743\n",
      "Epoch 2168/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.9386 - val_loss: 165.4200\n",
      "Epoch 2169/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 14.6884 - val_loss: 166.0941\n",
      "Epoch 2170/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.9860 - val_loss: 176.4874\n",
      "Epoch 2171/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 15.7013 - val_loss: 166.6781\n",
      "Epoch 2172/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.0603 - val_loss: 169.2528\n",
      "Epoch 2173/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.7012 - val_loss: 167.8777\n",
      "Epoch 2174/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.2787 - val_loss: 162.1742\n",
      "Epoch 2175/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.7526 - val_loss: 165.2454\n",
      "Epoch 2176/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.1749 - val_loss: 165.8924\n",
      "Epoch 2177/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.6285 - val_loss: 171.3485\n",
      "Epoch 2178/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.1110 - val_loss: 164.0597\n",
      "Epoch 2179/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 18.4512 - val_loss: 174.1293\n",
      "Epoch 2180/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 19.7666 - val_loss: 170.9479\n",
      "Epoch 2181/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 18.0451 - val_loss: 177.2815\n",
      "Epoch 2182/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 16.4727 - val_loss: 170.9944\n",
      "Epoch 2183/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 16.7265 - val_loss: 166.6895\n",
      "Epoch 2184/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.9423 - val_loss: 170.4628\n",
      "Epoch 2185/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.3157 - val_loss: 173.6277\n",
      "Epoch 2186/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.6844 - val_loss: 168.0067\n",
      "Epoch 2187/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.9862 - val_loss: 170.1316\n",
      "Epoch 2188/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 17.0945 - val_loss: 164.5303\n",
      "Epoch 2189/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.4324 - val_loss: 166.3965\n",
      "Epoch 2190/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 18.4573 - val_loss: 166.6434\n",
      "Epoch 2191/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.3074 - val_loss: 171.2290\n",
      "Epoch 2192/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.8853 - val_loss: 166.8018\n",
      "Epoch 2193/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.8424 - val_loss: 164.3408\n",
      "Epoch 2194/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.2102 - val_loss: 165.5017\n",
      "Epoch 2195/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 19.6357 - val_loss: 170.5690\n",
      "Epoch 2196/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 19.1015 - val_loss: 171.9478\n",
      "Epoch 2197/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 19.1294 - val_loss: 164.8865\n",
      "Epoch 2198/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.7044 - val_loss: 167.9439\n",
      "Epoch 2199/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.9520 - val_loss: 167.4454\n",
      "Epoch 2200/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.7649 - val_loss: 166.0294\n",
      "Epoch 2201/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 17.1365 - val_loss: 172.0381\n",
      "Epoch 2202/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 18.5466 - val_loss: 161.9423\n",
      "Epoch 2203/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.4764 - val_loss: 170.2640\n",
      "Epoch 2204/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.6701 - val_loss: 170.2540\n",
      "Epoch 2205/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.6647 - val_loss: 166.8710\n",
      "Epoch 2206/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 15.6204 - val_loss: 160.9335\n",
      "Epoch 2207/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 14.4544 - val_loss: 168.6463\n",
      "Epoch 2208/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 14.6311 - val_loss: 160.8411\n",
      "Epoch 2209/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 16.6980 - val_loss: 167.9287\n",
      "Epoch 2210/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 18.6062 - val_loss: 159.9470\n",
      "Epoch 2211/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 18.7416 - val_loss: 169.3680\n",
      "Epoch 2212/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 16.0851 - val_loss: 169.5244\n",
      "Epoch 2213/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 16.1979 - val_loss: 162.8877\n",
      "Epoch 2214/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 15.0645 - val_loss: 163.6281\n",
      "Epoch 2215/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 15.0406 - val_loss: 168.0975\n",
      "Epoch 2216/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 19.0049 - val_loss: 168.2823\n",
      "Epoch 2217/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 15.9564 - val_loss: 163.9089\n",
      "Epoch 2218/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 15.2163 - val_loss: 166.8782\n",
      "Epoch 2219/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 15.4145 - val_loss: 166.1593\n",
      "Epoch 2220/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 16.3039 - val_loss: 167.2410\n",
      "Epoch 2221/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 18.2694 - val_loss: 171.4926\n",
      "Epoch 2222/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 15.7661 - val_loss: 177.1269\n",
      "Epoch 2223/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.6373 - val_loss: 162.9052\n",
      "Epoch 2224/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.5605 - val_loss: 162.1001\n",
      "Epoch 2225/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.9933 - val_loss: 162.1786\n",
      "Epoch 2226/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.4716 - val_loss: 163.5152\n",
      "Epoch 2227/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.8087 - val_loss: 166.6621\n",
      "Epoch 2228/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.5126 - val_loss: 160.8923\n",
      "Epoch 2229/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.8653 - val_loss: 170.6590\n",
      "Epoch 2230/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.8334 - val_loss: 168.8196\n",
      "Epoch 2231/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.3953 - val_loss: 164.0499\n",
      "Epoch 2232/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 17.2256 - val_loss: 165.2550\n",
      "Epoch 2233/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.4828 - val_loss: 161.2420\n",
      "Epoch 2234/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 17.7798 - val_loss: 169.6322\n",
      "Epoch 2235/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 18.3868 - val_loss: 166.8383\n",
      "Epoch 2236/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.8239 - val_loss: 164.6867\n",
      "Epoch 2237/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.8456 - val_loss: 162.3372\n",
      "Epoch 2238/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 16.1882 - val_loss: 168.6464\n",
      "Epoch 2239/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.3989 - val_loss: 168.6128\n",
      "Epoch 2240/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.7181 - val_loss: 168.3909\n",
      "Epoch 2241/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 16.8319 - val_loss: 160.6803\n",
      "Epoch 2242/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.2078 - val_loss: 166.2003\n",
      "Epoch 2243/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.0663 - val_loss: 164.2524\n",
      "Epoch 2244/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.7031 - val_loss: 161.6098\n",
      "Epoch 2245/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 16.6152 - val_loss: 163.0616\n",
      "Epoch 2246/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 14.3866 - val_loss: 164.1769\n",
      "Epoch 2247/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 15.5400 - val_loss: 169.7916\n",
      "Epoch 2248/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 15.6999 - val_loss: 170.2065\n",
      "Epoch 2249/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.0866 - val_loss: 163.1985\n",
      "Epoch 2250/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.1175 - val_loss: 171.3958\n",
      "Epoch 2251/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 16.4362 - val_loss: 169.3850\n",
      "Epoch 2252/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.8623 - val_loss: 164.6945\n",
      "Epoch 2253/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 19.5156 - val_loss: 167.1388\n",
      "Epoch 2254/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.7759 - val_loss: 173.2627\n",
      "Epoch 2255/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.5810 - val_loss: 175.4420\n",
      "Epoch 2256/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.0318 - val_loss: 170.1824\n",
      "Epoch 2257/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 16.1602 - val_loss: 178.0867\n",
      "Epoch 2258/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 17.1616 - val_loss: 167.2578\n",
      "Epoch 2259/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.6234 - val_loss: 171.9947\n",
      "Epoch 2260/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 15.1685 - val_loss: 173.2746\n",
      "Epoch 2261/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 18.6767 - val_loss: 157.7057\n",
      "Epoch 2262/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 15.1850 - val_loss: 167.1571\n",
      "Epoch 2263/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.1365 - val_loss: 168.7297\n",
      "Epoch 2264/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 17.9739 - val_loss: 168.1065\n",
      "Epoch 2265/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 19.2021 - val_loss: 178.5432\n",
      "Epoch 2266/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 21.1900 - val_loss: 176.1000\n",
      "Epoch 2267/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 14.9941 - val_loss: 176.3289\n",
      "Epoch 2268/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 15.1603 - val_loss: 174.7170\n",
      "Epoch 2269/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.4193 - val_loss: 176.3343\n",
      "Epoch 2270/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.5038 - val_loss: 175.4281\n",
      "Epoch 2271/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 17.8096 - val_loss: 170.4870\n",
      "Epoch 2272/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 16.4194 - val_loss: 171.2305\n",
      "Epoch 2273/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 17.9518 - val_loss: 169.3245\n",
      "Epoch 2274/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.0769 - val_loss: 170.5609\n",
      "Epoch 2275/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.9039 - val_loss: 167.4579\n",
      "Epoch 2276/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.5492 - val_loss: 166.6505\n",
      "Epoch 2277/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.2648 - val_loss: 173.9856\n",
      "Epoch 2278/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 16.3628 - val_loss: 175.7551\n",
      "Epoch 2279/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.2137 - val_loss: 170.2112\n",
      "Epoch 2280/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.4514 - val_loss: 166.9046\n",
      "Epoch 2281/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 18.7716 - val_loss: 173.0494\n",
      "Epoch 2282/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.2853 - val_loss: 172.6594\n",
      "Epoch 2283/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.6609 - val_loss: 172.2377\n",
      "Epoch 2284/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 17.3941 - val_loss: 178.9927\n",
      "Epoch 2285/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.7837 - val_loss: 170.3371\n",
      "Epoch 2286/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.0995 - val_loss: 174.7221\n",
      "Epoch 2287/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.9718 - val_loss: 171.7921\n",
      "Epoch 2288/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.2963 - val_loss: 172.7088\n",
      "Epoch 2289/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.3894 - val_loss: 173.3061\n",
      "Epoch 2290/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.4299 - val_loss: 167.8501\n",
      "Epoch 2291/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.3478 - val_loss: 173.6693\n",
      "Epoch 2292/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.6859 - val_loss: 183.3325\n",
      "Epoch 2293/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.8117 - val_loss: 170.5957\n",
      "Epoch 2294/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.3700 - val_loss: 165.1809\n",
      "Epoch 2295/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 15.1027 - val_loss: 178.4159\n",
      "Epoch 2296/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.8982 - val_loss: 166.4690\n",
      "Epoch 2297/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 17.5648 - val_loss: 176.0074\n",
      "Epoch 2298/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.4156 - val_loss: 169.9118\n",
      "Epoch 2299/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.2554 - val_loss: 166.6878\n",
      "Epoch 2300/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 18.6025 - val_loss: 174.6606\n",
      "Epoch 2301/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.3963 - val_loss: 168.6344\n",
      "Epoch 2302/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 17.0066 - val_loss: 168.0640\n",
      "Epoch 2303/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 17.0859 - val_loss: 168.1683\n",
      "Epoch 2304/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.6769 - val_loss: 163.8762\n",
      "Epoch 2305/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.6047 - val_loss: 174.6463\n",
      "Epoch 2306/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.9841 - val_loss: 165.4436\n",
      "Epoch 2307/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.5126 - val_loss: 168.6094\n",
      "Epoch 2308/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.6699 - val_loss: 168.0163\n",
      "Epoch 2309/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.1928 - val_loss: 175.7963\n",
      "Epoch 2310/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 16.1131 - val_loss: 163.9390\n",
      "Epoch 2311/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 16.7319 - val_loss: 173.0520\n",
      "Epoch 2312/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 18.7295 - val_loss: 178.4549\n",
      "Epoch 2313/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 16.0406 - val_loss: 168.3693\n",
      "Epoch 2314/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 16.5172 - val_loss: 166.8930\n",
      "Epoch 2315/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 16.7294 - val_loss: 165.0607\n",
      "Epoch 2316/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 17.5893 - val_loss: 164.8791\n",
      "Epoch 2317/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 14.7788 - val_loss: 168.1542\n",
      "Epoch 2318/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 13.8655 - val_loss: 165.5163\n",
      "Epoch 2319/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 14.5874 - val_loss: 165.6778\n",
      "Epoch 2320/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 16.7254 - val_loss: 162.9514\n",
      "Epoch 2321/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 18.5253 - val_loss: 171.6774\n",
      "Epoch 2322/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 17.4828 - val_loss: 167.4419\n",
      "Epoch 2323/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 17.8326 - val_loss: 162.0868\n",
      "Epoch 2324/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.5689 - val_loss: 166.9171\n",
      "Epoch 2325/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 18.7107 - val_loss: 163.9543\n",
      "Epoch 2326/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 18.4545 - val_loss: 169.2802\n",
      "Epoch 2327/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.3290 - val_loss: 169.8514\n",
      "Epoch 2328/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.6410 - val_loss: 167.3040\n",
      "Epoch 2329/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.4980 - val_loss: 170.2860\n",
      "Epoch 2330/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 15.3855 - val_loss: 165.2205\n",
      "Epoch 2331/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.4154 - val_loss: 168.3558\n",
      "Epoch 2332/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.3801 - val_loss: 174.9061\n",
      "Epoch 2333/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 16.3860 - val_loss: 171.2516\n",
      "Epoch 2334/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.4750 - val_loss: 169.6858\n",
      "Epoch 2335/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.2617 - val_loss: 171.0360\n",
      "Epoch 2336/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 15.1358 - val_loss: 167.1250\n",
      "Epoch 2337/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 15.4223 - val_loss: 167.8838\n",
      "Epoch 2338/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 17.2850 - val_loss: 166.9812\n",
      "Epoch 2339/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 19.0428 - val_loss: 163.4879\n",
      "Epoch 2340/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 18.5856 - val_loss: 168.3686\n",
      "Epoch 2341/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.0654 - val_loss: 159.5228\n",
      "Epoch 2342/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.7430 - val_loss: 164.7165\n",
      "Epoch 2343/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.4970 - val_loss: 163.6999\n",
      "Epoch 2344/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 18.1904 - val_loss: 180.6162\n",
      "Epoch 2345/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 17.8774 - val_loss: 167.6732\n",
      "Epoch 2346/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 17.3231 - val_loss: 171.5281\n",
      "Epoch 2347/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.1485 - val_loss: 168.7714\n",
      "Epoch 2348/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.7949 - val_loss: 163.2421\n",
      "Epoch 2349/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.9571 - val_loss: 169.0324\n",
      "Epoch 2350/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 15.24 - 0s 51us/step - loss: 15.1309 - val_loss: 179.6711\n",
      "Epoch 2351/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 15.71 - 1s 63us/step - loss: 15.5408 - val_loss: 169.4055\n",
      "Epoch 2352/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 15.9871 - val_loss: 166.8440\n",
      "Epoch 2353/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 15.2512 - val_loss: 163.5423\n",
      "Epoch 2354/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 16.0838 - val_loss: 177.3110\n",
      "Epoch 2355/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 16.5780 - val_loss: 171.3784\n",
      "Epoch 2356/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 16.8467 - val_loss: 165.6820\n",
      "Epoch 2357/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 20.0904 - val_loss: 169.9594\n",
      "Epoch 2358/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 15.8282 - val_loss: 163.5935\n",
      "Epoch 2359/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 15.3388 - val_loss: 163.8856\n",
      "Epoch 2360/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 15.8731 - val_loss: 163.8067\n",
      "Epoch 2361/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.2409 - val_loss: 165.7544\n",
      "Epoch 2362/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.0158 - val_loss: 174.5312\n",
      "Epoch 2363/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.2816 - val_loss: 167.5048\n",
      "Epoch 2364/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.2628 - val_loss: 178.4490\n",
      "Epoch 2365/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 17.6904 - val_loss: 164.7850\n",
      "Epoch 2366/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 17.4300 - val_loss: 170.5117\n",
      "Epoch 2367/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.4452 - val_loss: 166.9540\n",
      "Epoch 2368/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 16.0731 - val_loss: 176.9289\n",
      "Epoch 2369/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.9431 - val_loss: 168.0991\n",
      "Epoch 2370/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 15.2215 - val_loss: 173.2263\n",
      "Epoch 2371/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 19.5793 - val_loss: 174.0998\n",
      "Epoch 2372/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.2288 - val_loss: 167.2580\n",
      "Epoch 2373/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.8006 - val_loss: 170.0981\n",
      "Epoch 2374/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.1804 - val_loss: 176.4310\n",
      "Epoch 2375/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.5346 - val_loss: 165.3043\n",
      "Epoch 2376/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.8679 - val_loss: 168.8485\n",
      "Epoch 2377/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.9231 - val_loss: 175.9647\n",
      "Epoch 2378/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.6422 - val_loss: 178.0576\n",
      "Epoch 2379/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 16.2770 - val_loss: 160.5523\n",
      "Epoch 2380/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 16.6755 - val_loss: 169.5966\n",
      "Epoch 2381/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 15.2549 - val_loss: 162.9750\n",
      "Epoch 2382/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 18.1082 - val_loss: 170.3565\n",
      "Epoch 2383/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 18.4247 - val_loss: 157.1460\n",
      "Epoch 2384/10000\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 14.5615 - val_loss: 160.9810\n",
      "Epoch 2385/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 14.8783 - val_loss: 170.0866\n",
      "Epoch 2386/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 16.9605 - val_loss: 166.4216\n",
      "Epoch 2387/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 15.2804 - val_loss: 161.1584\n",
      "Epoch 2388/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 14.4300 - val_loss: 167.4404\n",
      "Epoch 2389/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 15.0853 - val_loss: 163.9593\n",
      "Epoch 2390/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 15.2447 - val_loss: 162.0423\n",
      "Epoch 2391/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 15.3206 - val_loss: 170.1207\n",
      "Epoch 2392/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.9557 - val_loss: 162.7999\n",
      "Epoch 2393/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 17.4310 - val_loss: 170.9490\n",
      "Epoch 2394/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 18.5584 - val_loss: 165.1021\n",
      "Epoch 2395/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 17.5024 - val_loss: 163.6085\n",
      "Epoch 2396/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.0974 - val_loss: 171.2470\n",
      "Epoch 2397/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.7034 - val_loss: 171.8092\n",
      "Epoch 2398/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.5196 - val_loss: 163.5489\n",
      "Epoch 2399/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.3690 - val_loss: 168.2753\n",
      "Epoch 2400/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.2169 - val_loss: 168.3227\n",
      "Epoch 2401/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.3523 - val_loss: 168.4164\n",
      "Epoch 2402/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.1973 - val_loss: 172.3888\n",
      "Epoch 2403/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 18.0185 - val_loss: 168.4160\n",
      "Epoch 2404/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 16.6202 - val_loss: 179.3506\n",
      "Epoch 2405/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 17.0916 - val_loss: 162.9636\n",
      "Epoch 2406/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.7363 - val_loss: 175.2186\n",
      "Epoch 2407/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 15.3235 - val_loss: 180.3631\n",
      "Epoch 2408/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.1080 - val_loss: 172.4512\n",
      "Epoch 2409/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 14.4172 - val_loss: 171.2960\n",
      "Epoch 2410/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.5644 - val_loss: 173.7159\n",
      "Epoch 2411/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.1773 - val_loss: 168.5124\n",
      "Epoch 2412/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 17.8355 - val_loss: 164.0894\n",
      "Epoch 2413/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.4807 - val_loss: 169.7982\n",
      "Epoch 2414/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.8100 - val_loss: 162.5519\n",
      "Epoch 2415/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.0257 - val_loss: 164.2474\n",
      "Epoch 2416/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 17.4568 - val_loss: 164.3697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2417/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.4484 - val_loss: 173.7560\n",
      "Epoch 2418/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.3478 - val_loss: 164.8422\n",
      "Epoch 2419/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 15.8475 - val_loss: 176.4985\n",
      "Epoch 2420/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.3147 - val_loss: 163.0414\n",
      "Epoch 2421/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.9265 - val_loss: 159.0631\n",
      "Epoch 2422/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.4247 - val_loss: 163.7271\n",
      "Epoch 2423/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.6401 - val_loss: 159.1392\n",
      "Epoch 2424/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.1892 - val_loss: 157.4878\n",
      "Epoch 2425/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.8928 - val_loss: 161.4664\n",
      "Epoch 2426/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 18.3115 - val_loss: 171.9157\n",
      "Epoch 2427/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.9391 - val_loss: 171.0633\n",
      "Epoch 2428/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.4771 - val_loss: 169.5375\n",
      "Epoch 2429/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.4929 - val_loss: 170.2067\n",
      "Epoch 2430/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.9852 - val_loss: 172.3005\n",
      "Epoch 2431/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.9461 - val_loss: 162.9004\n",
      "Epoch 2432/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.8555 - val_loss: 165.6508\n",
      "Epoch 2433/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.8007 - val_loss: 161.4469\n",
      "Epoch 2434/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.4666 - val_loss: 158.1387\n",
      "Epoch 2435/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.1991 - val_loss: 163.6238\n",
      "Epoch 2436/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.1021 - val_loss: 161.6887\n",
      "Epoch 2437/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 15.6399 - val_loss: 154.6323\n",
      "Epoch 2438/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.2604 - val_loss: 163.9177\n",
      "Epoch 2439/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.0350 - val_loss: 172.1440\n",
      "Epoch 2440/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.2774 - val_loss: 166.0912\n",
      "Epoch 2441/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 19.6212 - val_loss: 171.0766\n",
      "Epoch 2442/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 16.8762 - val_loss: 164.9432\n",
      "Epoch 2443/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 16.5031 - val_loss: 162.3561\n",
      "Epoch 2444/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.0558 - val_loss: 165.9298\n",
      "Epoch 2445/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.1929 - val_loss: 180.0669\n",
      "Epoch 2446/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.9158 - val_loss: 164.8591\n",
      "Epoch 2447/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 15.3674 - val_loss: 157.6034\n",
      "Epoch 2448/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.7178 - val_loss: 166.2307\n",
      "Epoch 2449/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.3857 - val_loss: 169.5292\n",
      "Epoch 2450/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.5879 - val_loss: 169.1835\n",
      "Epoch 2451/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.7428 - val_loss: 175.1994\n",
      "Epoch 2452/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.1396 - val_loss: 179.1755\n",
      "Epoch 2453/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 15.1264 - val_loss: 170.5623\n",
      "Epoch 2454/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 13.7998 - val_loss: 166.7196\n",
      "Epoch 2455/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 16.7587 - val_loss: 171.3928\n",
      "Epoch 2456/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 14.6458 - val_loss: 163.2211\n",
      "Epoch 2457/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.7353 - val_loss: 168.7462\n",
      "Epoch 2458/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.3838 - val_loss: 169.9364\n",
      "Epoch 2459/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 18.8064 - val_loss: 163.3404\n",
      "Epoch 2460/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.8008 - val_loss: 165.9926\n",
      "Epoch 2461/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.3983 - val_loss: 167.6850\n",
      "Epoch 2462/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 16.0958 - val_loss: 173.7005\n",
      "Epoch 2463/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.2222 - val_loss: 175.3258\n",
      "Epoch 2464/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.8262 - val_loss: 172.1582\n",
      "Epoch 2465/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.5763 - val_loss: 163.2051\n",
      "Epoch 2466/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.4944 - val_loss: 170.9643\n",
      "Epoch 2467/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.5411 - val_loss: 173.6395\n",
      "Epoch 2468/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.7499 - val_loss: 170.5299\n",
      "Epoch 2469/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.7453 - val_loss: 171.6535\n",
      "Epoch 2470/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 14.0968 - val_loss: 177.0164\n",
      "Epoch 2471/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.6749 - val_loss: 166.2118\n",
      "Epoch 2472/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.9984 - val_loss: 178.3251\n",
      "Epoch 2473/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.6881 - val_loss: 172.5069\n",
      "Epoch 2474/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 16.9872 - val_loss: 176.4925\n",
      "Epoch 2475/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.5986 - val_loss: 165.7005\n",
      "Epoch 2476/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 16.4009 - val_loss: 180.1136\n",
      "Epoch 2477/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 15.3789 - val_loss: 173.7116\n",
      "Epoch 2478/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.0773 - val_loss: 169.6790\n",
      "Epoch 2479/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.5160 - val_loss: 177.9328\n",
      "Epoch 2480/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.2682 - val_loss: 174.6183\n",
      "Epoch 2481/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 14.7521 - val_loss: 169.2396\n",
      "Epoch 2482/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 18.0382 - val_loss: 172.5161\n",
      "Epoch 2483/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 17.5003 - val_loss: 166.0725\n",
      "Epoch 2484/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.2435 - val_loss: 168.2810\n",
      "Epoch 2485/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.7419 - val_loss: 166.4634\n",
      "Epoch 2486/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 16.1404 - val_loss: 172.4808\n",
      "Epoch 2487/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 16.6036 - val_loss: 158.7079\n",
      "Epoch 2488/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.8440 - val_loss: 171.1917\n",
      "Epoch 2489/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 13.5118 - val_loss: 169.9024\n",
      "Epoch 2490/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.4935 - val_loss: 170.6927\n",
      "Epoch 2491/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 15.5736 - val_loss: 169.6930\n",
      "Epoch 2492/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.9871 - val_loss: 167.6200\n",
      "Epoch 2493/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.7299 - val_loss: 168.3518\n",
      "Epoch 2494/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 16.9032 - val_loss: 167.8759\n",
      "Epoch 2495/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 16.0726 - val_loss: 175.5401\n",
      "Epoch 2496/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 19.1166 - val_loss: 173.5009\n",
      "Epoch 2497/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.8345 - val_loss: 171.0903\n",
      "Epoch 2498/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 15.6176 - val_loss: 169.8460\n",
      "Epoch 2499/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 19.2157 - val_loss: 165.6398\n",
      "Epoch 2500/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 17.1929 - val_loss: 170.3951\n",
      "Epoch 2501/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.7588 - val_loss: 167.9696\n",
      "Epoch 2502/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.9192 - val_loss: 169.7733\n",
      "Epoch 2503/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.7616 - val_loss: 176.1961\n",
      "Epoch 2504/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.9661 - val_loss: 172.3742\n",
      "Epoch 2505/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.8405 - val_loss: 173.6216\n",
      "Epoch 2506/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.9004 - val_loss: 171.0124\n",
      "Epoch 2507/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.9780 - val_loss: 160.5069\n",
      "Epoch 2508/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.9117 - val_loss: 170.8087\n",
      "Epoch 2509/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.4663 - val_loss: 175.4100\n",
      "Epoch 2510/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.4714 - val_loss: 175.9941\n",
      "Epoch 2511/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 15.0686 - val_loss: 176.6532\n",
      "Epoch 2512/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 14.5055 - val_loss: 165.1287\n",
      "Epoch 2513/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 18.0517 - val_loss: 169.1607\n",
      "Epoch 2514/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 15.4661 - val_loss: 176.8139\n",
      "Epoch 2515/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 14.1657 - val_loss: 167.9279\n",
      "Epoch 2516/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 14.7569 - val_loss: 170.9633\n",
      "Epoch 2517/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 17.0227 - val_loss: 170.5935\n",
      "Epoch 2518/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 14.1128 - val_loss: 172.4153\n",
      "Epoch 2519/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.4435 - val_loss: 169.4761\n",
      "Epoch 2520/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.7837 - val_loss: 169.1332\n",
      "Epoch 2521/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 16.4073 - val_loss: 161.9929\n",
      "Epoch 2522/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 14.3055 - val_loss: 173.0515\n",
      "Epoch 2523/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 13.4789 - val_loss: 178.1392\n",
      "Epoch 2524/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 14.7252 - val_loss: 174.5627\n",
      "Epoch 2525/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 16.1080 - val_loss: 169.0888\n",
      "Epoch 2526/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 13.7829 - val_loss: 169.6429\n",
      "Epoch 2527/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.7272 - val_loss: 169.2640\n",
      "Epoch 2528/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 16.7471 - val_loss: 160.7804\n",
      "Epoch 2529/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 17.0511 - val_loss: 163.5471\n",
      "Epoch 2530/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.5782 - val_loss: 170.4185\n",
      "Epoch 2531/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 14.2109 - val_loss: 174.3284\n",
      "Epoch 2532/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 17.6007 - val_loss: 158.8982\n",
      "Epoch 2533/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.8383 - val_loss: 170.0486\n",
      "Epoch 2534/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.9228 - val_loss: 184.1785\n",
      "Epoch 2535/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.3728 - val_loss: 165.0143\n",
      "Epoch 2536/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.4114 - val_loss: 171.7696\n",
      "Epoch 2537/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 15.7877 - val_loss: 163.1686\n",
      "Epoch 2538/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.5802 - val_loss: 160.5680\n",
      "Epoch 2539/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.7558 - val_loss: 169.3480\n",
      "Epoch 2540/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.9819 - val_loss: 165.7365\n",
      "Epoch 2541/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.9458 - val_loss: 168.3897\n",
      "Epoch 2542/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.7423 - val_loss: 163.8258\n",
      "Epoch 2543/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.2291 - val_loss: 161.7210\n",
      "Epoch 2544/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.7161 - val_loss: 173.5837\n",
      "Epoch 2545/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.2948 - val_loss: 175.4064\n",
      "Epoch 2546/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 18.9276 - val_loss: 163.9326\n",
      "Epoch 2547/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 15.5566 - val_loss: 169.6624\n",
      "Epoch 2548/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 14.6349 - val_loss: 171.7787\n",
      "Epoch 2549/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 15.3622 - val_loss: 160.4084\n",
      "Epoch 2550/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 15.7146 - val_loss: 163.4431\n",
      "Epoch 2551/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.2726 - val_loss: 172.0485\n",
      "Epoch 2552/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.3785 - val_loss: 160.7750\n",
      "Epoch 2553/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 16.2887 - val_loss: 162.4226\n",
      "Epoch 2554/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 15.0768 - val_loss: 164.4608\n",
      "Epoch 2555/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 16.9944 - val_loss: 164.7271\n",
      "Epoch 2556/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.6377 - val_loss: 165.5726\n",
      "Epoch 2557/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 17.5718 - val_loss: 168.7873\n",
      "Epoch 2558/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 15.5715 - val_loss: 164.7900\n",
      "Epoch 2559/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.6330 - val_loss: 158.0789\n",
      "Epoch 2560/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 15.8006 - val_loss: 164.3943\n",
      "Epoch 2561/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 16.4401 - val_loss: 178.2654\n",
      "Epoch 2562/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.6236 - val_loss: 174.3136\n",
      "Epoch 2563/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 17.3993 - val_loss: 168.1819\n",
      "Epoch 2564/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 17.1090 - val_loss: 164.1367\n",
      "Epoch 2565/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.9564 - val_loss: 172.1624\n",
      "Epoch 2566/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 14.3440 - val_loss: 168.2198\n",
      "Epoch 2567/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 15.1098 - val_loss: 161.9808\n",
      "Epoch 2568/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.1990 - val_loss: 166.3979\n",
      "Epoch 2569/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 14.8379 - val_loss: 160.5224\n",
      "Epoch 2570/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.9094 - val_loss: 162.7571\n",
      "Epoch 2571/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.9133 - val_loss: 167.4788\n",
      "Epoch 2572/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.6525 - val_loss: 163.0698\n",
      "Epoch 2573/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.1351 - val_loss: 171.9670\n",
      "Epoch 2574/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.2297 - val_loss: 169.8637\n",
      "Epoch 2575/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.8982 - val_loss: 162.3977\n",
      "Epoch 2576/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.9568 - val_loss: 170.7689\n",
      "Epoch 2577/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.4907 - val_loss: 174.0946\n",
      "Epoch 2578/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.6869 - val_loss: 165.1853\n",
      "Epoch 2579/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.7413 - val_loss: 172.0428\n",
      "Epoch 2580/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 15.3729 - val_loss: 170.7995\n",
      "Epoch 2581/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 16.5874 - val_loss: 168.8901\n",
      "Epoch 2582/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.8606 - val_loss: 174.0230\n",
      "Epoch 2583/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.3902 - val_loss: 169.3834\n",
      "Epoch 2584/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 18.8449 - val_loss: 165.6085\n",
      "Epoch 2585/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.7502 - val_loss: 172.0291\n",
      "Epoch 2586/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.6769 - val_loss: 168.3808\n",
      "Epoch 2587/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 15.0421 - val_loss: 170.6953\n",
      "Epoch 2588/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.0170 - val_loss: 168.7616\n",
      "Epoch 2589/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 15.0617 - val_loss: 166.7740\n",
      "Epoch 2590/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 14.1603 - val_loss: 163.3169\n",
      "Epoch 2591/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 17.2668 - val_loss: 170.7510\n",
      "Epoch 2592/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 15.5966 - val_loss: 163.1761\n",
      "Epoch 2593/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.1608 - val_loss: 180.7078\n",
      "Epoch 2594/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.4027 - val_loss: 174.9884\n",
      "Epoch 2595/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.3941 - val_loss: 172.5955\n",
      "Epoch 2596/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.3572 - val_loss: 161.3513\n",
      "Epoch 2597/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 14.8182 - val_loss: 163.6028\n",
      "Epoch 2598/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.3271 - val_loss: 166.9972\n",
      "Epoch 2599/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.6727 - val_loss: 167.5288\n",
      "Epoch 2600/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.1801 - val_loss: 156.7776\n",
      "Epoch 2601/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.4428 - val_loss: 172.2577\n",
      "Epoch 2602/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.4761 - val_loss: 168.1165\n",
      "Epoch 2603/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 14.1484 - val_loss: 163.8033\n",
      "Epoch 2604/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 14.8786 - val_loss: 166.4954\n",
      "Epoch 2605/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 16.0525 - val_loss: 165.1405\n",
      "Epoch 2606/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 18.0764 - val_loss: 169.0763\n",
      "Epoch 2607/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 15.7508 - val_loss: 153.0882\n",
      "Epoch 2608/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 14.3263 - val_loss: 167.6923\n",
      "Epoch 2609/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 16.1574 - val_loss: 161.4683\n",
      "Epoch 2610/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.8024 - val_loss: 176.5508\n",
      "Epoch 2611/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.1214 - val_loss: 173.5100\n",
      "Epoch 2612/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 16.4458 - val_loss: 172.6305\n",
      "Epoch 2613/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.8247 - val_loss: 172.0385\n",
      "Epoch 2614/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.6116 - val_loss: 161.5834\n",
      "Epoch 2615/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 15.2619 - val_loss: 164.0744\n",
      "Epoch 2616/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.7413 - val_loss: 175.3781\n",
      "Epoch 2617/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.0043 - val_loss: 164.4230\n",
      "Epoch 2618/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.2084 - val_loss: 166.4100\n",
      "Epoch 2619/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.3350 - val_loss: 172.0558\n",
      "Epoch 2620/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.8489 - val_loss: 168.9150\n",
      "Epoch 2621/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.2449 - val_loss: 174.3493\n",
      "Epoch 2622/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.7672 - val_loss: 166.5711\n",
      "Epoch 2623/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.8173 - val_loss: 163.4371\n",
      "Epoch 2624/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.8852 - val_loss: 170.2726\n",
      "Epoch 2625/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.4974 - val_loss: 167.2700\n",
      "Epoch 2626/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 16.7423 - val_loss: 173.2883\n",
      "Epoch 2627/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 16.1368 - val_loss: 168.4915\n",
      "Epoch 2628/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.8349 - val_loss: 159.9712\n",
      "Epoch 2629/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.5694 - val_loss: 165.8345\n",
      "Epoch 2630/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.2292 - val_loss: 168.0962\n",
      "Epoch 2631/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.2240 - val_loss: 166.7564\n",
      "Epoch 2632/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.5383 - val_loss: 169.9557\n",
      "Epoch 2633/10000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 13.4821 - val_loss: 169.6886\n",
      "Epoch 2634/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 13.6983 - val_loss: 170.2970\n",
      "Epoch 2635/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 14.2979 - val_loss: 173.5967\n",
      "Epoch 2636/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 56us/step - loss: 16.9412 - val_loss: 162.6389\n",
      "Epoch 2637/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 15.9914 - val_loss: 169.1721\n",
      "Epoch 2638/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.1547 - val_loss: 163.9788\n",
      "Epoch 2639/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.7929 - val_loss: 168.2782\n",
      "Epoch 2640/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 15.6845 - val_loss: 171.3359\n",
      "Epoch 2641/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.3968 - val_loss: 174.6305\n",
      "Epoch 2642/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.9007 - val_loss: 175.6396\n",
      "Epoch 2643/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.2778 - val_loss: 175.6415\n",
      "Epoch 2644/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.0877 - val_loss: 166.0269\n",
      "Epoch 2645/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.4563 - val_loss: 176.0336\n",
      "Epoch 2646/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 15.2811 - val_loss: 175.8045\n",
      "Epoch 2647/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.4450 - val_loss: 169.0206\n",
      "Epoch 2648/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.8819 - val_loss: 163.5652\n",
      "Epoch 2649/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.2068 - val_loss: 165.2526\n",
      "Epoch 2650/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.8055 - val_loss: 173.9116\n",
      "Epoch 2651/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 15.0823 - val_loss: 167.7402\n",
      "Epoch 2652/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 16.0254 - val_loss: 160.1039\n",
      "Epoch 2653/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.0972 - val_loss: 157.4398\n",
      "Epoch 2654/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.7739 - val_loss: 165.8182\n",
      "Epoch 2655/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.4163 - val_loss: 171.2886\n",
      "Epoch 2656/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 21.6695 - val_loss: 166.4840\n",
      "Epoch 2657/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 15.0111 - val_loss: 171.9780\n",
      "Epoch 2658/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 13.5138 - val_loss: 160.4599\n",
      "Epoch 2659/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.1353 - val_loss: 164.3708\n",
      "Epoch 2660/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 18.3822 - val_loss: 159.7320\n",
      "Epoch 2661/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.6535 - val_loss: 156.0061\n",
      "Epoch 2662/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 15.2297 - val_loss: 156.9609\n",
      "Epoch 2663/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.4643 - val_loss: 160.6306\n",
      "Epoch 2664/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 15.7013 - val_loss: 162.6001\n",
      "Epoch 2665/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.0047 - val_loss: 164.3683\n",
      "Epoch 2666/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.4584 - val_loss: 164.7715\n",
      "Epoch 2667/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.2667 - val_loss: 160.5414\n",
      "Epoch 2668/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 13.8435 - val_loss: 155.2873\n",
      "Epoch 2669/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 16.1991 - val_loss: 164.8523\n",
      "Epoch 2670/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 14.3213 - val_loss: 181.0990\n",
      "Epoch 2671/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 16.4868 - val_loss: 160.6925\n",
      "Epoch 2672/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.3591 - val_loss: 162.8481\n",
      "Epoch 2673/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.1619 - val_loss: 167.7166\n",
      "Epoch 2674/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.8752 - val_loss: 166.8917\n",
      "Epoch 2675/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.9646 - val_loss: 163.2071\n",
      "Epoch 2676/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.8637 - val_loss: 161.3262\n",
      "Epoch 2677/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 13.8621 - val_loss: 160.0189\n",
      "Epoch 2678/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.0501 - val_loss: 169.6775\n",
      "Epoch 2679/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.4737 - val_loss: 163.9663\n",
      "Epoch 2680/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.2707 - val_loss: 171.1008\n",
      "Epoch 2681/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.1938 - val_loss: 157.7732\n",
      "Epoch 2682/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.9929 - val_loss: 163.7685\n",
      "Epoch 2683/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.4106 - val_loss: 163.6018\n",
      "Epoch 2684/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.8481 - val_loss: 159.4223\n",
      "Epoch 2685/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.0951 - val_loss: 159.6215\n",
      "Epoch 2686/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 14.0663 - val_loss: 162.1597\n",
      "Epoch 2687/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.3473 - val_loss: 162.3817\n",
      "Epoch 2688/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.8617 - val_loss: 169.4647\n",
      "Epoch 2689/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.7217 - val_loss: 167.3407\n",
      "Epoch 2690/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.6776 - val_loss: 161.5071\n",
      "Epoch 2691/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.1151 - val_loss: 162.7102\n",
      "Epoch 2692/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 15.8121 - val_loss: 172.2290\n",
      "Epoch 2693/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.9552 - val_loss: 168.1182\n",
      "Epoch 2694/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.5408 - val_loss: 160.3755\n",
      "Epoch 2695/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.6278 - val_loss: 159.2921\n",
      "Epoch 2696/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 16.4066 - val_loss: 155.1296\n",
      "Epoch 2697/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.5557 - val_loss: 164.6635\n",
      "Epoch 2698/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.8909 - val_loss: 161.9594\n",
      "Epoch 2699/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.8527 - val_loss: 158.0528\n",
      "Epoch 2700/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.2870 - val_loss: 155.9490\n",
      "Epoch 2701/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.2164 - val_loss: 166.2202\n",
      "Epoch 2702/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 16.5002 - val_loss: 167.6629\n",
      "Epoch 2703/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 15.3475 - val_loss: 159.5305\n",
      "Epoch 2704/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.1020 - val_loss: 162.2905\n",
      "Epoch 2705/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.9909 - val_loss: 162.1103\n",
      "Epoch 2706/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.9832 - val_loss: 168.6252\n",
      "Epoch 2707/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.9575 - val_loss: 167.7462\n",
      "Epoch 2708/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 15.2927 - val_loss: 176.4814\n",
      "Epoch 2709/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.4706 - val_loss: 170.5518\n",
      "Epoch 2710/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.3273 - val_loss: 167.0606\n",
      "Epoch 2711/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.2843 - val_loss: 167.7530\n",
      "Epoch 2712/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.5401 - val_loss: 165.8707\n",
      "Epoch 2713/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.9244 - val_loss: 162.1720\n",
      "Epoch 2714/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 15.6742 - val_loss: 169.2359\n",
      "Epoch 2715/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 15.1785 - val_loss: 173.2666\n",
      "Epoch 2716/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.1843 - val_loss: 169.6727\n",
      "Epoch 2717/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.6325 - val_loss: 165.2565\n",
      "Epoch 2718/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 14.8352 - val_loss: 179.5802\n",
      "Epoch 2719/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 15.5363 - val_loss: 175.9234\n",
      "Epoch 2720/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 16.3962 - val_loss: 170.4476\n",
      "Epoch 2721/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 13.6710 - val_loss: 169.8740\n",
      "Epoch 2722/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 14.7858 - val_loss: 171.6840\n",
      "Epoch 2723/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.5657 - val_loss: 171.8088\n",
      "Epoch 2724/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 15.8641 - val_loss: 174.2174\n",
      "Epoch 2725/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 14.1486 - val_loss: 171.1959\n",
      "Epoch 2726/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.7542 - val_loss: 170.0861\n",
      "Epoch 2727/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 13.8015 - val_loss: 165.5540\n",
      "Epoch 2728/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 13.0768 - val_loss: 162.4429\n",
      "Epoch 2729/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 14.3084 - val_loss: 171.7992\n",
      "Epoch 2730/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 15.0209 - val_loss: 168.8077\n",
      "Epoch 2731/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 15.6887 - val_loss: 168.3031\n",
      "Epoch 2732/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 14.9149 - val_loss: 169.7421\n",
      "Epoch 2733/10000\n",
      "8000/8000 [==============================] - 1s 129us/step - loss: 15.2631 - val_loss: 172.6471\n",
      "Epoch 2734/10000\n",
      "8000/8000 [==============================] - 1s 127us/step - loss: 13.8196 - val_loss: 170.2697\n",
      "Epoch 2735/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 13.6931 - val_loss: 179.2782\n",
      "Epoch 2736/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 13.3090 - val_loss: 166.4376\n",
      "Epoch 2737/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.5080 - val_loss: 174.4935\n",
      "Epoch 2738/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.2072 - val_loss: 163.1151\n",
      "Epoch 2739/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.1962 - val_loss: 164.2136\n",
      "Epoch 2740/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.4537 - val_loss: 165.4819\n",
      "Epoch 2741/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 14.6350 - val_loss: 164.2103\n",
      "Epoch 2742/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 16.3647 - val_loss: 173.0388\n",
      "Epoch 2743/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 17.4331 - val_loss: 172.1676\n",
      "Epoch 2744/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 17.4168 - val_loss: 164.4706\n",
      "Epoch 2745/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.0968 - val_loss: 166.2191\n",
      "Epoch 2746/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 12.9241 - val_loss: 164.2067\n",
      "Epoch 2747/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 14.4157 - val_loss: 168.2080\n",
      "Epoch 2748/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 15.0676 - val_loss: 179.3187\n",
      "Epoch 2749/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.0382 - val_loss: 166.7474\n",
      "Epoch 2750/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.1010 - val_loss: 173.2893\n",
      "Epoch 2751/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.1168 - val_loss: 163.5370\n",
      "Epoch 2752/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 16.7374 - val_loss: 167.0396\n",
      "Epoch 2753/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 16.0333 - val_loss: 164.2248\n",
      "Epoch 2754/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 15.5729 - val_loss: 164.9426\n",
      "Epoch 2755/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.3818 - val_loss: 160.5636\n",
      "Epoch 2756/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.3655 - val_loss: 161.5173\n",
      "Epoch 2757/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 14.5328 - val_loss: 162.5932\n",
      "Epoch 2758/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 13.2480 - val_loss: 161.0459\n",
      "Epoch 2759/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 12.8625 - val_loss: 161.2258\n",
      "Epoch 2760/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 16.4192 - val_loss: 163.0396\n",
      "Epoch 2761/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 16.2829 - val_loss: 166.4070\n",
      "Epoch 2762/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 16.4494 - val_loss: 168.8582\n",
      "Epoch 2763/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 13.7817 - val_loss: 158.6822\n",
      "Epoch 2764/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.3140 - val_loss: 167.0102\n",
      "Epoch 2765/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.7937 - val_loss: 161.6084\n",
      "Epoch 2766/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.9120 - val_loss: 165.5814\n",
      "Epoch 2767/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.9978 - val_loss: 174.2613\n",
      "Epoch 2768/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.3901 - val_loss: 169.7640\n",
      "Epoch 2769/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.4766 - val_loss: 165.8254\n",
      "Epoch 2770/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.5524 - val_loss: 164.6343\n",
      "Epoch 2771/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.6129 - val_loss: 160.9007\n",
      "Epoch 2772/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 16.1973 - val_loss: 174.6434\n",
      "Epoch 2773/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 17.3316 - val_loss: 162.0108\n",
      "Epoch 2774/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.6301 - val_loss: 166.5212\n",
      "Epoch 2775/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.3824 - val_loss: 166.4298\n",
      "Epoch 2776/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.6085 - val_loss: 172.4816\n",
      "Epoch 2777/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.2504 - val_loss: 167.9336\n",
      "Epoch 2778/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 16.3929 - val_loss: 165.9870\n",
      "Epoch 2779/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.9375 - val_loss: 171.0504\n",
      "Epoch 2780/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.8201 - val_loss: 169.8917\n",
      "Epoch 2781/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 15.2292 - val_loss: 170.3408\n",
      "Epoch 2782/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.5584 - val_loss: 168.0775\n",
      "Epoch 2783/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.5142 - val_loss: 168.0721\n",
      "Epoch 2784/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.0899 - val_loss: 167.7870\n",
      "Epoch 2785/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.1247 - val_loss: 168.6794\n",
      "Epoch 2786/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.5284 - val_loss: 170.4177\n",
      "Epoch 2787/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.4742 - val_loss: 160.3880\n",
      "Epoch 2788/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 16.6866 - val_loss: 163.2582\n",
      "Epoch 2789/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 13.8832 - val_loss: 169.9950\n",
      "Epoch 2790/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 14.3528 - val_loss: 167.1103\n",
      "Epoch 2791/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 15.9582 - val_loss: 170.0437\n",
      "Epoch 2792/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.2294 - val_loss: 167.5340\n",
      "Epoch 2793/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 15.7073 - val_loss: 161.9362\n",
      "Epoch 2794/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.4947 - val_loss: 163.7258\n",
      "Epoch 2795/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.1123 - val_loss: 164.8712\n",
      "Epoch 2796/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.8940 - val_loss: 173.0566\n",
      "Epoch 2797/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.7043 - val_loss: 161.5068\n",
      "Epoch 2798/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.8556 - val_loss: 166.6842\n",
      "Epoch 2799/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.4289 - val_loss: 167.4861\n",
      "Epoch 2800/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.8337 - val_loss: 156.7193\n",
      "Epoch 2801/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.1642 - val_loss: 168.6502\n",
      "Epoch 2802/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.0934 - val_loss: 167.9433\n",
      "Epoch 2803/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.5451 - val_loss: 164.8195\n",
      "Epoch 2804/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.1997 - val_loss: 162.8370\n",
      "Epoch 2805/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 15.7231 - val_loss: 173.8000\n",
      "Epoch 2806/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.5807 - val_loss: 170.3068\n",
      "Epoch 2807/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.5376 - val_loss: 160.6454\n",
      "Epoch 2808/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.0088 - val_loss: 165.2526\n",
      "Epoch 2809/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.2583 - val_loss: 168.8177\n",
      "Epoch 2810/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.5628 - val_loss: 167.9662\n",
      "Epoch 2811/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.0733 - val_loss: 165.9573\n",
      "Epoch 2812/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.1645 - val_loss: 175.3894\n",
      "Epoch 2813/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 13.7126 - val_loss: 164.8521\n",
      "Epoch 2814/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 13.5406 - val_loss: 170.7195\n",
      "Epoch 2815/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 12.7672 - val_loss: 168.6578\n",
      "Epoch 2816/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 13.9846 - val_loss: 164.5511\n",
      "Epoch 2817/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 13.7703 - val_loss: 168.1069\n",
      "Epoch 2818/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 14.7064 - val_loss: 165.1508\n",
      "Epoch 2819/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 17.4155 - val_loss: 166.3854\n",
      "Epoch 2820/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.5777 - val_loss: 170.8883\n",
      "Epoch 2821/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.7094 - val_loss: 172.7040\n",
      "Epoch 2822/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.6259 - val_loss: 172.4976\n",
      "Epoch 2823/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 13.4950 - val_loss: 161.0891\n",
      "Epoch 2824/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.0775 - val_loss: 155.0831\n",
      "Epoch 2825/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.9216 - val_loss: 165.1195\n",
      "Epoch 2826/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.2213 - val_loss: 165.7043\n",
      "Epoch 2827/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.7189 - val_loss: 168.1223\n",
      "Epoch 2828/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.1487 - val_loss: 177.1792\n",
      "Epoch 2829/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.0703 - val_loss: 176.5695\n",
      "Epoch 2830/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.1746 - val_loss: 163.3511\n",
      "Epoch 2831/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 16.8526 - val_loss: 158.9335\n",
      "Epoch 2832/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.2747 - val_loss: 161.3926\n",
      "Epoch 2833/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.1371 - val_loss: 160.6252\n",
      "Epoch 2834/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.1590 - val_loss: 161.3720\n",
      "Epoch 2835/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.6709 - val_loss: 162.0303\n",
      "Epoch 2836/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 15.3034 - val_loss: 166.0309\n",
      "Epoch 2837/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.8554 - val_loss: 171.1749\n",
      "Epoch 2838/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.8110 - val_loss: 160.2369\n",
      "Epoch 2839/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 15.9411 - val_loss: 170.1565\n",
      "Epoch 2840/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.0714 - val_loss: 160.9847\n",
      "Epoch 2841/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.4102 - val_loss: 169.1872\n",
      "Epoch 2842/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 13.7868 - val_loss: 169.9914\n",
      "Epoch 2843/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.1139 - val_loss: 163.5228\n",
      "Epoch 2844/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.0057 - val_loss: 162.1379\n",
      "Epoch 2845/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.7131 - val_loss: 164.9220\n",
      "Epoch 2846/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.9981 - val_loss: 154.6365\n",
      "Epoch 2847/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.6905 - val_loss: 168.2358\n",
      "Epoch 2848/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.4022 - val_loss: 166.9730\n",
      "Epoch 2849/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.8449 - val_loss: 157.7375\n",
      "Epoch 2850/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.3247 - val_loss: 161.9437\n",
      "Epoch 2851/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.4651 - val_loss: 158.5505\n",
      "Epoch 2852/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.1589 - val_loss: 164.4287\n",
      "Epoch 2853/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.5257 - val_loss: 166.4735\n",
      "Epoch 2854/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.3779 - val_loss: 160.0845\n",
      "Epoch 2855/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 57us/step - loss: 19.1093 - val_loss: 164.2208\n",
      "Epoch 2856/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.7366 - val_loss: 163.5965\n",
      "Epoch 2857/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 13.5784 - val_loss: 163.7485\n",
      "Epoch 2858/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 15.0704 - val_loss: 152.8087\n",
      "Epoch 2859/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 12.7708 - val_loss: 153.5938\n",
      "Epoch 2860/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.7822 - val_loss: 158.4736\n",
      "Epoch 2861/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.0539 - val_loss: 164.3999\n",
      "Epoch 2862/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.8618 - val_loss: 161.2915\n",
      "Epoch 2863/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.5652 - val_loss: 167.7144\n",
      "Epoch 2864/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.2696 - val_loss: 157.1984\n",
      "Epoch 2865/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.1372 - val_loss: 159.9352\n",
      "Epoch 2866/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.5377 - val_loss: 174.0152\n",
      "Epoch 2867/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.8390 - val_loss: 167.5587\n",
      "Epoch 2868/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.6778 - val_loss: 160.2954\n",
      "Epoch 2869/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.6651 - val_loss: 156.3995\n",
      "Epoch 2870/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.5073 - val_loss: 157.6756\n",
      "Epoch 2871/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.6313 - val_loss: 164.0987\n",
      "Epoch 2872/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.2559 - val_loss: 159.9507\n",
      "Epoch 2873/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.7004 - val_loss: 163.4456\n",
      "Epoch 2874/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.9644 - val_loss: 161.8005\n",
      "Epoch 2875/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 16.2099 - val_loss: 157.9766\n",
      "Epoch 2876/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.7585 - val_loss: 159.2538\n",
      "Epoch 2877/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.6753 - val_loss: 163.8272\n",
      "Epoch 2878/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.5533 - val_loss: 155.8167\n",
      "Epoch 2879/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.3919 - val_loss: 166.4530\n",
      "Epoch 2880/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.6284 - val_loss: 161.3750\n",
      "Epoch 2881/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 12.8886 - val_loss: 163.5026\n",
      "Epoch 2882/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.7797 - val_loss: 168.9912\n",
      "Epoch 2883/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 14.4370 - val_loss: 166.2876\n",
      "Epoch 2884/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.3692 - val_loss: 158.1616\n",
      "Epoch 2885/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 12.6367 - val_loss: 171.8357\n",
      "Epoch 2886/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.2564 - val_loss: 162.3198\n",
      "Epoch 2887/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.1016 - val_loss: 162.0713\n",
      "Epoch 2888/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.0958 - val_loss: 163.7611\n",
      "Epoch 2889/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.7642 - val_loss: 163.4176\n",
      "Epoch 2890/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.9777 - val_loss: 162.3706\n",
      "Epoch 2891/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 14.6327 - val_loss: 161.6099\n",
      "Epoch 2892/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 14.0116 - val_loss: 164.2388\n",
      "Epoch 2893/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.9366 - val_loss: 164.0883\n",
      "Epoch 2894/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.9024 - val_loss: 164.6259\n",
      "Epoch 2895/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.0911 - val_loss: 161.8756\n",
      "Epoch 2896/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.0630 - val_loss: 162.7504\n",
      "Epoch 2897/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.7746 - val_loss: 156.2335\n",
      "Epoch 2898/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 13.2438 - val_loss: 162.8643\n",
      "Epoch 2899/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 13.5022 - val_loss: 166.9135\n",
      "Epoch 2900/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.1456 - val_loss: 156.4037\n",
      "Epoch 2901/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.0666 - val_loss: 160.3540\n",
      "Epoch 2902/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.0463 - val_loss: 166.3824\n",
      "Epoch 2903/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.8981 - val_loss: 168.0079\n",
      "Epoch 2904/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.5850 - val_loss: 166.4888\n",
      "Epoch 2905/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.1026 - val_loss: 163.9477\n",
      "Epoch 2906/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 13.4917 - val_loss: 163.6476\n",
      "Epoch 2907/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 18.2294 - val_loss: 163.0989\n",
      "Epoch 2908/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.4162 - val_loss: 154.5751\n",
      "Epoch 2909/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.6283 - val_loss: 167.9522\n",
      "Epoch 2910/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.6846 - val_loss: 168.6127\n",
      "Epoch 2911/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.6557 - val_loss: 165.5316\n",
      "Epoch 2912/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.4859 - val_loss: 166.0690\n",
      "Epoch 2913/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.2580 - val_loss: 154.6605\n",
      "Epoch 2914/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.7157 - val_loss: 164.3541\n",
      "Epoch 2915/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.1960 - val_loss: 163.3164\n",
      "Epoch 2916/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.6102 - val_loss: 162.2050\n",
      "Epoch 2917/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 14.1874 - val_loss: 169.7941\n",
      "Epoch 2918/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 15.4422 - val_loss: 163.4852\n",
      "Epoch 2919/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.6078 - val_loss: 162.7118\n",
      "Epoch 2920/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.7433 - val_loss: 168.1749\n",
      "Epoch 2921/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.7104 - val_loss: 166.1483\n",
      "Epoch 2922/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.8697 - val_loss: 164.4292\n",
      "Epoch 2923/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.7177 - val_loss: 168.2540\n",
      "Epoch 2924/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.1575 - val_loss: 165.0565\n",
      "Epoch 2925/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.4872 - val_loss: 161.9179\n",
      "Epoch 2926/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 13.4302 - val_loss: 162.2779\n",
      "Epoch 2927/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 13.0942 - val_loss: 159.2945\n",
      "Epoch 2928/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 65us/step - loss: 13.0884 - val_loss: 168.5111\n",
      "Epoch 2929/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 13.9656 - val_loss: 159.7348\n",
      "Epoch 2930/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.9563 - val_loss: 169.0987\n",
      "Epoch 2931/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.5414 - val_loss: 166.1837\n",
      "Epoch 2932/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.6406 - val_loss: 160.9406\n",
      "Epoch 2933/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 12.7483 - val_loss: 159.8491\n",
      "Epoch 2934/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.9124 - val_loss: 159.3528\n",
      "Epoch 2935/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.5850 - val_loss: 167.6415\n",
      "Epoch 2936/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.9581 - val_loss: 165.1868\n",
      "Epoch 2937/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.9383 - val_loss: 164.6497\n",
      "Epoch 2938/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.8972 - val_loss: 160.8027\n",
      "Epoch 2939/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 12.8043 - val_loss: 162.7612\n",
      "Epoch 2940/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.0908 - val_loss: 163.0056\n",
      "Epoch 2941/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.9683 - val_loss: 165.4049\n",
      "Epoch 2942/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.1518 - val_loss: 167.4158\n",
      "Epoch 2943/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.5835 - val_loss: 168.1645\n",
      "Epoch 2944/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 14.6867 - val_loss: 162.2684\n",
      "Epoch 2945/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.0894 - val_loss: 166.2486\n",
      "Epoch 2946/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.3161 - val_loss: 161.6504\n",
      "Epoch 2947/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.7724 - val_loss: 160.9549\n",
      "Epoch 2948/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.0524 - val_loss: 164.6385\n",
      "Epoch 2949/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.1207 - val_loss: 157.0732\n",
      "Epoch 2950/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.8172 - val_loss: 164.6206\n",
      "Epoch 2951/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.1291 - val_loss: 171.4500\n",
      "Epoch 2952/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.0019 - val_loss: 163.3608\n",
      "Epoch 2953/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.9187 - val_loss: 167.6793\n",
      "Epoch 2954/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.3788 - val_loss: 163.8764\n",
      "Epoch 2955/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 12.8398 - val_loss: 175.0349\n",
      "Epoch 2956/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.8505 - val_loss: 167.3182\n",
      "Epoch 2957/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.4398 - val_loss: 167.5085\n",
      "Epoch 2958/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.0288 - val_loss: 162.1331\n",
      "Epoch 2959/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 17.2898 - val_loss: 164.8900\n",
      "Epoch 2960/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.3314 - val_loss: 168.5324\n",
      "Epoch 2961/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.8274 - val_loss: 164.5529\n",
      "Epoch 2962/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.5602 - val_loss: 160.4075\n",
      "Epoch 2963/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.9914 - val_loss: 166.3600\n",
      "Epoch 2964/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 13.4389 - val_loss: 165.2369\n",
      "Epoch 2965/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.2639 - val_loss: 162.9071\n",
      "Epoch 2966/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.6947 - val_loss: 160.7100\n",
      "Epoch 2967/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 15.8370 - val_loss: 157.0410\n",
      "Epoch 2968/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 15.2245 - val_loss: 174.8876\n",
      "Epoch 2969/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.3954 - val_loss: 164.0985\n",
      "Epoch 2970/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.7115 - val_loss: 162.5679\n",
      "Epoch 2971/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 16.0241 - val_loss: 151.6275\n",
      "Epoch 2972/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.6372 - val_loss: 161.8978\n",
      "Epoch 2973/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.0527 - val_loss: 158.0819\n",
      "Epoch 2974/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.1324 - val_loss: 167.6574\n",
      "Epoch 2975/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 15.4311 - val_loss: 159.5278\n",
      "Epoch 2976/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.1833 - val_loss: 159.7999\n",
      "Epoch 2977/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.0703 - val_loss: 156.1865\n",
      "Epoch 2978/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.4451 - val_loss: 161.5619\n",
      "Epoch 2979/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.5540 - val_loss: 160.8914\n",
      "Epoch 2980/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.6941 - val_loss: 155.7106\n",
      "Epoch 2981/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.9403 - val_loss: 170.1746\n",
      "Epoch 2982/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.4019 - val_loss: 159.6126\n",
      "Epoch 2983/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.4130 - val_loss: 162.5443\n",
      "Epoch 2984/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 17.2029 - val_loss: 167.2118\n",
      "Epoch 2985/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.3420 - val_loss: 166.9222\n",
      "Epoch 2986/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.8074 - val_loss: 168.0287\n",
      "Epoch 2987/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.8951 - val_loss: 163.8276\n",
      "Epoch 2988/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.3259 - val_loss: 157.7939\n",
      "Epoch 2989/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.1032 - val_loss: 156.5982\n",
      "Epoch 2990/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.2239 - val_loss: 155.0044\n",
      "Epoch 2991/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.2066 - val_loss: 153.4961\n",
      "Epoch 2992/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.3360 - val_loss: 162.7866\n",
      "Epoch 2993/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.2135 - val_loss: 151.9914\n",
      "Epoch 2994/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.8549 - val_loss: 159.7110\n",
      "Epoch 2995/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.8558 - val_loss: 170.3742\n",
      "Epoch 2996/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.6439 - val_loss: 164.8411\n",
      "Epoch 2997/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 12.9655 - val_loss: 164.0149\n",
      "Epoch 2998/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 14.1594 - val_loss: 163.8609\n",
      "Epoch 2999/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 14.1167 - val_loss: 159.2793\n",
      "Epoch 3000/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.2645 - val_loss: 174.1405\n",
      "Epoch 3001/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.8553 - val_loss: 165.9517\n",
      "Epoch 3002/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.7230 - val_loss: 148.3025\n",
      "Epoch 3003/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.6140 - val_loss: 161.3457\n",
      "Epoch 3004/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.7455 - val_loss: 162.8862\n",
      "Epoch 3005/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.3191 - val_loss: 159.3437\n",
      "Epoch 3006/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.4738 - val_loss: 156.5192\n",
      "Epoch 3007/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.1184 - val_loss: 162.3734\n",
      "Epoch 3008/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 12.9119 - val_loss: 145.2528\n",
      "Epoch 3009/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.0914 - val_loss: 165.4608\n",
      "Epoch 3010/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 11.9118 - val_loss: 161.1755\n",
      "Epoch 3011/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.4438 - val_loss: 160.4208\n",
      "Epoch 3012/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.0907 - val_loss: 146.0442\n",
      "Epoch 3013/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.3655 - val_loss: 159.9210\n",
      "Epoch 3014/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.9019 - val_loss: 158.5666\n",
      "Epoch 3015/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 15.4869 - val_loss: 158.5677\n",
      "Epoch 3016/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 15.4877 - val_loss: 157.6825\n",
      "Epoch 3017/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.9034 - val_loss: 157.7614\n",
      "Epoch 3018/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.4801 - val_loss: 159.7722\n",
      "Epoch 3019/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.5418 - val_loss: 160.9539\n",
      "Epoch 3020/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.1052 - val_loss: 157.6981\n",
      "Epoch 3021/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.2395 - val_loss: 162.8453\n",
      "Epoch 3022/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.7220 - val_loss: 155.5781\n",
      "Epoch 3023/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 17.7905 - val_loss: 155.1752\n",
      "Epoch 3024/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.3257 - val_loss: 179.5072\n",
      "Epoch 3025/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 15.4310 - val_loss: 152.9350\n",
      "Epoch 3026/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.9772 - val_loss: 160.1956\n",
      "Epoch 3027/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 18.0938 - val_loss: 171.7620\n",
      "Epoch 3028/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.9177 - val_loss: 160.5672\n",
      "Epoch 3029/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.3389 - val_loss: 166.5590\n",
      "Epoch 3030/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.8569 - val_loss: 161.8756\n",
      "Epoch 3031/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.6145 - val_loss: 162.6183\n",
      "Epoch 3032/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.5611 - val_loss: 160.3832\n",
      "Epoch 3033/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.0515 - val_loss: 164.8333\n",
      "Epoch 3034/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.6424 - val_loss: 162.1198\n",
      "Epoch 3035/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.1612 - val_loss: 165.2934\n",
      "Epoch 3036/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.9189 - val_loss: 168.8616\n",
      "Epoch 3037/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.3733 - val_loss: 160.7453\n",
      "Epoch 3038/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.3938 - val_loss: 163.5616\n",
      "Epoch 3039/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 12.9159 - val_loss: 159.0412\n",
      "Epoch 3040/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 13.5859 - val_loss: 157.6836\n",
      "Epoch 3041/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.7226 - val_loss: 163.3141\n",
      "Epoch 3042/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.5066 - val_loss: 162.2045\n",
      "Epoch 3043/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.9972 - val_loss: 164.7251\n",
      "Epoch 3044/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.5643 - val_loss: 153.7749\n",
      "Epoch 3045/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.6653 - val_loss: 159.0417\n",
      "Epoch 3046/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 12.9723 - val_loss: 161.9298\n",
      "Epoch 3047/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.6473 - val_loss: 161.6218\n",
      "Epoch 3048/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.7773 - val_loss: 155.5178\n",
      "Epoch 3049/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 13.5391 - val_loss: 156.7692\n",
      "Epoch 3050/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 12.7278 - val_loss: 161.7837\n",
      "Epoch 3051/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.2291 - val_loss: 158.8967\n",
      "Epoch 3052/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.4373 - val_loss: 177.4223\n",
      "Epoch 3053/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.6476 - val_loss: 168.2881\n",
      "Epoch 3054/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.4820 - val_loss: 157.3323\n",
      "Epoch 3055/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.1756 - val_loss: 170.5478\n",
      "Epoch 3056/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.4233 - val_loss: 167.3645\n",
      "Epoch 3057/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.1636 - val_loss: 159.1800\n",
      "Epoch 3058/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 12.6970 - val_loss: 167.8647\n",
      "Epoch 3059/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 12.7050 - val_loss: 161.7118\n",
      "Epoch 3060/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.8976 - val_loss: 163.9967\n",
      "Epoch 3061/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 11.5988 - val_loss: 162.9289\n",
      "Epoch 3062/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 14.4627 - val_loss: 171.2248\n",
      "Epoch 3063/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 16.5640 - val_loss: 168.3911\n",
      "Epoch 3064/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 14.5640 - val_loss: 169.2435\n",
      "Epoch 3065/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 14.1167 - val_loss: 161.3580\n",
      "Epoch 3066/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 13.2339 - val_loss: 175.3628\n",
      "Epoch 3067/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 12.1883 - val_loss: 168.0915\n",
      "Epoch 3068/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 12.5531 - val_loss: 166.5881\n",
      "Epoch 3069/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.5796 - val_loss: 175.4638\n",
      "Epoch 3070/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.7177 - val_loss: 159.8407\n",
      "Epoch 3071/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.1929 - val_loss: 164.6411\n",
      "Epoch 3072/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 13.6414 - val_loss: 164.7922\n",
      "Epoch 3073/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.5712 - val_loss: 169.5216\n",
      "Epoch 3074/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.6617 - val_loss: 171.8266\n",
      "Epoch 3075/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.4902 - val_loss: 169.1816\n",
      "Epoch 3076/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.5353 - val_loss: 163.3390\n",
      "Epoch 3077/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.5477 - val_loss: 160.9220\n",
      "Epoch 3078/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.1441 - val_loss: 166.2665\n",
      "Epoch 3079/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.8367 - val_loss: 170.5537\n",
      "Epoch 3080/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.9315 - val_loss: 161.6929\n",
      "Epoch 3081/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.4166 - val_loss: 162.8358\n",
      "Epoch 3082/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.6298 - val_loss: 169.8949\n",
      "Epoch 3083/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.8365 - val_loss: 159.6332\n",
      "Epoch 3084/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.3471 - val_loss: 161.1185\n",
      "Epoch 3085/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.9378 - val_loss: 157.6602\n",
      "Epoch 3086/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.1845 - val_loss: 164.7861\n",
      "Epoch 3087/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.1769 - val_loss: 172.8711\n",
      "Epoch 3088/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.3821 - val_loss: 172.9825\n",
      "Epoch 3089/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.6338 - val_loss: 169.4285\n",
      "Epoch 3090/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.1241 - val_loss: 154.8194\n",
      "Epoch 3091/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.5040 - val_loss: 170.4093\n",
      "Epoch 3092/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.8790 - val_loss: 172.9717\n",
      "Epoch 3093/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.8128 - val_loss: 171.0110\n",
      "Epoch 3094/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 12.3205 - val_loss: 171.9942\n",
      "Epoch 3095/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.6975 - val_loss: 168.9736\n",
      "Epoch 3096/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.3810 - val_loss: 170.1604\n",
      "Epoch 3097/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 13.2584 - val_loss: 169.3593\n",
      "Epoch 3098/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 12.8922 - val_loss: 167.3682\n",
      "Epoch 3099/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.6615 - val_loss: 157.5629\n",
      "Epoch 3100/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.2618 - val_loss: 169.6103\n",
      "Epoch 3101/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 13.3464 - val_loss: 164.7068\n",
      "Epoch 3102/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 13.9606 - val_loss: 162.7002\n",
      "Epoch 3103/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 12.6404 - val_loss: 156.3809\n",
      "Epoch 3104/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 14.3062 - val_loss: 157.6203\n",
      "Epoch 3105/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 14.3700 - val_loss: 166.8891\n",
      "Epoch 3106/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.7115 - val_loss: 166.0298\n",
      "Epoch 3107/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 12.6588 - val_loss: 170.4360\n",
      "Epoch 3108/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.2684 - val_loss: 167.2031\n",
      "Epoch 3109/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.3977 - val_loss: 152.1449\n",
      "Epoch 3110/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.2920 - val_loss: 166.0748\n",
      "Epoch 3111/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.2540 - val_loss: 164.5445\n",
      "Epoch 3112/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 15.0746 - val_loss: 156.4038\n",
      "Epoch 3113/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.4843 - val_loss: 168.7488\n",
      "Epoch 3114/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.5457 - val_loss: 159.2821\n",
      "Epoch 3115/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.7213 - val_loss: 159.4092\n",
      "Epoch 3116/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.9933 - val_loss: 164.9211\n",
      "Epoch 3117/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.2363 - val_loss: 164.1846\n",
      "Epoch 3118/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.9239 - val_loss: 171.0140\n",
      "Epoch 3119/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.0752 - val_loss: 170.7523\n",
      "Epoch 3120/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 12.0389 - val_loss: 167.5858\n",
      "Epoch 3121/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 13.3994 - val_loss: 159.6268\n",
      "Epoch 3122/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.8829 - val_loss: 178.6762\n",
      "Epoch 3123/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 12.4188 - val_loss: 170.3510\n",
      "Epoch 3124/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 12.4150 - val_loss: 167.6992\n",
      "Epoch 3125/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 13.7500 - val_loss: 172.0466\n",
      "Epoch 3126/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.6367 - val_loss: 169.7664\n",
      "Epoch 3127/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.3883 - val_loss: 171.4765\n",
      "Epoch 3128/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.6975 - val_loss: 161.4648\n",
      "Epoch 3129/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 17.2848 - val_loss: 163.2504\n",
      "Epoch 3130/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 15.5369 - val_loss: 170.5810\n",
      "Epoch 3131/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 14.5956 - val_loss: 158.5227\n",
      "Epoch 3132/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.9119 - val_loss: 163.9915\n",
      "Epoch 3133/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 13.8446 - val_loss: 176.5938\n",
      "Epoch 3134/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 14.0343 - val_loss: 165.5413\n",
      "Epoch 3135/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 13.4289 - val_loss: 176.3730\n",
      "Epoch 3136/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.4753 - val_loss: 160.8346\n",
      "Epoch 3137/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 14.1817 - val_loss: 167.9157\n",
      "Epoch 3138/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.8787 - val_loss: 160.6574\n",
      "Epoch 3139/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.4717 - val_loss: 165.9046\n",
      "Epoch 3140/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.6954 - val_loss: 166.9209\n",
      "Epoch 3141/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.2787 - val_loss: 179.8000\n",
      "Epoch 3142/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.0379 - val_loss: 177.3396\n",
      "Epoch 3143/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.3483 - val_loss: 160.5122\n",
      "Epoch 3144/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.3808 - val_loss: 172.6540\n",
      "Epoch 3145/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.3368 - val_loss: 177.0502\n",
      "Epoch 3146/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.9091 - val_loss: 162.1509\n",
      "Epoch 3147/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.7155 - val_loss: 174.0427\n",
      "Epoch 3148/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.0063 - val_loss: 171.0584\n",
      "Epoch 3149/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.2666 - val_loss: 163.8077\n",
      "Epoch 3150/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.2025 - val_loss: 175.6338\n",
      "Epoch 3151/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 16.2609 - val_loss: 157.7622\n",
      "Epoch 3152/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.0251 - val_loss: 165.4492\n",
      "Epoch 3153/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.6437 - val_loss: 157.0872\n",
      "Epoch 3154/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.1637 - val_loss: 166.3870\n",
      "Epoch 3155/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.0246 - val_loss: 165.3908\n",
      "Epoch 3156/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.9974 - val_loss: 161.6704\n",
      "Epoch 3157/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.1855 - val_loss: 163.6437\n",
      "Epoch 3158/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.0768 - val_loss: 163.6650\n",
      "Epoch 3159/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.3739 - val_loss: 172.1570\n",
      "Epoch 3160/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.9307 - val_loss: 168.2347\n",
      "Epoch 3161/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.3256 - val_loss: 169.7459\n",
      "Epoch 3162/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.0135 - val_loss: 164.8483\n",
      "Epoch 3163/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.0204 - val_loss: 165.8723\n",
      "Epoch 3164/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 16.7173 - val_loss: 162.6022\n",
      "Epoch 3165/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.1588 - val_loss: 165.6467\n",
      "Epoch 3166/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.7365 - val_loss: 165.4246\n",
      "Epoch 3167/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.3334 - val_loss: 167.3824\n",
      "Epoch 3168/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.5326 - val_loss: 166.6346\n",
      "Epoch 3169/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.6996 - val_loss: 160.2147\n",
      "Epoch 3170/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.5980 - val_loss: 159.1702\n",
      "Epoch 3171/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.8112 - val_loss: 164.8755\n",
      "Epoch 3172/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.4954 - val_loss: 179.9935\n",
      "Epoch 3173/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.5638 - val_loss: 167.5682\n",
      "Epoch 3174/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.0141 - val_loss: 174.4165\n",
      "Epoch 3175/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.2749 - val_loss: 165.4671\n",
      "Epoch 3176/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.8452 - val_loss: 171.1303\n",
      "Epoch 3177/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.1697 - val_loss: 155.2690\n",
      "Epoch 3178/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.9110 - val_loss: 159.2665\n",
      "Epoch 3179/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.8008 - val_loss: 170.7987\n",
      "Epoch 3180/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.6242 - val_loss: 172.3962\n",
      "Epoch 3181/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 17.1390 - val_loss: 171.6113\n",
      "Epoch 3182/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 14.1052 - val_loss: 161.5765\n",
      "Epoch 3183/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.1931 - val_loss: 160.2311\n",
      "Epoch 3184/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.1866 - val_loss: 168.5863\n",
      "Epoch 3185/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.6680 - val_loss: 156.2230\n",
      "Epoch 3186/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.3197 - val_loss: 168.5347\n",
      "Epoch 3187/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.5676 - val_loss: 167.9962\n",
      "Epoch 3188/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 19.1670 - val_loss: 162.0043\n",
      "Epoch 3189/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 15.7894 - val_loss: 173.1188\n",
      "Epoch 3190/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 14.6003 - val_loss: 166.4182\n",
      "Epoch 3191/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.4789 - val_loss: 162.3477\n",
      "Epoch 3192/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 12.5132 - val_loss: 161.8338\n",
      "Epoch 3193/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 11.2310 - val_loss: 161.6384\n",
      "Epoch 3194/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 12.0571 - val_loss: 167.6748\n",
      "Epoch 3195/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 12.6184 - val_loss: 165.3325\n",
      "Epoch 3196/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.1728 - val_loss: 163.5305\n",
      "Epoch 3197/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 12.3898 - val_loss: 166.6588\n",
      "Epoch 3198/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.0261 - val_loss: 162.0640\n",
      "Epoch 3199/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 13.1751 - val_loss: 161.9448\n",
      "Epoch 3200/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.5154 - val_loss: 156.5009\n",
      "Epoch 3201/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 14.9266 - val_loss: 162.1310\n",
      "Epoch 3202/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 13.4715 - val_loss: 163.6692\n",
      "Epoch 3203/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 13.0541 - val_loss: 160.5663\n",
      "Epoch 3204/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 13.0947 - val_loss: 162.5650\n",
      "Epoch 3205/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.5621 - val_loss: 168.4126\n",
      "Epoch 3206/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.1735 - val_loss: 176.5314\n",
      "Epoch 3207/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.4560 - val_loss: 166.6991\n",
      "Epoch 3208/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.0823 - val_loss: 165.8988\n",
      "Epoch 3209/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.7730 - val_loss: 169.4770\n",
      "Epoch 3210/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.5350 - val_loss: 165.9626\n",
      "Epoch 3211/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.8488 - val_loss: 167.2614\n",
      "Epoch 3212/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.7998 - val_loss: 163.9408\n",
      "Epoch 3213/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.6352 - val_loss: 159.6612\n",
      "Epoch 3214/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.6073 - val_loss: 166.3723\n",
      "Epoch 3215/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.4198 - val_loss: 161.5406\n",
      "Epoch 3216/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.6202 - val_loss: 171.1234\n",
      "Epoch 3217/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.3489 - val_loss: 162.0816\n",
      "Epoch 3218/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.9319 - val_loss: 174.7316\n",
      "Epoch 3219/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.5889 - val_loss: 162.9517\n",
      "Epoch 3220/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.5263 - val_loss: 164.6376\n",
      "Epoch 3221/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.3098 - val_loss: 176.8222\n",
      "Epoch 3222/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.9388 - val_loss: 171.1066\n",
      "Epoch 3223/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.5315 - val_loss: 163.2188\n",
      "Epoch 3224/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.5916 - val_loss: 171.4866\n",
      "Epoch 3225/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.4621 - val_loss: 167.0724\n",
      "Epoch 3226/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.5574 - val_loss: 168.2964\n",
      "Epoch 3227/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 11.9492 - val_loss: 166.2821\n",
      "Epoch 3228/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 15.4771 - val_loss: 163.3942\n",
      "Epoch 3229/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 14.5688 - val_loss: 167.0340\n",
      "Epoch 3230/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.5990 - val_loss: 167.2917\n",
      "Epoch 3231/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.5225 - val_loss: 166.4421\n",
      "Epoch 3232/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 12.6160 - val_loss: 162.8370\n",
      "Epoch 3233/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 14.4390 - val_loss: 169.5337\n",
      "Epoch 3234/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 12.9429 - val_loss: 167.0055\n",
      "Epoch 3235/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 13.4814 - val_loss: 161.0864\n",
      "Epoch 3236/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 12.0141 - val_loss: 163.4217\n",
      "Epoch 3237/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 12.5444 - val_loss: 169.2112\n",
      "Epoch 3238/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 13.7461 - val_loss: 163.9294\n",
      "Epoch 3239/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.8524 - val_loss: 166.6243\n",
      "Epoch 3240/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.7120 - val_loss: 168.5467\n",
      "Epoch 3241/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.7760 - val_loss: 170.4399\n",
      "Epoch 3242/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.7503 - val_loss: 170.3935\n",
      "Epoch 3243/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.3604 - val_loss: 171.0653\n",
      "Epoch 3244/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 16.1375 - val_loss: 166.2474\n",
      "Epoch 3245/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.2576 - val_loss: 162.5329\n",
      "Epoch 3246/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.4950 - val_loss: 159.7534\n",
      "Epoch 3247/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.5390 - val_loss: 160.3521\n",
      "Epoch 3248/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.7941 - val_loss: 164.5889\n",
      "Epoch 3249/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.7143 - val_loss: 168.8187\n",
      "Epoch 3250/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.5116 - val_loss: 174.1735\n",
      "Epoch 3251/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.3338 - val_loss: 166.7791\n",
      "Epoch 3252/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.6555 - val_loss: 163.6689\n",
      "Epoch 3253/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.1958 - val_loss: 163.4138\n",
      "Epoch 3254/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 14.2945 - val_loss: 167.6498\n",
      "Epoch 3255/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.5123 - val_loss: 171.1923\n",
      "Epoch 3256/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.6776 - val_loss: 177.0292\n",
      "Epoch 3257/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.0109 - val_loss: 166.6280\n",
      "Epoch 3258/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.1649 - val_loss: 163.9100\n",
      "Epoch 3259/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.4975 - val_loss: 163.9986\n",
      "Epoch 3260/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.7757 - val_loss: 168.1388\n",
      "Epoch 3261/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.9915 - val_loss: 168.1532\n",
      "Epoch 3262/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.3308 - val_loss: 168.9828\n",
      "Epoch 3263/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.8621 - val_loss: 163.2842\n",
      "Epoch 3264/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.1346 - val_loss: 172.6705\n",
      "Epoch 3265/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.6700 - val_loss: 154.5908\n",
      "Epoch 3266/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.5387 - val_loss: 163.9045\n",
      "Epoch 3267/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.4427 - val_loss: 165.9668\n",
      "Epoch 3268/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 14.09 - 0s 51us/step - loss: 13.9262 - val_loss: 167.1109\n",
      "Epoch 3269/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.6421 - val_loss: 164.0370\n",
      "Epoch 3270/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.8525 - val_loss: 168.7892\n",
      "Epoch 3271/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 12.0103 - val_loss: 169.3865\n",
      "Epoch 3272/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 12.1540 - val_loss: 172.0371\n",
      "Epoch 3273/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 12.6076 - val_loss: 172.6359\n",
      "Epoch 3274/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 13.7490 - val_loss: 182.5893\n",
      "Epoch 3275/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.3684 - val_loss: 167.1873\n",
      "Epoch 3276/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.4417 - val_loss: 175.5940\n",
      "Epoch 3277/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.3577 - val_loss: 165.5258\n",
      "Epoch 3278/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 11.7057 - val_loss: 171.6994\n",
      "Epoch 3279/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.2791 - val_loss: 171.4891\n",
      "Epoch 3280/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.0147 - val_loss: 166.4732\n",
      "Epoch 3281/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 11.3296 - val_loss: 171.8132\n",
      "Epoch 3282/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.8020 - val_loss: 175.0179\n",
      "Epoch 3283/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.6756 - val_loss: 166.8318\n",
      "Epoch 3284/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.3891 - val_loss: 170.9263\n",
      "Epoch 3285/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.8733 - val_loss: 177.9773\n",
      "Epoch 3286/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.4838 - val_loss: 173.2391\n",
      "Epoch 3287/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.1277 - val_loss: 175.0844\n",
      "Epoch 3288/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.6881 - val_loss: 169.6169\n",
      "Epoch 3289/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.4628 - val_loss: 168.0362\n",
      "Epoch 3290/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.4907 - val_loss: 172.5354\n",
      "Epoch 3291/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.3886 - val_loss: 165.9211\n",
      "Epoch 3292/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.6843 - val_loss: 176.9266\n",
      "Epoch 3293/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.9721 - val_loss: 167.6218\n",
      "Epoch 3294/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.9183 - val_loss: 168.7005\n",
      "Epoch 3295/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.1795 - val_loss: 165.1930\n",
      "Epoch 3296/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 14.7957 - val_loss: 173.5370\n",
      "Epoch 3297/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.9804 - val_loss: 157.4633\n",
      "Epoch 3298/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.6016 - val_loss: 166.4607\n",
      "Epoch 3299/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.4280 - val_loss: 169.5615\n",
      "Epoch 3300/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.8074 - val_loss: 174.5878\n",
      "Epoch 3301/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.4364 - val_loss: 172.6635\n",
      "Epoch 3302/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.1833 - val_loss: 170.1999\n",
      "Epoch 3303/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.4870 - val_loss: 175.3252\n",
      "Epoch 3304/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.5675 - val_loss: 167.9271\n",
      "Epoch 3305/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.3968 - val_loss: 173.9368\n",
      "Epoch 3306/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.7768 - val_loss: 166.0810\n",
      "Epoch 3307/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.1681 - val_loss: 170.4922\n",
      "Epoch 3308/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.6719 - val_loss: 171.2252\n",
      "Epoch 3309/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.1315 - val_loss: 161.2785\n",
      "Epoch 3310/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.3147 - val_loss: 168.5056\n",
      "Epoch 3311/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.5687 - val_loss: 158.5610\n",
      "Epoch 3312/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.1513 - val_loss: 165.5391\n",
      "Epoch 3313/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.1777 - val_loss: 168.2850\n",
      "Epoch 3314/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.5541 - val_loss: 170.3930\n",
      "Epoch 3315/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 15.5577 - val_loss: 162.8274\n",
      "Epoch 3316/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 11.7840 - val_loss: 171.5966\n",
      "Epoch 3317/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 11.9462 - val_loss: 167.5609\n",
      "Epoch 3318/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.5908 - val_loss: 155.7469\n",
      "Epoch 3319/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.1637 - val_loss: 160.9703\n",
      "Epoch 3320/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.5763 - val_loss: 167.8784\n",
      "Epoch 3321/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.9882 - val_loss: 165.4576\n",
      "Epoch 3322/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.9461 - val_loss: 168.5141\n",
      "Epoch 3323/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.6911 - val_loss: 165.1019\n",
      "Epoch 3324/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.9343 - val_loss: 161.8913\n",
      "Epoch 3325/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.4056 - val_loss: 165.2783\n",
      "Epoch 3326/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.8698 - val_loss: 165.6860\n",
      "Epoch 3327/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.2365 - val_loss: 174.4926\n",
      "Epoch 3328/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.4027 - val_loss: 180.4641\n",
      "Epoch 3329/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.2999 - val_loss: 164.8551\n",
      "Epoch 3330/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.6195 - val_loss: 161.9207\n",
      "Epoch 3331/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.5475 - val_loss: 166.5468\n",
      "Epoch 3332/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.7057 - val_loss: 172.5173\n",
      "Epoch 3333/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 13.2775 - val_loss: 168.1908\n",
      "Epoch 3334/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.3507 - val_loss: 161.5144\n",
      "Epoch 3335/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.5734 - val_loss: 169.0727\n",
      "Epoch 3336/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.1587 - val_loss: 158.1549\n",
      "Epoch 3337/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 14.5018 - val_loss: 159.5685\n",
      "Epoch 3338/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.8515 - val_loss: 161.6469\n",
      "Epoch 3339/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.1345 - val_loss: 167.5687\n",
      "Epoch 3340/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.0799 - val_loss: 168.6248\n",
      "Epoch 3341/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.3328 - val_loss: 171.6784\n",
      "Epoch 3342/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 10.8564 - val_loss: 165.3773\n",
      "Epoch 3343/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 14.4190 - val_loss: 168.4550\n",
      "Epoch 3344/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 12.9549 - val_loss: 167.1572\n",
      "Epoch 3345/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 13.5541 - val_loss: 165.6482\n",
      "Epoch 3346/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 13.5621 - val_loss: 176.6578\n",
      "Epoch 3347/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 12.1164 - val_loss: 173.3949\n",
      "Epoch 3348/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 13.3282 - val_loss: 176.9468\n",
      "Epoch 3349/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 14.2082 - val_loss: 155.8596\n",
      "Epoch 3350/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.8626 - val_loss: 176.0145\n",
      "Epoch 3351/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.3799 - val_loss: 181.8508\n",
      "Epoch 3352/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 13.2518 - val_loss: 173.4585\n",
      "Epoch 3353/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 12.7304 - val_loss: 167.9061\n",
      "Epoch 3354/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 13.2237 - val_loss: 184.7845\n",
      "Epoch 3355/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 13.5513 - val_loss: 178.1902\n",
      "Epoch 3356/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 12.4729 - val_loss: 173.3818\n",
      "Epoch 3357/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.2876 - val_loss: 174.2235\n",
      "Epoch 3358/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.7826 - val_loss: 181.3444\n",
      "Epoch 3359/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.2426 - val_loss: 164.5986\n",
      "Epoch 3360/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 12.1658 - val_loss: 164.5797\n",
      "Epoch 3361/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.0394 - val_loss: 173.8551\n",
      "Epoch 3362/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.7199 - val_loss: 170.6590\n",
      "Epoch 3363/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.1920 - val_loss: 172.6987\n",
      "Epoch 3364/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.4343 - val_loss: 179.9507\n",
      "Epoch 3365/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.0413 - val_loss: 169.6237\n",
      "Epoch 3366/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.6934 - val_loss: 169.2304\n",
      "Epoch 3367/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.0038 - val_loss: 165.2854\n",
      "Epoch 3368/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.6639 - val_loss: 170.7423\n",
      "Epoch 3369/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.6034 - val_loss: 168.3859\n",
      "Epoch 3370/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.6514 - val_loss: 163.5537\n",
      "Epoch 3371/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.7409 - val_loss: 162.7107\n",
      "Epoch 3372/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.3490 - val_loss: 164.6824\n",
      "Epoch 3373/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.4410 - val_loss: 162.3460\n",
      "Epoch 3374/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.0165 - val_loss: 171.1426\n",
      "Epoch 3375/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 13.0261 - val_loss: 159.6031\n",
      "Epoch 3376/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.2596 - val_loss: 161.5652\n",
      "Epoch 3377/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.1071 - val_loss: 170.3402\n",
      "Epoch 3378/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 17.0394 - val_loss: 166.9572\n",
      "Epoch 3379/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.5869 - val_loss: 169.5461\n",
      "Epoch 3380/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.7895 - val_loss: 172.2293\n",
      "Epoch 3381/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.1750 - val_loss: 165.0430\n",
      "Epoch 3382/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.4862 - val_loss: 159.4782\n",
      "Epoch 3383/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 11.4989 - val_loss: 167.4304\n",
      "Epoch 3384/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.4794 - val_loss: 163.3975\n",
      "Epoch 3385/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.1258 - val_loss: 167.8156\n",
      "Epoch 3386/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.3958 - val_loss: 165.4652\n",
      "Epoch 3387/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 17.0737 - val_loss: 168.3018\n",
      "Epoch 3388/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.3636 - val_loss: 170.6575\n",
      "Epoch 3389/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 11.4307 - val_loss: 170.1785\n",
      "Epoch 3390/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.8098 - val_loss: 172.5783\n",
      "Epoch 3391/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.4444 - val_loss: 171.9459\n",
      "Epoch 3392/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.1404 - val_loss: 178.5249\n",
      "Epoch 3393/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 12.5808 - val_loss: 169.3991\n",
      "Epoch 3394/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.3881 - val_loss: 167.7935\n",
      "Epoch 3395/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.1199 - val_loss: 160.2942\n",
      "Epoch 3396/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 11.6542 - val_loss: 171.4489\n",
      "Epoch 3397/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.0462 - val_loss: 170.1597\n",
      "Epoch 3398/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.5552 - val_loss: 158.7192\n",
      "Epoch 3399/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.2307 - val_loss: 156.2231\n",
      "Epoch 3400/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.1187 - val_loss: 169.1534\n",
      "Epoch 3401/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.6410 - val_loss: 166.0427\n",
      "Epoch 3402/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.8238 - val_loss: 170.3659\n",
      "Epoch 3403/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.0727 - val_loss: 164.3199\n",
      "Epoch 3404/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.2384 - val_loss: 168.0385\n",
      "Epoch 3405/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.9237 - val_loss: 166.8038\n",
      "Epoch 3406/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.8361 - val_loss: 167.1392\n",
      "Epoch 3407/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.9247 - val_loss: 177.8777\n",
      "Epoch 3408/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.7759 - val_loss: 166.6326\n",
      "Epoch 3409/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.3121 - val_loss: 167.6237\n",
      "Epoch 3410/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.3075 - val_loss: 173.4133\n",
      "Epoch 3411/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.2611 - val_loss: 177.4430\n",
      "Epoch 3412/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.9874 - val_loss: 169.7906\n",
      "Epoch 3413/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 11.7907 - val_loss: 175.3839\n",
      "Epoch 3414/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 12.4829 - val_loss: 167.2120\n",
      "Epoch 3415/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 15.4532 - val_loss: 165.0897\n",
      "Epoch 3416/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.2689 - val_loss: 165.8918\n",
      "Epoch 3417/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.4776 - val_loss: 164.4219\n",
      "Epoch 3418/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.1681 - val_loss: 167.8226\n",
      "Epoch 3419/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.1798 - val_loss: 169.4647\n",
      "Epoch 3420/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.2180 - val_loss: 164.7089\n",
      "Epoch 3421/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.7348 - val_loss: 172.3427\n",
      "Epoch 3422/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.0295 - val_loss: 172.6127\n",
      "Epoch 3423/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.2935 - val_loss: 162.5001\n",
      "Epoch 3424/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.5817 - val_loss: 172.2887\n",
      "Epoch 3425/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.8989 - val_loss: 177.0084\n",
      "Epoch 3426/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.9727 - val_loss: 185.2865\n",
      "Epoch 3427/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.0952 - val_loss: 170.7040\n",
      "Epoch 3428/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 13.0980 - val_loss: 165.8847\n",
      "Epoch 3429/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.8039 - val_loss: 175.1966\n",
      "Epoch 3430/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.5302 - val_loss: 169.4349\n",
      "Epoch 3431/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.5921 - val_loss: 169.8473\n",
      "Epoch 3432/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.7800 - val_loss: 172.0603\n",
      "Epoch 3433/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.7277 - val_loss: 172.4523\n",
      "Epoch 3434/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.3792 - val_loss: 163.0360\n",
      "Epoch 3435/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.0956 - val_loss: 183.9663\n",
      "Epoch 3436/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.1444 - val_loss: 166.1282\n",
      "Epoch 3437/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.6437 - val_loss: 169.0381\n",
      "Epoch 3438/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 12.8669 - val_loss: 167.5349\n",
      "Epoch 3439/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.2144 - val_loss: 171.1512\n",
      "Epoch 3440/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 11.9846 - val_loss: 167.0284\n",
      "Epoch 3441/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.1571 - val_loss: 166.8856\n",
      "Epoch 3442/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.3562 - val_loss: 181.8941\n",
      "Epoch 3443/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.7629 - val_loss: 173.4093\n",
      "Epoch 3444/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 16.2816 - val_loss: 165.4394\n",
      "Epoch 3445/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.9889 - val_loss: 175.2970\n",
      "Epoch 3446/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.1858 - val_loss: 175.2732\n",
      "Epoch 3447/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.4102 - val_loss: 168.7976\n",
      "Epoch 3448/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 11.1216 - val_loss: 162.6302\n",
      "Epoch 3449/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 10.0633 - val_loss: 169.6595\n",
      "Epoch 3450/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 11.7736 - val_loss: 168.4261\n",
      "Epoch 3451/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 15.7441 - val_loss: 165.2929\n",
      "Epoch 3452/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.8866 - val_loss: 174.5568\n",
      "Epoch 3453/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.3974 - val_loss: 179.8405\n",
      "Epoch 3454/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.7351 - val_loss: 164.3616\n",
      "Epoch 3455/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.9190 - val_loss: 174.5991\n",
      "Epoch 3456/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.0897 - val_loss: 167.9165\n",
      "Epoch 3457/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.3973 - val_loss: 167.5978\n",
      "Epoch 3458/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.6119 - val_loss: 158.8489\n",
      "Epoch 3459/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.1666 - val_loss: 174.3430\n",
      "Epoch 3460/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 11.7860 - val_loss: 167.1508\n",
      "Epoch 3461/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.8154 - val_loss: 167.6435\n",
      "Epoch 3462/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.3272 - val_loss: 169.5945\n",
      "Epoch 3463/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.2575 - val_loss: 172.9680\n",
      "Epoch 3464/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.5853 - val_loss: 162.1684\n",
      "Epoch 3465/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 12.8645 - val_loss: 167.6345\n",
      "Epoch 3466/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.7456 - val_loss: 167.4133\n",
      "Epoch 3467/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 12.5878 - val_loss: 170.8248\n",
      "Epoch 3468/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.7877 - val_loss: 171.5953\n",
      "Epoch 3469/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 11.4316 - val_loss: 163.5149\n",
      "Epoch 3470/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.1535 - val_loss: 162.6844\n",
      "Epoch 3471/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.3741 - val_loss: 163.7272\n",
      "Epoch 3472/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.1342 - val_loss: 169.5308\n",
      "Epoch 3473/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.1718 - val_loss: 164.1284\n",
      "Epoch 3474/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 13.1905 - val_loss: 171.2827\n",
      "Epoch 3475/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 12.1815 - val_loss: 171.3909\n",
      "Epoch 3476/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.3044 - val_loss: 168.4646\n",
      "Epoch 3477/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.6237 - val_loss: 164.6504\n",
      "Epoch 3478/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.9961 - val_loss: 159.9664\n",
      "Epoch 3479/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.7959 - val_loss: 169.4032\n",
      "Epoch 3480/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 12.4617 - val_loss: 165.9188\n",
      "Epoch 3481/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 13.0369 - val_loss: 168.4030\n",
      "Epoch 3482/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 16.4222 - val_loss: 175.1986\n",
      "Epoch 3483/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 14.7168 - val_loss: 167.9483\n",
      "Epoch 3484/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 13.8255 - val_loss: 174.8072\n",
      "Epoch 3485/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 11.8931 - val_loss: 168.4625\n",
      "Epoch 3486/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 11.7269 - val_loss: 167.7520\n",
      "Epoch 3487/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.3472 - val_loss: 172.3311\n",
      "Epoch 3488/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 11.7654 - val_loss: 173.4929\n",
      "Epoch 3489/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 12.7972 - val_loss: 177.6775\n",
      "Epoch 3490/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.7786 - val_loss: 178.9253\n",
      "Epoch 3491/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.1865 - val_loss: 169.8588\n",
      "Epoch 3492/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 13.1115 - val_loss: 174.3121\n",
      "Epoch 3493/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.0045 - val_loss: 171.7340\n",
      "Epoch 3494/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 12.3095 - val_loss: 171.7600\n",
      "Epoch 3495/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 11.8251 - val_loss: 163.2071\n",
      "Epoch 3496/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 15.3532 - val_loss: 166.6380\n",
      "Epoch 3497/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 11.5319 - val_loss: 172.2234\n",
      "Epoch 3498/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 11.3247 - val_loss: 171.6973\n",
      "Epoch 3499/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.6058 - val_loss: 170.7411\n",
      "Epoch 3500/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.9122 - val_loss: 190.1575\n",
      "Epoch 3501/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 14.5802 - val_loss: 170.2746\n",
      "Epoch 3502/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.2662 - val_loss: 170.4910\n",
      "Epoch 3503/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.3408 - val_loss: 164.8812\n",
      "Epoch 3504/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.9709 - val_loss: 165.0093\n",
      "Epoch 3505/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 11.1480 - val_loss: 164.3854\n",
      "Epoch 3506/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 12.3257 - val_loss: 160.7861\n",
      "Epoch 3507/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.0228 - val_loss: 161.6440\n",
      "Epoch 3508/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.2967 - val_loss: 168.3726\n",
      "Epoch 3509/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 11.9613 - val_loss: 167.0644\n",
      "Epoch 3510/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 11.5321 - val_loss: 171.8189\n",
      "Epoch 3511/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 12.5192 - val_loss: 168.9245\n",
      "Epoch 3512/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.2802 - val_loss: 167.2367\n",
      "Epoch 3513/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.5732 - val_loss: 174.9871\n",
      "Epoch 3514/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 12.3027 - val_loss: 165.6982\n",
      "Epoch 3515/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 12.8282 - val_loss: 172.5415\n",
      "Epoch 3516/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 12.6467 - val_loss: 163.6847\n",
      "Epoch 3517/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.2040 - val_loss: 161.6130\n",
      "Epoch 3518/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.1397 - val_loss: 168.9497\n",
      "Epoch 3519/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.1642 - val_loss: 165.0436\n",
      "Epoch 3520/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 10.9264 - val_loss: 167.2423\n",
      "Epoch 3521/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.4647 - val_loss: 164.6183\n",
      "Epoch 3522/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.2218 - val_loss: 167.2385\n",
      "Epoch 3523/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 11.7599 - val_loss: 165.7555\n",
      "Epoch 3524/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.3797 - val_loss: 175.9771\n",
      "Epoch 3525/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 13.3831 - val_loss: 165.7254\n",
      "Epoch 3526/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 13.0805 - val_loss: 172.2194\n",
      "Epoch 3527/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 11.0740 - val_loss: 170.2494\n",
      "Epoch 3528/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.1772 - val_loss: 165.5860\n",
      "Epoch 3529/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 12.4987 - val_loss: 168.8533\n",
      "Epoch 3530/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.6430 - val_loss: 174.4295\n",
      "Epoch 3531/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.5796 - val_loss: 171.9288\n",
      "Epoch 3532/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.2287 - val_loss: 172.7916\n",
      "Epoch 3533/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.0713 - val_loss: 173.3860\n",
      "Epoch 3534/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 15.4942 - val_loss: 181.5568\n",
      "Epoch 3535/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.6189 - val_loss: 171.1598\n",
      "Epoch 3536/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.6107 - val_loss: 175.8359\n",
      "Epoch 3537/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.7837 - val_loss: 177.4612\n",
      "Epoch 3538/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 12.4424 - val_loss: 170.0453\n",
      "Epoch 3539/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.0176 - val_loss: 173.8135\n",
      "Epoch 3540/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 11.4236 - val_loss: 164.1655\n",
      "Epoch 3541/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 12.3092 - val_loss: 165.6030\n",
      "Epoch 3542/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.8778 - val_loss: 172.2013\n",
      "Epoch 3543/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 11.6962 - val_loss: 175.4625\n",
      "Epoch 3544/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 11.7322 - val_loss: 174.9923\n",
      "Epoch 3545/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.8900 - val_loss: 174.1872\n",
      "Epoch 3546/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 12.5476 - val_loss: 174.4131\n",
      "Epoch 3547/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 12.6042 - val_loss: 165.6512\n",
      "Epoch 3548/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.1471 - val_loss: 169.4146\n",
      "Epoch 3549/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 11.9078 - val_loss: 166.9054\n",
      "Epoch 3550/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 17.0578 - val_loss: 175.0103\n",
      "Epoch 3551/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 12.4216 - val_loss: 162.6494\n",
      "Epoch 3552/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 12.4649 - val_loss: 168.8531\n",
      "Epoch 3553/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 12.3043 - val_loss: 165.9887\n",
      "Epoch 3554/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 11.9617 - val_loss: 172.8919\n",
      "Epoch 3555/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.9285 - val_loss: 163.8331\n",
      "Epoch 3556/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 11.3818 - val_loss: 171.2460\n",
      "Epoch 3557/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 12.7798 - val_loss: 168.0314\n",
      "Epoch 3558/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 11.5591 - val_loss: 166.6444\n",
      "Epoch 3559/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.3532 - val_loss: 180.2293\n",
      "Epoch 3560/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.6016 - val_loss: 171.8464\n",
      "Epoch 3561/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.0135 - val_loss: 177.4552\n",
      "Epoch 3562/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.1939 - val_loss: 174.8481\n",
      "Epoch 3563/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.3436 - val_loss: 166.8454\n",
      "Epoch 3564/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.9135 - val_loss: 177.8084\n",
      "Epoch 3565/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 16.2753 - val_loss: 175.8564\n",
      "Epoch 3566/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 12.8498 - val_loss: 173.5763\n",
      "Epoch 3567/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.4415 - val_loss: 169.5373\n",
      "Epoch 3568/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.4901 - val_loss: 172.9182\n",
      "Epoch 3569/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.2735 - val_loss: 176.1813\n",
      "Epoch 3570/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.1216 - val_loss: 177.0735\n",
      "Epoch 3571/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.3336 - val_loss: 175.5583\n",
      "Epoch 3572/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.4439 - val_loss: 177.4399\n",
      "Epoch 3573/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.7770 - val_loss: 167.0170\n",
      "Epoch 3574/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.7201 - val_loss: 167.1990\n",
      "Epoch 3575/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.0395 - val_loss: 173.1468\n",
      "Epoch 3576/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 11.8860 - val_loss: 166.5149\n",
      "Epoch 3577/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 12.1644 - val_loss: 172.3378\n",
      "Epoch 3578/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 13.3275 - val_loss: 172.9654\n",
      "Epoch 3579/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.9671 - val_loss: 171.6489\n",
      "Epoch 3580/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.8777 - val_loss: 172.7482\n",
      "Epoch 3581/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.1152 - val_loss: 175.2118\n",
      "Epoch 3582/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.5304 - val_loss: 170.2328\n",
      "Epoch 3583/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 11.9721 - val_loss: 166.5786\n",
      "Epoch 3584/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 10.8360 - val_loss: 176.2996\n",
      "Epoch 3585/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 64us/step - loss: 12.0013 - val_loss: 165.6329\n",
      "Epoch 3586/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 14.6281 - val_loss: 166.6801\n",
      "Epoch 3587/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.0262 - val_loss: 159.2864\n",
      "Epoch 3588/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 13.8447 - val_loss: 177.0962\n",
      "Epoch 3589/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 16.9067 - val_loss: 171.3842\n",
      "Epoch 3590/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.1015 - val_loss: 169.5761\n",
      "Epoch 3591/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.3624 - val_loss: 167.8878\n",
      "Epoch 3592/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.0835 - val_loss: 177.7011\n",
      "Epoch 3593/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 12.0621 - val_loss: 176.0270\n",
      "Epoch 3594/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.4980 - val_loss: 175.8866\n",
      "Epoch 3595/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.1007 - val_loss: 175.5432\n",
      "Epoch 3596/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.3331 - val_loss: 167.3699\n",
      "Epoch 3597/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 13.3572 - val_loss: 175.4463\n",
      "Epoch 3598/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 11.7077 - val_loss: 170.0248\n",
      "Epoch 3599/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 12.7692 - val_loss: 175.7100\n",
      "Epoch 3600/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.4941 - val_loss: 169.6579\n",
      "Epoch 3601/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 12.6446 - val_loss: 176.8829\n",
      "Epoch 3602/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 12.2236 - val_loss: 176.5436\n",
      "Epoch 3603/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 17.7383 - val_loss: 181.7058\n",
      "Epoch 3604/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 15.1253 - val_loss: 174.2407\n",
      "Epoch 3605/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.7779 - val_loss: 174.9340\n",
      "Epoch 3606/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 11.2540 - val_loss: 163.4231\n",
      "Epoch 3607/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 12.3094 - val_loss: 167.5323\n",
      "Epoch 3608/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.5740 - val_loss: 162.5656\n",
      "Epoch 3609/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.3173 - val_loss: 171.9807\n",
      "Epoch 3610/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.5365 - val_loss: 165.3691\n",
      "Epoch 3611/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 12.4763 - val_loss: 160.9961\n",
      "Epoch 3612/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 14.5234 - val_loss: 169.4309\n",
      "Epoch 3613/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.5954 - val_loss: 156.0132\n",
      "Epoch 3614/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 12.2752 - val_loss: 165.4866\n",
      "Epoch 3615/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.6670 - val_loss: 163.6174\n",
      "Epoch 3616/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.1443 - val_loss: 165.8593\n",
      "Epoch 3617/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 11.5087 - val_loss: 168.8268\n",
      "Epoch 3618/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 12.0826 - val_loss: 164.7990\n",
      "Epoch 3619/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 12.8579 - val_loss: 158.2086\n",
      "Epoch 3620/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 11.7847 - val_loss: 174.4319\n",
      "Epoch 3621/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.9781 - val_loss: 164.3496\n",
      "Epoch 3622/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.2358 - val_loss: 167.4467\n",
      "Epoch 3623/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.2343 - val_loss: 166.6831\n",
      "Epoch 3624/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.3858 - val_loss: 171.2902\n",
      "Epoch 3625/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.3415 - val_loss: 176.2896\n",
      "Epoch 3626/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.2463 - val_loss: 169.4746\n",
      "Epoch 3627/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.6062 - val_loss: 171.1326\n",
      "Epoch 3628/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.7665 - val_loss: 175.3106\n",
      "Epoch 3629/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 12.3584 - val_loss: 166.4639\n",
      "Epoch 3630/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 11.3762 - val_loss: 185.3400\n",
      "Epoch 3631/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 11.3892 - val_loss: 173.1171\n",
      "Epoch 3632/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 12.6828 - val_loss: 175.3807\n",
      "Epoch 3633/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 11.7623 - val_loss: 174.6381\n",
      "Epoch 3634/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.5375 - val_loss: 165.9045\n",
      "Epoch 3635/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.0980 - val_loss: 168.7618\n",
      "Epoch 3636/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 16.0835 - val_loss: 173.5931\n",
      "Epoch 3637/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.1929 - val_loss: 163.7063\n",
      "Epoch 3638/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.5585 - val_loss: 157.0348\n",
      "Epoch 3639/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.8662 - val_loss: 157.1642\n",
      "Epoch 3640/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 12.7034 - val_loss: 170.9511\n",
      "Epoch 3641/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.6888 - val_loss: 174.1691\n",
      "Epoch 3642/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 14.3304 - val_loss: 168.0547\n",
      "Epoch 3643/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.6596 - val_loss: 178.5440\n",
      "Epoch 3644/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 12.3191 - val_loss: 165.5650\n",
      "Epoch 3645/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 11.1509 - val_loss: 167.7411\n",
      "Epoch 3646/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.3224 - val_loss: 174.2322\n",
      "Epoch 3647/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.0165 - val_loss: 184.0350\n",
      "Epoch 3648/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.7255 - val_loss: 170.1807\n",
      "Epoch 3649/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 12.3555 - val_loss: 179.9188\n",
      "Epoch 3650/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.4017 - val_loss: 171.3262\n",
      "Epoch 3651/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 15.7377 - val_loss: 188.1608\n",
      "Epoch 3652/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 13.1553 - val_loss: 173.4169\n",
      "Epoch 3653/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.9474 - val_loss: 167.1196\n",
      "Epoch 3654/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.0977 - val_loss: 166.4888\n",
      "Epoch 3655/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 12.1555 - val_loss: 172.8974\n",
      "Epoch 3656/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 12.0607 - val_loss: 179.6251\n",
      "Epoch 3657/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 11.4993 - val_loss: 175.0056\n",
      "Epoch 3658/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 91us/step - loss: 11.4129 - val_loss: 167.9633\n",
      "Epoch 3659/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.3450 - val_loss: 165.3650\n",
      "Epoch 3660/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.0512 - val_loss: 174.5001\n",
      "Epoch 3661/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 14.1876 - val_loss: 171.2301\n",
      "Epoch 3662/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 11.7157 - val_loss: 174.9969\n",
      "Epoch 3663/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 11.6904 - val_loss: 176.9486\n",
      "Epoch 3664/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.6434 - val_loss: 174.3469\n",
      "Epoch 3665/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 11.6863 - val_loss: 172.3723\n",
      "Epoch 3666/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 12.5968 - val_loss: 177.1872\n",
      "Epoch 3667/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 12.6316 - val_loss: 162.3813\n",
      "Epoch 3668/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 11.6275 - val_loss: 177.5981\n",
      "Epoch 3669/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 12.1561 - val_loss: 175.7434\n",
      "Epoch 3670/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 13.9545 - val_loss: 168.1621\n",
      "Epoch 3671/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.4688 - val_loss: 179.4000\n",
      "Epoch 3672/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.9887 - val_loss: 165.8297\n",
      "Epoch 3673/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 13.2831 - val_loss: 176.7821\n",
      "Epoch 3674/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 11.6324 - val_loss: 167.9763\n",
      "Epoch 3675/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 12.66 - 0s 57us/step - loss: 12.6748 - val_loss: 184.4870\n",
      "Epoch 3676/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 12.4167 - val_loss: 171.0798\n",
      "Epoch 3677/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 12.1453 - val_loss: 174.2590\n",
      "Epoch 3678/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.2439 - val_loss: 174.9823\n",
      "Epoch 3679/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 12.8803 - val_loss: 175.3352\n",
      "Epoch 3680/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 11.9708 - val_loss: 172.5870\n",
      "Epoch 3681/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.2722 - val_loss: 169.2863\n",
      "Epoch 3682/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.6123 - val_loss: 177.3441\n",
      "Epoch 3683/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 11.9290 - val_loss: 173.1898\n",
      "Epoch 3684/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 12.1614 - val_loss: 173.1181\n",
      "Epoch 3685/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 12.3056 - val_loss: 170.5065\n",
      "Epoch 3686/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.4286 - val_loss: 169.0500\n",
      "Epoch 3687/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 12.6688 - val_loss: 178.4595\n",
      "Epoch 3688/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.8525 - val_loss: 168.8689\n",
      "Epoch 3689/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.2267 - val_loss: 167.2347\n",
      "Epoch 3690/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.8391 - val_loss: 178.7082\n",
      "Epoch 3691/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.8589 - val_loss: 177.7399\n",
      "Epoch 3692/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.1481 - val_loss: 168.1979\n",
      "Epoch 3693/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.9059 - val_loss: 165.9946\n",
      "Epoch 3694/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.4505 - val_loss: 169.6817\n",
      "Epoch 3695/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.2220 - val_loss: 184.8225\n",
      "Epoch 3696/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.1574 - val_loss: 176.3739\n",
      "Epoch 3697/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.1923 - val_loss: 181.2424\n",
      "Epoch 3698/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.4468 - val_loss: 173.5377\n",
      "Epoch 3699/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 15.1148 - val_loss: 179.1569\n",
      "Epoch 3700/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.5397 - val_loss: 175.2103\n",
      "Epoch 3701/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.5974 - val_loss: 172.7124\n",
      "Epoch 3702/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.3338 - val_loss: 170.7717\n",
      "Epoch 3703/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.9229 - val_loss: 175.8050\n",
      "Epoch 3704/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.9006 - val_loss: 175.2273\n",
      "Epoch 3705/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.8883 - val_loss: 176.6852\n",
      "Epoch 3706/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.9629 - val_loss: 173.7841\n",
      "Epoch 3707/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.5628 - val_loss: 162.7409\n",
      "Epoch 3708/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.3324 - val_loss: 171.2956\n",
      "Epoch 3709/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.4297 - val_loss: 167.6146\n",
      "Epoch 3710/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 11.4185 - val_loss: 175.9722\n",
      "Epoch 3711/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 12.1366 - val_loss: 172.9804\n",
      "Epoch 3712/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.3035 - val_loss: 174.4972\n",
      "Epoch 3713/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.9372 - val_loss: 174.7535\n",
      "Epoch 3714/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.6667 - val_loss: 173.3690\n",
      "Epoch 3715/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.7360 - val_loss: 172.3549\n",
      "Epoch 3716/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.1976 - val_loss: 168.7702\n",
      "Epoch 3717/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.5656 - val_loss: 168.3873\n",
      "Epoch 3718/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.4251 - val_loss: 160.8833\n",
      "Epoch 3719/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.7412 - val_loss: 174.1204\n",
      "Epoch 3720/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.3573 - val_loss: 168.3526\n",
      "Epoch 3721/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.3818 - val_loss: 170.8216\n",
      "Epoch 3722/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.0952 - val_loss: 162.6549\n",
      "Epoch 3723/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.1461 - val_loss: 165.4738\n",
      "Epoch 3724/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.4104 - val_loss: 174.8311\n",
      "Epoch 3725/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.0174 - val_loss: 176.8088\n",
      "Epoch 3726/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.2560 - val_loss: 179.4506\n",
      "Epoch 3727/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.8673 - val_loss: 172.5040\n",
      "Epoch 3728/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.6628 - val_loss: 185.0725\n",
      "Epoch 3729/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.0335 - val_loss: 173.6718\n",
      "Epoch 3730/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.0573 - val_loss: 165.3976\n",
      "Epoch 3731/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.7108 - val_loss: 171.6483\n",
      "Epoch 3732/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.0140 - val_loss: 178.7083\n",
      "Epoch 3733/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.0709 - val_loss: 168.7765\n",
      "Epoch 3734/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.1173 - val_loss: 183.7880\n",
      "Epoch 3735/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.4307 - val_loss: 166.4216\n",
      "Epoch 3736/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.9765 - val_loss: 176.0076\n",
      "Epoch 3737/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.5601 - val_loss: 173.0223\n",
      "Epoch 3738/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.6855 - val_loss: 173.6112\n",
      "Epoch 3739/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.0218 - val_loss: 174.5406\n",
      "Epoch 3740/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.3020 - val_loss: 177.0686\n",
      "Epoch 3741/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 11.9318 - val_loss: 173.2590\n",
      "Epoch 3742/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 12.2945 - val_loss: 171.2324\n",
      "Epoch 3743/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.1632 - val_loss: 175.8118\n",
      "Epoch 3744/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.7470 - val_loss: 168.9659\n",
      "Epoch 3745/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.9420 - val_loss: 169.6835\n",
      "Epoch 3746/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.1828 - val_loss: 182.9906\n",
      "Epoch 3747/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.9474 - val_loss: 183.9848\n",
      "Epoch 3748/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.9994 - val_loss: 169.1476\n",
      "Epoch 3749/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.5952 - val_loss: 167.0283\n",
      "Epoch 3750/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.5052 - val_loss: 172.2028\n",
      "Epoch 3751/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.6108 - val_loss: 169.1983\n",
      "Epoch 3752/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.0616 - val_loss: 165.3839\n",
      "Epoch 3753/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 12.3320 - val_loss: 171.0586\n",
      "Epoch 3754/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 11.2469 - val_loss: 174.9417\n",
      "Epoch 3755/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 12.0355 - val_loss: 168.7395\n",
      "Epoch 3756/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 12.9838 - val_loss: 173.7715\n",
      "Epoch 3757/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.5389 - val_loss: 166.9975\n",
      "Epoch 3758/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.1415 - val_loss: 169.1540\n",
      "Epoch 3759/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.8666 - val_loss: 171.9425\n",
      "Epoch 3760/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.9031 - val_loss: 181.6580\n",
      "Epoch 3761/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.5848 - val_loss: 182.1686\n",
      "Epoch 3762/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.8871 - val_loss: 180.5841\n",
      "Epoch 3763/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.8058 - val_loss: 173.4235\n",
      "Epoch 3764/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.1874 - val_loss: 180.1059\n",
      "Epoch 3765/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 10.8565 - val_loss: 170.5504\n",
      "Epoch 3766/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.2917 - val_loss: 177.2603\n",
      "Epoch 3767/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.6487 - val_loss: 177.0871\n",
      "Epoch 3768/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.3946 - val_loss: 165.4955\n",
      "Epoch 3769/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.9642 - val_loss: 180.3953\n",
      "Epoch 3770/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.5907 - val_loss: 178.5388\n",
      "Epoch 3771/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 11.1519 - val_loss: 176.4407\n",
      "Epoch 3772/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 14.1801 - val_loss: 170.9513\n",
      "Epoch 3773/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.9170 - val_loss: 171.5839\n",
      "Epoch 3774/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 11.5628 - val_loss: 178.3893\n",
      "Epoch 3775/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.9254 - val_loss: 177.2659\n",
      "Epoch 3776/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.4604 - val_loss: 185.5320\n",
      "Epoch 3777/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.9512 - val_loss: 171.4705\n",
      "Epoch 3778/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 12.2250 - val_loss: 174.0518\n",
      "Epoch 3779/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.3284 - val_loss: 172.7887\n",
      "Epoch 3780/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.2260 - val_loss: 175.1308\n",
      "Epoch 3781/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 12.2778 - val_loss: 172.2058\n",
      "Epoch 3782/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.4836 - val_loss: 176.5194\n",
      "Epoch 3783/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.0106 - val_loss: 178.0954\n",
      "Epoch 3784/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 14.2327 - val_loss: 174.2006\n",
      "Epoch 3785/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.1612 - val_loss: 175.7939\n",
      "Epoch 3786/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.7316 - val_loss: 174.0252\n",
      "Epoch 3787/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.3758 - val_loss: 170.3353\n",
      "Epoch 3788/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.6702 - val_loss: 160.9676\n",
      "Epoch 3789/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.6738 - val_loss: 174.0908\n",
      "Epoch 3790/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 11.8684 - val_loss: 167.4279\n",
      "Epoch 3791/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.1884 - val_loss: 166.0240\n",
      "Epoch 3792/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 10.5707 - val_loss: 173.9495\n",
      "Epoch 3793/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.7974 - val_loss: 171.2010\n",
      "Epoch 3794/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.8097 - val_loss: 183.7996\n",
      "Epoch 3795/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.6505 - val_loss: 172.7680\n",
      "Epoch 3796/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.8149 - val_loss: 163.7387\n",
      "Epoch 3797/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 10.8193 - val_loss: 171.7512\n",
      "Epoch 3798/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 10.9379 - val_loss: 166.4759\n",
      "Epoch 3799/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 10.6926 - val_loss: 169.3782\n",
      "Epoch 3800/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.2725 - val_loss: 178.0228\n",
      "Epoch 3801/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 13.9541 - val_loss: 177.3331\n",
      "Epoch 3802/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.0888 - val_loss: 175.5777\n",
      "Epoch 3803/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.2349 - val_loss: 166.4569\n",
      "Epoch 3804/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.4679 - val_loss: 169.7936\n",
      "Epoch 3805/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.1590 - val_loss: 174.5525\n",
      "Epoch 3806/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.0051 - val_loss: 169.4526\n",
      "Epoch 3807/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.1714 - val_loss: 179.1809\n",
      "Epoch 3808/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.6099 - val_loss: 178.5253\n",
      "Epoch 3809/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 13.1711 - val_loss: 168.4289\n",
      "Epoch 3810/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.2262 - val_loss: 168.8557\n",
      "Epoch 3811/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.0246 - val_loss: 169.1310\n",
      "Epoch 3812/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.1405 - val_loss: 169.7669\n",
      "Epoch 3813/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.9920 - val_loss: 180.7448\n",
      "Epoch 3814/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.1909 - val_loss: 167.7647\n",
      "Epoch 3815/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.4240 - val_loss: 170.9151\n",
      "Epoch 3816/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.0640 - val_loss: 168.5026\n",
      "Epoch 3817/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.2286 - val_loss: 169.0579\n",
      "Epoch 3818/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 14.2118 - val_loss: 169.7076\n",
      "Epoch 3819/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.4433 - val_loss: 173.6412\n",
      "Epoch 3820/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 10.1076 - val_loss: 175.7570\n",
      "Epoch 3821/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.9770 - val_loss: 167.8415\n",
      "Epoch 3822/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.8137 - val_loss: 169.1383\n",
      "Epoch 3823/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 11.4969 - val_loss: 176.6369\n",
      "Epoch 3824/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 11.0286 - val_loss: 176.6261\n",
      "Epoch 3825/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 12.4715 - val_loss: 169.4672\n",
      "Epoch 3826/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 13.5584 - val_loss: 171.5923\n",
      "Epoch 3827/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.8223 - val_loss: 174.4752\n",
      "Epoch 3828/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.8532 - val_loss: 171.9476\n",
      "Epoch 3829/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.5551 - val_loss: 174.5979\n",
      "Epoch 3830/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.0898 - val_loss: 171.8764\n",
      "Epoch 3831/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.9672 - val_loss: 171.7940\n",
      "Epoch 3832/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.0915 - val_loss: 166.3590\n",
      "Epoch 3833/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.4289 - val_loss: 178.7495\n",
      "Epoch 3834/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.9217 - val_loss: 177.3042\n",
      "Epoch 3835/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.3707 - val_loss: 170.3102\n",
      "Epoch 3836/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.6452 - val_loss: 172.1576\n",
      "Epoch 3837/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.6172 - val_loss: 178.5839\n",
      "Epoch 3838/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.1452 - val_loss: 174.6642\n",
      "Epoch 3839/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.7502 - val_loss: 176.8059\n",
      "Epoch 3840/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.8686 - val_loss: 171.8429\n",
      "Epoch 3841/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.1172 - val_loss: 181.1806\n",
      "Epoch 3842/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 15.3374 - val_loss: 186.1880\n",
      "Epoch 3843/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.1304 - val_loss: 171.1003\n",
      "Epoch 3844/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.8055 - val_loss: 182.2322\n",
      "Epoch 3845/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.6671 - val_loss: 186.1320\n",
      "Epoch 3846/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 13.5258 - val_loss: 171.9359\n",
      "Epoch 3847/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.5525 - val_loss: 178.1962\n",
      "Epoch 3848/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.6506 - val_loss: 177.7586\n",
      "Epoch 3849/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.1422 - val_loss: 177.9448\n",
      "Epoch 3850/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.9011 - val_loss: 176.5294\n",
      "Epoch 3851/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.1000 - val_loss: 168.0767\n",
      "Epoch 3852/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.4570 - val_loss: 169.2813\n",
      "Epoch 3853/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 13.1995 - val_loss: 173.0091\n",
      "Epoch 3854/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.4918 - val_loss: 178.0536\n",
      "Epoch 3855/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 13.3480 - val_loss: 174.7297\n",
      "Epoch 3856/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.2046 - val_loss: 180.3203\n",
      "Epoch 3857/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.3307 - val_loss: 182.7293\n",
      "Epoch 3858/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.2640 - val_loss: 181.4355\n",
      "Epoch 3859/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.8770 - val_loss: 170.4093\n",
      "Epoch 3860/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 13.1947 - val_loss: 178.4741\n",
      "Epoch 3861/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 12.2102 - val_loss: 175.2452\n",
      "Epoch 3862/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.4694 - val_loss: 178.6983\n",
      "Epoch 3863/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.6544 - val_loss: 179.0787\n",
      "Epoch 3864/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 11.7582 - val_loss: 179.8249\n",
      "Epoch 3865/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.7471 - val_loss: 176.0035\n",
      "Epoch 3866/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.2986 - val_loss: 179.4077\n",
      "Epoch 3867/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.3336 - val_loss: 173.9222\n",
      "Epoch 3868/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.2472 - val_loss: 178.8454\n",
      "Epoch 3869/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.5204 - val_loss: 180.4080\n",
      "Epoch 3870/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.4800 - val_loss: 174.9494\n",
      "Epoch 3871/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.5369 - val_loss: 181.8000\n",
      "Epoch 3872/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 14.8627 - val_loss: 177.9815\n",
      "Epoch 3873/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.8022 - val_loss: 183.0561\n",
      "Epoch 3874/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 11.7460 - val_loss: 176.2857\n",
      "Epoch 3875/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.3265 - val_loss: 165.9066\n",
      "Epoch 3876/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.6498 - val_loss: 173.2217\n",
      "Epoch 3877/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 12.2983 - val_loss: 175.9407\n",
      "Epoch 3878/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 12.1965 - val_loss: 176.6629\n",
      "Epoch 3879/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.6141 - val_loss: 176.2963\n",
      "Epoch 3880/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 12.0151 - val_loss: 177.1894\n",
      "Epoch 3881/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 11.7172 - val_loss: 170.8060\n",
      "Epoch 3882/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 12.3136 - val_loss: 172.1996\n",
      "Epoch 3883/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 12.8818 - val_loss: 175.3732\n",
      "Epoch 3884/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 14.6189 - val_loss: 178.0910\n",
      "Epoch 3885/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 14.0879 - val_loss: 171.2517\n",
      "Epoch 3886/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 11.7081 - val_loss: 176.8819\n",
      "Epoch 3887/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 11.4732 - val_loss: 180.6610\n",
      "Epoch 3888/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 11.4697 - val_loss: 173.1297\n",
      "Epoch 3889/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 11.9303 - val_loss: 177.3041\n",
      "Epoch 3890/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 13.5807 - val_loss: 173.1911\n",
      "Epoch 3891/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 13.4201 - val_loss: 169.6446\n",
      "Epoch 3892/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 11.7136 - val_loss: 175.3934\n",
      "Epoch 3893/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 11.3638 - val_loss: 178.8616\n",
      "Epoch 3894/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 12.6792 - val_loss: 171.6402\n",
      "Epoch 3895/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 11.4780 - val_loss: 172.8388\n",
      "Epoch 3896/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 13.6677 - val_loss: 182.3745\n",
      "Epoch 3897/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 13.0075 - val_loss: 183.1236\n",
      "Epoch 3898/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 13.7829 - val_loss: 170.1351\n",
      "Epoch 3899/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 14.9290 - val_loss: 175.0440\n",
      "Epoch 3900/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 12.4978 - val_loss: 173.8765\n",
      "Epoch 3901/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 11.3398 - val_loss: 179.3553\n",
      "Epoch 3902/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 10.6532 - val_loss: 172.1155\n",
      "Epoch 3903/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 10.4653 - val_loss: 182.5963\n",
      "Epoch 3904/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 11.3178 - val_loss: 176.6793\n",
      "Epoch 3905/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 13.1106 - val_loss: 165.2853\n",
      "Epoch 3906/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 11.9154 - val_loss: 174.7119\n",
      "Epoch 3907/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 13.7881 - val_loss: 181.4633\n",
      "Epoch 3908/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 12.2771 - val_loss: 169.2857\n",
      "Epoch 3909/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 12.2250 - val_loss: 173.9198\n",
      "Epoch 3910/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 13.6163 - val_loss: 180.5962\n",
      "Epoch 3911/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 11.3248 - val_loss: 174.9907\n",
      "Epoch 3912/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 12.5364 - val_loss: 173.0391\n",
      "Epoch 3913/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 11.9961 - val_loss: 185.7411\n",
      "Epoch 3914/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 14.6440 - val_loss: 166.9822\n",
      "Epoch 3915/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 11.7669 - val_loss: 168.1269\n",
      "Epoch 3916/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 10.6213 - val_loss: 175.9204\n",
      "Epoch 3917/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 11.3577 - val_loss: 178.7691\n",
      "Epoch 3918/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 11.5480 - val_loss: 178.4717\n",
      "Epoch 3919/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 11.3124 - val_loss: 176.9331\n",
      "Epoch 3920/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 10.8908 - val_loss: 177.1315\n",
      "Epoch 3921/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 13.1758 - val_loss: 171.5482\n",
      "Epoch 3922/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 14.2095 - val_loss: 184.6663\n",
      "Epoch 3923/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 12.4902 - val_loss: 171.3872\n",
      "Epoch 3924/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 12.8149 - val_loss: 181.1074\n",
      "Epoch 3925/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 10.8804 - val_loss: 171.9145\n",
      "Epoch 3926/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 11.5089 - val_loss: 183.1332\n",
      "Epoch 3927/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 13.3319 - val_loss: 180.3008\n",
      "Epoch 3928/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 14.1407 - val_loss: 187.4742\n",
      "Epoch 3929/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 14.2411 - val_loss: 177.2140\n",
      "Epoch 3930/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 14.1027 - val_loss: 184.2680\n",
      "Epoch 3931/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 14.2749 - val_loss: 172.1789\n",
      "Epoch 3932/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 12.1663 - val_loss: 184.2316\n",
      "Epoch 3933/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 12.7626 - val_loss: 170.9770\n",
      "Epoch 3934/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 11.6661 - val_loss: 172.4560\n",
      "Epoch 3935/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 10.7670 - val_loss: 179.0881\n",
      "Epoch 3936/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 11.1617 - val_loss: 173.7760\n",
      "Epoch 3937/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 12.8244 - val_loss: 176.5104\n",
      "Epoch 3938/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 13.0616 - val_loss: 175.4356\n",
      "Epoch 3939/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 13.3921 - val_loss: 176.4566\n",
      "Epoch 3940/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 11.7185 - val_loss: 176.1503\n",
      "Epoch 3941/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 11.4328 - val_loss: 171.1102\n",
      "Epoch 3942/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 11.5102 - val_loss: 174.9518\n",
      "Epoch 3943/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 15.4884 - val_loss: 168.7565\n",
      "Epoch 3944/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 12.1817 - val_loss: 174.4179\n",
      "Epoch 3945/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 12.1909 - val_loss: 171.0662\n",
      "Epoch 3946/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 12.4112 - val_loss: 180.4030\n",
      "Epoch 3947/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 11.4896 - val_loss: 169.9523\n",
      "Epoch 3948/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 12.3962 - val_loss: 177.2570\n",
      "Epoch 3949/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 11.7581 - val_loss: 175.4535\n",
      "Epoch 3950/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 80us/step - loss: 14.5005 - val_loss: 170.6092\n",
      "Epoch 3951/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 12.3216 - val_loss: 166.4905\n",
      "Epoch 3952/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 11.2564 - val_loss: 177.6852\n",
      "Epoch 3953/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 12.3392 - val_loss: 169.4913\n",
      "Epoch 3954/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 12.6235 - val_loss: 167.7465\n",
      "Epoch 3955/10000\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 12.1903 - val_loss: 162.8959\n",
      "Epoch 3956/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 13.3336 - val_loss: 175.5632\n",
      "Epoch 3957/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 12.3498 - val_loss: 181.4231\n",
      "Epoch 3958/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 13.0946 - val_loss: 170.2938\n",
      "Epoch 3959/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 13.0832 - val_loss: 178.0276\n",
      "Epoch 3960/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 12.8187 - val_loss: 171.7984\n",
      "Epoch 3961/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 11.6086 - val_loss: 171.4513\n",
      "Epoch 3962/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 11.8401 - val_loss: 160.0031\n",
      "Epoch 3963/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 12.6609 - val_loss: 177.8099\n",
      "Epoch 3964/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 11.2698 - val_loss: 173.3473\n",
      "Epoch 3965/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 13.8929 - val_loss: 162.0188\n",
      "Epoch 3966/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 12.1021 - val_loss: 169.8674\n",
      "Epoch 3967/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 11.4974 - val_loss: 167.8978\n",
      "Epoch 3968/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 13.1618 - val_loss: 173.1788\n",
      "Epoch 3969/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 11.8656 - val_loss: 174.5513\n",
      "Epoch 3970/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 11.7863 - val_loss: 168.5185\n",
      "Epoch 3971/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 12.6415 - val_loss: 171.2660\n",
      "Epoch 3972/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 11.5152 - val_loss: 170.5334\n",
      "Epoch 3973/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 11.10 - 0s 57us/step - loss: 11.0914 - val_loss: 176.2386\n",
      "Epoch 3974/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 12.0314 - val_loss: 180.9575\n",
      "Epoch 3975/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 11.4198 - val_loss: 172.2860\n",
      "Epoch 3976/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 14.4178 - val_loss: 183.2518\n",
      "Epoch 3977/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 11.8283 - val_loss: 168.7195\n",
      "Epoch 3978/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 11.4920 - val_loss: 165.5493\n",
      "Epoch 3979/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 14.1028 - val_loss: 173.5183\n",
      "Epoch 3980/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 11.4172 - val_loss: 174.0606\n",
      "Epoch 3981/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 11.3832 - val_loss: 163.8105\n",
      "Epoch 3982/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 12.0384 - val_loss: 169.0522\n",
      "Epoch 3983/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 15.0500 - val_loss: 174.3801\n",
      "Epoch 3984/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 14.2702 - val_loss: 167.5246\n",
      "Epoch 3985/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 11.4750 - val_loss: 177.2964\n",
      "Epoch 3986/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 13.4672 - val_loss: 184.2153\n",
      "Epoch 3987/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 14.0253 - val_loss: 171.4498\n",
      "Epoch 3988/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 12.6092 - val_loss: 173.3428\n",
      "Epoch 3989/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 12.8577 - val_loss: 188.6947\n",
      "Epoch 3990/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 13.6893 - val_loss: 172.2287\n",
      "Epoch 3991/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 10.6591 - val_loss: 181.5791\n",
      "Epoch 3992/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 11.4988 - val_loss: 183.5419\n",
      "Epoch 3993/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 12.4765 - val_loss: 179.3340\n",
      "Epoch 3994/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 13.3438 - val_loss: 178.4580\n",
      "Epoch 3995/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 14.3464 - val_loss: 173.4886\n",
      "Epoch 3996/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 11.7963 - val_loss: 178.3544\n",
      "Epoch 3997/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 12.1143 - val_loss: 168.3780\n",
      "Epoch 3998/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 10.3396 - val_loss: 178.6574\n",
      "Epoch 3999/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 10.6959 - val_loss: 175.9920\n",
      "Epoch 4000/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 11.2423 - val_loss: 175.9729\n",
      "Epoch 4001/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 11.2537 - val_loss: 175.0049\n",
      "Epoch 4002/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 12.5402 - val_loss: 172.8016\n",
      "Epoch 4003/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 13.4013 - val_loss: 165.0267\n",
      "Epoch 4004/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 12.6187 - val_loss: 171.3023\n",
      "Epoch 4005/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 13.3363 - val_loss: 171.3568\n",
      "Epoch 4006/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 11.2679 - val_loss: 178.1874\n",
      "Epoch 4007/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 14.6346 - val_loss: 180.7718\n",
      "Epoch 4008/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 12.5358 - val_loss: 173.2265\n",
      "Epoch 04008: early stopping\n",
      "Fold score (RMSE): 13.096099853515625\n",
      "Fold #2\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 126us/step - loss: 11291.0647 - val_loss: 1079.5850\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 715.0195 - val_loss: 614.6360\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 458.7736 - val_loss: 413.8498\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 343.8986 - val_loss: 333.4592\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 302.8923 - val_loss: 314.3304\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 290.9109 - val_loss: 310.8749\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 283.1471 - val_loss: 299.3864\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 272.3930 - val_loss: 288.5922\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 267.0648 - val_loss: 304.5374\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 266.0517 - val_loss: 286.7766\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 257.8517 - val_loss: 274.2591\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 262.0473 - val_loss: 279.4188\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 250.5798 - val_loss: 278.7034\n",
      "Epoch 14/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 248.7506 - val_loss: 268.5744\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 247.1398 - val_loss: 267.5228\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 245.8365 - val_loss: 284.2679\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 237.5168 - val_loss: 265.0933\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 240.6503 - val_loss: 304.5660\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 233.7129 - val_loss: 269.8227\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 232.6629 - val_loss: 258.4986\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 232.5696 - val_loss: 261.2154\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 230.4181 - val_loss: 254.8688\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 229.1440 - val_loss: 259.1541\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 228.6158 - val_loss: 256.4743\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 224.3518 - val_loss: 256.2470\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 223.7591 - val_loss: 258.9489\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 221.1108 - val_loss: 277.2014\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 228.8986 - val_loss: 252.1324\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 221.1081 - val_loss: 254.8477\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 216.7646 - val_loss: 247.8106\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 212.0850 - val_loss: 253.0472\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 219.2148 - val_loss: 253.4114\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 214.4025 - val_loss: 246.1894\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 216.0333 - val_loss: 262.6737\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 213.5938 - val_loss: 277.0907\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 212.0571 - val_loss: 242.6772\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 209.2679 - val_loss: 249.3325\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 211.1722 - val_loss: 242.0634\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 204.6219 - val_loss: 243.8045\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 202.5171 - val_loss: 243.4062\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 203.9213 - val_loss: 270.3150\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 206.1009 - val_loss: 252.1311\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 201.4608 - val_loss: 251.9347\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 197.9303 - val_loss: 244.0174\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 195.6806 - val_loss: 250.7527\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 196.8971 - val_loss: 255.5838\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 195.5269 - val_loss: 243.7971\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 203.1950 - val_loss: 238.4116\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 192.7191 - val_loss: 236.3689\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 192.9894 - val_loss: 314.0588\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 196.1441 - val_loss: 237.5008\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 188.6459 - val_loss: 241.8626\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 191.9102 - val_loss: 235.4776\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 186.7828 - val_loss: 239.1270\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 190.1851 - val_loss: 247.1392\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 183.9714 - val_loss: 242.4531\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 185.7698 - val_loss: 237.8881\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 180.2758 - val_loss: 231.5086\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 179.7482 - val_loss: 222.2322\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 178.5159 - val_loss: 241.3148\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 181.0970 - val_loss: 238.2867\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 184.4320 - val_loss: 224.0661\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 172.4711 - val_loss: 249.9648\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 174.6256 - val_loss: 224.9799\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 170.2671 - val_loss: 217.2519\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 175.6846 - val_loss: 230.8070\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 170.7954 - val_loss: 213.7952\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 171.0069 - val_loss: 226.3383\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 165.9820 - val_loss: 236.2805\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 168.4991 - val_loss: 244.9579\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 164.4789 - val_loss: 229.8435\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 165.1308 - val_loss: 219.0851\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 159.0531 - val_loss: 231.1437\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 164.3591 - val_loss: 215.7635\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 162.6205 - val_loss: 219.3661\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 159.8335 - val_loss: 213.5636\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 161.7407 - val_loss: 214.1742\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 155.5769 - val_loss: 209.5506\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 155.5609 - val_loss: 211.8638\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 151.2821 - val_loss: 214.9480\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 153.0166 - val_loss: 203.2776\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 151.2561 - val_loss: 211.0436\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 152.1592 - val_loss: 198.9902\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 140.4111 - val_loss: 195.0972\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 140.2135 - val_loss: 209.0012\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 142.6196 - val_loss: 213.1043\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 145.2583 - val_loss: 200.6531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 142.3366 - val_loss: 203.6859\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 138.9309 - val_loss: 200.8204\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 135.842 - 0s 57us/step - loss: 136.2917 - val_loss: 188.5538\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 140.4041 - val_loss: 219.4605\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 134.6953 - val_loss: 186.7996\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 136.9480 - val_loss: 208.0536\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 134.9348 - val_loss: 184.9702\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 126.2741 - val_loss: 194.4032\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 128.2100 - val_loss: 228.3740\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 136.3721 - val_loss: 196.0455\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 128.5718 - val_loss: 187.6480\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 123.0731 - val_loss: 186.8899\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 125.5711 - val_loss: 176.2336\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 125.2817 - val_loss: 180.8735\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 127.4555 - val_loss: 173.2042\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 119.9406 - val_loss: 171.2277\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 118.8356 - val_loss: 176.8270\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 114.7694 - val_loss: 177.4283\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 112.6810 - val_loss: 182.8989\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 120.5331 - val_loss: 174.7007\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 111.0536 - val_loss: 176.9386\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 115.7927 - val_loss: 175.5201\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 114.0436 - val_loss: 171.7306\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 113.5616 - val_loss: 180.7211\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 108.2303 - val_loss: 176.7704\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 118.6130 - val_loss: 191.7424\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 111.0303 - val_loss: 166.5633\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 113.7660 - val_loss: 180.2340\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 114.5814 - val_loss: 180.2363\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 106.5416 - val_loss: 170.4358\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 104.8446 - val_loss: 169.9009\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 113.8560 - val_loss: 170.5499\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 107.1430 - val_loss: 171.9154\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 112.2028 - val_loss: 177.3953\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 105.7763 - val_loss: 195.1340\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 110.3732 - val_loss: 191.8532\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 104.7729 - val_loss: 175.0922\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 104.8944 - val_loss: 171.3243\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 102.8850 - val_loss: 200.3508\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 111.8750 - val_loss: 178.8221\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 99.5261 - val_loss: 174.1634\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 105.2409 - val_loss: 162.6243\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 98.1622 - val_loss: 179.7710\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 101.2990 - val_loss: 173.3012\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 103.9781 - val_loss: 175.9802\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 101.6845 - val_loss: 166.5377\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 2s 188us/step - loss: 95.7186 - val_loss: 201.4196\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 100.0213 - val_loss: 181.6415\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 97.5238 - val_loss: 188.0875\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 101.5599 - val_loss: 175.8093\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 100.8966 - val_loss: 204.7323\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 95.1456 - val_loss: 179.9861\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 95.3880 - val_loss: 196.4747\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 101.3595 - val_loss: 173.4675\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 96.1094 - val_loss: 167.3338\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 94.0646 - val_loss: 171.6550\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 93.5122 - val_loss: 168.1224\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 93.3011 - val_loss: 181.4743\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 93.3274 - val_loss: 192.9307\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 94.3253 - val_loss: 178.8751\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 97.8325 - val_loss: 172.5350\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 94.0644 - val_loss: 173.6633\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 90.0433 - val_loss: 167.0219\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 87.2414 - val_loss: 169.9271\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 1s 127us/step - loss: 93.8306 - val_loss: 174.7537\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 91.9766 - val_loss: 176.6299\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 89.2549 - val_loss: 189.4366\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 90.3670 - val_loss: 172.0649\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 91.9582 - val_loss: 184.6236\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 94.8683 - val_loss: 177.8619\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 85.8160 - val_loss: 177.6240\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 88.9961 - val_loss: 168.5554\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 92.6264 - val_loss: 173.0662\n",
      "Epoch 161/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 135us/step - loss: 88.0539 - val_loss: 182.5837\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 85.2184 - val_loss: 170.9550\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 93.2058 - val_loss: 180.9199\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 88.3094 - val_loss: 201.4541\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 87.1322 - val_loss: 169.4194\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 89.6382 - val_loss: 180.2826\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 87.7433 - val_loss: 183.4382\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 81.4542 - val_loss: 184.9929\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 85.6204 - val_loss: 174.8105\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 82.8847 - val_loss: 162.1039\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 81.7002 - val_loss: 177.4875\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 81.9394 - val_loss: 180.5484\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 80.1096 - val_loss: 176.7230\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 83.0616 - val_loss: 175.4099\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 86.1622 - val_loss: 201.4863\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 81.0515 - val_loss: 178.4200\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 78.5011 - val_loss: 172.8933\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 84.8798 - val_loss: 179.8980\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 79.8977 - val_loss: 178.7189\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 84.7833 - val_loss: 181.7133\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 80.8857 - val_loss: 175.3375\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 81.8693 - val_loss: 168.0366\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 79.1796 - val_loss: 167.9619\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 83.2370 - val_loss: 175.2271\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 75.7876 - val_loss: 181.4602\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 79.5063 - val_loss: 186.1953\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 77.1029 - val_loss: 177.6212\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 77.0777 - val_loss: 167.4059\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 83.5109 - val_loss: 176.6289\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 77.8676 - val_loss: 173.2874\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 75.4340 - val_loss: 173.0111\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 77.8374 - val_loss: 176.4094\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 81.2790 - val_loss: 177.1067\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 74.6085 - val_loss: 175.2702\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 72.5927 - val_loss: 190.1257\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 77.1651 - val_loss: 170.3162\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 73.1479 - val_loss: 171.3967\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 76.0889 - val_loss: 180.6873\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 75.8473 - val_loss: 181.1003\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 73.9403 - val_loss: 163.9305\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 70.0207 - val_loss: 171.1877\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 68.5195 - val_loss: 170.6909\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 73.4445 - val_loss: 195.6353\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 76.9777 - val_loss: 182.5738\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 77.9861 - val_loss: 171.9692\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 73.4494 - val_loss: 168.9113\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 71.0965 - val_loss: 168.6008\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 74.3919 - val_loss: 175.5617\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 73.8916 - val_loss: 171.6759\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 72.2997 - val_loss: 177.4846\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 74.9673 - val_loss: 174.3654\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 67.8888 - val_loss: 173.0459\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 69.4043 - val_loss: 164.1527\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 69.5361 - val_loss: 171.0642\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 68.3346 - val_loss: 173.1753\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 71.8578 - val_loss: 176.3230\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 69.9068 - val_loss: 179.9603\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 69.3493 - val_loss: 185.7977\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 71.8151 - val_loss: 167.3032\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 66.1094 - val_loss: 182.9525\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 79.6866 - val_loss: 178.0303\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 68.0326 - val_loss: 175.7765\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 74.4926 - val_loss: 166.5499\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 67.1368 - val_loss: 176.1792\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 65.7428 - val_loss: 176.8606\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 63.3422 - val_loss: 186.8564\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 77.1354 - val_loss: 185.0467\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 66.7714 - val_loss: 178.1173\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 63.8465 - val_loss: 181.7881\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 67.3116 - val_loss: 172.9982\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 65.7242 - val_loss: 189.7446\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 70.7154 - val_loss: 207.1979\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 69.6944 - val_loss: 177.0984\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 65.3329 - val_loss: 176.1416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 70.3046 - val_loss: 165.3140\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 70.6260 - val_loss: 204.4693\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 66.3834 - val_loss: 177.0330\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 66.0440 - val_loss: 178.0237\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 62.2834 - val_loss: 191.0935\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 64.4947 - val_loss: 172.1220\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 61.8629 - val_loss: 202.0391\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 62.0128 - val_loss: 184.4433\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 66.9320 - val_loss: 183.2695\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 64.3819 - val_loss: 182.2916\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 66.0578 - val_loss: 167.2536\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 66.8271 - val_loss: 175.4142\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 62.2515 - val_loss: 176.3012\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 60.6394 - val_loss: 189.8290\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 67.1306 - val_loss: 174.2457\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 68.6467 - val_loss: 178.0097\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 62.2717 - val_loss: 182.9224\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 64.5143 - val_loss: 175.9219\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 64.4635 - val_loss: 188.0375\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 68.0656 - val_loss: 179.2552\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 64.6389 - val_loss: 199.4117\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 68.6273 - val_loss: 180.6802\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 61.1125 - val_loss: 176.3335\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 60.4273 - val_loss: 174.5715\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 60.2259 - val_loss: 183.1311\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 59.5591 - val_loss: 170.7253\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 61.3802 - val_loss: 172.9595\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 59.9361 - val_loss: 174.0828\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 62.9568 - val_loss: 184.4728\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 62.3027 - val_loss: 186.3650\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 65.1740 - val_loss: 177.9659\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 60.9802 - val_loss: 170.9167\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 63.2735 - val_loss: 183.8025\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 59.6711 - val_loss: 174.6111\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 65.4945 - val_loss: 192.2711\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 62.1758 - val_loss: 179.8946\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 60.6852 - val_loss: 174.6812\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 59.0706 - val_loss: 196.9132\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 54.9590 - val_loss: 181.0630\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 61.2577 - val_loss: 182.8239\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 63.3802 - val_loss: 198.6687\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 59.4332 - val_loss: 181.3494\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 61.2404 - val_loss: 202.2198\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 60.3982 - val_loss: 182.7079\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 58.3119 - val_loss: 197.0807\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 68.3754 - val_loss: 187.7897\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 56.4443 - val_loss: 181.4200\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 57.3937 - val_loss: 179.9129\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 62.6285 - val_loss: 200.2890\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 56.4503 - val_loss: 198.1405\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 53.2208 - val_loss: 189.1045\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 60.2023 - val_loss: 186.0580\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 57.2793 - val_loss: 190.7789\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 56.8942 - val_loss: 203.1119\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 59.2414 - val_loss: 222.6855\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 64.4365 - val_loss: 184.0384\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 54.9773 - val_loss: 178.6404\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 56.8917 - val_loss: 180.8856\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 56.1879 - val_loss: 181.4103\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 55.7651 - val_loss: 185.9570\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 59.9118 - val_loss: 194.2190\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 59.7755 - val_loss: 178.3916\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 57.8884 - val_loss: 173.6853\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 60.3323 - val_loss: 179.4873\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 54.7095 - val_loss: 187.8852\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 53.6803 - val_loss: 192.7790\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 57.2571 - val_loss: 184.7792\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 55.5701 - val_loss: 183.6327\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 54.3218 - val_loss: 185.3264\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 55.1258 - val_loss: 179.5202\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 52.4324 - val_loss: 183.1344\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 65.5717 - val_loss: 190.7194\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 51.6511 - val_loss: 183.9938\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 55.5302 - val_loss: 186.7086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 54.2893 - val_loss: 198.6822\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 55.0797 - val_loss: 192.6413\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 56.0317 - val_loss: 183.3727\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 54.0374 - val_loss: 186.7767\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 58.6827 - val_loss: 180.3027\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 55.5541 - val_loss: 190.7185\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 57.8842 - val_loss: 184.3861\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 53.0276 - val_loss: 191.3852\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 56.2162 - val_loss: 182.7368\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 55.0893 - val_loss: 184.4840\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 52.3585 - val_loss: 188.4135\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 54.2065 - val_loss: 193.9814\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 56.1587 - val_loss: 176.5816\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 49.5565 - val_loss: 194.4776\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 53.5720 - val_loss: 183.8543\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 51.1975 - val_loss: 202.2054\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 55.4069 - val_loss: 179.2452\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 51.5487 - val_loss: 188.3438\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 52.1704 - val_loss: 193.5569\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 50.1063 - val_loss: 195.5250\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 57.5214 - val_loss: 193.7733\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 55.8144 - val_loss: 174.6340\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 53.1016 - val_loss: 183.0627\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 55.2082 - val_loss: 204.2957\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 55.7846 - val_loss: 188.6625\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 52.1988 - val_loss: 197.0671\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 53.1173 - val_loss: 189.4328\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 53.9947 - val_loss: 198.3489\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 54.4057 - val_loss: 205.4054\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 50.2966 - val_loss: 186.5486\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 52.7045 - val_loss: 186.6676\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 50.8227 - val_loss: 186.4788\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 52.7140 - val_loss: 189.3529\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 49.7434 - val_loss: 198.7525\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 51.4595 - val_loss: 209.4785\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 46.7738 - val_loss: 187.2105\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 50.9590 - val_loss: 180.0933\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 55.9674 - val_loss: 191.9537\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 50.0844 - val_loss: 190.1780\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 51.7125 - val_loss: 186.5822\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 53.8267 - val_loss: 181.8586\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 48.9265 - val_loss: 197.0706\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 51.9695 - val_loss: 185.3579\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 48.8368 - val_loss: 203.7564\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 56.7197 - val_loss: 183.9604\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 54.4872 - val_loss: 187.1639\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 51.3030 - val_loss: 200.1492\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 49.7107 - val_loss: 198.3016\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 52.8182 - val_loss: 178.4328\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 52.3684 - val_loss: 193.3995\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 48.3295 - val_loss: 181.2368\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 53.3452 - val_loss: 192.9333\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 48.7757 - val_loss: 199.1462\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 49.8945 - val_loss: 216.5911\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 46.2037 - val_loss: 190.2780\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 51.5431 - val_loss: 181.1865\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 56.0009 - val_loss: 187.9019\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 50.8401 - val_loss: 202.5440\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 47.4210 - val_loss: 185.7628\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 49.3156 - val_loss: 198.4103\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 51.5997 - val_loss: 204.0567\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 50.0339 - val_loss: 190.1384\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 47.4607 - val_loss: 192.0138\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 52.2316 - val_loss: 201.1511\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 50.7374 - val_loss: 179.8346\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 46.7800 - val_loss: 182.9559\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 47.7047 - val_loss: 196.5056\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 48.0756 - val_loss: 197.4578\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 48.8904 - val_loss: 194.6274\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 51.6097 - val_loss: 184.9672\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 55.6140 - val_loss: 197.3850\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 51.9727 - val_loss: 204.7207\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 52.6994 - val_loss: 204.1652\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 46.2605 - val_loss: 182.4316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 48.4730 - val_loss: 196.4431\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 52.2677 - val_loss: 197.0821\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 47.35 - 0s 57us/step - loss: 47.1954 - val_loss: 194.2023\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 47.6683 - val_loss: 194.5352\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 43.9116 - val_loss: 180.3014\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 46.2145 - val_loss: 185.5501\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 51.2365 - val_loss: 203.0759\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 46.6466 - val_loss: 197.5281\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 47.6654 - val_loss: 190.2540\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 49.1405 - val_loss: 198.9203\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 45.4176 - val_loss: 203.2157\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 46.7909 - val_loss: 213.2708\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 47.7018 - val_loss: 205.2527\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 46.2322 - val_loss: 218.2229\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 50.2215 - val_loss: 206.3124\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 51.5306 - val_loss: 209.3737\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 45.5261 - val_loss: 192.0506\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 43.0504 - val_loss: 189.4955\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 48.4524 - val_loss: 190.1446\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 48.6645 - val_loss: 198.9103\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 48.5109 - val_loss: 192.5275\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 51.5464 - val_loss: 190.9412\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 46.8579 - val_loss: 207.7380\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 45.4027 - val_loss: 194.4556\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 44.4398 - val_loss: 191.0834\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 48.7613 - val_loss: 194.0224\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 46.6885 - val_loss: 192.6606\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 54.3806 - val_loss: 185.1984\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 44.8037 - val_loss: 202.1364\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 48.5484 - val_loss: 186.1609\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 51.8596 - val_loss: 215.3309\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 50.1015 - val_loss: 194.8209\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 47.3823 - val_loss: 190.1693\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 44.4340 - val_loss: 191.3903\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 41.6266 - val_loss: 201.6589\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 1s 134us/step - loss: 45.9083 - val_loss: 213.4811\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 46.7340 - val_loss: 189.8598\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 49.1795 - val_loss: 192.1696\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 42.5135 - val_loss: 207.6245\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 51.9023 - val_loss: 207.9396\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 42.6926 - val_loss: 192.0206\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 48.6344 - val_loss: 186.2797\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 43.8492 - val_loss: 196.4540\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 46.0076 - val_loss: 183.3798\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 45.6871 - val_loss: 221.1481\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 41.5156 - val_loss: 194.9754\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 45.2713 - val_loss: 211.7771\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 44.3978 - val_loss: 197.4396\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 46.0364 - val_loss: 201.1491\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 39.6166 - val_loss: 193.9063\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 46.4416 - val_loss: 195.5888\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 45.4723 - val_loss: 196.1607\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 50.1107 - val_loss: 211.2381\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 44.6891 - val_loss: 192.2700\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 44.2506 - val_loss: 206.3463\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 42.8209 - val_loss: 183.8018\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 48.0474 - val_loss: 192.9225\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 44.9681 - val_loss: 205.7596\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 44.3447 - val_loss: 197.3096\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 47.4368 - val_loss: 198.1395\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 44.6848 - val_loss: 198.1994\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 46.2765 - val_loss: 196.2819\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 44.1630 - val_loss: 192.4044\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 45.5770 - val_loss: 208.6199\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 43.1312 - val_loss: 199.1942\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 44.5064 - val_loss: 198.0322\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 44.0775 - val_loss: 202.4330\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 43.4919 - val_loss: 193.8728\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 41.9593 - val_loss: 206.5449\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 43.8795 - val_loss: 196.8356\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 45.9574 - val_loss: 204.0151\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 47.5901 - val_loss: 207.3735\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 43.8686 - val_loss: 189.2082\n",
      "Epoch 456/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 61us/step - loss: 41.4970 - val_loss: 188.0254\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 43.3750 - val_loss: 199.1683\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 43.4022 - val_loss: 193.6238\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 47.5711 - val_loss: 200.3766\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 49.7637 - val_loss: 202.0290\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 38.8635 - val_loss: 194.8382\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 48.0208 - val_loss: 212.7495\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 48.8512 - val_loss: 193.0207\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 43.2952 - val_loss: 197.5106\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 43.1418 - val_loss: 204.2170\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 43.0408 - val_loss: 208.1892\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 43.0630 - val_loss: 208.8194\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 42.4309 - val_loss: 199.6772\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 41.6451 - val_loss: 209.3037\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 47.3531 - val_loss: 203.7729\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 42.5980 - val_loss: 198.3235\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 45.0154 - val_loss: 200.3615\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 42.6381 - val_loss: 202.5355\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 42.9656 - val_loss: 182.3586\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 42.3970 - val_loss: 196.0254\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 42.5570 - val_loss: 193.8488\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 44.3663 - val_loss: 205.3757\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 40.3977 - val_loss: 188.9187\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 48.4391 - val_loss: 181.0393\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 43.0019 - val_loss: 202.5402\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 41.9150 - val_loss: 196.1177\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 41.8330 - val_loss: 195.3446\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 41.0563 - val_loss: 201.0242\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 43.6015 - val_loss: 205.4164\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 43.3917 - val_loss: 206.1685\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 40.5182 - val_loss: 196.0890\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 40.3202 - val_loss: 193.6392\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 40.9856 - val_loss: 197.9933\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 39.9549 - val_loss: 205.8982\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 47.4954 - val_loss: 206.0063\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 42.0810 - val_loss: 203.6068\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 40.5988 - val_loss: 211.7983\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 40.0463 - val_loss: 197.6797\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 40.5099 - val_loss: 203.8723\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 42.7418 - val_loss: 207.2340\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 43.6478 - val_loss: 201.5720\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 55.0501 - val_loss: 196.0733\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 42.7115 - val_loss: 194.0489\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 41.1154 - val_loss: 194.4633\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 43.8671 - val_loss: 197.3254\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 40.9611 - val_loss: 221.8687\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 41.3880 - val_loss: 195.8547\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 42.7685 - val_loss: 210.9049\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 41.0662 - val_loss: 194.0653\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 39.2990 - val_loss: 196.9839\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 40.1309 - val_loss: 199.8001\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 43.7382 - val_loss: 224.7389\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 42.7345 - val_loss: 197.2670\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 42.6794 - val_loss: 197.8168\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 40.4578 - val_loss: 212.4127\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 40.2392 - val_loss: 191.0217\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 39.8805 - val_loss: 201.8298\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.1307 - val_loss: 207.4646\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 46.2082 - val_loss: 217.6052\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 40.3112 - val_loss: 213.0713\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 45.3864 - val_loss: 194.6156\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 41.5962 - val_loss: 197.0264\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 40.8797 - val_loss: 205.0148\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 41.8200 - val_loss: 199.7955\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.5623 - val_loss: 202.4071\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 40.9306 - val_loss: 198.3232\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 41.6598 - val_loss: 198.0415\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 44.4624 - val_loss: 200.2663\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 42.3668 - val_loss: 195.9694\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 43.8689 - val_loss: 202.7347\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 39.0903 - val_loss: 196.4050\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 37.1019 - val_loss: 209.8441\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 48.3698 - val_loss: 226.7628\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 40.7118 - val_loss: 193.4938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 38.9638 - val_loss: 194.1640\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 37.9053 - val_loss: 207.7234\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 39.5690 - val_loss: 203.8105\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 41.2366 - val_loss: 204.0524\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.7929 - val_loss: 195.0146\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 40.5607 - val_loss: 198.7009\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 39.9439 - val_loss: 204.4511\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 39.9986 - val_loss: 200.9508\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 45.2453 - val_loss: 199.4652\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 41.3053 - val_loss: 192.4525\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 39.5086 - val_loss: 214.8102\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 47.0885 - val_loss: 195.1034\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 40.2699 - val_loss: 208.7217\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 40.2816 - val_loss: 197.2017\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 39.0813 - val_loss: 203.2633\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 43.5087 - val_loss: 198.7787\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 41.5934 - val_loss: 197.3582\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 43.0539 - val_loss: 196.9580\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 47.7495 - val_loss: 186.4601\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 41.7390 - val_loss: 201.1266\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 40.1351 - val_loss: 186.4916\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.2420 - val_loss: 187.0103\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 42.0791 - val_loss: 190.2371\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 37.6420 - val_loss: 190.3525\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 42.9640 - val_loss: 191.2563\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 40.6464 - val_loss: 197.7352\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 39.2580 - val_loss: 198.1605\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 37.5969 - val_loss: 197.2178\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 41.0869 - val_loss: 186.5963\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 39.2930 - val_loss: 208.1099\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 37.6762 - val_loss: 194.9040\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 38.5410 - val_loss: 203.0104\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 42.9267 - val_loss: 199.5152\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 37.9619 - val_loss: 202.8931\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 40.3367 - val_loss: 189.5103\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 38.7672 - val_loss: 201.4984\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 44.4215 - val_loss: 197.1466\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 38.2146 - val_loss: 185.9200\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 34.6267 - val_loss: 204.4026\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 40.2731 - val_loss: 197.7977\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 37.3213 - val_loss: 192.5463\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.8505 - val_loss: 195.7378\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 41.8862 - val_loss: 203.9912\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 42.3493 - val_loss: 216.7233\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 39.7672 - val_loss: 194.5671\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 37.2140 - val_loss: 214.8919\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 35.9905 - val_loss: 199.3818\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 39.9173 - val_loss: 195.8389\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 42.4111 - val_loss: 217.6454\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 38.1841 - val_loss: 203.8336\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 35.3120 - val_loss: 202.4790\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.2769 - val_loss: 209.6597\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 39.4150 - val_loss: 197.2139\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 37.8818 - val_loss: 190.6924\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 36.6864 - val_loss: 216.2416\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 38.5674 - val_loss: 205.4468\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 37.5883 - val_loss: 206.1522\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 38.5532 - val_loss: 203.3132\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 37.1427 - val_loss: 199.8701\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 42.8067 - val_loss: 208.7249\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 37.2326 - val_loss: 193.3181\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.1677 - val_loss: 193.1012\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.3473 - val_loss: 210.8765\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.9093 - val_loss: 207.5876\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 35.8010 - val_loss: 202.4764\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 40.0965 - val_loss: 200.2944\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 36.5664 - val_loss: 205.8054\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 37.2811 - val_loss: 211.2764\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 42.4464 - val_loss: 196.9538\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 42.6589 - val_loss: 207.7445\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.2234 - val_loss: 194.9987\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 38.0264 - val_loss: 205.3954\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 36.1537 - val_loss: 197.8905\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.5034 - val_loss: 202.5562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 38.4240 - val_loss: 205.3459\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 36.7559 - val_loss: 220.1864\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 37.6384 - val_loss: 208.4656\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.4866 - val_loss: 202.6342\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 38.9884 - val_loss: 200.6748\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.3763 - val_loss: 209.6193\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 36.9735 - val_loss: 199.4871\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.6545 - val_loss: 204.3358\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 42.7802 - val_loss: 205.0512\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 37.5768 - val_loss: 193.6544\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 40.6364 - val_loss: 209.7313\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 37.6208 - val_loss: 216.2921\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 37.4386 - val_loss: 192.0551\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 34.9990 - val_loss: 225.9275\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.1934 - val_loss: 188.0780\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 45.1084 - val_loss: 223.5794\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 38.7647 - val_loss: 197.8968\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 36.0358 - val_loss: 209.2321\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.1278 - val_loss: 218.1791\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 38.0446 - val_loss: 203.6405\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 38.0606 - val_loss: 191.1159\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 34.8220 - val_loss: 203.9225\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 38.7994 - val_loss: 213.1625\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.6918 - val_loss: 204.1698\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 36.1913 - val_loss: 230.8714\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 38.6473 - val_loss: 205.0116\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 43.8313 - val_loss: 225.5485\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.7902 - val_loss: 201.5712\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 34.7873 - val_loss: 192.6565\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.9509 - val_loss: 204.4511\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.6855 - val_loss: 207.5132\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 41.6055 - val_loss: 195.1149\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 34.5480 - val_loss: 187.5265\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 39.1906 - val_loss: 202.0176\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.4067 - val_loss: 202.3038\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 36.8648 - val_loss: 224.4096\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 39.3713 - val_loss: 198.9599\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 36.7940 - val_loss: 197.5800\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 35.9004 - val_loss: 193.1918\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 34.9298 - val_loss: 192.2673\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 36.1986 - val_loss: 205.6170\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 36.2739 - val_loss: 200.7936\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 37.8051 - val_loss: 199.6624\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 38.4912 - val_loss: 195.6144\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 36.7891 - val_loss: 199.0393\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 40.0446 - val_loss: 207.4599\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 36.4539 - val_loss: 208.5384\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 38.2123 - val_loss: 197.0973\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 40.2410 - val_loss: 215.8457\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 35.5206 - val_loss: 192.3697\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.8297 - val_loss: 174.3630\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.5898 - val_loss: 185.5833\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 38.6322 - val_loss: 192.8630\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 38.5600 - val_loss: 189.4942\n",
      "Epoch 658/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 35.3211 - val_loss: 205.4381\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.6440 - val_loss: 189.4273\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.8126 - val_loss: 195.2446\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.1022 - val_loss: 192.4484\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 36.6539 - val_loss: 196.7876\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.3721 - val_loss: 191.1453\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.9988 - val_loss: 204.3560\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 39.1981 - val_loss: 209.0646\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 38.9049 - val_loss: 210.2586\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.5505 - val_loss: 189.9709\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.0717 - val_loss: 210.6441\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 36.7252 - val_loss: 204.8848\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 37.9527 - val_loss: 200.6309\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.5098 - val_loss: 196.5153\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 35.3147 - val_loss: 204.4114\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 34.8360 - val_loss: 203.6774\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 37.0486 - val_loss: 199.1646\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 40.3527 - val_loss: 198.3897\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 34.1413 - val_loss: 212.2530\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 35.9026 - val_loss: 205.7328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.7827 - val_loss: 214.0960\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 35.1314 - val_loss: 194.2302\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.9427 - val_loss: 197.7553\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.0244 - val_loss: 197.9238\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 39.7163 - val_loss: 192.1978\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.1568 - val_loss: 218.1448\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 40.8462 - val_loss: 192.8877\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 36.2088 - val_loss: 200.5200\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 36.2771 - val_loss: 198.1667\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 36.8259 - val_loss: 200.3852\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.9391 - val_loss: 192.3696\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.2367 - val_loss: 202.7100\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 36.2033 - val_loss: 208.6297\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.0000 - val_loss: 205.8480\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 38.5466 - val_loss: 205.8179\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 34.9457 - val_loss: 206.0832\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 40.3452 - val_loss: 198.9603\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.6092 - val_loss: 216.0785\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 35.0649 - val_loss: 198.3908\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.8898 - val_loss: 220.3013\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 34.4843 - val_loss: 202.0894\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 36.3734 - val_loss: 203.5909\n",
      "Epoch 700/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 36.7919 - val_loss: 202.9651\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 31.6438 - val_loss: 200.4047\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 37.2787 - val_loss: 216.3143\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 33.3098 - val_loss: 222.1898\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 41.4978 - val_loss: 212.7234\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.6317 - val_loss: 205.8243\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 34.4027 - val_loss: 218.3116\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 37.0928 - val_loss: 199.7557\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.9834 - val_loss: 196.8211\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.2163 - val_loss: 198.3935\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.3905 - val_loss: 206.0671\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.0657 - val_loss: 208.5743\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 36.7961 - val_loss: 197.5971\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.8513 - val_loss: 202.5713\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.5346 - val_loss: 206.1038\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 33.7631 - val_loss: 201.1041\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 32.7074 - val_loss: 199.0078\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 36.7055 - val_loss: 198.2331\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 37.3034 - val_loss: 205.0398\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.2574 - val_loss: 195.5621\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.7248 - val_loss: 202.2379\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 37.9723 - val_loss: 203.8644\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 36.8064 - val_loss: 203.7200\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 39.5719 - val_loss: 198.7007\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 34.3057 - val_loss: 191.9294\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 33.3499 - val_loss: 197.1967\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.5617 - val_loss: 213.4476\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.8113 - val_loss: 208.0259\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 33.2953 - val_loss: 194.9926\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.0060 - val_loss: 195.1128\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.9878 - val_loss: 197.0119\n",
      "Epoch 731/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 38.2514 - val_loss: 197.7981\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.8534 - val_loss: 205.4118\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 34.2420 - val_loss: 209.3675\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 38.7212 - val_loss: 218.2475\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.2597 - val_loss: 210.1889TA: 0s - los\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 33.8802 - val_loss: 222.2845\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 36.9565 - val_loss: 193.3147\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 39.0154 - val_loss: 212.2969\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.6966 - val_loss: 201.5612\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 35.3583 - val_loss: 204.3409\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 38.4039 - val_loss: 200.4068\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.5311 - val_loss: 206.6766\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 34.9381 - val_loss: 213.8143\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.7153 - val_loss: 207.7410\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 32.7712 - val_loss: 215.7930\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 32.9041 - val_loss: 206.2939\n",
      "Epoch 747/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 35.6020 - val_loss: 214.4301\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.7824 - val_loss: 196.6789\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 36.5258 - val_loss: 197.1210\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.4354 - val_loss: 198.4533\n",
      "Epoch 751/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step - loss: 35.0523 - val_loss: 195.8279\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 32.3028 - val_loss: 201.8818\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.8669 - val_loss: 196.9594\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 35.0703 - val_loss: 209.0810\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.4691 - val_loss: 204.0237\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 36.3528 - val_loss: 206.5157\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 37.6256 - val_loss: 196.3962\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 33.9705 - val_loss: 205.7635\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.8964 - val_loss: 207.3519\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 33.8853 - val_loss: 206.7531\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 34.6860 - val_loss: 221.5640\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 34.9692 - val_loss: 207.0691\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.0621 - val_loss: 210.3373\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.2091 - val_loss: 207.8740\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 36.4318 - val_loss: 207.5762\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.1560 - val_loss: 202.0910\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.8269 - val_loss: 199.1056\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 33.5586 - val_loss: 195.8510\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.1390 - val_loss: 202.9708\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.3451 - val_loss: 208.6776\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.5423 - val_loss: 199.8790\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 33.1762 - val_loss: 217.7080\n",
      "Epoch 773/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 33.5187 - val_loss: 208.5938\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.8149 - val_loss: 211.6791\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.3843 - val_loss: 204.0296\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 32.9680 - val_loss: 204.0451\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.8699 - val_loss: 220.2652\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 34.5796 - val_loss: 200.5211\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 36.3816 - val_loss: 203.7312\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.7987 - val_loss: 205.2304\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.4924 - val_loss: 208.0272\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 35.7032 - val_loss: 206.9251\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.8705 - val_loss: 215.6918\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 32.3993 - val_loss: 209.8309\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.6284 - val_loss: 199.3897\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 33.4555 - val_loss: 203.7672\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 35.2475 - val_loss: 204.9954\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 36.3192 - val_loss: 205.7222\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 33.2166 - val_loss: 197.6279\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.4315 - val_loss: 205.0493\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.3539 - val_loss: 201.6821\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 34.2447 - val_loss: 208.1969\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 34.3967 - val_loss: 200.9452\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.0198 - val_loss: 219.4455\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.8108 - val_loss: 193.6084\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.1873 - val_loss: 200.2690\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 36.5374 - val_loss: 213.1582\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.1526 - val_loss: 192.5639\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 33.2357 - val_loss: 223.8994\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.2747 - val_loss: 219.8024\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 35.7318 - val_loss: 214.6782\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.8555 - val_loss: 203.6810\n",
      "Epoch 803/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.9043 - val_loss: 211.9079\n",
      "Epoch 804/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.2339 - val_loss: 211.7162\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.5161 - val_loss: 207.0393\n",
      "Epoch 806/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.3296 - val_loss: 212.2926\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 34.4676 - val_loss: 210.7614\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.1368 - val_loss: 208.2448\n",
      "Epoch 809/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.6024 - val_loss: 203.5921\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.0069 - val_loss: 219.7454\n",
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 32.4410 - val_loss: 196.3656\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.3907 - val_loss: 201.2462\n",
      "Epoch 813/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.7196 - val_loss: 198.2403\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 38.9011 - val_loss: 219.7410\n",
      "Epoch 815/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 32.1431 - val_loss: 220.3526\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 36.9851 - val_loss: 196.9202\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 34.9681 - val_loss: 193.8550\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 34.4601 - val_loss: 215.5571\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.0857 - val_loss: 201.5240\n",
      "Epoch 820/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.1327 - val_loss: 198.3968\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 31.8561 - val_loss: 202.9790\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 31.2594 - val_loss: 197.8767\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 36.9337 - val_loss: 201.4829\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.7837 - val_loss: 210.6020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.0735 - val_loss: 200.6062\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 31.6620 - val_loss: 214.5653\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.2915 - val_loss: 194.8684\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 39.0773 - val_loss: 202.9790\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 34.4397 - val_loss: 211.4391\n",
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.0899 - val_loss: 204.6315\n",
      "Epoch 831/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.2098 - val_loss: 212.3272\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 34.0683 - val_loss: 210.9808\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.8947 - val_loss: 201.9248\n",
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 34.6802 - val_loss: 205.5162\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.4214 - val_loss: 211.6388\n",
      "Epoch 836/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 37.6059 - val_loss: 214.1225\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.4093 - val_loss: 202.4454\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 32.8588 - val_loss: 213.4328\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 34.7031 - val_loss: 219.0403\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 31.8327 - val_loss: 210.1913\n",
      "Epoch 841/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 33.9047 - val_loss: 219.6259\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.7171 - val_loss: 217.0431\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.2454 - val_loss: 193.2768\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 38.0917 - val_loss: 207.4600\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.0373 - val_loss: 206.4130\n",
      "Epoch 846/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 33.1814 - val_loss: 221.9808\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.4720 - val_loss: 218.9913\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 32.6821 - val_loss: 203.3104\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.3304 - val_loss: 223.9890\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 33.6022 - val_loss: 211.2238\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 38.9244 - val_loss: 204.7348\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 34.1076 - val_loss: 202.2883\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.7037 - val_loss: 199.1349\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 33.7033 - val_loss: 206.9286\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.8317 - val_loss: 221.8897\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.3769 - val_loss: 210.5411\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.0379 - val_loss: 202.5864\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 32.0204 - val_loss: 227.0524\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 33.9177 - val_loss: 220.7397\n",
      "Epoch 860/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 30.7073 - val_loss: 205.6623\n",
      "Epoch 861/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.0448 - val_loss: 199.4554\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.8598 - val_loss: 218.7641\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 34.6000 - val_loss: 226.4557\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 31.8728 - val_loss: 203.0972\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 35.7396 - val_loss: 205.5308\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.4243 - val_loss: 204.0968\n",
      "Epoch 867/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 31.3263 - val_loss: 205.8531\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 28.3670 - val_loss: 207.2793\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.8893 - val_loss: 211.4304\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 34.0300 - val_loss: 198.5684\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 37.8609 - val_loss: 206.0323\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 34.4194 - val_loss: 211.6142\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.0869 - val_loss: 205.0374\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.4297 - val_loss: 220.6888\n",
      "Epoch 875/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 29.7094 - val_loss: 209.9428\n",
      "Epoch 876/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.5001 - val_loss: 199.8422\n",
      "Epoch 877/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.8444 - val_loss: 221.3217\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.0919 - val_loss: 212.3445\n",
      "Epoch 879/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.3753 - val_loss: 205.3641\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.5428 - val_loss: 203.9621\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 34.8032 - val_loss: 196.8699\n",
      "Epoch 882/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 30.8507 - val_loss: 211.9240\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.5183 - val_loss: 205.1654\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.8975 - val_loss: 221.6858\n",
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 33.1125 - val_loss: 210.4215\n",
      "Epoch 886/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.2502 - val_loss: 205.9058\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 32.2842 - val_loss: 203.6820\n",
      "Epoch 888/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 31.5197 - val_loss: 215.0728\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.5714 - val_loss: 203.7360\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 34.5262 - val_loss: 216.3093\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.6927 - val_loss: 201.9610\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.0199 - val_loss: 208.0538\n",
      "Epoch 893/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.6757 - val_loss: 214.1578\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.1835 - val_loss: 242.4913\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 36.9294 - val_loss: 220.0069\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.7773 - val_loss: 205.9854\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.5762 - val_loss: 198.1339\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.0218 - val_loss: 205.1411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 33.0697 - val_loss: 210.6673\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 31.7477 - val_loss: 209.9942\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.1331 - val_loss: 210.3764\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 34.4662 - val_loss: 205.6774\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.1399 - val_loss: 235.2431\n",
      "Epoch 904/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 32.3991 - val_loss: 209.4220\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 32.8800 - val_loss: 214.9319\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 30.7798 - val_loss: 208.8603\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.3916 - val_loss: 208.2434\n",
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 32.3017 - val_loss: 216.7375\n",
      "Epoch 909/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 34.5893 - val_loss: 212.0691\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.0174 - val_loss: 201.1893\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 32.1735 - val_loss: 209.3551\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 32.4655 - val_loss: 200.6272\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.4594 - val_loss: 209.5429\n",
      "Epoch 914/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.3538 - val_loss: 216.0982\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.9730 - val_loss: 214.9732\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 35.5669 - val_loss: 206.5455\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 33.8619 - val_loss: 211.5151\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 30.3739 - val_loss: 201.2247\n",
      "Epoch 919/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 32.6292 - val_loss: 202.4804\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 33.42 - 0s 57us/step - loss: 33.3059 - val_loss: 199.5341\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 29.3102 - val_loss: 210.9645\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 35.2902 - val_loss: 207.2642\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 29.1577 - val_loss: 214.1316\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.5661 - val_loss: 197.2421\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 31.9500 - val_loss: 208.5473\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 30.3651 - val_loss: 216.7350\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 32.1047 - val_loss: 208.0636\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 31.5078 - val_loss: 223.8410\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 32.8248 - val_loss: 209.2988\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 30.5098 - val_loss: 215.5604\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.4854 - val_loss: 211.3706\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 30.3915 - val_loss: 198.5956\n",
      "Epoch 933/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 36.4039 - val_loss: 214.7933\n",
      "Epoch 934/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 34.6415 - val_loss: 204.0005\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.9809 - val_loss: 216.4052\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 31.4443 - val_loss: 210.6862\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.5233 - val_loss: 210.6159\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 30.2997 - val_loss: 202.0787\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.4254 - val_loss: 190.8378\n",
      "Epoch 940/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 35.2200 - val_loss: 199.7751\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 33.6805 - val_loss: 202.1883\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 31.1934 - val_loss: 205.2567\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.3508 - val_loss: 236.7207\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 32.4815 - val_loss: 212.1516\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.4315 - val_loss: 205.2742\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 34.4831 - val_loss: 209.6221\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.0687 - val_loss: 206.9932\n",
      "Epoch 948/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 28.8432 - val_loss: 200.7159\n",
      "Epoch 949/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.5819 - val_loss: 209.4550\n",
      "Epoch 950/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.0784 - val_loss: 214.3914\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 33.1909 - val_loss: 221.9687\n",
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 32.7524 - val_loss: 224.1640\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.0591 - val_loss: 198.3051\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.4747 - val_loss: 222.0315\n",
      "Epoch 955/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 33.3274 - val_loss: 234.5729\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.6401 - val_loss: 214.3632\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 36.1696 - val_loss: 213.0751\n",
      "Epoch 958/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.3164 - val_loss: 217.3256\n",
      "Epoch 959/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.1863 - val_loss: 203.9931\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 31.7029 - val_loss: 206.0475\n",
      "Epoch 961/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.8573 - val_loss: 208.0364\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 28.8722 - val_loss: 221.2754\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 32.4813 - val_loss: 211.0553\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 32.3375 - val_loss: 208.9312\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.3057 - val_loss: 221.7940\n",
      "Epoch 966/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.1426 - val_loss: 211.6933\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.9310 - val_loss: 222.3141\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.6167 - val_loss: 217.5081\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.2287 - val_loss: 204.3995\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.6357 - val_loss: 213.7407\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.7822 - val_loss: 212.6194\n",
      "Epoch 972/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.6304 - val_loss: 204.4130\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.5081 - val_loss: 220.6707\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.5008 - val_loss: 216.6073\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.3150 - val_loss: 218.1499\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 33.8361 - val_loss: 230.3963\n",
      "Epoch 977/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 37.8567 - val_loss: 206.4994\n",
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.2788 - val_loss: 202.9325\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 29.4376 - val_loss: 219.0764\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.2696 - val_loss: 219.2882\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.6303 - val_loss: 219.7706\n",
      "Epoch 982/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 32.3280 - val_loss: 208.2502\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.2259 - val_loss: 220.1380\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.4947 - val_loss: 208.8867\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.9535 - val_loss: 227.2737\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.5664 - val_loss: 218.3243\n",
      "Epoch 987/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 34.1232 - val_loss: 229.2045\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.4203 - val_loss: 216.7040\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.5727 - val_loss: 217.0587\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 33.2538 - val_loss: 203.3180\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.5325 - val_loss: 218.6028\n",
      "Epoch 992/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.4085 - val_loss: 210.2086\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.1921 - val_loss: 203.6569\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.6653 - val_loss: 205.1577\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 29.5001 - val_loss: 235.4132\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 32.0848 - val_loss: 217.6846\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 31.7816 - val_loss: 207.0163\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 32.3705 - val_loss: 212.2830\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 29.3516 - val_loss: 207.6277\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.1243 - val_loss: 227.9767\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.7233 - val_loss: 213.8636\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.9679 - val_loss: 217.6963\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 33.5257 - val_loss: 217.8858\n",
      "Epoch 1004/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.9881 - val_loss: 219.4221\n",
      "Epoch 1005/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.0859 - val_loss: 204.7119\n",
      "Epoch 1006/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 32.0292 - val_loss: 208.2093\n",
      "Epoch 1007/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.3502 - val_loss: 206.3995\n",
      "Epoch 1008/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.8101 - val_loss: 219.4280\n",
      "Epoch 1009/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.2355 - val_loss: 216.0139\n",
      "Epoch 1010/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 33.2106 - val_loss: 231.9853\n",
      "Epoch 1011/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 31.0626 - val_loss: 212.3247\n",
      "Epoch 1012/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.1928 - val_loss: 218.8545\n",
      "Epoch 1013/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 29.2765 - val_loss: 213.7488\n",
      "Epoch 1014/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 30.0194 - val_loss: 207.2972\n",
      "Epoch 1015/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.3380 - val_loss: 208.6220\n",
      "Epoch 1016/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.0001 - val_loss: 207.6789\n",
      "Epoch 1017/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 34.0008 - val_loss: 212.9122\n",
      "Epoch 1018/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.2587 - val_loss: 210.3443\n",
      "Epoch 1019/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 31.6964 - val_loss: 199.3686\n",
      "Epoch 1020/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.9342 - val_loss: 203.4369\n",
      "Epoch 1021/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.0475 - val_loss: 197.2059\n",
      "Epoch 1022/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 30.2270 - val_loss: 204.3764\n",
      "Epoch 1023/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 32.2925 - val_loss: 204.2739\n",
      "Epoch 1024/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 28.7017 - val_loss: 210.7564\n",
      "Epoch 1025/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 29.1096 - val_loss: 204.2987\n",
      "Epoch 1026/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.3145 - val_loss: 197.9038\n",
      "Epoch 1027/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 30.8491 - val_loss: 204.9482\n",
      "Epoch 1028/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.6101 - val_loss: 210.2199\n",
      "Epoch 1029/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 28.5085 - val_loss: 218.5324\n",
      "Epoch 1030/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 29.9815 - val_loss: 200.5529\n",
      "Epoch 1031/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.8468 - val_loss: 205.4977\n",
      "Epoch 1032/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.8013 - val_loss: 215.7583\n",
      "Epoch 1033/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 28.2806 - val_loss: 210.8720\n",
      "Epoch 1034/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 29.6220 - val_loss: 215.8652\n",
      "Epoch 1035/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.1592 - val_loss: 220.5996\n",
      "Epoch 1036/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.7179 - val_loss: 226.6586\n",
      "Epoch 1037/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.0097 - val_loss: 229.7026\n",
      "Epoch 1038/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.9690 - val_loss: 205.3698\n",
      "Epoch 1039/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 32.0678 - val_loss: 211.5699\n",
      "Epoch 1040/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 30.6796 - val_loss: 205.2999\n",
      "Epoch 1041/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 32.2105 - val_loss: 224.4767\n",
      "Epoch 1042/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 36.7674 - val_loss: 203.5517\n",
      "Epoch 1043/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 29.9161 - val_loss: 218.1725\n",
      "Epoch 1044/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 29.2840 - val_loss: 209.1242\n",
      "Epoch 1045/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 29.7753 - val_loss: 229.4236\n",
      "Epoch 1046/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 32.7405 - val_loss: 206.4429\n",
      "Epoch 1047/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.7694 - val_loss: 225.7711\n",
      "Epoch 1048/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 30.9629 - val_loss: 202.3564\n",
      "Epoch 1049/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 29.0192 - val_loss: 203.8860\n",
      "Epoch 1050/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 29.7410 - val_loss: 206.4345\n",
      "Epoch 1051/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.6339 - val_loss: 208.6750\n",
      "Epoch 1052/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.0625 - val_loss: 217.5342\n",
      "Epoch 1053/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 27.0234 - val_loss: 199.5678\n",
      "Epoch 1054/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.2867 - val_loss: 211.1093\n",
      "Epoch 1055/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 31.0881 - val_loss: 205.4435\n",
      "Epoch 1056/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 35.8036 - val_loss: 210.7638\n",
      "Epoch 1057/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.5617 - val_loss: 206.5562\n",
      "Epoch 1058/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.6640 - val_loss: 220.0621\n",
      "Epoch 1059/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 29.2963 - val_loss: 199.2058\n",
      "Epoch 1060/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 29.6573 - val_loss: 194.5751\n",
      "Epoch 1061/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 30.4745 - val_loss: 209.8022\n",
      "Epoch 1062/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 29.7055 - val_loss: 202.8550\n",
      "Epoch 1063/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.7414 - val_loss: 209.8209\n",
      "Epoch 1064/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 30.5929 - val_loss: 207.5613\n",
      "Epoch 1065/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 31.3994 - val_loss: 212.3967\n",
      "Epoch 1066/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 28.1740 - val_loss: 198.1039\n",
      "Epoch 1067/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 28.8581 - val_loss: 208.1183\n",
      "Epoch 1068/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 26.8164 - val_loss: 204.9115\n",
      "Epoch 1069/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.4359 - val_loss: 203.7551\n",
      "Epoch 1070/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 31.2484 - val_loss: 208.7202\n",
      "Epoch 1071/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 31.0777 - val_loss: 214.3527\n",
      "Epoch 1072/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 35.0934 - val_loss: 208.2523\n",
      "Epoch 1073/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 30.9980 - val_loss: 211.0769\n",
      "Epoch 1074/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 30.3574 - val_loss: 220.0061\n",
      "Epoch 1075/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 29.5568 - val_loss: 199.1643\n",
      "Epoch 1076/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.7038 - val_loss: 199.6292\n",
      "Epoch 1077/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 32.6765 - val_loss: 220.3960\n",
      "Epoch 1078/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 29.3099 - val_loss: 205.7853\n",
      "Epoch 1079/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 28.5134 - val_loss: 204.7418\n",
      "Epoch 1080/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 28.1851 - val_loss: 209.2463\n",
      "Epoch 1081/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 29.2337 - val_loss: 208.7584\n",
      "Epoch 1082/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.6869 - val_loss: 200.8347\n",
      "Epoch 1083/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.3054 - val_loss: 209.0666\n",
      "Epoch 1084/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 28.6691 - val_loss: 205.1389\n",
      "Epoch 1085/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 28.9059 - val_loss: 214.2052\n",
      "Epoch 1086/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.3234 - val_loss: 215.4599\n",
      "Epoch 1087/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 32.3515 - val_loss: 218.8533\n",
      "Epoch 1088/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 34.5859 - val_loss: 208.8117\n",
      "Epoch 1089/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 31.4428 - val_loss: 213.2561\n",
      "Epoch 1090/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 29.1744 - val_loss: 207.9010\n",
      "Epoch 1091/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.5460 - val_loss: 209.1673\n",
      "Epoch 1092/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.0450 - val_loss: 203.3173\n",
      "Epoch 1093/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.6405 - val_loss: 207.0763\n",
      "Epoch 1094/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 28.6807 - val_loss: 215.4502\n",
      "Epoch 1095/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 28.7237 - val_loss: 209.3197\n",
      "Epoch 1096/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.7002 - val_loss: 222.3819\n",
      "Epoch 1097/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 31.3792 - val_loss: 213.0787\n",
      "Epoch 1098/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.8350 - val_loss: 209.2806\n",
      "Epoch 1099/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 28.2465 - val_loss: 212.2288\n",
      "Epoch 1100/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 32.4381 - val_loss: 200.7123\n",
      "Epoch 1101/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.9646 - val_loss: 211.0777\n",
      "Epoch 1102/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 26.9945 - val_loss: 204.9446\n",
      "Epoch 1103/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.7382 - val_loss: 213.7723\n",
      "Epoch 1104/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.6678 - val_loss: 206.0954\n",
      "Epoch 1105/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 29.2446 - val_loss: 197.5807\n",
      "Epoch 1106/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 30.8232 - val_loss: 208.1006\n",
      "Epoch 1107/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.4770 - val_loss: 223.0371\n",
      "Epoch 1108/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.4358 - val_loss: 211.7894\n",
      "Epoch 1109/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 29.0692 - val_loss: 215.3015\n",
      "Epoch 1110/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 27.9870 - val_loss: 204.0671\n",
      "Epoch 1111/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 28.6624 - val_loss: 213.7049\n",
      "Epoch 1112/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 33.9784 - val_loss: 200.2766\n",
      "Epoch 1113/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.8954 - val_loss: 212.0801\n",
      "Epoch 1114/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.6409 - val_loss: 204.0714\n",
      "Epoch 1115/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 29.2627 - val_loss: 198.2023\n",
      "Epoch 1116/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.0008 - val_loss: 199.3158\n",
      "Epoch 1117/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 30.9195 - val_loss: 199.5168\n",
      "Epoch 1118/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 27.5165 - val_loss: 195.5083\n",
      "Epoch 1119/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 27.6759 - val_loss: 199.9282\n",
      "Epoch 1120/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.3986 - val_loss: 217.4807\n",
      "Epoch 1121/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.9524 - val_loss: 197.8025\n",
      "Epoch 1122/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.5765 - val_loss: 211.3487\n",
      "Epoch 1123/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 29.7239 - val_loss: 213.1855\n",
      "Epoch 1124/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 29.4112 - val_loss: 202.7733\n",
      "Epoch 1125/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 28.6545 - val_loss: 203.9091\n",
      "Epoch 1126/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 27.8139 - val_loss: 209.8850\n",
      "Epoch 1127/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 28.2859 - val_loss: 193.5264\n",
      "Epoch 1128/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.6018 - val_loss: 207.6239\n",
      "Epoch 1129/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.1009 - val_loss: 203.1832\n",
      "Epoch 1130/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 30.2765 - val_loss: 201.0073\n",
      "Epoch 1131/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 27.3395 - val_loss: 210.9497\n",
      "Epoch 1132/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.2702 - val_loss: 214.3972\n",
      "Epoch 1133/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 29.0604 - val_loss: 216.0700\n",
      "Epoch 1134/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 32.7784 - val_loss: 222.4160\n",
      "Epoch 1135/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 29.9213 - val_loss: 194.4828\n",
      "Epoch 1136/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 30.7413 - val_loss: 213.6853\n",
      "Epoch 1137/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.6565 - val_loss: 208.9381\n",
      "Epoch 1138/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 28.3704 - val_loss: 206.8252\n",
      "Epoch 1139/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 26.3569 - val_loss: 223.7256\n",
      "Epoch 1140/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.8484 - val_loss: 216.3422\n",
      "Epoch 1141/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 27.4528 - val_loss: 212.0567\n",
      "Epoch 1142/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 27.3565 - val_loss: 207.9252\n",
      "Epoch 1143/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.8599 - val_loss: 205.2800\n",
      "Epoch 1144/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.8142 - val_loss: 217.8358\n",
      "Epoch 1145/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.5353 - val_loss: 210.3530\n",
      "Epoch 1146/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 27.9495 - val_loss: 212.0303\n",
      "Epoch 1147/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.6995 - val_loss: 201.6713\n",
      "Epoch 1148/10000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 31.8383 - val_loss: 199.9951\n",
      "Epoch 1149/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.2208 - val_loss: 212.5539\n",
      "Epoch 1150/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.4523 - val_loss: 204.8335\n",
      "Epoch 1151/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 29.0627 - val_loss: 222.6088\n",
      "Epoch 1152/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 30.6131 - val_loss: 232.6879\n",
      "Epoch 1153/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 26.5504 - val_loss: 203.5781\n",
      "Epoch 1154/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 26.1465 - val_loss: 202.2662\n",
      "Epoch 1155/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.4191 - val_loss: 210.8411\n",
      "Epoch 1156/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.1412 - val_loss: 196.3580\n",
      "Epoch 1157/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 30.1761 - val_loss: 204.2593\n",
      "Epoch 1158/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.7827 - val_loss: 211.1285\n",
      "Epoch 1159/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 27.5371 - val_loss: 222.0247\n",
      "Epoch 1160/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 27.6536 - val_loss: 205.9164\n",
      "Epoch 1161/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 27.1918 - val_loss: 204.2955\n",
      "Epoch 1162/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 27.9990 - val_loss: 223.7994\n",
      "Epoch 1163/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.8324 - val_loss: 187.8452\n",
      "Epoch 1164/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 29.2246 - val_loss: 212.1306\n",
      "Epoch 1165/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 29.4761 - val_loss: 197.2739\n",
      "Epoch 1166/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.5363 - val_loss: 211.5728\n",
      "Epoch 1167/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 27.8490 - val_loss: 201.6080\n",
      "Epoch 1168/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 27.8803 - val_loss: 218.0471\n",
      "Epoch 1169/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 28.9736 - val_loss: 211.2610\n",
      "Epoch 1170/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.6412 - val_loss: 200.9709\n",
      "Epoch 01170: early stopping\n",
      "Fold score (RMSE): 14.035025596618652\n",
      "Fold #3\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 12656.4257 - val_loss: 1141.1303\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 720.7009 - val_loss: 511.4164\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 431.2958 - val_loss: 362.8233\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 344.2127 - val_loss: 320.5864\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 323.6801 - val_loss: 300.5338\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 303.9261 - val_loss: 307.2477\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 291.9957 - val_loss: 284.1080\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 285.2469 - val_loss: 285.3891\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 287.5920 - val_loss: 278.1349\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 275.1226 - val_loss: 271.0604\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 276.8262 - val_loss: 269.0283\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 269.2069 - val_loss: 286.5719\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 270.7884 - val_loss: 260.2135\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 260.8441 - val_loss: 264.2331\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 258.9123 - val_loss: 280.4426\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 257.2565 - val_loss: 262.8333\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 257.5545 - val_loss: 263.3085\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 250.9262 - val_loss: 260.7922\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 253.6770 - val_loss: 269.3676\n",
      "Epoch 20/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 248.5860 - val_loss: 345.2128\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 249.0600 - val_loss: 253.1913\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 241.2673 - val_loss: 252.7699\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 241.2747 - val_loss: 256.6079\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 235.8821 - val_loss: 255.8443\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 237.0454 - val_loss: 266.8197\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 236.9427 - val_loss: 276.7065\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 236.8685 - val_loss: 261.8771\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 239.9113 - val_loss: 248.9400\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 231.0221 - val_loss: 249.2658\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 227.8799 - val_loss: 268.5133\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 227.5552 - val_loss: 269.7769\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 222.5308 - val_loss: 257.8577\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 220.5272 - val_loss: 246.5940\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 222.9766 - val_loss: 310.5440\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 224.1739 - val_loss: 262.5568\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 223.0702 - val_loss: 260.7517\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 218.2573 - val_loss: 306.6477\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 221.4436 - val_loss: 257.0086\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 221.6480 - val_loss: 241.7408\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 211.5658 - val_loss: 259.3672\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 219.4705 - val_loss: 245.0374\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 207.1923 - val_loss: 240.4312\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 204.8524 - val_loss: 241.7583\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 205.0235 - val_loss: 247.0725\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 209.7995 - val_loss: 233.2382\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 208.0194 - val_loss: 232.7795\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 201.1679 - val_loss: 235.7546\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 207.0105 - val_loss: 239.0341\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 198.2777 - val_loss: 241.3655\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 200.7621 - val_loss: 240.3387\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 197.0611 - val_loss: 252.8754\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 194.7521 - val_loss: 234.0243\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 199.1091 - val_loss: 234.9786\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 197.1353 - val_loss: 240.4649\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 194.0952 - val_loss: 227.0702\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 188.4602 - val_loss: 227.3621\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 196.1007 - val_loss: 241.0427\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 191.3182 - val_loss: 228.6693\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 186.8329 - val_loss: 220.2963\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 189.5279 - val_loss: 228.1572\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 186.5669 - val_loss: 240.5185\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 186.9704 - val_loss: 221.5810\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 186.8341 - val_loss: 258.2980\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 184.2855 - val_loss: 225.7482\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 182.2535 - val_loss: 232.9633\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 179.1318 - val_loss: 260.2189\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 180.6053 - val_loss: 227.1327\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 180.0926 - val_loss: 226.9320\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 174.4846 - val_loss: 219.4743\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 178.7143 - val_loss: 257.2489\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 170.5401 - val_loss: 217.7020\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 174.1826 - val_loss: 215.4246\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 170.6195 - val_loss: 226.7433\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 173.1194 - val_loss: 221.9387\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 178.2924 - val_loss: 229.1332\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.7774 - val_loss: 214.3386\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 167.8154 - val_loss: 250.2548\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.5920 - val_loss: 212.9590\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 170.6309 - val_loss: 208.1826\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 166.3725 - val_loss: 208.8230\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 165.7359 - val_loss: 231.5403\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 159.5926 - val_loss: 216.6432\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.5690 - val_loss: 206.0173\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 163.3540 - val_loss: 211.9070\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 162.1174 - val_loss: 213.9345\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 163.7382 - val_loss: 209.2915\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 161.1712 - val_loss: 213.9512\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 156.0136 - val_loss: 218.6823\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 152.6489 - val_loss: 211.2256\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 147.1660 - val_loss: 225.1455\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.2909 - val_loss: 210.9709\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 153.4690 - val_loss: 215.4952\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 149.6703 - val_loss: 205.8675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 148.5838 - val_loss: 196.7632\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 144.3748 - val_loss: 216.4864\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 151.3280 - val_loss: 214.4711\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 139.2097 - val_loss: 211.3555\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 141.1315 - val_loss: 203.8537\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 140.4592 - val_loss: 223.2736\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 142.4788 - val_loss: 198.5507\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 144.6337 - val_loss: 213.0271\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 140.1633 - val_loss: 195.7723\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 132.4297 - val_loss: 209.8532\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 145.3650 - val_loss: 197.0355\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.5948 - val_loss: 265.0159\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 136.7606 - val_loss: 202.9952\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 139.8824 - val_loss: 213.0531\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.6170 - val_loss: 196.0528\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.7821 - val_loss: 212.1017\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 134.8033 - val_loss: 197.9014\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 126.3626 - val_loss: 204.4515\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 129.3468 - val_loss: 200.3744\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 126.8594 - val_loss: 200.9498\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 121.3352 - val_loss: 180.6755\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 125.0026 - val_loss: 208.7676\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 128.7204 - val_loss: 197.7389\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 123.7366 - val_loss: 203.6511\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 122.7505 - val_loss: 185.4726\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 129.0191 - val_loss: 226.6148\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 112.2352 - val_loss: 191.1987\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 119.8136 - val_loss: 201.4111\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 120.2712 - val_loss: 189.1307\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 118.4595 - val_loss: 177.1162\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 118.8098 - val_loss: 187.7849\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 116.9387 - val_loss: 183.6010\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 110.8189 - val_loss: 188.6852\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 117.3798 - val_loss: 191.1571\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 117.4580 - val_loss: 183.3014\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 111.7027 - val_loss: 190.0630\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 114.0428 - val_loss: 182.7620\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 108.4223 - val_loss: 188.0609\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 114.8535 - val_loss: 182.5820\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 108.0469 - val_loss: 181.2430\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 116.6275 - val_loss: 201.3503\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 109.8022 - val_loss: 188.9382\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 103.4612 - val_loss: 180.9104\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 105.6147 - val_loss: 180.7979\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 105.6801 - val_loss: 193.8667\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 108.6149 - val_loss: 174.2819\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 107.4646 - val_loss: 177.3115\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 102.1169 - val_loss: 188.9443\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 108.2078 - val_loss: 172.9425\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 104.1914 - val_loss: 212.3012\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 98.2467 - val_loss: 177.4763\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 105.4987 - val_loss: 189.1171\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 95.7173 - val_loss: 167.0064\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 105.1800 - val_loss: 178.7652\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 97.8691 - val_loss: 178.9433\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 93.7655 - val_loss: 184.1960\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 94.0629 - val_loss: 179.1437\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 96.2684 - val_loss: 187.6653\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 95.8470 - val_loss: 188.5068\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 103.7768 - val_loss: 177.7170\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 98.3721 - val_loss: 202.1561\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 97.0220 - val_loss: 173.0371\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 97.7803 - val_loss: 228.0413\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 96.2600 - val_loss: 181.2628\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 93.9873 - val_loss: 165.7278\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 96.6049 - val_loss: 181.3416\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 89.6244 - val_loss: 192.8136\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 91.6518 - val_loss: 175.4214\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 94.3414 - val_loss: 170.4499\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 88.2161 - val_loss: 182.2983\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 87.9579 - val_loss: 194.1243\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 87.9027 - val_loss: 214.1546\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 90.1592 - val_loss: 190.0083\n",
      "Epoch 167/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 100.1321 - val_loss: 184.9099\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 86.0539 - val_loss: 178.7951\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 88.3187 - val_loss: 177.0021\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 80.9055 - val_loss: 227.7165\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 85.0459 - val_loss: 183.9369\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 85.3603 - val_loss: 172.0644\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 89.6829 - val_loss: 184.3932\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 86.1053 - val_loss: 168.0413\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 86.1402 - val_loss: 187.4169\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 89.6145 - val_loss: 185.4303\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 85.6553 - val_loss: 181.5302\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 82.7812 - val_loss: 188.0739\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 85.3793 - val_loss: 201.1395\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 82.9250 - val_loss: 187.7503\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 86.5543 - val_loss: 196.0417\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 82.5855 - val_loss: 174.1493\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 83.5366 - val_loss: 198.2985\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 84.9984 - val_loss: 180.0731\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 80.7436 - val_loss: 187.3479\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 81.8005 - val_loss: 292.2026\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 82.5106 - val_loss: 182.1676\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 80.2189 - val_loss: 181.2456\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 76.7911 - val_loss: 201.0578\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 87.3004 - val_loss: 208.8718\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 86.9489 - val_loss: 188.8885\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 79.8147 - val_loss: 176.5775\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 75.9154 - val_loss: 198.6792\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 85.3980 - val_loss: 212.9749\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 81.0000 - val_loss: 194.0892\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 81.6909 - val_loss: 188.5813\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 84.1390 - val_loss: 182.6412\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 77.2434 - val_loss: 193.2123\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 80.8688 - val_loss: 204.9597\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 81.0093 - val_loss: 199.7403\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 74.2516 - val_loss: 196.1309\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 77.2850 - val_loss: 216.5971\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 73.1813 - val_loss: 188.1741\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 77.3635 - val_loss: 183.5618\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 77.3678 - val_loss: 168.6870\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 78.7228 - val_loss: 178.8221\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 72.0285 - val_loss: 183.8351\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 79.4938 - val_loss: 178.6365\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 72.4531 - val_loss: 165.9124\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 74.1931 - val_loss: 185.8613\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 74.2351 - val_loss: 190.2492\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 75.8457 - val_loss: 194.1158\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 72.1442 - val_loss: 187.3546\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 77.8644 - val_loss: 180.3435\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 73.6009 - val_loss: 188.7370\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 73.3184 - val_loss: 181.1622\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 71.8048 - val_loss: 193.6779\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 75.0900 - val_loss: 176.7668\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 72.6392 - val_loss: 165.5772\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 65.4593 - val_loss: 182.2905\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 69.7201 - val_loss: 178.2639\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 74.1068 - val_loss: 172.5946\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 75.6550 - val_loss: 219.9699\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 76.9233 - val_loss: 216.4204\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 71.4463 - val_loss: 183.2871\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 71.8923 - val_loss: 169.2544\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 69.0134 - val_loss: 181.9131\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 71.0673 - val_loss: 181.6463\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 70.7949 - val_loss: 185.7044\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 73.9326 - val_loss: 186.2616\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 70.8703 - val_loss: 170.9882\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 65.2690 - val_loss: 191.3891\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 70.7463 - val_loss: 177.2499\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 66.9987 - val_loss: 178.8547\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 68.9689 - val_loss: 168.7337\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 67.0547 - val_loss: 181.4611\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 65.3498 - val_loss: 179.2142\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 67.9599 - val_loss: 198.3920\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 69.9848 - val_loss: 181.7368\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 67.3221 - val_loss: 174.8883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 67.4316 - val_loss: 202.4747\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 69.4451 - val_loss: 199.1350\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 68.0996 - val_loss: 181.4482\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 62.7936 - val_loss: 191.7366\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 60.4128 - val_loss: 193.1884\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 65.3720 - val_loss: 192.9313\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 60.7766 - val_loss: 170.8736\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 67.8653 - val_loss: 204.9152\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 61.8756 - val_loss: 184.5054\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 67.3768 - val_loss: 174.9969\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 69.4500 - val_loss: 185.5401\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 68.0402 - val_loss: 179.5294\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 64.7025 - val_loss: 179.3804\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 65.0228 - val_loss: 179.1395\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 65.2881 - val_loss: 177.5714\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 70.5371 - val_loss: 216.5572\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 65.0901 - val_loss: 205.1197\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 65.2472 - val_loss: 188.7305\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 61.2142 - val_loss: 169.2003\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 62.4334 - val_loss: 184.1326\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 59.6705 - val_loss: 194.6608\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 66.2616 - val_loss: 180.6508\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 66.2499 - val_loss: 175.8170\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 60.8940 - val_loss: 190.8585\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 62.9971 - val_loss: 191.2264\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 67.7428 - val_loss: 190.7286\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 62.1377 - val_loss: 195.2534\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 62.7391 - val_loss: 179.3656\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 61.7772 - val_loss: 185.5594\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 62.99 - 0s 51us/step - loss: 62.9025 - val_loss: 194.1847\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 62.3817 - val_loss: 166.2025\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 63.1690 - val_loss: 188.2035\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 64.0728 - val_loss: 181.8077\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 64.0085 - val_loss: 188.0976\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 63.3723 - val_loss: 187.7084\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 63.2502 - val_loss: 189.2332\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 57.6623 - val_loss: 192.7532\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 61.0389 - val_loss: 171.4441\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 59.1332 - val_loss: 193.3006\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 59.0104 - val_loss: 181.4670\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 61.1055 - val_loss: 189.4346\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 63.7402 - val_loss: 179.5748\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 57.7891 - val_loss: 193.9366\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 63.1378 - val_loss: 176.0480\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 62.3458 - val_loss: 189.4388\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 58.6750 - val_loss: 177.3694\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 57.7366 - val_loss: 185.7273\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 57.0866 - val_loss: 188.1089\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 55.8074 - val_loss: 181.9177\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 57.2525 - val_loss: 190.2519\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 59.8131 - val_loss: 193.8694\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 61.7905 - val_loss: 188.5926\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 59.4446 - val_loss: 177.7411\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 54.7731 - val_loss: 194.8544\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 60.9833 - val_loss: 193.6326\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 57.9760 - val_loss: 192.3448\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 57.2779 - val_loss: 180.5685\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 55.0671 - val_loss: 189.4521\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 56.8643 - val_loss: 180.8458\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 53.3595 - val_loss: 179.3065\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 54.9107 - val_loss: 192.2953\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 56.8802 - val_loss: 189.3324\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 55.8358 - val_loss: 178.7289\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 59.0283 - val_loss: 193.6949\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 59.5871 - val_loss: 183.4390\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 55.3180 - val_loss: 183.3135\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 58.3595 - val_loss: 188.2714\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 53.8870 - val_loss: 193.8933\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 54.7872 - val_loss: 178.3984\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 61.7231 - val_loss: 186.4592\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 56.0045 - val_loss: 204.3887\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 57.5912 - val_loss: 191.5616\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 57.3159 - val_loss: 179.6184\n",
      "Epoch 314/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 55.7419 - val_loss: 213.7210\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 55.2215 - val_loss: 183.5558\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 60.3932 - val_loss: 200.0361\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 54.2148 - val_loss: 196.5037\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 56.1672 - val_loss: 193.2215\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 55.8490 - val_loss: 185.5834\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 56.9846 - val_loss: 178.5597\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 53.2003 - val_loss: 180.2982\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 58.3456 - val_loss: 215.7140\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 54.6400 - val_loss: 232.5598\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 54.4167 - val_loss: 194.1037\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 55.9275 - val_loss: 203.0145\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 54.3739 - val_loss: 186.1704\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 49.5731 - val_loss: 190.6970\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 57.7642 - val_loss: 188.8983\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 52.4970 - val_loss: 194.6121\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 58.0385 - val_loss: 186.2247\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 54.1617 - val_loss: 188.1136\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 56.7609 - val_loss: 185.1858\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 51.6549 - val_loss: 183.7002\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 52.4922 - val_loss: 178.7363\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 51.2931 - val_loss: 183.8690\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 53.1969 - val_loss: 203.6066\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 53.1809 - val_loss: 191.5561\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 54.0804 - val_loss: 190.7444\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 52.9498 - val_loss: 189.1399\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 53.9874 - val_loss: 192.9324\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 55.4795 - val_loss: 194.3466\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 49.6515 - val_loss: 206.9927\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 52.4751 - val_loss: 188.2828\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 59.4219 - val_loss: 189.4040\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 49.9166 - val_loss: 175.3754\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 48.9509 - val_loss: 192.7794\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 51.8308 - val_loss: 192.2894\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 51.3609 - val_loss: 187.4935\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 53.1806 - val_loss: 191.0502\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 48.9686 - val_loss: 185.2130\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 47.5872 - val_loss: 191.0337\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 55.9238 - val_loss: 197.5918\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 52.3445 - val_loss: 185.1941\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 50.3525 - val_loss: 187.6766\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 51.0475 - val_loss: 192.3845\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 47.0813 - val_loss: 181.8542\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 51.4939 - val_loss: 199.8749\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 52.4059 - val_loss: 204.8273\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 52.6619 - val_loss: 189.6613\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 48.9475 - val_loss: 190.6805\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 50.4400 - val_loss: 197.7138\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 53.2814 - val_loss: 179.8613\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 51.3605 - val_loss: 204.9132\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 50.7515 - val_loss: 187.4375\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 54.9217 - val_loss: 188.3192\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 51.3082 - val_loss: 193.2828\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 51.7313 - val_loss: 204.6432\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 53.7872 - val_loss: 188.8447\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 49.2738 - val_loss: 184.6066\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 49.2807 - val_loss: 190.9149\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 46.7917 - val_loss: 186.2206\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 46.7129 - val_loss: 194.6665\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 48.9094 - val_loss: 195.7105\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 49.7564 - val_loss: 191.4041\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 48.1226 - val_loss: 192.8284\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 51.8855 - val_loss: 200.8166\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 48.0025 - val_loss: 197.5235\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 52.3735 - val_loss: 194.2976\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 50.6916 - val_loss: 211.5096\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 45.8113 - val_loss: 194.6143\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 52.4157 - val_loss: 179.8689\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 49.7450 - val_loss: 203.2568\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 47.2482 - val_loss: 198.5562\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 52.7415 - val_loss: 178.4952\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 50.2102 - val_loss: 201.9601\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 46.8746 - val_loss: 183.8865\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 50.2190 - val_loss: 201.6224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 47.4327 - val_loss: 197.4679\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 50.4642 - val_loss: 194.7827\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 47.2794 - val_loss: 194.0269\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 49.0948 - val_loss: 201.1046\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 53.5922 - val_loss: 190.9347\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 44.2999 - val_loss: 194.8379\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 46.2322 - val_loss: 204.9577\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 49.7061 - val_loss: 184.4718TA: 0s - l\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 46.9125 - val_loss: 198.7556\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 45.3789 - val_loss: 192.9912\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 48.0418 - val_loss: 179.6757\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 46.3987 - val_loss: 194.0985\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 50.4967 - val_loss: 178.7811\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 47.0632 - val_loss: 203.5789\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 50.8426 - val_loss: 190.8359\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 50.4083 - val_loss: 182.0367\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 47.2659 - val_loss: 203.2711\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 44.5430 - val_loss: 194.5844\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 45.0924 - val_loss: 193.7083\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 45.6751 - val_loss: 193.6532\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 43.1086 - val_loss: 191.8917\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 46.1036 - val_loss: 189.5244\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 46.9100 - val_loss: 182.7057\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 46.3847 - val_loss: 204.8294\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 47.0346 - val_loss: 193.0854\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 46.0381 - val_loss: 184.5490\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 47.1590 - val_loss: 190.6341\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 49.9382 - val_loss: 192.3000\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 44.4520 - val_loss: 193.7253\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 46.8276 - val_loss: 207.1609\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 48.8599 - val_loss: 188.8531\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 44.5996 - val_loss: 199.5637\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 46.7426 - val_loss: 187.3744\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 43.0066 - val_loss: 192.7603\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 46.3223 - val_loss: 185.3773\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 46.9160 - val_loss: 184.5629\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 44.0863 - val_loss: 190.9284\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 45.2003 - val_loss: 186.2997\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 43.0489 - val_loss: 195.4986\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 48.5103 - val_loss: 192.2304\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 44.6917 - val_loss: 195.3491\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 44.6708 - val_loss: 206.3613\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 46.4836 - val_loss: 188.9282\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 49.2528 - val_loss: 196.9926\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 46.0089 - val_loss: 197.5114\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 46.4580 - val_loss: 199.8219\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 49.9950 - val_loss: 209.8584\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 1s 151us/step - loss: 42.6941 - val_loss: 203.0037\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 1s 129us/step - loss: 46.1601 - val_loss: 185.5784\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 43.2027 - val_loss: 190.1199\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 41.0108 - val_loss: 195.7781\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 46.5598 - val_loss: 197.2198\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 42.8422 - val_loss: 195.4580\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 45.1241 - val_loss: 179.4934\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 43.5666 - val_loss: 197.8362\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 42.3916 - val_loss: 207.6388\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 44.4976 - val_loss: 186.4313\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 46.2066 - val_loss: 194.4104\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 47.1761 - val_loss: 212.6435\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 48.5120 - val_loss: 201.0159\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 42.7116 - val_loss: 187.7381\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 43.4936 - val_loss: 205.8406\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 46.6318 - val_loss: 185.8344\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 43.8420 - val_loss: 194.3817\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 44.5689 - val_loss: 197.4200\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 41.5677 - val_loss: 190.6480\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 46.2301 - val_loss: 205.9390\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 45.9543 - val_loss: 193.4124\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 40.9098 - val_loss: 202.3003\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 46.5096 - val_loss: 187.3437\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 42.3311 - val_loss: 180.7193\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 41.6413 - val_loss: 188.9921\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 43.5719 - val_loss: 178.1861\n",
      "Epoch 461/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 56us/step - loss: 45.9541 - val_loss: 192.1319\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 42.9753 - val_loss: 188.5417\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 45.3846 - val_loss: 214.9261\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 40.5467 - val_loss: 193.4123\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 41.9114 - val_loss: 191.2366\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 42.3322 - val_loss: 181.8296\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 43.5706 - val_loss: 189.9160\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 46.4776 - val_loss: 199.5949\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 41.7624 - val_loss: 187.7362\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 41.4031 - val_loss: 185.4034\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 44.6684 - val_loss: 196.5421\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 45.5459 - val_loss: 188.9989\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 40.1205 - val_loss: 197.8448\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 42.0796 - val_loss: 222.8071\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 44.2219 - val_loss: 186.5375\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 39.4456 - val_loss: 197.8219\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 43.0472 - val_loss: 184.5409\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 43.9958 - val_loss: 211.4323\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 43.2398 - val_loss: 205.4380\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 42.4160 - val_loss: 192.9750\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 40.1698 - val_loss: 194.9209\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 39.4772 - val_loss: 202.3672\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 42.8491 - val_loss: 197.5147\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 41.9445 - val_loss: 190.2575\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 40.7502 - val_loss: 190.6930\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 40.0823 - val_loss: 213.0440\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 40.3801 - val_loss: 211.9760\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 39.1261 - val_loss: 197.0397\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 43.3907 - val_loss: 186.5040\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 43.9691 - val_loss: 209.0183\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 43.2009 - val_loss: 194.8194\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 45.7786 - val_loss: 191.5175\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 40.9864 - val_loss: 192.4713\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 39.3342 - val_loss: 194.3807\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 43.5241 - val_loss: 213.0421\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 44.5603 - val_loss: 207.8680\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 42.5955 - val_loss: 192.9039\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 43.2058 - val_loss: 195.2152\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 41.3257 - val_loss: 188.6882\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 40.9027 - val_loss: 193.6958\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 38.0883 - val_loss: 191.2578\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 42.4216 - val_loss: 190.1436\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 39.7441 - val_loss: 184.9514\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 42.6277 - val_loss: 194.5594\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 38.5103 - val_loss: 213.9446\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 39.2469 - val_loss: 196.4514\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 40.7985 - val_loss: 187.5885\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 42.8326 - val_loss: 197.7155\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 41.9797 - val_loss: 198.4481\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 38.8440 - val_loss: 187.7017\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 42.4296 - val_loss: 200.3462\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 39.9847 - val_loss: 185.5353\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 37.6276 - val_loss: 187.4115\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 38.9893 - val_loss: 199.1176\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 39.1205 - val_loss: 199.5198\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 41.2305 - val_loss: 194.4445\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 43.1801 - val_loss: 197.2999\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 39.2404 - val_loss: 187.4815\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 41.5699 - val_loss: 187.8784\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 41.9752 - val_loss: 201.1791\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 38.8914 - val_loss: 202.5814\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 38.8365 - val_loss: 189.1527\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 41.4415 - val_loss: 185.5346\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 39.9339 - val_loss: 193.3240\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 40.7867 - val_loss: 198.4898\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 41.2888 - val_loss: 215.1186\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.5393 - val_loss: 186.8943\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.7616 - val_loss: 194.3438\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 40.0430 - val_loss: 206.5256\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 40.5859 - val_loss: 206.3452\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 43.3780 - val_loss: 184.9276\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 40.9843 - val_loss: 217.0589\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 44.0080 - val_loss: 187.6007\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 37.6003 - val_loss: 189.1786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 40.0905 - val_loss: 201.7625\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 40.1295 - val_loss: 189.1972\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 44.8048 - val_loss: 203.0698\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 38.6035 - val_loss: 206.7380\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 38.1455 - val_loss: 187.9853\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.4052 - val_loss: 197.9027\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 40.7603 - val_loss: 212.4627\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 41.1576 - val_loss: 209.0627\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 38.8036 - val_loss: 183.9862\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 39.7119 - val_loss: 185.0893\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 39.6720 - val_loss: 194.5007\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 38.8655 - val_loss: 191.4946\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 42.1572 - val_loss: 196.8860\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 39.2699 - val_loss: 202.3594\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 38.7988 - val_loss: 190.4458\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 36.7958 - val_loss: 192.2997\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 38.7251 - val_loss: 202.9937\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 39.4190 - val_loss: 197.3609\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 40.4359 - val_loss: 187.3683\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 36.4969 - val_loss: 199.2308\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 37.2939 - val_loss: 188.2863\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 36.2091 - val_loss: 198.6695\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 37.4082 - val_loss: 182.9845\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 37.2467 - val_loss: 192.8584\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 39.3209 - val_loss: 199.8774\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 43.2334 - val_loss: 203.1008\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 37.0909 - val_loss: 193.6218\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 41.0602 - val_loss: 194.8131\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.2209 - val_loss: 194.9723\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 39.4493 - val_loss: 188.8547\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 36.2129 - val_loss: 193.7486\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 37.4096 - val_loss: 189.1723\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 39.6633 - val_loss: 198.7720\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 38.0515 - val_loss: 192.5282\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 35.1394 - val_loss: 195.8528\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 37.1452 - val_loss: 206.1688\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.7328 - val_loss: 202.6399\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.8570 - val_loss: 198.6424\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 36.9939 - val_loss: 192.2473\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 38.8472 - val_loss: 200.8550\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 41.4210 - val_loss: 192.2365\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 37.2840 - val_loss: 201.3590\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 37.4479 - val_loss: 190.8000\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 36.4254 - val_loss: 197.6452\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 34.4275 - val_loss: 185.9078\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 37.7794 - val_loss: 185.2828\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 36.7275 - val_loss: 192.9058\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 40.8058 - val_loss: 192.4652\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 37.0649 - val_loss: 193.8073\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 37.9688 - val_loss: 191.8388\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 36.9197 - val_loss: 203.2498\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 39.1954 - val_loss: 192.1190\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 40.5632 - val_loss: 200.3317\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 35.2105 - val_loss: 196.3385\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 40.7259 - val_loss: 190.5561\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 36.7698 - val_loss: 189.4704\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 34.8685 - val_loss: 208.8394\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 36.4474 - val_loss: 182.0861\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 36.9920 - val_loss: 194.0079\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 43.0873 - val_loss: 193.3313\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 39.5707 - val_loss: 190.7720\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.1715 - val_loss: 188.5085\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 37.6246 - val_loss: 184.4141\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 37.9131 - val_loss: 189.6850\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 33.5499 - val_loss: 195.0569\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 39.0496 - val_loss: 197.4198\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.4506 - val_loss: 184.8986\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 37.2630 - val_loss: 191.0588\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 37.2018 - val_loss: 186.9174\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 34.7930 - val_loss: 196.6812\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 37.0902 - val_loss: 193.3328\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 36.7660 - val_loss: 193.1982\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 39.3008 - val_loss: 191.1339\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 36.8549 - val_loss: 187.9910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 37.8579 - val_loss: 206.9955\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 36.7478 - val_loss: 204.0253\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.3100 - val_loss: 190.9458\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.4893 - val_loss: 199.9791\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 34.9479 - val_loss: 196.3267\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 37.2986 - val_loss: 192.2198\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 34.4086 - val_loss: 185.5241\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.2863 - val_loss: 204.2028\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.2300 - val_loss: 189.0331\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 37.2091 - val_loss: 196.8057\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.4101 - val_loss: 194.5149\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 38.5874 - val_loss: 194.9580\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 39.1955 - val_loss: 196.2667\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 38.5983 - val_loss: 182.6545\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 40.1753 - val_loss: 193.8871\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 37.3874 - val_loss: 207.5641\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 34.6029 - val_loss: 202.6779\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 37.7545 - val_loss: 189.0131\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 37.3965 - val_loss: 196.5962\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 36.8637 - val_loss: 194.1925\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 34.9905 - val_loss: 189.8703\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 33.6884 - val_loss: 203.1547\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 1s 106us/step - loss: 33.5663 - val_loss: 195.9070\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 33.9640 - val_loss: 190.0601\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 36.3107 - val_loss: 195.2631\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 34.9019 - val_loss: 188.9655\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 35.4552 - val_loss: 198.2038\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 37.0707 - val_loss: 190.8165\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 35.9740 - val_loss: 193.2420\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 34.7011 - val_loss: 193.9713\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 35.6821 - val_loss: 193.1589\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 36.2985 - val_loss: 202.8961\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 35.9033 - val_loss: 188.0450\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 37.1869 - val_loss: 192.8752\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 36.0292 - val_loss: 191.1004\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 36.7713 - val_loss: 191.3469\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.3672 - val_loss: 212.6045\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 34.2438 - val_loss: 196.2491\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 33.8922 - val_loss: 187.6887\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 34.3576 - val_loss: 192.5690\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 36.5754 - val_loss: 194.5821\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 35.4904 - val_loss: 197.7749\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.5532 - val_loss: 195.3745\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.9932 - val_loss: 206.1293\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 38.5308 - val_loss: 189.1416\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 35.7749 - val_loss: 199.6289\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 43.1376 - val_loss: 192.3247\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 1s 172us/step - loss: 35.1110 - val_loss: 189.6381\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 33.2824 - val_loss: 200.4842\n",
      "Epoch 658/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 35.9154 - val_loss: 185.5763\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 35.7221 - val_loss: 195.7768\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 37.1693 - val_loss: 192.8749\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 35.5633 - val_loss: 194.0550\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 35.7393 - val_loss: 184.1443\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 32.1242 - val_loss: 194.9671\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 1s 144us/step - loss: 33.1855 - val_loss: 198.0888\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 35.1656 - val_loss: 198.8779\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 36.8421 - val_loss: 202.6366\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 1s 127us/step - loss: 35.5645 - val_loss: 198.4112\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 32.2102 - val_loss: 196.8566\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 35.1406 - val_loss: 182.3040\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 33.3536 - val_loss: 192.6476\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 32.8212 - val_loss: 192.1696\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 34.5827 - val_loss: 201.2783\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 36.6332 - val_loss: 199.6125\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 31.4332 - val_loss: 183.7871\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 35.6361 - val_loss: 194.1564\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 32.3948 - val_loss: 193.8688\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 35.5285 - val_loss: 190.5584\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 34.4851 - val_loss: 201.1345\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 34.0979 - val_loss: 194.4705\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 35.9988 - val_loss: 189.9936\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 34.9463 - val_loss: 191.7956\n",
      "Epoch 682/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 65us/step - loss: 37.9131 - val_loss: 193.8544\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 33.1895 - val_loss: 200.3251\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 34.3584 - val_loss: 186.5439\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 34.9492 - val_loss: 190.0384\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 35.1317 - val_loss: 192.4130\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 33.3944 - val_loss: 197.5548\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 34.0893 - val_loss: 192.1067\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 33.5023 - val_loss: 192.1900\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 31.4349 - val_loss: 190.7963\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.9871 - val_loss: 198.5010\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 35.2430 - val_loss: 188.9582\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 33.6009 - val_loss: 189.4869\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 34.4094 - val_loss: 197.1815\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 36.4014 - val_loss: 192.0475\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 30.9626 - val_loss: 186.4334\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 35.7989 - val_loss: 189.5499\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 36.0228 - val_loss: 198.7467\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 35.9771 - val_loss: 184.5800\n",
      "Epoch 700/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 33.8913 - val_loss: 184.8070\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 31.8304 - val_loss: 192.3395\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 33.2339 - val_loss: 197.4743\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 34.4616 - val_loss: 176.8245\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 32.3596 - val_loss: 202.5461\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 33.5532 - val_loss: 200.1689\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 34.6636 - val_loss: 189.4316\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 35.3906 - val_loss: 184.6659\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 33.7213 - val_loss: 196.4340\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.5624 - val_loss: 185.3010\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 33.1275 - val_loss: 203.5992\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.6261 - val_loss: 184.0657\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 32.9042 - val_loss: 195.5991\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.9244 - val_loss: 203.3124\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 33.3986 - val_loss: 190.0496\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 33.4708 - val_loss: 198.2707\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 35.4576 - val_loss: 189.6780\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 32.7898 - val_loss: 193.3800\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 30.8570 - val_loss: 187.4271\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 34.6355 - val_loss: 204.0791\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 31.2327 - val_loss: 201.8090\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 32.4017 - val_loss: 186.2031\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 33.6028 - val_loss: 188.7077\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 35.1706 - val_loss: 186.8581\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 30.4108 - val_loss: 206.0614\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 33.9940 - val_loss: 194.5625\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 31.7269 - val_loss: 189.5301\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 32.2061 - val_loss: 188.8699\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 32.4567 - val_loss: 191.0202\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 32.3483 - val_loss: 179.3312\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.6695 - val_loss: 176.3777\n",
      "Epoch 731/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.3945 - val_loss: 200.7419\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 31.5567 - val_loss: 189.2472\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 32.8825 - val_loss: 187.0684\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 35.0286 - val_loss: 192.5785\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 32.6939 - val_loss: 185.3607\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 30.6040 - val_loss: 187.8714\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 33.4391 - val_loss: 190.2828\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 34.0460 - val_loss: 183.1459\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 34.1974 - val_loss: 181.4931\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 30.7035 - val_loss: 186.6973\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 32.3035 - val_loss: 184.2455\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 30.1540 - val_loss: 184.2465\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 32.6702 - val_loss: 185.6946\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 34.7640 - val_loss: 185.7119\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.6508 - val_loss: 181.4927\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 31.1328 - val_loss: 182.2286\n",
      "Epoch 747/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 32.3998 - val_loss: 201.0148\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 37.2869 - val_loss: 185.7912\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.3177 - val_loss: 192.1048\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.3127 - val_loss: 197.8514\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.2094 - val_loss: 185.3300\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 32.6607 - val_loss: 194.9630\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 30.5837 - val_loss: 194.4295\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 29.6683 - val_loss: 190.8041\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 30.2847 - val_loss: 195.6756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 31.4840 - val_loss: 191.1698\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 29.6883 - val_loss: 189.6670\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 33.3683 - val_loss: 179.7707\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 31.7968 - val_loss: 190.2725\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.7362 - val_loss: 185.6100\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 32.7038 - val_loss: 194.0117\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 32.8486 - val_loss: 183.8872\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 30.6934 - val_loss: 188.9690\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 33.0514 - val_loss: 186.8464\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 34.6728 - val_loss: 185.6245\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 29.4617 - val_loss: 185.9152\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 33.6641 - val_loss: 189.1649\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 32.5001 - val_loss: 188.8718\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 31.3556 - val_loss: 184.3096\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.2835 - val_loss: 186.1102\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 35.7146 - val_loss: 191.5703\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 31.4319 - val_loss: 185.6219\n",
      "Epoch 773/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 30.1357 - val_loss: 192.4905\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 31.7537 - val_loss: 183.2721\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 32.2689 - val_loss: 193.9148\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.7072 - val_loss: 184.3895\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.3484 - val_loss: 191.8144\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.8850 - val_loss: 191.5452\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 32.6842 - val_loss: 200.5841\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.2784 - val_loss: 179.7730\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.8432 - val_loss: 183.2699\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 34.1060 - val_loss: 184.5266\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.8769 - val_loss: 188.9571\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 29.7844 - val_loss: 185.8460\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.1969 - val_loss: 191.4351\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.4208 - val_loss: 186.4177\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 32.3711 - val_loss: 184.4778\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.0196 - val_loss: 192.6557\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.0429 - val_loss: 187.6066\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 29.4736 - val_loss: 179.9488\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.4080 - val_loss: 194.5499\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.5298 - val_loss: 194.2005\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.9499 - val_loss: 208.0537\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.6476 - val_loss: 191.8033\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.2510 - val_loss: 189.5562\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.1259 - val_loss: 202.1866\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.0986 - val_loss: 188.8317\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.0944 - val_loss: 195.1881\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.7261 - val_loss: 193.0301\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.7591 - val_loss: 185.1892\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 31.4865 - val_loss: 194.5902\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.6360 - val_loss: 201.9947\n",
      "Epoch 803/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 29.2830 - val_loss: 193.3064\n",
      "Epoch 804/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 28.6249 - val_loss: 196.9461\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 30.2504 - val_loss: 197.8142\n",
      "Epoch 806/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 33.4098 - val_loss: 200.2747\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 30.1409 - val_loss: 194.1492\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.8021 - val_loss: 192.2746\n",
      "Epoch 809/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 31.6011 - val_loss: 188.9070\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 34.3200 - val_loss: 193.8379\n",
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 28.9909 - val_loss: 192.6868\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 33.1162 - val_loss: 184.7981\n",
      "Epoch 813/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 29.7880 - val_loss: 188.3102\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.5934 - val_loss: 190.3293\n",
      "Epoch 815/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.4626 - val_loss: 188.0902\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.1516 - val_loss: 190.9208\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.7992 - val_loss: 187.0587\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.3690 - val_loss: 179.5323\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.2581 - val_loss: 191.8683\n",
      "Epoch 820/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.9186 - val_loss: 191.9423\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.1322 - val_loss: 186.6190\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.6236 - val_loss: 181.8630\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.2515 - val_loss: 200.0829\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.9457 - val_loss: 188.3774\n",
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.0370 - val_loss: 193.6823\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.9459 - val_loss: 194.4286\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.4635 - val_loss: 194.6947\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.4393 - val_loss: 189.0355\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.8344 - val_loss: 181.8241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.1922 - val_loss: 189.4319\n",
      "Epoch 831/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.6433 - val_loss: 187.2270\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.1314 - val_loss: 192.0996\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 27.9778 - val_loss: 189.9696\n",
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 31.2219 - val_loss: 186.5850\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.4270 - val_loss: 187.4435\n",
      "Epoch 836/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.1217 - val_loss: 187.5485\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.8773 - val_loss: 193.3551\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 29.8130 - val_loss: 196.5779\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 29.4975 - val_loss: 191.9014\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 28.9190 - val_loss: 189.3539\n",
      "Epoch 841/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 29.6502 - val_loss: 185.0054\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 31.8207 - val_loss: 186.5017\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 31.7518 - val_loss: 188.3825\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.8294 - val_loss: 191.2456\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 27.3831 - val_loss: 186.6689\n",
      "Epoch 846/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 30.2316 - val_loss: 190.0606\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 30.4777 - val_loss: 189.1986\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 31.3014 - val_loss: 184.4911\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 30.2575 - val_loss: 197.5841\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.9966 - val_loss: 192.5562\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 29.6227 - val_loss: 183.8072\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 30.4769 - val_loss: 192.9210\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 27.8797 - val_loss: 195.8724\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 31.4450 - val_loss: 187.4399\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 28.6649 - val_loss: 191.5147\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 31.0165 - val_loss: 183.4474\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 33.9729 - val_loss: 184.6021\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 29.8151 - val_loss: 195.0807\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.6063 - val_loss: 196.1911\n",
      "Epoch 860/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.5952 - val_loss: 180.3846\n",
      "Epoch 861/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 27.2156 - val_loss: 188.6689\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 28.3953 - val_loss: 183.9243\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 26.2512 - val_loss: 188.7903\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 30.6195 - val_loss: 194.5484\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 28.4360 - val_loss: 185.7633\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 29.5868 - val_loss: 182.0475\n",
      "Epoch 867/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 30.2094 - val_loss: 175.8090\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 29.1341 - val_loss: 193.5754\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.7337 - val_loss: 187.0857\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 29.3621 - val_loss: 192.7817\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 32.9748 - val_loss: 191.0681\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 28.0908 - val_loss: 187.3503\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 27.1688 - val_loss: 205.5763\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 27.6421 - val_loss: 181.4103\n",
      "Epoch 875/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 30.5568 - val_loss: 189.8959\n",
      "Epoch 876/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 28.8110 - val_loss: 186.8154\n",
      "Epoch 877/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 26.6900 - val_loss: 183.1403\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 27.9536 - val_loss: 194.7639\n",
      "Epoch 879/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 29.4829 - val_loss: 184.3719\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.5485 - val_loss: 193.6795\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.0138 - val_loss: 194.9224\n",
      "Epoch 882/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.1473 - val_loss: 196.3255\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.4658 - val_loss: 182.4952\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 28.0555 - val_loss: 180.6533\n",
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.1735 - val_loss: 175.4342\n",
      "Epoch 886/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.3578 - val_loss: 183.7420\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 28.1226 - val_loss: 184.7578\n",
      "Epoch 888/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 29.7009 - val_loss: 183.2692\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.5776 - val_loss: 190.3651\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.9690 - val_loss: 188.0482\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.1053 - val_loss: 181.0372\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 26.8076 - val_loss: 185.9928\n",
      "Epoch 893/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 31.9639 - val_loss: 181.7838\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 27.6820 - val_loss: 185.2709\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 32.0393 - val_loss: 181.4139\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 27.8559 - val_loss: 186.2575\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 27.9891 - val_loss: 189.0197\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.2356 - val_loss: 195.0602\n",
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 27.5814 - val_loss: 201.7703\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 27.6435 - val_loss: 183.9375\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.9557 - val_loss: 188.9989\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.6734 - val_loss: 209.5936\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 29.8741 - val_loss: 190.8729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 904/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 27.1721 - val_loss: 184.9732\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 27.8668 - val_loss: 183.7321\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 31.1160 - val_loss: 186.5822\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 28.6812 - val_loss: 188.2070\n",
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 28.0591 - val_loss: 188.2169\n",
      "Epoch 909/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 27.2539 - val_loss: 185.2183\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 28.3317 - val_loss: 194.3061\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 29.3240 - val_loss: 190.9255\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 27.6606 - val_loss: 187.1945\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 28.1074 - val_loss: 182.7374\n",
      "Epoch 914/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 28.0752 - val_loss: 186.5272\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 27.9950 - val_loss: 185.5168\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 29.5811 - val_loss: 187.8351\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.2327 - val_loss: 187.7614\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.5829 - val_loss: 189.7316\n",
      "Epoch 919/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.9480 - val_loss: 200.1054\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.3430 - val_loss: 190.6051\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.4738 - val_loss: 181.0987\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.2468 - val_loss: 190.2908\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 27.5359 - val_loss: 192.3772\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.6991 - val_loss: 195.3462\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.4650 - val_loss: 181.0139\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.0064 - val_loss: 187.7579\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 25.8950 - val_loss: 185.6088\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.2424 - val_loss: 195.1907\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 35.0614 - val_loss: 188.1072\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 26.0828 - val_loss: 196.1663\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 25.9677 - val_loss: 189.8334\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 28.4708 - val_loss: 194.3185\n",
      "Epoch 933/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 26.3879 - val_loss: 199.0736\n",
      "Epoch 934/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 30.8952 - val_loss: 183.7629\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 28.5881 - val_loss: 180.6596\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 29.1417 - val_loss: 185.4433\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 25.8865 - val_loss: 193.7510\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 29.6367 - val_loss: 182.9748\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 31.4959 - val_loss: 201.2556\n",
      "Epoch 940/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 30.6906 - val_loss: 188.2891\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 25.0530 - val_loss: 186.2030\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 25.4733 - val_loss: 179.7781\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 27.1798 - val_loss: 189.0014\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 26.6047 - val_loss: 188.6783\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 28.3574 - val_loss: 179.6401\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.0082 - val_loss: 185.3779\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.9981 - val_loss: 182.6233\n",
      "Epoch 948/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 27.4041 - val_loss: 191.0066\n",
      "Epoch 949/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 29.1112 - val_loss: 197.9230\n",
      "Epoch 950/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.4903 - val_loss: 184.1728\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.3594 - val_loss: 180.5198\n",
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.0120 - val_loss: 181.1575\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.0103 - val_loss: 184.6679\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.3864 - val_loss: 197.0965\n",
      "Epoch 955/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 27.8317 - val_loss: 190.0009\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 31.6917 - val_loss: 182.3942\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.1515 - val_loss: 185.7779\n",
      "Epoch 958/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.5813 - val_loss: 198.6643\n",
      "Epoch 959/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 26.5867 - val_loss: 184.8998\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.4756 - val_loss: 182.6544\n",
      "Epoch 961/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 27.9169 - val_loss: 178.6184\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 26.0760 - val_loss: 193.4474\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 25.8837 - val_loss: 196.8604\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.1378 - val_loss: 185.8206\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 30.5290 - val_loss: 189.6940\n",
      "Epoch 966/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 28.9946 - val_loss: 184.6604\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 28.4870 - val_loss: 197.4427\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 29.3694 - val_loss: 189.0918\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 28.6693 - val_loss: 190.2845\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 27.8425 - val_loss: 197.8543\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 27.5770 - val_loss: 189.8527\n",
      "Epoch 972/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 26.6666 - val_loss: 206.8632\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 25.5748 - val_loss: 183.5053\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 25.4646 - val_loss: 185.8963\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 26.3784 - val_loss: 182.9276\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.1953 - val_loss: 182.1701\n",
      "Epoch 977/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.4648 - val_loss: 194.6754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 28.0729 - val_loss: 184.0338\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 28.1065 - val_loss: 195.6602\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 27.1712 - val_loss: 190.4129\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.3126 - val_loss: 183.1446\n",
      "Epoch 982/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 26.3375 - val_loss: 192.2323\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.6684 - val_loss: 196.1902\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 24.5923 - val_loss: 189.2292\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 28.5945 - val_loss: 216.3882\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 27.3906 - val_loss: 198.7261\n",
      "Epoch 987/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 29.8125 - val_loss: 188.7071\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 29.1036 - val_loss: 191.5464\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 29.1955 - val_loss: 187.6983\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 30.7075 - val_loss: 190.3713\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 27.7967 - val_loss: 195.4092\n",
      "Epoch 992/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 29.8233 - val_loss: 189.2871\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.0712 - val_loss: 189.7452\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 27.5404 - val_loss: 182.8645\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 27.2140 - val_loss: 188.7735\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 28.1395 - val_loss: 186.0899\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 26.0222 - val_loss: 191.7871\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 25.4350 - val_loss: 192.7023\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 25.6426 - val_loss: 192.3473\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 26.1034 - val_loss: 218.7175\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 29.3444 - val_loss: 192.6815\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 26.9379 - val_loss: 193.0975\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 23.8820 - val_loss: 184.8043\n",
      "Epoch 1004/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 27.9571 - val_loss: 193.1394\n",
      "Epoch 1005/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.3355 - val_loss: 192.7735\n",
      "Epoch 1006/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 26.0870 - val_loss: 188.9312\n",
      "Epoch 1007/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 28.7983 - val_loss: 187.9067\n",
      "Epoch 1008/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.7434 - val_loss: 184.0038\n",
      "Epoch 1009/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.9393 - val_loss: 188.9459\n",
      "Epoch 1010/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 25.2769 - val_loss: 182.9036\n",
      "Epoch 1011/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 28.3315 - val_loss: 181.4436\n",
      "Epoch 1012/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 26.9307 - val_loss: 183.2812\n",
      "Epoch 1013/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 26.1399 - val_loss: 192.7512\n",
      "Epoch 1014/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 28.5594 - val_loss: 189.9937\n",
      "Epoch 1015/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.9086 - val_loss: 196.4015\n",
      "Epoch 1016/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.1120 - val_loss: 186.1264\n",
      "Epoch 1017/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.6179 - val_loss: 188.7326\n",
      "Epoch 1018/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 25.2300 - val_loss: 184.6626\n",
      "Epoch 1019/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 26.5563 - val_loss: 184.6493\n",
      "Epoch 1020/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.1661 - val_loss: 194.0012\n",
      "Epoch 1021/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 30.6418 - val_loss: 192.1487\n",
      "Epoch 1022/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.7697 - val_loss: 188.9954\n",
      "Epoch 1023/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 25.3797 - val_loss: 193.3981\n",
      "Epoch 1024/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 25.1632 - val_loss: 185.5176\n",
      "Epoch 1025/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.7236 - val_loss: 191.4172\n",
      "Epoch 1026/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 28.8880 - val_loss: 187.9901\n",
      "Epoch 1027/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 27.3519 - val_loss: 191.4171\n",
      "Epoch 1028/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.0482 - val_loss: 188.4254\n",
      "Epoch 1029/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 29.7747 - val_loss: 187.2683\n",
      "Epoch 1030/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 28.9684 - val_loss: 188.9971\n",
      "Epoch 1031/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 24.3565 - val_loss: 184.0930\n",
      "Epoch 1032/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 23.4616 - val_loss: 181.2503\n",
      "Epoch 1033/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 23.7725 - val_loss: 187.3735\n",
      "Epoch 1034/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.8158 - val_loss: 201.2386\n",
      "Epoch 1035/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 26.1681 - val_loss: 188.9930\n",
      "Epoch 1036/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 32.7244 - val_loss: 183.3867\n",
      "Epoch 1037/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 31.1550 - val_loss: 190.0939\n",
      "Epoch 1038/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 27.3545 - val_loss: 192.0743\n",
      "Epoch 1039/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 25.2014 - val_loss: 183.5081\n",
      "Epoch 1040/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 25.7412 - val_loss: 190.6415\n",
      "Epoch 1041/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 24.2760 - val_loss: 184.2246\n",
      "Epoch 1042/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 27.0403 - val_loss: 177.4522\n",
      "Epoch 1043/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 24.7877 - val_loss: 177.9412\n",
      "Epoch 1044/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 24.8961 - val_loss: 183.7573\n",
      "Epoch 1045/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.3678 - val_loss: 185.8964\n",
      "Epoch 1046/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 24.8685 - val_loss: 182.4890\n",
      "Epoch 1047/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 25.3062 - val_loss: 188.9942\n",
      "Epoch 1048/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 24.8001 - val_loss: 198.3496\n",
      "Epoch 1049/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 27.3044 - val_loss: 187.0124\n",
      "Epoch 1050/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 30.0740 - val_loss: 183.3410\n",
      "Epoch 1051/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 52us/step - loss: 24.4578 - val_loss: 179.1955\n",
      "Epoch 1052/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 24.5289 - val_loss: 177.7281\n",
      "Epoch 1053/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.7958 - val_loss: 189.8373\n",
      "Epoch 1054/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 24.0184 - val_loss: 184.1782\n",
      "Epoch 1055/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.3482 - val_loss: 179.3047\n",
      "Epoch 1056/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 24.4486 - val_loss: 181.4513\n",
      "Epoch 1057/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.4897 - val_loss: 188.8356\n",
      "Epoch 1058/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 26.0054 - val_loss: 183.4668\n",
      "Epoch 1059/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.1720 - val_loss: 178.9493\n",
      "Epoch 1060/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 25.0518 - val_loss: 190.5635\n",
      "Epoch 1061/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 24.9963 - val_loss: 180.8795\n",
      "Epoch 1062/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 27.7407 - val_loss: 186.7008\n",
      "Epoch 1063/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 26.9482 - val_loss: 189.7172\n",
      "Epoch 1064/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 24.9575 - val_loss: 185.7457\n",
      "Epoch 1065/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 22.9884 - val_loss: 189.9131\n",
      "Epoch 1066/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.2291 - val_loss: 188.7106\n",
      "Epoch 1067/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 27.1140 - val_loss: 192.1380\n",
      "Epoch 1068/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 24.6367 - val_loss: 194.4307\n",
      "Epoch 1069/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 26.2642 - val_loss: 193.5724\n",
      "Epoch 1070/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.1896 - val_loss: 184.2219\n",
      "Epoch 1071/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 26.5121 - val_loss: 189.5912\n",
      "Epoch 1072/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 24.6376 - val_loss: 194.1249\n",
      "Epoch 1073/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 24.4808 - val_loss: 191.5959\n",
      "Epoch 1074/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 30.2385 - val_loss: 192.2802\n",
      "Epoch 1075/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 24.7437 - val_loss: 197.8891\n",
      "Epoch 1076/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 25.1781 - val_loss: 180.5827\n",
      "Epoch 1077/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.9002 - val_loss: 188.6866\n",
      "Epoch 1078/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 24.6903 - val_loss: 181.1210\n",
      "Epoch 1079/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 25.9542 - val_loss: 199.9496\n",
      "Epoch 1080/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 24.6714 - val_loss: 182.9487\n",
      "Epoch 1081/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 25.9648 - val_loss: 193.2705\n",
      "Epoch 1082/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 26.5463 - val_loss: 189.8455\n",
      "Epoch 1083/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 24.0968 - val_loss: 189.9576\n",
      "Epoch 1084/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.6084 - val_loss: 181.8511\n",
      "Epoch 1085/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.4317 - val_loss: 183.3616\n",
      "Epoch 1086/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 25.5619 - val_loss: 191.8152\n",
      "Epoch 1087/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 25.9772 - val_loss: 207.0715\n",
      "Epoch 1088/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 27.4088 - val_loss: 190.1895\n",
      "Epoch 1089/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 25.5835 - val_loss: 194.2884\n",
      "Epoch 1090/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 25.1954 - val_loss: 202.0406\n",
      "Epoch 1091/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 26.1109 - val_loss: 204.0343\n",
      "Epoch 1092/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 26.8351 - val_loss: 185.9172\n",
      "Epoch 1093/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.8054 - val_loss: 195.6412\n",
      "Epoch 1094/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.6919 - val_loss: 196.7716\n",
      "Epoch 1095/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 25.2916 - val_loss: 183.9397\n",
      "Epoch 1096/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 26.5067 - val_loss: 181.7144\n",
      "Epoch 1097/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 24.9288 - val_loss: 183.2108\n",
      "Epoch 1098/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 25.5680 - val_loss: 193.6480\n",
      "Epoch 1099/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 25.8033 - val_loss: 190.0106\n",
      "Epoch 1100/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.6645 - val_loss: 189.0339\n",
      "Epoch 1101/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 24.7069 - val_loss: 190.0263\n",
      "Epoch 1102/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 25.7484 - val_loss: 192.9185\n",
      "Epoch 1103/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.5328 - val_loss: 193.3764\n",
      "Epoch 1104/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.6234 - val_loss: 181.6482\n",
      "Epoch 1105/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 28.8653 - val_loss: 192.5316\n",
      "Epoch 1106/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 26.5803 - val_loss: 181.6742\n",
      "Epoch 1107/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 27.9910 - val_loss: 190.2198\n",
      "Epoch 1108/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 23.8065 - val_loss: 186.2758\n",
      "Epoch 1109/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 22.9996 - val_loss: 187.6438\n",
      "Epoch 1110/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 25.2015 - val_loss: 186.9057\n",
      "Epoch 1111/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 22.9753 - val_loss: 194.5482\n",
      "Epoch 1112/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.0330 - val_loss: 187.5226\n",
      "Epoch 1113/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 25.3944 - val_loss: 191.1419\n",
      "Epoch 1114/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 24.8711 - val_loss: 198.3422\n",
      "Epoch 1115/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.0155 - val_loss: 200.2724\n",
      "Epoch 1116/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 25.0097 - val_loss: 193.3116\n",
      "Epoch 1117/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.2750 - val_loss: 187.6504\n",
      "Epoch 1118/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 24.7223 - val_loss: 190.2677\n",
      "Epoch 1119/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 24.0033 - val_loss: 190.5541\n",
      "Epoch 1120/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.8857 - val_loss: 196.3229\n",
      "Epoch 1121/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.9806 - val_loss: 190.9514\n",
      "Epoch 1122/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 25.5522 - val_loss: 181.1781\n",
      "Epoch 1123/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 28.6717 - val_loss: 190.3455\n",
      "Epoch 1124/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.5006 - val_loss: 183.6474\n",
      "Epoch 1125/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 23.8058 - val_loss: 202.3554\n",
      "Epoch 1126/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 25.2174 - val_loss: 186.8730\n",
      "Epoch 1127/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 23.3156 - val_loss: 188.3430\n",
      "Epoch 1128/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.2298 - val_loss: 193.6686\n",
      "Epoch 1129/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.3928 - val_loss: 185.7057\n",
      "Epoch 1130/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 25.5527 - val_loss: 190.6082\n",
      "Epoch 1131/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 23.3378 - val_loss: 191.6729\n",
      "Epoch 1132/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.1681 - val_loss: 197.4617\n",
      "Epoch 1133/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.0656 - val_loss: 197.9520\n",
      "Epoch 1134/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.1330 - val_loss: 193.5630\n",
      "Epoch 1135/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 27.0790 - val_loss: 197.8802\n",
      "Epoch 1136/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 23.8657 - val_loss: 189.5725\n",
      "Epoch 1137/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 24.2071 - val_loss: 199.0590\n",
      "Epoch 1138/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 25.3758 - val_loss: 194.1442\n",
      "Epoch 1139/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 26.1086 - val_loss: 208.6955\n",
      "Epoch 1140/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 23.6834 - val_loss: 193.8252\n",
      "Epoch 1141/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 26.4305 - val_loss: 191.1819\n",
      "Epoch 1142/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 25.5728 - val_loss: 190.6276\n",
      "Epoch 1143/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 24.2442 - val_loss: 189.8909\n",
      "Epoch 1144/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 22.4711 - val_loss: 185.2615\n",
      "Epoch 1145/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 27.4176 - val_loss: 201.8196\n",
      "Epoch 1146/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 27.9309 - val_loss: 183.6085\n",
      "Epoch 1147/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 26.4916 - val_loss: 192.0170\n",
      "Epoch 1148/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 25.1546 - val_loss: 187.2675\n",
      "Epoch 1149/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.1342 - val_loss: 181.7303\n",
      "Epoch 1150/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 22.6807 - val_loss: 191.9956\n",
      "Epoch 1151/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 22.1736 - val_loss: 193.8017\n",
      "Epoch 1152/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 27.4438 - val_loss: 189.3856\n",
      "Epoch 1153/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 24.9280 - val_loss: 189.6136\n",
      "Epoch 1154/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 23.1586 - val_loss: 185.5992\n",
      "Epoch 1155/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 22.8538 - val_loss: 187.4311\n",
      "Epoch 1156/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 23.9427 - val_loss: 186.6403\n",
      "Epoch 1157/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 23.0333 - val_loss: 189.2405\n",
      "Epoch 1158/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 24.6901 - val_loss: 184.3081\n",
      "Epoch 1159/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 27.6089 - val_loss: 193.5709\n",
      "Epoch 1160/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 30.1627 - val_loss: 193.7986\n",
      "Epoch 1161/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 22.9616 - val_loss: 190.8304\n",
      "Epoch 1162/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 21.2102 - val_loss: 193.3226\n",
      "Epoch 1163/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 23.9554 - val_loss: 192.4318\n",
      "Epoch 1164/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 25.3153 - val_loss: 181.0016\n",
      "Epoch 1165/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 26.0205 - val_loss: 194.8104\n",
      "Epoch 1166/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.7337 - val_loss: 177.0024\n",
      "Epoch 1167/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 24.0500 - val_loss: 183.5476\n",
      "Epoch 1168/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.5018 - val_loss: 186.1518\n",
      "Epoch 1169/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 23.3609 - val_loss: 186.1062\n",
      "Epoch 1170/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 23.4211 - val_loss: 194.3180\n",
      "Epoch 1171/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 23.9074 - val_loss: 193.3653\n",
      "Epoch 1172/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 25.6133 - val_loss: 181.1129\n",
      "Epoch 1173/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.0279 - val_loss: 202.3128\n",
      "Epoch 1174/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 25.8580 - val_loss: 192.5608\n",
      "Epoch 1175/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.2162 - val_loss: 188.9685\n",
      "Epoch 1176/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 24.3956 - val_loss: 183.8494\n",
      "Epoch 1177/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 26.9049 - val_loss: 192.0117\n",
      "Epoch 1178/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 24.3807 - val_loss: 187.8760\n",
      "Epoch 1179/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 24.9809 - val_loss: 187.7991\n",
      "Epoch 1180/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 22.8666 - val_loss: 189.1965\n",
      "Epoch 1181/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.6684 - val_loss: 185.1147\n",
      "Epoch 1182/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 25.3976 - val_loss: 188.9872\n",
      "Epoch 1183/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 24.9172 - val_loss: 183.7205\n",
      "Epoch 1184/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.8112 - val_loss: 186.0930\n",
      "Epoch 1185/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 25.0565 - val_loss: 195.9270\n",
      "Epoch 1186/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.5344 - val_loss: 198.5652\n",
      "Epoch 1187/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 25.0735 - val_loss: 193.1475\n",
      "Epoch 1188/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 24.6028 - val_loss: 193.8859\n",
      "Epoch 1189/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 24.2354 - val_loss: 183.2622\n",
      "Epoch 1190/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 24.4726 - val_loss: 181.9985\n",
      "Epoch 1191/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 27.5835 - val_loss: 203.1547\n",
      "Epoch 1192/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 23.5552 - val_loss: 196.5928\n",
      "Epoch 1193/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 24.5782 - val_loss: 200.8965\n",
      "Epoch 1194/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 25.1981 - val_loss: 199.6757\n",
      "Epoch 1195/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 23.1128 - val_loss: 193.5245\n",
      "Epoch 1196/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 22.0980 - val_loss: 188.5154\n",
      "Epoch 1197/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 24.9318 - val_loss: 198.5286\n",
      "Epoch 1198/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 24.8520 - val_loss: 187.5163\n",
      "Epoch 1199/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.3617 - val_loss: 205.8546\n",
      "Epoch 1200/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 26.0152 - val_loss: 190.1031\n",
      "Epoch 1201/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.0160 - val_loss: 203.7046\n",
      "Epoch 1202/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 22.7338 - val_loss: 188.8047\n",
      "Epoch 1203/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 24.1984 - val_loss: 194.0501\n",
      "Epoch 1204/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 25.1414 - val_loss: 181.9188\n",
      "Epoch 1205/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 24.1389 - val_loss: 193.4260\n",
      "Epoch 1206/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 22.2507 - val_loss: 198.1144\n",
      "Epoch 1207/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.5287 - val_loss: 186.6751\n",
      "Epoch 1208/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 22.8517 - val_loss: 183.4885\n",
      "Epoch 1209/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 22.70 - 0s 51us/step - loss: 22.4971 - val_loss: 181.7047\n",
      "Epoch 1210/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.4840 - val_loss: 186.9842\n",
      "Epoch 1211/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 26.3067 - val_loss: 188.0058\n",
      "Epoch 1212/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 22.3096 - val_loss: 184.9798\n",
      "Epoch 1213/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.3655 - val_loss: 192.5358\n",
      "Epoch 1214/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 23.8579 - val_loss: 206.9920\n",
      "Epoch 1215/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.8866 - val_loss: 188.9751\n",
      "Epoch 1216/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.3892 - val_loss: 194.0161\n",
      "Epoch 1217/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 26.0844 - val_loss: 189.5282\n",
      "Epoch 1218/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.8671 - val_loss: 197.0310\n",
      "Epoch 1219/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 22.2036 - val_loss: 195.9960\n",
      "Epoch 01219: early stopping\n",
      "Fold score (RMSE): 13.888583183288574\n",
      "Fold #4\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 13973.3144 - val_loss: 1322.8670\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 864.5741 - val_loss: 639.8474\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 600.9887 - val_loss: 461.9664\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 446.5731 - val_loss: 351.3060\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 356.3063 - val_loss: 309.2019\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 322.9646 - val_loss: 288.7703\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 306.7676 - val_loss: 284.7628\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 295.2515 - val_loss: 268.9838\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 288.1021 - val_loss: 267.0127\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 285.3500 - val_loss: 257.6068\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 275.6925 - val_loss: 256.0747\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 271.5240 - val_loss: 249.5297\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 265.6061 - val_loss: 247.0935\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 263.0701 - val_loss: 243.8663\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 257.1565 - val_loss: 244.9487\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 255.4599 - val_loss: 242.3750\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 254.1947 - val_loss: 247.8175\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 253.7697 - val_loss: 239.0431\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 244.4238 - val_loss: 235.1699\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 248.2283 - val_loss: 242.6547\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 242.3926 - val_loss: 236.9922\n",
      "Epoch 22/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 241.4715 - val_loss: 233.7063\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 237.8323 - val_loss: 242.2877\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 232.6444 - val_loss: 243.2818\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 231.3698 - val_loss: 235.5428\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 231.5704 - val_loss: 224.0638\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 235.1655 - val_loss: 239.4880\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 226.1701 - val_loss: 228.4585\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 223.7507 - val_loss: 238.9380\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 228.3966 - val_loss: 290.0741\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 227.4322 - val_loss: 239.3682\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 226.2724 - val_loss: 233.6862\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 224.2279 - val_loss: 263.4765\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 221.4266 - val_loss: 233.4601\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 217.4734 - val_loss: 231.5614\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 223.6545 - val_loss: 245.8454\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 220.8231 - val_loss: 226.0889\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 210.4004 - val_loss: 315.3831\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 215.2104 - val_loss: 221.8845\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 207.6736 - val_loss: 233.4721\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 211.3424 - val_loss: 220.6456\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 206.4038 - val_loss: 218.6919\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 209.6772 - val_loss: 218.9397\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 200.4574 - val_loss: 223.6627\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 201.8387 - val_loss: 225.1801\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 201.3201 - val_loss: 226.4922\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 198.3894 - val_loss: 216.9139\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 198.0581 - val_loss: 228.6092\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 197.5638 - val_loss: 222.1138\n",
      "Epoch 50/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 51us/step - loss: 195.9337 - val_loss: 217.0048\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 195.8398 - val_loss: 213.7130\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 192.1076 - val_loss: 246.2658\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 192.5681 - val_loss: 219.4672\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 193.1684 - val_loss: 218.1325\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 189.4024 - val_loss: 214.5189\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 194.6984 - val_loss: 229.6886\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 186.9098 - val_loss: 219.7550\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 189.1961 - val_loss: 223.9031\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 182.0893 - val_loss: 257.7845\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 183.5122 - val_loss: 223.7933\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 187.8047 - val_loss: 215.9026\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 179.0124 - val_loss: 210.7078\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 176.4261 - val_loss: 216.5702\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 173.2354 - val_loss: 216.2729\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 180.3026 - val_loss: 214.3622\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 176.8508 - val_loss: 208.6587\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 169.4479 - val_loss: 200.1726\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 171.0619 - val_loss: 202.4628\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 168.3285 - val_loss: 193.2375\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 171.5488 - val_loss: 209.4191\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 166.2235 - val_loss: 215.3561\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 164.0903 - val_loss: 203.5066\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 164.0267 - val_loss: 201.3937\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 161.2025 - val_loss: 210.8512\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 158.5804 - val_loss: 230.5575\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 158.4900 - val_loss: 191.7435\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 168.0802 - val_loss: 194.5499\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 162.8614 - val_loss: 192.9915\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 161.6829 - val_loss: 192.1561\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 154.7317 - val_loss: 190.2767\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 155.7911 - val_loss: 192.6844\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 152.3081 - val_loss: 194.6299\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 153.1555 - val_loss: 185.9315\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 146.0877 - val_loss: 183.8103\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 147.8916 - val_loss: 168.4290\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.3263 - val_loss: 185.0097\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 143.6824 - val_loss: 183.3594\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 142.4499 - val_loss: 199.6037\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 146.0781 - val_loss: 193.2661\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 137.2375 - val_loss: 171.4745\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 140.6616 - val_loss: 176.1707\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 138.0225 - val_loss: 170.5076\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.4632 - val_loss: 178.0771\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 136.7608 - val_loss: 169.9738\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 139.1156 - val_loss: 170.5465\n",
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 131.6795 - val_loss: 165.8874\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 132.5398 - val_loss: 161.0497\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 128.0438 - val_loss: 160.8562\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 131.1224 - val_loss: 163.5409\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.8297 - val_loss: 177.8958\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 128.4278 - val_loss: 161.7578\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 124.5760 - val_loss: 168.2195\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.4853 - val_loss: 183.0869\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 125.3342 - val_loss: 157.8995\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 126.3220 - val_loss: 164.0556\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 118.1337 - val_loss: 174.3273\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 124.1102 - val_loss: 166.1431\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 118.7676 - val_loss: 155.4416\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 121.1934 - val_loss: 182.4989\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 122.3163 - val_loss: 168.9160\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 118.6094 - val_loss: 159.4493\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 116.8518 - val_loss: 164.9654\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 116.6600 - val_loss: 160.5482\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 119.6069 - val_loss: 158.3771\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 121.8862 - val_loss: 173.3343\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 115.4276 - val_loss: 153.7198\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 113.8655 - val_loss: 184.0705\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 111.0804 - val_loss: 155.4072\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 112.2013 - val_loss: 161.3295\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 110.7381 - val_loss: 161.8008\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 114.6631 - val_loss: 152.5219\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 115.3246 - val_loss: 155.3701\n",
      "Epoch 123/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 76us/step - loss: 106.6750 - val_loss: 163.6853\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 109.8372 - val_loss: 149.6730\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 104.9094 - val_loss: 160.7938\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 111.8052 - val_loss: 164.8010\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 107.4399 - val_loss: 162.4473\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 109.0828 - val_loss: 166.7975\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 104.4599 - val_loss: 155.0663\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 114.3798 - val_loss: 146.0188\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 104.3951 - val_loss: 153.2456\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 105.5511 - val_loss: 168.3554\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 106.5549 - val_loss: 166.4524\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 112.943 - 1s 76us/step - loss: 112.7689 - val_loss: 167.4730\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 103.3623 - val_loss: 152.9314\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 102.5673 - val_loss: 212.4337\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 101.6150 - val_loss: 171.5020\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 102.4166 - val_loss: 161.0566\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 98.6924 - val_loss: 150.3502\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 101.6645 - val_loss: 151.7760\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 100.5612 - val_loss: 161.8587\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 98.7084 - val_loss: 174.9034\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 107.2911 - val_loss: 154.8102\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 98.7093 - val_loss: 172.9054\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 102.6506 - val_loss: 171.1901\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 103.8260 - val_loss: 163.0882\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 96.0897 - val_loss: 153.2821\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 95.8463 - val_loss: 159.0910\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 101.7778 - val_loss: 159.3765\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 97.3158 - val_loss: 159.6970\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 96.7453 - val_loss: 177.1363\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 93.2579 - val_loss: 149.2343\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 90.1317 - val_loss: 173.4107\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 98.3896 - val_loss: 153.0774\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 98.7365 - val_loss: 170.1391\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 95.0694 - val_loss: 161.4874\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 99.3902 - val_loss: 162.7942\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 100.2356 - val_loss: 162.8202\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 93.5567 - val_loss: 164.1195\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 91.7679 - val_loss: 151.9104\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 93.6938 - val_loss: 163.4104\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 101.4351 - val_loss: 160.9659\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 91.3386 - val_loss: 156.8833\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 90.6824 - val_loss: 154.6761\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 88.0022 - val_loss: 161.9447\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 90.7940 - val_loss: 156.8341\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 91.1571 - val_loss: 193.4626\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 94.0649 - val_loss: 159.3647\n",
      "Epoch 169/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 93.3138 - val_loss: 161.6702\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 88.4040 - val_loss: 169.0365\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 97.2340 - val_loss: 166.4191\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 88.7257 - val_loss: 160.0276\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 89.1230 - val_loss: 154.4319\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 83.2666 - val_loss: 164.5226\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 90.0552 - val_loss: 157.7175\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 91.4523 - val_loss: 157.7511\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 89.0780 - val_loss: 168.8080\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 87.3537 - val_loss: 193.4618\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 85.3250 - val_loss: 157.0351\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 82.3508 - val_loss: 160.3690\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 81.7311 - val_loss: 162.1712\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 88.3768 - val_loss: 160.1271\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 82.1302 - val_loss: 181.1437\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 92.2190 - val_loss: 168.5290\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 87.5369 - val_loss: 168.8333\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 86.3359 - val_loss: 154.6242\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 85.6195 - val_loss: 153.2232\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 83.6471 - val_loss: 149.8362\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 88.0372 - val_loss: 154.4176\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 84.6876 - val_loss: 169.4921\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 88.0242 - val_loss: 164.9613\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 83.2854 - val_loss: 166.0608\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 80.6431 - val_loss: 177.2023\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 83.3645 - val_loss: 149.4525\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 78.6080 - val_loss: 181.7575\n",
      "Epoch 196/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 72us/step - loss: 89.2383 - val_loss: 168.4732\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 78.5105 - val_loss: 150.4362\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 83.1822 - val_loss: 162.2974\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 80.6680 - val_loss: 163.4187\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 86.3296 - val_loss: 168.7295\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 81.7253 - val_loss: 168.9879\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 83.3520 - val_loss: 181.3214\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 82.9232 - val_loss: 152.1299\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 80.2668 - val_loss: 163.0990\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 80.5402 - val_loss: 176.9893\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 81.4007 - val_loss: 153.5056\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 77.5503 - val_loss: 177.8366\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 76.9536 - val_loss: 153.9275\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 80.8449 - val_loss: 163.5129\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 79.6225 - val_loss: 154.4942\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 79.0753 - val_loss: 160.8214\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 74.7826 - val_loss: 160.9515\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 78.4237 - val_loss: 166.5422\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 76.3433 - val_loss: 176.0965\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 84.2055 - val_loss: 168.2397\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 79.2028 - val_loss: 165.3333\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 78.1003 - val_loss: 160.4488\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 76.6223 - val_loss: 164.8450\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 72.4682 - val_loss: 173.0994\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 81.1181 - val_loss: 175.1385\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 76.4645 - val_loss: 166.0923\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 74.3947 - val_loss: 165.5971\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 77.5110 - val_loss: 168.5018\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 73.8721 - val_loss: 154.5012\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 79.2578 - val_loss: 157.4959\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 74.9128 - val_loss: 158.5457\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 72.2742 - val_loss: 164.0486\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 72.1512 - val_loss: 166.7473\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 77.2769 - val_loss: 153.6550\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 77.8939 - val_loss: 163.4189\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 75.5811 - val_loss: 160.9822\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 74.8477 - val_loss: 167.9432\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 76.1899 - val_loss: 157.7118\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 73.9943 - val_loss: 153.1050\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 70.4320 - val_loss: 173.7476\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 74.3289 - val_loss: 157.7989\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 74.1778 - val_loss: 152.8287\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 72.9399 - val_loss: 181.4281\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 74.2732 - val_loss: 165.3626\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 71.0199 - val_loss: 169.7855\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 72.0182 - val_loss: 175.8815\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 69.8265 - val_loss: 166.1906\n",
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 72.4430 - val_loss: 157.0141\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 71.6846 - val_loss: 161.8391\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 69.4871 - val_loss: 171.6544\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 68.8038 - val_loss: 157.3643\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 72.1947 - val_loss: 172.4152\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 71.3147 - val_loss: 194.4632\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 74.2773 - val_loss: 170.5581\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 67.8141 - val_loss: 161.0512\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 68.1790 - val_loss: 170.9161\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 70.7999 - val_loss: 170.8155\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 73.7398 - val_loss: 160.6600\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 70.3481 - val_loss: 165.2889\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 66.3203 - val_loss: 173.0314\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 71.6940 - val_loss: 208.5799\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 68.6360 - val_loss: 167.0998\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 67.6925 - val_loss: 181.9291\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 68.2126 - val_loss: 154.1133\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 66.2941 - val_loss: 167.9704\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 64.6042 - val_loss: 170.0508\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 70.5253 - val_loss: 163.3599\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 66.9105 - val_loss: 196.0449\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 67.7719 - val_loss: 161.4111\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 67.1129 - val_loss: 188.7556\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 66.5160 - val_loss: 159.0654\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 67.5156 - val_loss: 165.9751\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 65.3454 - val_loss: 159.5117\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 67.0064 - val_loss: 166.8751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 66.1780 - val_loss: 168.9203\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 63.4500 - val_loss: 171.1503\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 69.2554 - val_loss: 189.0572\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 66.4948 - val_loss: 159.1250\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 66.3863 - val_loss: 174.3072\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 67.4926 - val_loss: 164.4809\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 63.0638 - val_loss: 170.1499\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 68.8426 - val_loss: 166.4802\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 65.6697 - val_loss: 172.3653\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 67.6107 - val_loss: 202.0194\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 62.4902 - val_loss: 176.9227\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 66.1567 - val_loss: 178.5377\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 68.6002 - val_loss: 180.5037\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 62.7987 - val_loss: 165.4961\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 66.3679 - val_loss: 201.5526\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 67.8455 - val_loss: 191.3168\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 63.4809 - val_loss: 164.4578\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 65.4581 - val_loss: 166.3208\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 62.3425 - val_loss: 178.6303\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 61.1965 - val_loss: 166.3138\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 66.1975 - val_loss: 170.2259\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 67.6254 - val_loss: 166.1413\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 64.6111 - val_loss: 167.8729\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 65.7909 - val_loss: 167.9932\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 58.7707 - val_loss: 172.8859\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 61.4910 - val_loss: 202.6901\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 63.2141 - val_loss: 159.7353\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 65.0646 - val_loss: 190.5335\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 63.0555 - val_loss: 160.5381\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 63.7857 - val_loss: 171.7512\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 65.0519 - val_loss: 173.4546\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 63.5809 - val_loss: 160.5435\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 64.6727 - val_loss: 172.7747\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 61.2380 - val_loss: 166.1878\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 59.6092 - val_loss: 164.5233\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 65.5575 - val_loss: 167.0419\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 61.0922 - val_loss: 169.7573\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 61.0234 - val_loss: 182.1364\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 59.3101 - val_loss: 165.6576\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 59.3088 - val_loss: 187.8116\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 62.6806 - val_loss: 173.2888\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 59.8815 - val_loss: 176.6985\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 62.2240 - val_loss: 171.9157\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 61.0925 - val_loss: 196.3303\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 60.1702 - val_loss: 166.5741\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 60.3122 - val_loss: 161.1682\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 58.4792 - val_loss: 160.6041\n",
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 55.4844 - val_loss: 166.1616\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 57.1245 - val_loss: 179.1891\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 62.1123 - val_loss: 179.2132\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 58.2885 - val_loss: 203.9397\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 58.4597 - val_loss: 177.8121\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 61.1746 - val_loss: 172.1857\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 57.1086 - val_loss: 177.0746\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 62.3839 - val_loss: 205.4902\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 62.9489 - val_loss: 171.4995\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 58.4417 - val_loss: 175.9394\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 56.8169 - val_loss: 170.6418\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 57.2293 - val_loss: 181.4982\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 58.8922 - val_loss: 170.9283\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 57.1795 - val_loss: 176.7435\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 59.1169 - val_loss: 176.5007\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 59.3917 - val_loss: 172.0454\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 60.6240 - val_loss: 180.7042\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 57.8154 - val_loss: 182.8053\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 57.8996 - val_loss: 185.2747\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 60.0193 - val_loss: 180.9030\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 55.7553 - val_loss: 185.2226\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 58.0479 - val_loss: 176.2098\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 56.4872 - val_loss: 189.3817\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 56.8221 - val_loss: 177.3956\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 57.5570 - val_loss: 169.0188\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 53.2021 - val_loss: 181.0529\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 58.2613 - val_loss: 173.3475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 54.8327 - val_loss: 176.8676\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 58.9492 - val_loss: 170.9433\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 57.8295 - val_loss: 177.7679\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 61.0559 - val_loss: 178.6004\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 55.2171 - val_loss: 215.4907\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 52.8023 - val_loss: 173.6406\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 56.3517 - val_loss: 173.1698\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 58.6758 - val_loss: 176.8963\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 51.3824 - val_loss: 182.4796\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 59.4664 - val_loss: 202.1240\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 57.1394 - val_loss: 182.3109\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 56.3149 - val_loss: 176.5054\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 55.0874 - val_loss: 176.7832\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 56.3682 - val_loss: 182.0050\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 59.0694 - val_loss: 179.7705\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 56.3794 - val_loss: 174.6699\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 53.9452 - val_loss: 180.8175\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 56.8400 - val_loss: 173.1600\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 55.4566 - val_loss: 183.2779\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 56.6519 - val_loss: 162.2761\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 56.2835 - val_loss: 173.2835\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 52.0756 - val_loss: 176.5804\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 52.6687 - val_loss: 188.0105\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 52.8202 - val_loss: 183.4498\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 53.9537 - val_loss: 180.4799\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 54.1516 - val_loss: 180.2916\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 56.1327 - val_loss: 166.4254\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 58.2046 - val_loss: 176.9096\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 57.3305 - val_loss: 186.1222\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 52.8751 - val_loss: 186.2186\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 57.1334 - val_loss: 182.2005\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 54.2945 - val_loss: 167.7176\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 55.4736 - val_loss: 174.3875\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 51.3353 - val_loss: 170.2289\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 47.9277 - val_loss: 182.6024\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 51.7644 - val_loss: 176.1057\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 53.4838 - val_loss: 165.6471\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 55.3761 - val_loss: 178.7104\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 56.8055 - val_loss: 177.9497\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 55.3554 - val_loss: 177.2748\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 54.6599 - val_loss: 179.1884\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 56.6191 - val_loss: 172.9743\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 49.4700 - val_loss: 172.4909\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 53.7828 - val_loss: 182.7127\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 56.3177 - val_loss: 176.4920\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 54.1897 - val_loss: 173.8600\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 55.3197 - val_loss: 185.4340\n",
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 52.1884 - val_loss: 176.0169\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 51.9462 - val_loss: 185.7752\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 54.0806 - val_loss: 179.5373\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 53.1280 - val_loss: 189.2100\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 55.8102 - val_loss: 178.8444\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 53.1879 - val_loss: 171.5492\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 58.5195 - val_loss: 179.0192\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 54.1305 - val_loss: 173.0288\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 50.1633 - val_loss: 182.6555\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 50.7166 - val_loss: 173.6571\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 50.4864 - val_loss: 172.8856\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 50.6674 - val_loss: 169.5843\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 51.4054 - val_loss: 183.8321\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 52.9291 - val_loss: 167.3411\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 52.8845 - val_loss: 177.3516\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 51.1685 - val_loss: 175.2210\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 56.2056 - val_loss: 170.3712\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 49.8865 - val_loss: 168.6527\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 51.9127 - val_loss: 186.0604\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 51.4398 - val_loss: 181.5377\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 54.6950 - val_loss: 176.5726\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 51.6829 - val_loss: 171.1935\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 50.1754 - val_loss: 196.7793\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 53.8267 - val_loss: 165.1972\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 51.1787 - val_loss: 182.1853\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 49.4431 - val_loss: 196.3343\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 48.4920 - val_loss: 177.4078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 48.9922 - val_loss: 181.0839\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 51.3806 - val_loss: 198.8992\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 52.7938 - val_loss: 186.5443\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 53.8831 - val_loss: 189.2914\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 54.1748 - val_loss: 170.1724\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 53.1347 - val_loss: 189.4910\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 51.5458 - val_loss: 180.4995\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 46.5330 - val_loss: 184.5834\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 50.7750 - val_loss: 178.8791\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 49.2328 - val_loss: 177.6728\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 49.9653 - val_loss: 203.7451\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 51.6725 - val_loss: 177.5307\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 50.6179 - val_loss: 190.3664\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 55.1130 - val_loss: 173.6925\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 49.9511 - val_loss: 173.8611\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 53.3515 - val_loss: 188.7335\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 48.1275 - val_loss: 186.0700\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 47.6625 - val_loss: 182.1072\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 49.2136 - val_loss: 199.2375\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 54.2621 - val_loss: 176.3254\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 50.0708 - val_loss: 187.0401\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 49.6231 - val_loss: 185.8224\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 47.9790 - val_loss: 171.1234\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 47.6999 - val_loss: 172.2287\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 50.0096 - val_loss: 181.2745\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 48.6706 - val_loss: 180.0832\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 52.7180 - val_loss: 171.6376\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 56.3955 - val_loss: 174.4609\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 46.9088 - val_loss: 170.1446\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 46.6805 - val_loss: 193.4893\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 49.3797 - val_loss: 201.4503\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 47.8813 - val_loss: 191.9801\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 50.7194 - val_loss: 188.8319\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 51.6312 - val_loss: 193.0551\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 47.0023 - val_loss: 189.5904\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 46.7776 - val_loss: 179.8107\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 49.7384 - val_loss: 184.3439\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 49.2635 - val_loss: 191.0655\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 46.4549 - val_loss: 176.9423\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 48.2925 - val_loss: 176.0200\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 51.3395 - val_loss: 190.6818\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 47.7494 - val_loss: 193.4237\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 47.7633 - val_loss: 183.6632\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 46.9835 - val_loss: 181.6579\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 51.4576 - val_loss: 185.4017\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 46.5779 - val_loss: 182.8485\n",
      "Epoch 464/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 45.7953 - val_loss: 196.0122\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 47.6297 - val_loss: 182.7515\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 44.3775 - val_loss: 190.5472\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 45.6886 - val_loss: 192.9366\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 46.8777 - val_loss: 189.5912\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 50.7095 - val_loss: 184.7471\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 47.8819 - val_loss: 202.6013\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 46.4056 - val_loss: 218.0573\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 49.8442 - val_loss: 180.1821\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 45.2084 - val_loss: 188.3636\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 47.0991 - val_loss: 188.2306\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 50.8417 - val_loss: 183.0766\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 49.3950 - val_loss: 183.3156\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 45.2312 - val_loss: 173.3900\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 46.6279 - val_loss: 196.3321\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 48.8006 - val_loss: 191.6590\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 48.0345 - val_loss: 183.7973\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 45.9753 - val_loss: 191.4681\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 46.5025 - val_loss: 173.3904\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 50.4719 - val_loss: 191.3917\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 44.9205 - val_loss: 183.5173\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 48.1126 - val_loss: 179.8650\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 44.8881 - val_loss: 177.1437\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 46.5531 - val_loss: 181.4234\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 46.3325 - val_loss: 181.3009\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 44.6408 - val_loss: 200.7755\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 47.2257 - val_loss: 188.6464\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 46.6639 - val_loss: 188.3488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 50.3368 - val_loss: 171.6421\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 43.7408 - val_loss: 185.7687\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 46.5098 - val_loss: 174.2188\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 46.1655 - val_loss: 183.5736\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 49.1736 - val_loss: 186.0737\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 47.9207 - val_loss: 173.7128\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 49.1841 - val_loss: 183.5703\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 47.3656 - val_loss: 180.8328\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 48.6121 - val_loss: 184.5032\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 46.5573 - val_loss: 200.2049\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 44.0775 - val_loss: 176.1297\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 49.6203 - val_loss: 179.0759\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 47.4057 - val_loss: 173.4296\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 44.1083 - val_loss: 185.7351\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 42.5963 - val_loss: 185.0038\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 42.5176 - val_loss: 187.2801\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 44.8505 - val_loss: 171.2004\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 48.9291 - val_loss: 188.9210\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 44.5458 - val_loss: 183.4246\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 41.0856 - val_loss: 184.0321\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 47.3886 - val_loss: 194.4240\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 46.0947 - val_loss: 195.8432\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 45.2828 - val_loss: 172.2503\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 43.9942 - val_loss: 183.6794\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 45.5538 - val_loss: 192.8911\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 46.7306 - val_loss: 184.9809\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 42.5122 - val_loss: 171.2760\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 44.5431 - val_loss: 170.2298\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 47.6373 - val_loss: 179.6356\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 43.6995 - val_loss: 193.2748\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 44.1049 - val_loss: 173.6625\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 46.0304 - val_loss: 190.2664\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 44.8554 - val_loss: 179.7933\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 47.4888 - val_loss: 179.1570\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 44.0446 - val_loss: 178.7651\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 42.9455 - val_loss: 196.2694\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 42.3714 - val_loss: 190.4998\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 45.1939 - val_loss: 177.9736\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 46.0266 - val_loss: 191.6040\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 43.7438 - val_loss: 184.0194\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 45.1414 - val_loss: 193.5066\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 43.4799 - val_loss: 166.5015\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 46.6483 - val_loss: 177.0667\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 42.9817 - val_loss: 185.1734\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 44.3471 - val_loss: 185.5704\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 42.7882 - val_loss: 189.1540\n",
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 42.9963 - val_loss: 182.6546\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 44.8522 - val_loss: 190.0124\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 43.8562 - val_loss: 177.9502\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 42.8076 - val_loss: 180.6215\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 43.4129 - val_loss: 188.7213\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 41.2726 - val_loss: 174.1090\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 45.7539 - val_loss: 189.5710\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 41.5825 - val_loss: 184.4631\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 42.4085 - val_loss: 185.9747\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 45.7141 - val_loss: 194.8213\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 42.8120 - val_loss: 190.2920\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 45.9393 - val_loss: 190.2122\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 40.1682 - val_loss: 188.0743\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 45.1190 - val_loss: 185.1498\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 40.8972 - val_loss: 181.9101\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 45.7861 - val_loss: 183.8074\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 50.2959 - val_loss: 178.6644\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 40.1007 - val_loss: 179.6505\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 42.9609 - val_loss: 179.0749\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 46.2006 - val_loss: 176.8378\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 43.1038 - val_loss: 182.3375\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 43.5700 - val_loss: 202.7983\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 41.1618 - val_loss: 176.8142\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 40.7735 - val_loss: 194.5347\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 41.8649 - val_loss: 191.3019\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 43.7749 - val_loss: 179.6214\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 42.9765 - val_loss: 189.5619\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 41.9517 - val_loss: 185.4879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 45.4002 - val_loss: 178.5693\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 45.2598 - val_loss: 189.2222\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 43.0809 - val_loss: 181.1355\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 42.5182 - val_loss: 174.3523\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 44.5755 - val_loss: 197.4140\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 40.8919 - val_loss: 181.8381\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 44.1497 - val_loss: 192.9939\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 44.6356 - val_loss: 176.5013\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 46.2469 - val_loss: 176.5912\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 42.5492 - val_loss: 177.7469\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 44.9116 - val_loss: 178.1004\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 41.2872 - val_loss: 184.2975\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 41.9813 - val_loss: 183.1749\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 40.3829 - val_loss: 177.0175\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 42.3700 - val_loss: 185.5520\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 44.3728 - val_loss: 192.0181\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 45.4798 - val_loss: 184.2339\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 44.7696 - val_loss: 175.8444\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 44.8435 - val_loss: 184.3503\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 42.2375 - val_loss: 196.0488\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 44.6874 - val_loss: 217.6875\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 41.0396 - val_loss: 186.6652\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 40.5234 - val_loss: 186.1474\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 39.6077 - val_loss: 180.0710\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 38.8806 - val_loss: 187.7317\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 40.8380 - val_loss: 198.8733\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 40.8718 - val_loss: 173.8972\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 46.7309 - val_loss: 181.2732\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 41.0154 - val_loss: 177.2771\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 43.1366 - val_loss: 184.3223\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 40.4839 - val_loss: 178.1673\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 40.6838 - val_loss: 185.4055\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 41.9544 - val_loss: 185.6578\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 42.5594 - val_loss: 177.9075\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 44.2425 - val_loss: 182.9600\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 44.2044 - val_loss: 184.3410\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 44.7915 - val_loss: 184.5062\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 42.0711 - val_loss: 185.6801\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 39.7395 - val_loss: 195.0632\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 41.7479 - val_loss: 192.4155\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 46.0774 - val_loss: 189.7483\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 44.8314 - val_loss: 183.4345\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 39.5632 - val_loss: 201.4501\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 42.9966 - val_loss: 188.4699\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 39.9032 - val_loss: 192.1437\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 40.9157 - val_loss: 182.0778\n",
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 38.1732 - val_loss: 183.5925\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 40.4464 - val_loss: 188.3143\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 45.0830 - val_loss: 183.4264\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 41.7616 - val_loss: 174.3037\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 42.2148 - val_loss: 192.5698\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 40.1138 - val_loss: 189.3619\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 41.6454 - val_loss: 208.0208\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 41.6027 - val_loss: 176.8359\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 40.1049 - val_loss: 189.9946\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 41.9384 - val_loss: 185.0437\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 38.8594 - val_loss: 195.2803\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 40.1944 - val_loss: 189.3949\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 40.9113 - val_loss: 185.2411\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 45.3567 - val_loss: 194.5956\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 45.0563 - val_loss: 203.5000\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 40.0806 - val_loss: 201.7973\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 40.5709 - val_loss: 187.1240\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 41.6156 - val_loss: 184.5361\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 40.0621 - val_loss: 195.0438\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 43.2587 - val_loss: 189.6271\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 42.7451 - val_loss: 196.1129\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 39.4471 - val_loss: 183.3528\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 38.8589 - val_loss: 186.3624\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 41.2534 - val_loss: 190.8018\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 39.9379 - val_loss: 192.4891\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 40.1729 - val_loss: 183.5924\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 40.7890 - val_loss: 184.9329\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 40.5688 - val_loss: 191.1534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 38.6442 - val_loss: 180.2666\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 39.8598 - val_loss: 174.9478\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 36.6829 - val_loss: 178.1768\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 41.4901 - val_loss: 190.7174\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 40.9315 - val_loss: 185.8522\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 40.2873 - val_loss: 183.4831\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 39.3326 - val_loss: 189.9717\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 42.1438 - val_loss: 188.4225\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 38.9140 - val_loss: 173.8393\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 41.2443 - val_loss: 194.9583\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 38.4635 - val_loss: 180.5237\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 47.2478 - val_loss: 185.6599\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 38.9414 - val_loss: 187.1922\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 40.3340 - val_loss: 180.4732\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 39.8089 - val_loss: 177.9534\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 37.9898 - val_loss: 176.2174\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 43.6034 - val_loss: 179.1963\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 39.9918 - val_loss: 175.6936\n",
      "Epoch 658/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 40.9389 - val_loss: 182.0732\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 40.3169 - val_loss: 190.2429\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 43.3588 - val_loss: 176.1974\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.4393 - val_loss: 179.0600\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 42.6934 - val_loss: 178.4330\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 40.3850 - val_loss: 186.3385\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 38.9737 - val_loss: 180.6582\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 42.9114 - val_loss: 181.6650\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 38.4726 - val_loss: 187.6377\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 38.2340 - val_loss: 196.7871\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 39.1064 - val_loss: 185.2481\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 39.3465 - val_loss: 179.5835\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 37.3443 - val_loss: 194.1512\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 37.8368 - val_loss: 175.5195\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 40.3903 - val_loss: 196.5881\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 38.0306 - val_loss: 184.0062\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 38.5315 - val_loss: 185.8262\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 38.6041 - val_loss: 191.6576\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 38.0987 - val_loss: 172.0654\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 37.9388 - val_loss: 193.8428\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 40.6670 - val_loss: 182.7850\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 39.3168 - val_loss: 178.0456\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 40.1614 - val_loss: 189.0867\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 38.4476 - val_loss: 189.0849\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 42.0587 - val_loss: 190.6728\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 37.7895 - val_loss: 175.2838\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 37.0437 - val_loss: 193.3670\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 40.2642 - val_loss: 178.6946\n",
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 40.6004 - val_loss: 182.7749\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 39.0467 - val_loss: 181.9941\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 41.5721 - val_loss: 187.8992\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 39.5463 - val_loss: 177.8957\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 37.2283 - val_loss: 197.7088\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 37.1895 - val_loss: 190.0772\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 42.7623 - val_loss: 181.9112\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 41.9492 - val_loss: 180.0969\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 38.9443 - val_loss: 184.6604\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.7582 - val_loss: 182.2943\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.0401 - val_loss: 180.0781\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 41.0284 - val_loss: 178.7815\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 36.8715 - val_loss: 180.9146\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 39.0836 - val_loss: 188.0740\n",
      "Epoch 700/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 40.3466 - val_loss: 183.9838\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 37.0989 - val_loss: 173.9639\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 42.1169 - val_loss: 177.1957\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 40.3364 - val_loss: 187.5872\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 39.1779 - val_loss: 177.8836\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 35.1864 - val_loss: 188.3734\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 35.6968 - val_loss: 184.8528\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 36.8253 - val_loss: 191.5952\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 40.8223 - val_loss: 195.4932\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 39.9613 - val_loss: 185.2816\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 1s 143us/step - loss: 41.9013 - val_loss: 176.6588\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 36.3776 - val_loss: 189.4245\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 43.0199 - val_loss: 206.2842\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 39.3799 - val_loss: 184.6271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 39.5807 - val_loss: 194.3429\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 39.1334 - val_loss: 190.7375\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.1829 - val_loss: 183.5593\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.3094 - val_loss: 187.9556\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 36.1282 - val_loss: 181.1928\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 39.0100 - val_loss: 181.0369\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 39.2869 - val_loss: 182.0136\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 37.8400 - val_loss: 180.9590\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 37.7507 - val_loss: 176.7501\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.0905 - val_loss: 186.7399\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.1190 - val_loss: 185.2148\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 38.6227 - val_loss: 194.1849\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 41.9560 - val_loss: 197.9687\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 39.4306 - val_loss: 190.4908\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 37.0535 - val_loss: 181.5470\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 36.4462 - val_loss: 183.0696\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 37.2632 - val_loss: 184.1797\n",
      "Epoch 731/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 39.0059 - val_loss: 189.8171\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 37.1925 - val_loss: 182.3185\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 39.4087 - val_loss: 184.9636\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 39.8596 - val_loss: 187.7858\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 37.6070 - val_loss: 186.6823\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 37.2547 - val_loss: 197.2620\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 39.7350 - val_loss: 175.2667\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 38.5889 - val_loss: 176.5307\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 37.95 - 1s 144us/step - loss: 37.5979 - val_loss: 197.5753\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 34.3802 - val_loss: 184.0949\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 38.7266 - val_loss: 189.9158\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 37.0675 - val_loss: 191.9458\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 37.7273 - val_loss: 176.7904\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 1s 129us/step - loss: 37.3315 - val_loss: 192.4839\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 37.0762 - val_loss: 187.4273\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 37.6367 - val_loss: 181.6426\n",
      "Epoch 747/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 39.5067 - val_loss: 188.1167\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 38.2550 - val_loss: 185.9141\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 39.5330 - val_loss: 199.9829\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 39.0958 - val_loss: 198.7928\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 35.0813 - val_loss: 188.8030\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 1s 109us/step - loss: 39.1115 - val_loss: 181.5946\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 36.9283 - val_loss: 182.2480\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 39.6652 - val_loss: 183.9843\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 39.4772 - val_loss: 188.4276\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 37.1989 - val_loss: 198.7463\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 35.5879 - val_loss: 179.1877\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 38.3633 - val_loss: 186.5375\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 39.0212 - val_loss: 180.1461\n",
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 37.3494 - val_loss: 186.9973\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 32.4136 - val_loss: 189.0242\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 36.9594 - val_loss: 174.1234\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 36.8794 - val_loss: 192.0704\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 38.2737 - val_loss: 184.9933\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 39.6687 - val_loss: 188.6623\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 38.1027 - val_loss: 184.5591\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 37.7474 - val_loss: 179.3989\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 35.6978 - val_loss: 180.3247\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 37.3000 - val_loss: 188.1630\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 37.6385 - val_loss: 194.0115\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 37.9281 - val_loss: 175.7240\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 37.9120 - val_loss: 182.9446\n",
      "Epoch 773/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 40.3604 - val_loss: 189.0052\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 39.2180 - val_loss: 190.2732\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 34.0488 - val_loss: 200.3573\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 36.5976 - val_loss: 194.3461\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 38.3984 - val_loss: 182.6654\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 34.9319 - val_loss: 175.9222\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 33.9468 - val_loss: 175.8788\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 37.1713 - val_loss: 188.1161\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 37.7430 - val_loss: 189.6151\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 38.0368 - val_loss: 189.6388\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 36.6901 - val_loss: 195.9402\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 35.3961 - val_loss: 184.4001\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 35.4041 - val_loss: 186.1206\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 35.5046 - val_loss: 178.3075\n",
      "Epoch 787/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 83us/step - loss: 38.9171 - val_loss: 187.2981\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 38.7738 - val_loss: 196.7825\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 39.4096 - val_loss: 189.6099\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 37.0774 - val_loss: 187.8669\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 35.9359 - val_loss: 179.4950\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 35.2050 - val_loss: 188.8538\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 37.0708 - val_loss: 179.8639\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 36.6374 - val_loss: 187.6879\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 37.3119 - val_loss: 180.4680\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 36.0291 - val_loss: 190.6681\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 40.4521 - val_loss: 187.1205\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 38.0987 - val_loss: 189.6317\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 34.3338 - val_loss: 185.8638\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 44.8833 - val_loss: 180.2415\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 35.9009 - val_loss: 180.6201\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 36.4063 - val_loss: 187.3228\n",
      "Epoch 803/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 38.0664 - val_loss: 182.9882\n",
      "Epoch 804/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 35.6846 - val_loss: 184.6418\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 35.3795 - val_loss: 194.5333\n",
      "Epoch 806/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 34.3867 - val_loss: 188.7094\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.9828 - val_loss: 184.7584\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.4747 - val_loss: 185.6457\n",
      "Epoch 809/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 37.0540 - val_loss: 187.9765\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 36.0043 - val_loss: 189.1528\n",
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 35.0005 - val_loss: 190.6569\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 36.9143 - val_loss: 188.4258\n",
      "Epoch 813/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.4858 - val_loss: 183.6134\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 35.6799 - val_loss: 179.8015\n",
      "Epoch 815/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 37.5153 - val_loss: 192.0130\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.1890 - val_loss: 185.8519\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 32.9114 - val_loss: 189.4103\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 38.0334 - val_loss: 184.9300\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 38.1487 - val_loss: 186.3979\n",
      "Epoch 820/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 36.8113 - val_loss: 195.9824\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 34.0085 - val_loss: 190.8012\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 34.3921 - val_loss: 177.8391\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 35.8923 - val_loss: 184.0990\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 38.4917 - val_loss: 192.5401\n",
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 37.4407 - val_loss: 195.5332\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 38.4788 - val_loss: 182.3081\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 38.5299 - val_loss: 176.6822\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 35.9653 - val_loss: 181.9369\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.1493 - val_loss: 180.1791\n",
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 35.9224 - val_loss: 173.8319\n",
      "Epoch 831/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 36.4791 - val_loss: 182.3715\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 33.7094 - val_loss: 186.8396\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 36.9635 - val_loss: 185.5475\n",
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 36.3909 - val_loss: 184.4133\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 34.2484 - val_loss: 182.4912\n",
      "Epoch 836/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 33.0696 - val_loss: 179.5201\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 37.0541 - val_loss: 193.4388\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 34.9983 - val_loss: 179.7518\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 35.0074 - val_loss: 176.7750\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 35.9625 - val_loss: 183.8599\n",
      "Epoch 841/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 36.2347 - val_loss: 190.6692\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 41.2055 - val_loss: 183.1373\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 36.2124 - val_loss: 180.4744\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.2434 - val_loss: 187.3494\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 34.2972 - val_loss: 188.6119\n",
      "Epoch 846/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 35.1992 - val_loss: 188.4379\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 34.0482 - val_loss: 196.4125\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 33.7460 - val_loss: 190.3450\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 37.6112 - val_loss: 176.3889\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 35.60 - 1s 63us/step - loss: 35.3341 - val_loss: 197.7259\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 37.5634 - val_loss: 193.3422\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 34.8815 - val_loss: 185.2836\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 35.7019 - val_loss: 184.8449\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 32.1212 - val_loss: 185.0673\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 33.9145 - val_loss: 195.3511\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 35.54 - 0s 57us/step - loss: 36.4361 - val_loss: 198.8509\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 37.8095 - val_loss: 186.9426\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 33.4147 - val_loss: 182.7997\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 43.7642 - val_loss: 179.2217\n",
      "Epoch 860/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 56us/step - loss: 37.5992 - val_loss: 178.4788\n",
      "Epoch 861/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 34.8674 - val_loss: 187.3835\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.6956 - val_loss: 181.0806\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 35.8469 - val_loss: 187.3025\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 37.2629 - val_loss: 188.4337\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 36.3407 - val_loss: 192.1703\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 34.4383 - val_loss: 196.5339\n",
      "Epoch 867/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 33.6615 - val_loss: 188.0291\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 36.3414 - val_loss: 186.9394\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 33.8655 - val_loss: 180.5031\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 38.7065 - val_loss: 177.4179\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 34.4588 - val_loss: 186.0723\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 33.7717 - val_loss: 179.8703\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 35.66 - 0s 57us/step - loss: 35.6308 - val_loss: 188.8258\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 36.1269 - val_loss: 184.9124\n",
      "Epoch 875/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 35.9782 - val_loss: 188.3019\n",
      "Epoch 876/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 37.1435 - val_loss: 185.7653\n",
      "Epoch 877/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.6444 - val_loss: 184.1547\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 33.6628 - val_loss: 190.9333\n",
      "Epoch 879/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 40.4052 - val_loss: 192.9043\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 34.9692 - val_loss: 188.3661\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 33.2846 - val_loss: 191.5790\n",
      "Epoch 882/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 35.3099 - val_loss: 189.1971\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 35.4505 - val_loss: 185.7318\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 35.4797 - val_loss: 191.1470\n",
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 35.0764 - val_loss: 186.7113\n",
      "Epoch 886/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 37.0270 - val_loss: 187.5227\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 37.1071 - val_loss: 183.1202\n",
      "Epoch 888/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 33.6235 - val_loss: 192.9998\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 35.1024 - val_loss: 179.0130\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 34.3605 - val_loss: 189.8466\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 33.8013 - val_loss: 176.4054\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 32.6794 - val_loss: 181.8303\n",
      "Epoch 893/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 38.6849 - val_loss: 187.2361\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 34.6738 - val_loss: 185.3922\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 34.9630 - val_loss: 177.0917\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 34.2222 - val_loss: 188.2210\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 34.2652 - val_loss: 179.3859\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 36.2618 - val_loss: 205.3110\n",
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 36.6365 - val_loss: 179.8703\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 36.9318 - val_loss: 176.4905\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 35.6039 - val_loss: 178.8250\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 34.2180 - val_loss: 182.3520\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 34.4112 - val_loss: 188.4579\n",
      "Epoch 904/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 34.3446 - val_loss: 183.5526\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 34.0420 - val_loss: 189.5111\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 35.2141 - val_loss: 188.1992\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 35.9839 - val_loss: 179.2940\n",
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 33.5409 - val_loss: 196.2805\n",
      "Epoch 909/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 35.6372 - val_loss: 182.0953\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 34.7189 - val_loss: 182.5155\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 35.5370 - val_loss: 183.7768\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 36.4479 - val_loss: 184.3431\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 31.8981 - val_loss: 181.0772\n",
      "Epoch 914/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 34.5399 - val_loss: 190.9204\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 41.0041 - val_loss: 185.2474\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 32.1130 - val_loss: 187.1393\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.1119 - val_loss: 184.1349\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 32.8736 - val_loss: 184.9737\n",
      "Epoch 919/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 32.7952 - val_loss: 188.2384\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 36.3293 - val_loss: 184.7828\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 34.3205 - val_loss: 184.3706\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 34.0501 - val_loss: 184.8703\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 37.7158 - val_loss: 174.5051\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 32.7762 - val_loss: 190.0146\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 32.4384 - val_loss: 196.4006\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 32.5299 - val_loss: 185.6875\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.1762 - val_loss: 179.1867\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 33.3140 - val_loss: 184.3937\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 32.3591 - val_loss: 187.2220\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 33.6147 - val_loss: 181.8308\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 33.7302 - val_loss: 183.0635\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 33.0313 - val_loss: 185.4063\n",
      "Epoch 933/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 64us/step - loss: 38.5063 - val_loss: 175.7511\n",
      "Epoch 934/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.1632 - val_loss: 187.8730\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.0309 - val_loss: 193.1075\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 35.5884 - val_loss: 187.9777\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 32.3254 - val_loss: 186.4192\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 32.0738 - val_loss: 182.7709\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 36.1863 - val_loss: 187.9785\n",
      "Epoch 940/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 30.6213 - val_loss: 181.6409\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.8771 - val_loss: 183.0247\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 33.5994 - val_loss: 191.4659\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 34.2028 - val_loss: 194.9880\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 37.0316 - val_loss: 180.2585\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 36.0243 - val_loss: 185.4313\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.9518 - val_loss: 184.0798\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.3517 - val_loss: 187.1020\n",
      "Epoch 948/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 33.0941 - val_loss: 186.3518\n",
      "Epoch 949/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 33.4291 - val_loss: 183.9846\n",
      "Epoch 950/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.6361 - val_loss: 186.9084\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.8657 - val_loss: 189.4000\n",
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 37.1532 - val_loss: 184.1092\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.1591 - val_loss: 187.8079\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.5682 - val_loss: 186.7097\n",
      "Epoch 955/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 32.7717 - val_loss: 181.9840\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 32.4332 - val_loss: 181.4480\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 33.2541 - val_loss: 179.7718\n",
      "Epoch 958/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 32.9405 - val_loss: 181.0293\n",
      "Epoch 959/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 34.4806 - val_loss: 179.3498\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.8948 - val_loss: 181.4326\n",
      "Epoch 961/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.8997 - val_loss: 178.7659\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 36.9851 - val_loss: 178.9782\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 33.0062 - val_loss: 189.2732\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.1323 - val_loss: 187.0714\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 34.5824 - val_loss: 190.0415\n",
      "Epoch 966/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 30.8016 - val_loss: 180.8471\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.2696 - val_loss: 182.0607\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 36.5636 - val_loss: 177.3189\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 32.6268 - val_loss: 184.5134\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 35.9860 - val_loss: 179.6953\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 34.3318 - val_loss: 174.6203\n",
      "Epoch 972/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 33.0939 - val_loss: 177.1141\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 34.4610 - val_loss: 177.2866\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 32.3746 - val_loss: 174.6509\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 33.7555 - val_loss: 187.8135\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 33.7792 - val_loss: 181.0697\n",
      "Epoch 977/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 34.1019 - val_loss: 180.5811\n",
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 31.1639 - val_loss: 174.1662\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 34.6143 - val_loss: 181.0080\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.4264 - val_loss: 178.1221\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 31.9456 - val_loss: 189.8251\n",
      "Epoch 982/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 36.7480 - val_loss: 177.0648\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.0934 - val_loss: 178.6363\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 31.9969 - val_loss: 185.7718\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 31.6132 - val_loss: 181.2374\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.9186 - val_loss: 172.6888\n",
      "Epoch 987/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 31.3833 - val_loss: 183.5296\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 38.0586 - val_loss: 182.2363\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.3025 - val_loss: 185.8900\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 32.6843 - val_loss: 174.8068\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 33.8195 - val_loss: 178.2636\n",
      "Epoch 992/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 33.2001 - val_loss: 179.9231\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.0747 - val_loss: 179.7318\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 33.1957 - val_loss: 188.8951\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 33.7136 - val_loss: 175.0726\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 33.1134 - val_loss: 181.9162\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 34.2742 - val_loss: 186.2399\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 32.5947 - val_loss: 186.6163\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 33.2081 - val_loss: 186.1961\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 34.2000 - val_loss: 197.8167\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.8659 - val_loss: 176.0994\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 32.7712 - val_loss: 182.6441\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 33.0258 - val_loss: 182.1605\n",
      "Epoch 1004/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 32.4395 - val_loss: 177.1095\n",
      "Epoch 1005/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 30.7820 - val_loss: 188.1761\n",
      "Epoch 1006/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 36.2631 - val_loss: 179.2915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1007/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 32.3318 - val_loss: 180.6435\n",
      "Epoch 1008/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 30.0467 - val_loss: 181.6832\n",
      "Epoch 1009/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 33.3646 - val_loss: 186.6344\n",
      "Epoch 1010/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 32.0002 - val_loss: 191.2200\n",
      "Epoch 1011/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 39.5779 - val_loss: 182.8679\n",
      "Epoch 1012/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 32.8408 - val_loss: 188.7925\n",
      "Epoch 1013/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 34.6221 - val_loss: 178.1155\n",
      "Epoch 1014/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 30.4188 - val_loss: 177.9973\n",
      "Epoch 1015/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.4773 - val_loss: 177.5892\n",
      "Epoch 1016/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 31.5772 - val_loss: 197.4239\n",
      "Epoch 1017/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 33.0341 - val_loss: 183.3076\n",
      "Epoch 1018/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 30.1665 - val_loss: 176.5386\n",
      "Epoch 1019/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 34.8800 - val_loss: 176.7180\n",
      "Epoch 1020/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 36.2040 - val_loss: 190.9703\n",
      "Epoch 1021/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 34.6645 - val_loss: 181.9390\n",
      "Epoch 1022/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 32.9898 - val_loss: 173.4467\n",
      "Epoch 1023/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 33.8425 - val_loss: 173.6973\n",
      "Epoch 1024/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 31.3996 - val_loss: 176.3903\n",
      "Epoch 1025/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.8015 - val_loss: 192.4949\n",
      "Epoch 1026/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 30.8907 - val_loss: 179.3695\n",
      "Epoch 1027/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 33.3772 - val_loss: 182.5589\n",
      "Epoch 1028/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 34.4785 - val_loss: 177.0090\n",
      "Epoch 1029/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 34.5697 - val_loss: 179.2441\n",
      "Epoch 1030/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.9788 - val_loss: 180.8726\n",
      "Epoch 1031/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 32.8777 - val_loss: 178.5363\n",
      "Epoch 1032/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 30.8107 - val_loss: 180.8764\n",
      "Epoch 1033/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.8302 - val_loss: 186.2928\n",
      "Epoch 1034/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 33.5340 - val_loss: 183.2799\n",
      "Epoch 1035/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 35.4694 - val_loss: 179.2593\n",
      "Epoch 1036/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 30.6235 - val_loss: 177.6833\n",
      "Epoch 1037/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 34.4830 - val_loss: 180.5357\n",
      "Epoch 1038/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.7260 - val_loss: 177.0261\n",
      "Epoch 1039/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 29.9200 - val_loss: 200.1191\n",
      "Epoch 1040/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 35.2347 - val_loss: 184.9619\n",
      "Epoch 1041/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.1814 - val_loss: 175.8185\n",
      "Epoch 1042/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.7564 - val_loss: 174.3119\n",
      "Epoch 1043/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.5036 - val_loss: 182.8694\n",
      "Epoch 1044/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 30.1031 - val_loss: 179.2710\n",
      "Epoch 1045/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.7827 - val_loss: 191.1466\n",
      "Epoch 1046/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 31.6998 - val_loss: 189.7381\n",
      "Epoch 1047/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 32.7868 - val_loss: 179.8097\n",
      "Epoch 1048/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 34.4974 - val_loss: 171.9461\n",
      "Epoch 1049/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 31.0001 - val_loss: 170.9765\n",
      "Epoch 1050/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 31.45 - 1s 98us/step - loss: 31.2859 - val_loss: 181.3608\n",
      "Epoch 1051/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 32.5700 - val_loss: 178.5637\n",
      "Epoch 1052/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 31.7649 - val_loss: 186.4461\n",
      "Epoch 1053/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 30.8105 - val_loss: 191.6375\n",
      "Epoch 1054/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.4772 - val_loss: 183.8721\n",
      "Epoch 1055/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.2592 - val_loss: 174.9593\n",
      "Epoch 1056/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 34.3582 - val_loss: 172.0036\n",
      "Epoch 1057/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 31.4622 - val_loss: 179.9744\n",
      "Epoch 1058/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 29.7303 - val_loss: 177.6224\n",
      "Epoch 1059/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.6897 - val_loss: 185.6141\n",
      "Epoch 1060/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 31.6205 - val_loss: 180.7096\n",
      "Epoch 1061/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 33.7374 - val_loss: 177.8692\n",
      "Epoch 1062/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 34.6001 - val_loss: 175.0066\n",
      "Epoch 1063/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 30.9696 - val_loss: 184.4667\n",
      "Epoch 1064/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.5079 - val_loss: 177.3235\n",
      "Epoch 1065/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 32.5577 - val_loss: 196.0249\n",
      "Epoch 1066/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 31.4042 - val_loss: 176.4629\n",
      "Epoch 1067/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 28.8545 - val_loss: 177.1801\n",
      "Epoch 1068/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 30.9170 - val_loss: 181.5222\n",
      "Epoch 1069/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.9317 - val_loss: 176.4677\n",
      "Epoch 1070/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.3850 - val_loss: 182.5906\n",
      "Epoch 1071/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 31.9252 - val_loss: 174.6089\n",
      "Epoch 1072/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 31.3780 - val_loss: 177.9243\n",
      "Epoch 1073/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 32.9632 - val_loss: 185.2332\n",
      "Epoch 1074/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 29.9833 - val_loss: 181.2193\n",
      "Epoch 1075/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 31.8292 - val_loss: 179.4583\n",
      "Epoch 1076/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 31.1112 - val_loss: 173.1876\n",
      "Epoch 1077/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 32.6825 - val_loss: 175.7829\n",
      "Epoch 1078/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 30.6312 - val_loss: 178.0416\n",
      "Epoch 1079/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.7371 - val_loss: 186.6596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1080/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 33.8885 - val_loss: 177.6749\n",
      "Epoch 1081/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 32.6403 - val_loss: 188.0219\n",
      "Epoch 1082/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.3260 - val_loss: 175.0579\n",
      "Epoch 1083/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 32.6713 - val_loss: 180.1016\n",
      "Epoch 1084/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.6167 - val_loss: 178.7192\n",
      "Epoch 1085/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 30.9784 - val_loss: 182.7034\n",
      "Epoch 1086/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.7914 - val_loss: 179.8316\n",
      "Epoch 1087/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.6306 - val_loss: 189.8878\n",
      "Epoch 1088/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.9265 - val_loss: 181.5052\n",
      "Epoch 1089/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.3280 - val_loss: 183.0629\n",
      "Epoch 1090/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.4671 - val_loss: 176.1165\n",
      "Epoch 1091/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.6184 - val_loss: 174.3183\n",
      "Epoch 1092/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 31.4725 - val_loss: 195.5946\n",
      "Epoch 1093/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 35.2863 - val_loss: 179.8222\n",
      "Epoch 1094/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 32.7842 - val_loss: 187.8389\n",
      "Epoch 1095/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 31.8723 - val_loss: 181.8500\n",
      "Epoch 1096/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 35.6289 - val_loss: 183.1817\n",
      "Epoch 1097/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.2158 - val_loss: 187.9636\n",
      "Epoch 1098/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 31.1511 - val_loss: 179.8221\n",
      "Epoch 1099/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 32.1163 - val_loss: 189.1381\n",
      "Epoch 1100/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 30.3539 - val_loss: 187.9950\n",
      "Epoch 1101/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 31.8156 - val_loss: 187.4244\n",
      "Epoch 1102/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.4877 - val_loss: 179.9200\n",
      "Epoch 1103/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.6406 - val_loss: 190.4385\n",
      "Epoch 1104/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.8217 - val_loss: 179.1112\n",
      "Epoch 1105/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.9368 - val_loss: 186.4657\n",
      "Epoch 1106/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 34.0892 - val_loss: 185.6689\n",
      "Epoch 1107/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 29.0460 - val_loss: 182.2430\n",
      "Epoch 1108/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 29.1254 - val_loss: 179.6054\n",
      "Epoch 1109/10000\n",
      "8000/8000 [==============================] - 1s 141us/step - loss: 35.4504 - val_loss: 184.8230\n",
      "Epoch 1110/10000\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 36.3342 - val_loss: 178.9657\n",
      "Epoch 1111/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 32.6546 - val_loss: 179.2800\n",
      "Epoch 1112/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 31.0320 - val_loss: 177.2617\n",
      "Epoch 1113/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 31.8124 - val_loss: 185.8550\n",
      "Epoch 1114/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 31.8337 - val_loss: 180.2577\n",
      "Epoch 1115/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 33.8134 - val_loss: 188.2358\n",
      "Epoch 1116/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 30.4393 - val_loss: 191.2929\n",
      "Epoch 1117/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 30.1836 - val_loss: 176.6338\n",
      "Epoch 1118/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 31.8365 - val_loss: 189.4232\n",
      "Epoch 1119/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 30.9970 - val_loss: 190.4302\n",
      "Epoch 1120/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 32.1065 - val_loss: 183.6094\n",
      "Epoch 1121/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 31.1807 - val_loss: 189.0012\n",
      "Epoch 1122/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 32.9349 - val_loss: 178.9119\n",
      "Epoch 1123/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 30.4062 - val_loss: 181.9481\n",
      "Epoch 1124/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 30.4741 - val_loss: 176.5376\n",
      "Epoch 1125/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 32.3790 - val_loss: 181.3797\n",
      "Epoch 1126/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 32.8659 - val_loss: 187.6930\n",
      "Epoch 1127/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 30.8474 - val_loss: 181.9186\n",
      "Epoch 1128/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 31.8096 - val_loss: 188.0038\n",
      "Epoch 1129/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 30.4408 - val_loss: 173.1928\n",
      "Epoch 1130/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 30.0642 - val_loss: 178.8777\n",
      "Epoch 01130: early stopping\n",
      "Fold score (RMSE): 13.244854927062988\n",
      "Fold #5\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10000\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 12878.9317 - val_loss: 1306.5406\n",
      "Epoch 2/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 912.5509 - val_loss: 657.5189\n",
      "Epoch 3/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 563.9307 - val_loss: 417.6703\n",
      "Epoch 4/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 415.0835 - val_loss: 341.9781\n",
      "Epoch 5/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 359.4084 - val_loss: 309.4177\n",
      "Epoch 6/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 335.5715 - val_loss: 302.0002\n",
      "Epoch 7/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 329.1479 - val_loss: 308.2963\n",
      "Epoch 8/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 312.3041 - val_loss: 290.1078\n",
      "Epoch 9/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 302.0193 - val_loss: 276.1831\n",
      "Epoch 10/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 294.6359 - val_loss: 291.1013\n",
      "Epoch 11/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 289.9412 - val_loss: 276.6929\n",
      "Epoch 12/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 288.4988 - val_loss: 290.7097\n",
      "Epoch 13/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 289.0282 - val_loss: 262.3862\n",
      "Epoch 14/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 280.6950 - val_loss: 261.8538\n",
      "Epoch 15/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 276.3267 - val_loss: 258.6204\n",
      "Epoch 16/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 271.7687 - val_loss: 262.2718\n",
      "Epoch 17/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 267.0425 - val_loss: 256.8384\n",
      "Epoch 18/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 268.5683 - val_loss: 250.1473\n",
      "Epoch 19/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 265.5730 - val_loss: 246.2769\n",
      "Epoch 20/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 261.6699 - val_loss: 279.9520\n",
      "Epoch 21/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 255.2025 - val_loss: 253.8780\n",
      "Epoch 22/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 71us/step - loss: 253.0168 - val_loss: 252.1846\n",
      "Epoch 23/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 249.4256 - val_loss: 252.6415\n",
      "Epoch 24/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 248.6104 - val_loss: 262.2497\n",
      "Epoch 25/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 243.4705 - val_loss: 239.2338\n",
      "Epoch 26/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 244.8910 - val_loss: 245.2096\n",
      "Epoch 27/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 243.8513 - val_loss: 280.2684\n",
      "Epoch 28/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 242.9110 - val_loss: 244.1420\n",
      "Epoch 29/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 244.0516 - val_loss: 256.4846\n",
      "Epoch 30/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 234.2992 - val_loss: 239.3493\n",
      "Epoch 31/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 237.0385 - val_loss: 247.4417\n",
      "Epoch 32/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 233.5897 - val_loss: 236.3627\n",
      "Epoch 33/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 236.0921 - val_loss: 235.8055\n",
      "Epoch 34/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 231.6878 - val_loss: 235.0081\n",
      "Epoch 35/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 230.0568 - val_loss: 233.1567\n",
      "Epoch 36/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 231.8291 - val_loss: 235.9451\n",
      "Epoch 37/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 223.8679 - val_loss: 230.3486\n",
      "Epoch 38/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 226.3360 - val_loss: 237.4281\n",
      "Epoch 39/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 226.6744 - val_loss: 247.7346\n",
      "Epoch 40/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 223.1688 - val_loss: 230.1094\n",
      "Epoch 41/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 218.4589 - val_loss: 247.7979\n",
      "Epoch 42/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 214.2901 - val_loss: 231.6967\n",
      "Epoch 43/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 214.8238 - val_loss: 234.5589\n",
      "Epoch 44/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 215.6155 - val_loss: 242.4893\n",
      "Epoch 45/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 214.1183 - val_loss: 221.7066\n",
      "Epoch 46/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 212.6515 - val_loss: 223.9888\n",
      "Epoch 47/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 219.7260 - val_loss: 253.2533\n",
      "Epoch 48/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 210.1031 - val_loss: 231.0220\n",
      "Epoch 49/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 209.2027 - val_loss: 231.5930\n",
      "Epoch 50/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 210.7945 - val_loss: 243.2832\n",
      "Epoch 51/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 207.8651 - val_loss: 225.1297\n",
      "Epoch 52/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 204.8241 - val_loss: 231.1373\n",
      "Epoch 53/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 205.9641 - val_loss: 235.7746\n",
      "Epoch 54/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 205.3876 - val_loss: 219.7824\n",
      "Epoch 55/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 202.7710 - val_loss: 217.0407\n",
      "Epoch 56/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 199.0219 - val_loss: 232.6869\n",
      "Epoch 57/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 198.3275 - val_loss: 232.4480\n",
      "Epoch 58/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 197.1617 - val_loss: 232.7671\n",
      "Epoch 59/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 196.2014 - val_loss: 217.1752\n",
      "Epoch 60/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 196.2755 - val_loss: 227.6588\n",
      "Epoch 61/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 196.0944 - val_loss: 244.3829\n",
      "Epoch 62/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 194.4823 - val_loss: 220.2888\n",
      "Epoch 63/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 195.1903 - val_loss: 223.2143\n",
      "Epoch 64/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 194.1707 - val_loss: 220.7148\n",
      "Epoch 65/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 189.9431 - val_loss: 213.4236\n",
      "Epoch 66/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 189.6290 - val_loss: 243.6973\n",
      "Epoch 67/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 186.0181 - val_loss: 231.5565\n",
      "Epoch 68/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 185.9793 - val_loss: 214.5660\n",
      "Epoch 69/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 185.1461 - val_loss: 217.0312\n",
      "Epoch 70/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 185.0182 - val_loss: 237.6423\n",
      "Epoch 71/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 181.4636 - val_loss: 211.3273\n",
      "Epoch 72/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 183.5654 - val_loss: 211.4945\n",
      "Epoch 73/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 180.5438 - val_loss: 220.8985\n",
      "Epoch 74/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 181.3113 - val_loss: 211.8972\n",
      "Epoch 75/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 175.0038 - val_loss: 223.3102\n",
      "Epoch 76/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 180.7365 - val_loss: 214.5177\n",
      "Epoch 77/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 177.1235 - val_loss: 214.7714\n",
      "Epoch 78/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 173.0942 - val_loss: 207.2930\n",
      "Epoch 79/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 171.6009 - val_loss: 207.9644\n",
      "Epoch 80/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 176.2588 - val_loss: 242.5588\n",
      "Epoch 81/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 171.6593 - val_loss: 211.4244\n",
      "Epoch 82/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 170.0670 - val_loss: 212.6501\n",
      "Epoch 83/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 167.2178 - val_loss: 204.1486\n",
      "Epoch 84/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 169.8449 - val_loss: 202.8167\n",
      "Epoch 85/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 164.5187 - val_loss: 211.4410\n",
      "Epoch 86/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 165.3252 - val_loss: 213.1977\n",
      "Epoch 87/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 161.2303 - val_loss: 203.2830\n",
      "Epoch 88/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 160.6055 - val_loss: 208.4009\n",
      "Epoch 89/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 162.0138 - val_loss: 218.4251\n",
      "Epoch 90/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 161.6470 - val_loss: 207.4523\n",
      "Epoch 91/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 163.4917 - val_loss: 212.6000\n",
      "Epoch 92/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 158.8555 - val_loss: 202.6813\n",
      "Epoch 93/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 157.3671 - val_loss: 197.9039\n",
      "Epoch 94/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 157.8770 - val_loss: 201.8118\n",
      "Epoch 95/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 150.3182 - val_loss: 210.6834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 150.9994 - val_loss: 196.4885\n",
      "Epoch 97/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 154.0887 - val_loss: 191.8411\n",
      "Epoch 98/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 150.5508 - val_loss: 198.0153\n",
      "Epoch 99/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 147.1294 - val_loss: 193.8089\n",
      "Epoch 100/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 146.6126 - val_loss: 178.0136\n",
      "Epoch 101/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 144.1095 - val_loss: 194.7065\n",
      "Epoch 102/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 141.8274 - val_loss: 183.3933\n",
      "Epoch 103/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 138.3370 - val_loss: 210.3295\n",
      "Epoch 104/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 144.1433 - val_loss: 186.0785\n",
      "Epoch 105/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 149.3996 - val_loss: 190.3683\n",
      "Epoch 106/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 143.4116 - val_loss: 184.0657\n",
      "Epoch 107/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 140.6387 - val_loss: 199.9933\n",
      "Epoch 108/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 142.6527 - val_loss: 196.1496\n",
      "Epoch 109/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 135.2914 - val_loss: 178.6181\n",
      "Epoch 110/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 129.1849 - val_loss: 188.3428\n",
      "Epoch 111/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 131.0816 - val_loss: 181.5595\n",
      "Epoch 112/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 131.2184 - val_loss: 190.3334\n",
      "Epoch 113/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 130.1444 - val_loss: 168.7879\n",
      "Epoch 114/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 130.9247 - val_loss: 205.3842\n",
      "Epoch 115/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 125.8157 - val_loss: 198.3426\n",
      "Epoch 116/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 129.5318 - val_loss: 188.5318\n",
      "Epoch 117/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 129.4759 - val_loss: 172.5400\n",
      "Epoch 118/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 127.1779 - val_loss: 173.1209\n",
      "Epoch 119/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 123.9212 - val_loss: 174.8697\n",
      "Epoch 120/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 118.9402 - val_loss: 167.3637\n",
      "Epoch 121/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 125.1902 - val_loss: 201.8375\n",
      "Epoch 122/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 122.0926 - val_loss: 195.8293\n",
      "Epoch 123/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 118.3674 - val_loss: 194.6257\n",
      "Epoch 124/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 116.2699 - val_loss: 177.6201\n",
      "Epoch 125/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 114.6410 - val_loss: 174.1099\n",
      "Epoch 126/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 114.1126 - val_loss: 177.8493\n",
      "Epoch 127/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 111.8797 - val_loss: 206.5244\n",
      "Epoch 128/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 113.2520 - val_loss: 185.9326\n",
      "Epoch 129/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 109.7399 - val_loss: 183.4077\n",
      "Epoch 130/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 108.9617 - val_loss: 181.3218\n",
      "Epoch 131/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 107.8363 - val_loss: 187.0300\n",
      "Epoch 132/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 110.0557 - val_loss: 167.5777\n",
      "Epoch 133/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 113.4818 - val_loss: 178.6621\n",
      "Epoch 134/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 112.7754 - val_loss: 175.4125\n",
      "Epoch 135/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 110.1671 - val_loss: 163.4753\n",
      "Epoch 136/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 112.7681 - val_loss: 178.9103\n",
      "Epoch 137/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 106.1484 - val_loss: 179.1230\n",
      "Epoch 138/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 105.8581 - val_loss: 167.2464\n",
      "Epoch 139/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 109.4217 - val_loss: 168.7703\n",
      "Epoch 140/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 106.4533 - val_loss: 173.4253\n",
      "Epoch 141/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 102.2215 - val_loss: 191.2415\n",
      "Epoch 142/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 108.3967 - val_loss: 168.5493\n",
      "Epoch 143/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 102.3971 - val_loss: 175.2216\n",
      "Epoch 144/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 101.4728 - val_loss: 170.4771\n",
      "Epoch 145/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 99.6771 - val_loss: 191.2793\n",
      "Epoch 146/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 98.9891 - val_loss: 214.0591\n",
      "Epoch 147/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 101.2089 - val_loss: 168.1059\n",
      "Epoch 148/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 100.4894 - val_loss: 161.0463\n",
      "Epoch 149/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 99.9393 - val_loss: 179.2088\n",
      "Epoch 150/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 102.3679 - val_loss: 162.6066\n",
      "Epoch 151/10000\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 94.9793 - val_loss: 157.3524\n",
      "Epoch 152/10000\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 96.2246 - val_loss: 171.2478\n",
      "Epoch 153/10000\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 94.3390 - val_loss: 190.5030\n",
      "Epoch 154/10000\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 95.3242 - val_loss: 165.8983\n",
      "Epoch 155/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 91.8909 - val_loss: 179.0994\n",
      "Epoch 156/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 89.9549 - val_loss: 166.6335\n",
      "Epoch 157/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 97.0233 - val_loss: 170.7343\n",
      "Epoch 158/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 91.1383 - val_loss: 155.7429\n",
      "Epoch 159/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 96.8813 - val_loss: 164.4575\n",
      "Epoch 160/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 97.3076 - val_loss: 165.2948\n",
      "Epoch 161/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 94.1728 - val_loss: 154.4702\n",
      "Epoch 162/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 90.7290 - val_loss: 165.8981\n",
      "Epoch 163/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 86.5969 - val_loss: 183.1131\n",
      "Epoch 164/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 93.8564 - val_loss: 187.2198\n",
      "Epoch 165/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 98.3318 - val_loss: 188.6583\n",
      "Epoch 166/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 90.1657 - val_loss: 166.2093\n",
      "Epoch 167/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 90.1779 - val_loss: 178.2492\n",
      "Epoch 168/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 88.4381 - val_loss: 178.8784\n",
      "Epoch 169/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 86.1098 - val_loss: 171.1501\n",
      "Epoch 170/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 86.3391 - val_loss: 162.1232\n",
      "Epoch 171/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 88.7748 - val_loss: 181.2299\n",
      "Epoch 172/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 90.7825 - val_loss: 170.9322\n",
      "Epoch 173/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 90.0369 - val_loss: 165.0389\n",
      "Epoch 174/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 86.8096 - val_loss: 166.0251\n",
      "Epoch 175/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 84.9461 - val_loss: 174.0775\n",
      "Epoch 176/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 88.0330 - val_loss: 160.7760\n",
      "Epoch 177/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 87.0467 - val_loss: 187.8137\n",
      "Epoch 178/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 84.5186 - val_loss: 164.6144\n",
      "Epoch 179/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 82.1816 - val_loss: 179.0745\n",
      "Epoch 180/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 80.8389 - val_loss: 169.4794\n",
      "Epoch 181/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 85.6696 - val_loss: 163.5702\n",
      "Epoch 182/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 85.1166 - val_loss: 180.2691\n",
      "Epoch 183/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 81.1244 - val_loss: 182.7108\n",
      "Epoch 184/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 83.6822 - val_loss: 162.5081\n",
      "Epoch 185/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 87.4248 - val_loss: 174.3569\n",
      "Epoch 186/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 84.4097 - val_loss: 170.3231\n",
      "Epoch 187/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 82.7432 - val_loss: 168.8317\n",
      "Epoch 188/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 82.2337 - val_loss: 165.5393\n",
      "Epoch 189/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 82.5127 - val_loss: 159.9792\n",
      "Epoch 190/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 84.1161 - val_loss: 185.6923\n",
      "Epoch 191/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 79.4384 - val_loss: 165.4241\n",
      "Epoch 192/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 75.6563 - val_loss: 168.5778\n",
      "Epoch 193/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 83.7378 - val_loss: 177.4900\n",
      "Epoch 194/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 77.5404 - val_loss: 161.3598\n",
      "Epoch 195/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 79.9957 - val_loss: 169.6380\n",
      "Epoch 196/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 82.2572 - val_loss: 167.2064\n",
      "Epoch 197/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 82.0775 - val_loss: 173.1564\n",
      "Epoch 198/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 77.2804 - val_loss: 171.8358\n",
      "Epoch 199/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 77.5304 - val_loss: 166.7016\n",
      "Epoch 200/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 77.6005 - val_loss: 164.1287\n",
      "Epoch 201/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 79.4866 - val_loss: 165.0306\n",
      "Epoch 202/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 73.0665 - val_loss: 163.6202\n",
      "Epoch 203/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 79.9915 - val_loss: 163.1023\n",
      "Epoch 204/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 80.8758 - val_loss: 155.6657\n",
      "Epoch 205/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 76.7098 - val_loss: 151.9740\n",
      "Epoch 206/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 74.8198 - val_loss: 164.4125\n",
      "Epoch 207/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 73.1008 - val_loss: 164.6143\n",
      "Epoch 208/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 72.8561 - val_loss: 151.1304\n",
      "Epoch 209/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 72.2399 - val_loss: 181.4276\n",
      "Epoch 210/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 77.5494 - val_loss: 163.1553\n",
      "Epoch 211/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 69.1884 - val_loss: 159.8036\n",
      "Epoch 212/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 72.1311 - val_loss: 162.6378\n",
      "Epoch 213/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 73.5401 - val_loss: 149.0171\n",
      "Epoch 214/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 75.1139 - val_loss: 153.2062\n",
      "Epoch 215/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 71.3386 - val_loss: 151.2208\n",
      "Epoch 216/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 77.1315 - val_loss: 153.4330\n",
      "Epoch 217/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 74.9301 - val_loss: 160.1052\n",
      "Epoch 218/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 76.4840 - val_loss: 157.3285\n",
      "Epoch 219/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 79.6453 - val_loss: 155.5140\n",
      "Epoch 220/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 69.3744 - val_loss: 156.3192\n",
      "Epoch 221/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 78.7617 - val_loss: 194.0274\n",
      "Epoch 222/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 70.5051 - val_loss: 152.3755\n",
      "Epoch 223/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 67.1747 - val_loss: 156.3946\n",
      "Epoch 224/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 75.0730 - val_loss: 155.7316\n",
      "Epoch 225/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 67.2486 - val_loss: 150.6502\n",
      "Epoch 226/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 72.0976 - val_loss: 179.3767\n",
      "Epoch 227/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 74.3768 - val_loss: 158.4523\n",
      "Epoch 228/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 74.5617 - val_loss: 161.3463\n",
      "Epoch 229/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 71.2592 - val_loss: 165.3197\n",
      "Epoch 230/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 73.2728 - val_loss: 159.3709\n",
      "Epoch 231/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 69.5420 - val_loss: 167.0800\n",
      "Epoch 232/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 71.6868 - val_loss: 184.1744\n",
      "Epoch 233/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 68.7185 - val_loss: 150.8451\n",
      "Epoch 234/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 67.5256 - val_loss: 159.8110\n",
      "Epoch 235/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 69.1625 - val_loss: 160.6059\n",
      "Epoch 236/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 72.0890 - val_loss: 158.5882\n",
      "Epoch 237/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 66.9611 - val_loss: 153.6581\n",
      "Epoch 238/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 69.2471 - val_loss: 154.6882\n",
      "Epoch 239/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 67.5131 - val_loss: 172.7662\n",
      "Epoch 240/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 65.8067 - val_loss: 164.4713\n",
      "Epoch 241/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 65.9941 - val_loss: 175.9833\n",
      "Epoch 242/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 70.4742 - val_loss: 171.0490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 243/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 66.2572 - val_loss: 167.1068\n",
      "Epoch 244/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 67.7539 - val_loss: 161.5036\n",
      "Epoch 245/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 67.4410 - val_loss: 163.7614\n",
      "Epoch 246/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 65.2071 - val_loss: 160.0620\n",
      "Epoch 247/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 71.7479 - val_loss: 157.5700\n",
      "Epoch 248/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 71.3960 - val_loss: 171.8652\n",
      "Epoch 249/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 72.0253 - val_loss: 171.7961\n",
      "Epoch 250/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 65.6819 - val_loss: 157.2985\n",
      "Epoch 251/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 64.9855 - val_loss: 164.6643\n",
      "Epoch 252/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 66.7688 - val_loss: 146.7107\n",
      "Epoch 253/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 64.5641 - val_loss: 163.2391\n",
      "Epoch 254/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 63.5852 - val_loss: 163.4854\n",
      "Epoch 255/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 59.9032 - val_loss: 164.4847\n",
      "Epoch 256/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 63.2917 - val_loss: 163.2844\n",
      "Epoch 257/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 66.3758 - val_loss: 166.2699\n",
      "Epoch 258/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 65.7104 - val_loss: 170.1543\n",
      "Epoch 259/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 66.6504 - val_loss: 158.6963\n",
      "Epoch 260/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 67.6538 - val_loss: 165.9180\n",
      "Epoch 261/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 62.4939 - val_loss: 152.4668\n",
      "Epoch 262/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 62.0256 - val_loss: 178.4784\n",
      "Epoch 263/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 66.7681 - val_loss: 157.7117\n",
      "Epoch 264/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 65.2123 - val_loss: 156.8684\n",
      "Epoch 265/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 60.5315 - val_loss: 173.9983\n",
      "Epoch 266/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 62.0597 - val_loss: 151.2914\n",
      "Epoch 267/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 67.8071 - val_loss: 160.4199\n",
      "Epoch 268/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 66.3569 - val_loss: 181.0111\n",
      "Epoch 269/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 63.0370 - val_loss: 186.3580\n",
      "Epoch 270/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 63.2867 - val_loss: 183.1959\n",
      "Epoch 271/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 63.0404 - val_loss: 152.4561\n",
      "Epoch 272/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 66.3410 - val_loss: 158.3088\n",
      "Epoch 273/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 61.5991 - val_loss: 155.4269\n",
      "Epoch 274/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 62.4596 - val_loss: 157.4097\n",
      "Epoch 275/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 60.5358 - val_loss: 178.2566\n",
      "Epoch 276/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 60.7481 - val_loss: 162.3177\n",
      "Epoch 277/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 63.0026 - val_loss: 173.8624\n",
      "Epoch 278/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 62.8881 - val_loss: 157.9771\n",
      "Epoch 279/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 60.4208 - val_loss: 155.7092\n",
      "Epoch 280/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 61.3094 - val_loss: 165.8476\n",
      "Epoch 281/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 64.2223 - val_loss: 156.0714\n",
      "Epoch 282/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 67.1666 - val_loss: 160.6981\n",
      "Epoch 283/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 66.2718 - val_loss: 177.2900\n",
      "Epoch 284/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 62.7626 - val_loss: 148.6800\n",
      "Epoch 285/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 60.1618 - val_loss: 163.8327\n",
      "Epoch 286/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 67.6027 - val_loss: 164.1904\n",
      "Epoch 287/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 61.1412 - val_loss: 158.1201\n",
      "Epoch 288/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 59.7216 - val_loss: 160.5561\n",
      "Epoch 289/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 58.0806 - val_loss: 168.1378\n",
      "Epoch 290/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 60.1584 - val_loss: 178.9210\n",
      "Epoch 291/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 58.5047 - val_loss: 156.8318\n",
      "Epoch 292/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 56.7548 - val_loss: 162.1338\n",
      "Epoch 293/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 64.0632 - val_loss: 172.9024\n",
      "Epoch 294/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 62.0266 - val_loss: 156.8982\n",
      "Epoch 295/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 60.6893 - val_loss: 163.8165\n",
      "Epoch 296/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 64.3689 - val_loss: 150.0195\n",
      "Epoch 297/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 58.5438 - val_loss: 173.3724\n",
      "Epoch 298/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 63.1992 - val_loss: 150.0559\n",
      "Epoch 299/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 60.3430 - val_loss: 155.5901\n",
      "Epoch 300/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 57.6399 - val_loss: 164.9042\n",
      "Epoch 301/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 60.2412 - val_loss: 165.2640\n",
      "Epoch 302/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 62.6261 - val_loss: 162.2182\n",
      "Epoch 303/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 57.8982 - val_loss: 154.0345\n",
      "Epoch 304/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 60.3857 - val_loss: 158.2813\n",
      "Epoch 305/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 59.6366 - val_loss: 169.4394\n",
      "Epoch 306/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 58.1780 - val_loss: 169.2345\n",
      "Epoch 307/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 57.6779 - val_loss: 161.0482\n",
      "Epoch 308/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 57.0442 - val_loss: 152.3486\n",
      "Epoch 309/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 61.3659 - val_loss: 163.9317\n",
      "Epoch 310/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 58.8756 - val_loss: 167.0371\n",
      "Epoch 311/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 59.4675 - val_loss: 156.4756\n",
      "Epoch 312/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 60.6164 - val_loss: 165.0216\n",
      "Epoch 313/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 54.1513 - val_loss: 158.2776\n",
      "Epoch 314/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 60.3071 - val_loss: 147.9069\n",
      "Epoch 315/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 56.1865 - val_loss: 155.2773\n",
      "Epoch 316/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 52.9937 - val_loss: 174.1305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 317/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 57.2092 - val_loss: 151.0470\n",
      "Epoch 318/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 54.0011 - val_loss: 151.7806\n",
      "Epoch 319/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 57.8357 - val_loss: 164.7364\n",
      "Epoch 320/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 60.2409 - val_loss: 169.7859\n",
      "Epoch 321/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 56.5731 - val_loss: 160.2899\n",
      "Epoch 322/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 58.1939 - val_loss: 157.2850\n",
      "Epoch 323/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 58.0053 - val_loss: 165.5199\n",
      "Epoch 324/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 63.7221 - val_loss: 173.6920\n",
      "Epoch 325/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 53.9545 - val_loss: 166.7908\n",
      "Epoch 326/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 56.7089 - val_loss: 155.5925\n",
      "Epoch 327/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 58.6326 - val_loss: 157.3145\n",
      "Epoch 328/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 53.7767 - val_loss: 160.2790\n",
      "Epoch 329/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 57.4743 - val_loss: 184.8076\n",
      "Epoch 330/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 56.3732 - val_loss: 171.1295\n",
      "Epoch 331/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 58.4214 - val_loss: 158.9192\n",
      "Epoch 332/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 57.0349 - val_loss: 170.8956\n",
      "Epoch 333/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 56.5428 - val_loss: 160.7664\n",
      "Epoch 334/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 56.5594 - val_loss: 162.9130\n",
      "Epoch 335/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 55.1354 - val_loss: 155.7688\n",
      "Epoch 336/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 53.2542 - val_loss: 157.9894\n",
      "Epoch 337/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 53.8816 - val_loss: 173.3955\n",
      "Epoch 338/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 55.5276 - val_loss: 163.6057\n",
      "Epoch 339/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 56.5598 - val_loss: 156.3520\n",
      "Epoch 340/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 59.4545 - val_loss: 166.4702\n",
      "Epoch 341/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 51.5690 - val_loss: 163.0972\n",
      "Epoch 342/10000\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 54.1453 - val_loss: 181.3661\n",
      "Epoch 343/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 52.0663 - val_loss: 171.5528\n",
      "Epoch 344/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 54.4513 - val_loss: 159.9427\n",
      "Epoch 345/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 54.0119 - val_loss: 165.9121\n",
      "Epoch 346/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 51.7243 - val_loss: 165.6021\n",
      "Epoch 347/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 54.8078 - val_loss: 162.5772\n",
      "Epoch 348/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 55.1011 - val_loss: 155.7207\n",
      "Epoch 349/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 49.0120 - val_loss: 156.5873\n",
      "Epoch 350/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 50.1885 - val_loss: 157.0808\n",
      "Epoch 351/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 57.5805 - val_loss: 160.3218\n",
      "Epoch 352/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 55.7703 - val_loss: 165.3304\n",
      "Epoch 353/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 52.3921 - val_loss: 160.1232\n",
      "Epoch 354/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 56.3013 - val_loss: 171.5171\n",
      "Epoch 355/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 52.8742 - val_loss: 168.6314\n",
      "Epoch 356/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 57.7406 - val_loss: 147.6831\n",
      "Epoch 357/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 54.4879 - val_loss: 159.6953\n",
      "Epoch 358/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 52.0905 - val_loss: 162.5154\n",
      "Epoch 359/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 58.1176 - val_loss: 167.8526\n",
      "Epoch 360/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 52.6396 - val_loss: 161.8913\n",
      "Epoch 361/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 52.8260 - val_loss: 162.8390\n",
      "Epoch 362/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 56.4908 - val_loss: 189.1203\n",
      "Epoch 363/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 55.8801 - val_loss: 165.1713\n",
      "Epoch 364/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 50.4481 - val_loss: 180.1751\n",
      "Epoch 365/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 51.8726 - val_loss: 168.7065\n",
      "Epoch 366/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 52.9410 - val_loss: 181.7757\n",
      "Epoch 367/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 53.5256 - val_loss: 158.0183\n",
      "Epoch 368/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 57.2526 - val_loss: 166.7292\n",
      "Epoch 369/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 52.4129 - val_loss: 163.2431\n",
      "Epoch 370/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 47.7321 - val_loss: 181.2510\n",
      "Epoch 371/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 53.2544 - val_loss: 167.0926\n",
      "Epoch 372/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 50.6054 - val_loss: 166.2352\n",
      "Epoch 373/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 50.0160 - val_loss: 162.5442\n",
      "Epoch 374/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 50.6044 - val_loss: 162.6422\n",
      "Epoch 375/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 50.8020 - val_loss: 162.1643\n",
      "Epoch 376/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 52.9223 - val_loss: 163.7334\n",
      "Epoch 377/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 51.3859 - val_loss: 199.6837\n",
      "Epoch 378/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 51.7512 - val_loss: 158.1237\n",
      "Epoch 379/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 48.4064 - val_loss: 164.8739\n",
      "Epoch 380/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 53.5653 - val_loss: 169.0386\n",
      "Epoch 381/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 51.0208 - val_loss: 156.4979\n",
      "Epoch 382/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 52.8937 - val_loss: 170.5977\n",
      "Epoch 383/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 52.6095 - val_loss: 155.6993\n",
      "Epoch 384/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 49.9714 - val_loss: 176.8606\n",
      "Epoch 385/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 48.8585 - val_loss: 167.5135\n",
      "Epoch 386/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 50.4896 - val_loss: 180.4915\n",
      "Epoch 387/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 49.9611 - val_loss: 162.1461\n",
      "Epoch 388/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 51.0648 - val_loss: 158.9267\n",
      "Epoch 389/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 55.6273 - val_loss: 170.7616\n",
      "Epoch 390/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 47.7952 - val_loss: 176.9699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 391/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 53.8206 - val_loss: 165.4642\n",
      "Epoch 392/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 47.4379 - val_loss: 174.7123\n",
      "Epoch 393/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 54.4410 - val_loss: 164.6213\n",
      "Epoch 394/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 52.38 - 0s 57us/step - loss: 52.4785 - val_loss: 164.5319\n",
      "Epoch 395/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 53.0346 - val_loss: 169.9355\n",
      "Epoch 396/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 49.3055 - val_loss: 165.9821\n",
      "Epoch 397/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 48.7917 - val_loss: 177.0001\n",
      "Epoch 398/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 51.1894 - val_loss: 172.3701\n",
      "Epoch 399/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 47.8528 - val_loss: 157.7843\n",
      "Epoch 400/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 50.7889 - val_loss: 170.9738\n",
      "Epoch 401/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 52.9910 - val_loss: 168.1627\n",
      "Epoch 402/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 51.8952 - val_loss: 164.2845\n",
      "Epoch 403/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 53.4397 - val_loss: 177.5782\n",
      "Epoch 404/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 47.3688 - val_loss: 165.3969\n",
      "Epoch 405/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 51.4860 - val_loss: 159.0993\n",
      "Epoch 406/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 50.4526 - val_loss: 171.0491\n",
      "Epoch 407/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 48.8225 - val_loss: 181.9014\n",
      "Epoch 408/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 51.8523 - val_loss: 185.0369\n",
      "Epoch 409/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 51.1356 - val_loss: 161.0310\n",
      "Epoch 410/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 49.2419 - val_loss: 170.5907\n",
      "Epoch 411/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 47.6511 - val_loss: 163.1814\n",
      "Epoch 412/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 50.0465 - val_loss: 188.9947\n",
      "Epoch 413/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 59.8943 - val_loss: 172.1598\n",
      "Epoch 414/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 52.9813 - val_loss: 160.8246\n",
      "Epoch 415/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 45.1920 - val_loss: 157.6345\n",
      "Epoch 416/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 46.9137 - val_loss: 169.8747\n",
      "Epoch 417/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 48.9812 - val_loss: 163.7639\n",
      "Epoch 418/10000\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 47.0926 - val_loss: 169.9470\n",
      "Epoch 419/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 45.5816 - val_loss: 175.5815\n",
      "Epoch 420/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 48.6538 - val_loss: 177.9503\n",
      "Epoch 421/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 49.3034 - val_loss: 165.0718\n",
      "Epoch 422/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 47.9829 - val_loss: 186.2618\n",
      "Epoch 423/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 49.7121 - val_loss: 166.0812\n",
      "Epoch 424/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 47.7517 - val_loss: 169.9407\n",
      "Epoch 425/10000\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 48.2104 - val_loss: 166.4151\n",
      "Epoch 426/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 48.8770 - val_loss: 171.2974\n",
      "Epoch 427/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 46.4886 - val_loss: 166.7855\n",
      "Epoch 428/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 49.9726 - val_loss: 172.3920\n",
      "Epoch 429/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 50.6395 - val_loss: 166.2212\n",
      "Epoch 430/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 48.3584 - val_loss: 167.0763\n",
      "Epoch 431/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 47.7118 - val_loss: 189.5568\n",
      "Epoch 432/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 47.1659 - val_loss: 169.1110\n",
      "Epoch 433/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 45.1639 - val_loss: 167.8905\n",
      "Epoch 434/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 47.8396 - val_loss: 166.2329\n",
      "Epoch 435/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 48.0303 - val_loss: 169.6258\n",
      "Epoch 436/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 46.2960 - val_loss: 166.9132\n",
      "Epoch 437/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 49.6084 - val_loss: 177.6216\n",
      "Epoch 438/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 46.7533 - val_loss: 192.2854\n",
      "Epoch 439/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 46.5061 - val_loss: 174.5945\n",
      "Epoch 440/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 49.3426 - val_loss: 164.0157\n",
      "Epoch 441/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 50.2120 - val_loss: 165.6757\n",
      "Epoch 442/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 47.9896 - val_loss: 170.1848\n",
      "Epoch 443/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 46.2968 - val_loss: 165.1592\n",
      "Epoch 444/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 42.8395 - val_loss: 179.8703\n",
      "Epoch 445/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 49.3536 - val_loss: 164.9205\n",
      "Epoch 446/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 49.4211 - val_loss: 153.5416\n",
      "Epoch 447/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 52.0379 - val_loss: 160.7965\n",
      "Epoch 448/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 43.5316 - val_loss: 154.6612\n",
      "Epoch 449/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 46.3226 - val_loss: 176.4339\n",
      "Epoch 450/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 44.5565 - val_loss: 167.0214\n",
      "Epoch 451/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 51.1521 - val_loss: 160.8451\n",
      "Epoch 452/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 44.4406 - val_loss: 179.7173\n",
      "Epoch 453/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 53.2010 - val_loss: 169.5658\n",
      "Epoch 454/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 46.3869 - val_loss: 168.2383\n",
      "Epoch 455/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 47.0940 - val_loss: 164.8832\n",
      "Epoch 456/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 48.6822 - val_loss: 161.1894\n",
      "Epoch 457/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 46.8814 - val_loss: 160.5901\n",
      "Epoch 458/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 44.9235 - val_loss: 169.0093\n",
      "Epoch 459/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 42.3475 - val_loss: 173.3561\n",
      "Epoch 460/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 45.2105 - val_loss: 157.4738\n",
      "Epoch 461/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 43.8480 - val_loss: 162.8328\n",
      "Epoch 462/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 43.4356 - val_loss: 165.4278\n",
      "Epoch 463/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 44.9016 - val_loss: 161.5247\n",
      "Epoch 464/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 44.7671 - val_loss: 167.1367\n",
      "Epoch 465/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 48.2637 - val_loss: 163.1895\n",
      "Epoch 466/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 45.4285 - val_loss: 176.7745\n",
      "Epoch 467/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 45.4482 - val_loss: 160.4067\n",
      "Epoch 468/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 48.2435 - val_loss: 175.6873\n",
      "Epoch 469/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 50.3013 - val_loss: 180.4084\n",
      "Epoch 470/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 43.1669 - val_loss: 161.8509\n",
      "Epoch 471/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 43.2916 - val_loss: 162.5445\n",
      "Epoch 472/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 41.6067 - val_loss: 178.0289\n",
      "Epoch 473/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 44.7566 - val_loss: 185.8277\n",
      "Epoch 474/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 44.7629 - val_loss: 185.3323\n",
      "Epoch 475/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 46.6036 - val_loss: 173.3573\n",
      "Epoch 476/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 44.3968 - val_loss: 176.7007\n",
      "Epoch 477/10000\n",
      "8000/8000 [==============================] - 1s 130us/step - loss: 48.2503 - val_loss: 171.8108\n",
      "Epoch 478/10000\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 45.6482 - val_loss: 161.7551\n",
      "Epoch 479/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 41.4797 - val_loss: 178.7739\n",
      "Epoch 480/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 44.8841 - val_loss: 173.9851\n",
      "Epoch 481/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 45.7248 - val_loss: 167.9017\n",
      "Epoch 482/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 47.3604 - val_loss: 164.3833\n",
      "Epoch 483/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 46.3866 - val_loss: 169.8107\n",
      "Epoch 484/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 43.6676 - val_loss: 169.0190\n",
      "Epoch 485/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 41.6720 - val_loss: 165.7735\n",
      "Epoch 486/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 45.5585 - val_loss: 171.4269\n",
      "Epoch 487/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 43.9862 - val_loss: 163.3347\n",
      "Epoch 488/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 44.3154 - val_loss: 172.0816\n",
      "Epoch 489/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 47.7271 - val_loss: 168.9565\n",
      "Epoch 490/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 48.6472 - val_loss: 163.3084\n",
      "Epoch 491/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 45.8699 - val_loss: 168.9635\n",
      "Epoch 492/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 46.2636 - val_loss: 171.7324\n",
      "Epoch 493/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 45.4374 - val_loss: 169.5230\n",
      "Epoch 494/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 44.0657 - val_loss: 167.2279\n",
      "Epoch 495/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 43.4637 - val_loss: 170.5153\n",
      "Epoch 496/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 44.2570 - val_loss: 164.9385\n",
      "Epoch 497/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 43.8945 - val_loss: 201.5136\n",
      "Epoch 498/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 44.6431 - val_loss: 179.9818\n",
      "Epoch 499/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 45.7576 - val_loss: 181.1396\n",
      "Epoch 500/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 43.5802 - val_loss: 168.9160\n",
      "Epoch 501/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 41.7913 - val_loss: 159.7921\n",
      "Epoch 502/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 46.9004 - val_loss: 181.9288\n",
      "Epoch 503/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 46.1712 - val_loss: 165.3757\n",
      "Epoch 504/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 42.1334 - val_loss: 190.7346\n",
      "Epoch 505/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 45.1846 - val_loss: 169.7606\n",
      "Epoch 506/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 44.0843 - val_loss: 183.0604\n",
      "Epoch 507/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 46.4146 - val_loss: 173.5525\n",
      "Epoch 508/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 48.6619 - val_loss: 178.3073\n",
      "Epoch 509/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 46.9934 - val_loss: 171.2631\n",
      "Epoch 510/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 43.0298 - val_loss: 176.9382\n",
      "Epoch 511/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 41.3169 - val_loss: 164.3793\n",
      "Epoch 512/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 48.2105 - val_loss: 181.4674\n",
      "Epoch 513/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 43.2641 - val_loss: 160.3014\n",
      "Epoch 514/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 45.5737 - val_loss: 172.3430\n",
      "Epoch 515/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 43.7620 - val_loss: 167.2286\n",
      "Epoch 516/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 41.8288 - val_loss: 156.1430\n",
      "Epoch 517/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 42.5223 - val_loss: 178.8408\n",
      "Epoch 518/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 43.3463 - val_loss: 181.6074\n",
      "Epoch 519/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 45.0974 - val_loss: 166.3850\n",
      "Epoch 520/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 41.6063 - val_loss: 176.6376\n",
      "Epoch 521/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 44.7037 - val_loss: 159.6729\n",
      "Epoch 522/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 44.7882 - val_loss: 169.6340\n",
      "Epoch 523/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 41.7293 - val_loss: 167.2593\n",
      "Epoch 524/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 41.9676 - val_loss: 178.9906\n",
      "Epoch 525/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 42.4739 - val_loss: 160.8232\n",
      "Epoch 526/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 46.6869 - val_loss: 164.0286\n",
      "Epoch 527/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 45.0834 - val_loss: 179.4883\n",
      "Epoch 528/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 43.9947 - val_loss: 173.9992\n",
      "Epoch 529/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 41.2544 - val_loss: 190.1743\n",
      "Epoch 530/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 45.7170 - val_loss: 178.3626\n",
      "Epoch 531/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 43.7590 - val_loss: 168.4130\n",
      "Epoch 532/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.4046 - val_loss: 179.9735\n",
      "Epoch 533/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 44.8081 - val_loss: 165.6422\n",
      "Epoch 534/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 44.5947 - val_loss: 164.8342\n",
      "Epoch 535/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 42.3637 - val_loss: 164.6678\n",
      "Epoch 536/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 42.2716 - val_loss: 178.9926\n",
      "Epoch 537/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 42.5662 - val_loss: 165.8740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 538/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 44.0294 - val_loss: 165.8939\n",
      "Epoch 539/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 45.8758 - val_loss: 188.4173\n",
      "Epoch 540/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 43.6596 - val_loss: 175.6392\n",
      "Epoch 541/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 38.8725 - val_loss: 163.9136\n",
      "Epoch 542/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 43.6981 - val_loss: 195.4518\n",
      "Epoch 543/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 41.8699 - val_loss: 162.4966\n",
      "Epoch 544/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 42.4983 - val_loss: 169.2139\n",
      "Epoch 545/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 44.1714 - val_loss: 161.6045\n",
      "Epoch 546/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 40.7627 - val_loss: 168.7881\n",
      "Epoch 547/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 43.5428 - val_loss: 161.7194\n",
      "Epoch 548/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 40.8292 - val_loss: 164.7566\n",
      "Epoch 549/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 44.2132 - val_loss: 180.1128\n",
      "Epoch 550/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 43.7895 - val_loss: 163.8488\n",
      "Epoch 551/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.3970 - val_loss: 173.3454\n",
      "Epoch 552/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 41.1196 - val_loss: 170.2215\n",
      "Epoch 553/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 40.8052 - val_loss: 167.8063\n",
      "Epoch 554/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.8507 - val_loss: 176.5396\n",
      "Epoch 555/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 42.5151 - val_loss: 171.7504\n",
      "Epoch 556/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 40.4670 - val_loss: 180.9068\n",
      "Epoch 557/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 42.2513 - val_loss: 169.0628\n",
      "Epoch 558/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 40.3315 - val_loss: 179.9486\n",
      "Epoch 559/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 44.6631 - val_loss: 165.4563\n",
      "Epoch 560/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 37.7504 - val_loss: 165.9947\n",
      "Epoch 561/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 42.4583 - val_loss: 169.9705\n",
      "Epoch 562/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 46.1913 - val_loss: 163.8340\n",
      "Epoch 563/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 43.8201 - val_loss: 166.8350\n",
      "Epoch 564/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 40.4659 - val_loss: 163.3983\n",
      "Epoch 565/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 38.6540 - val_loss: 168.9243\n",
      "Epoch 566/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 46.6056 - val_loss: 174.8887\n",
      "Epoch 567/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 39.3876 - val_loss: 161.6568\n",
      "Epoch 568/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 45.9304 - val_loss: 165.4566\n",
      "Epoch 569/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 44.4868 - val_loss: 175.8986\n",
      "Epoch 570/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 42.2094 - val_loss: 171.0876\n",
      "Epoch 571/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 43.8982 - val_loss: 184.2200\n",
      "Epoch 572/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 39.3732 - val_loss: 162.9443\n",
      "Epoch 573/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 39.2009 - val_loss: 172.3809\n",
      "Epoch 574/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 40.1193 - val_loss: 177.8381\n",
      "Epoch 575/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 43.7299 - val_loss: 166.0969\n",
      "Epoch 576/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 42.9076 - val_loss: 172.2148\n",
      "Epoch 577/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 36.1062 - val_loss: 180.0652\n",
      "Epoch 578/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.5765 - val_loss: 171.6475\n",
      "Epoch 579/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 40.2843 - val_loss: 182.5067\n",
      "Epoch 580/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 40.8046 - val_loss: 181.8058\n",
      "Epoch 581/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 43.4857 - val_loss: 177.7681\n",
      "Epoch 582/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 41.6136 - val_loss: 166.6956\n",
      "Epoch 583/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.7314 - val_loss: 165.4779\n",
      "Epoch 584/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 42.8841 - val_loss: 177.6107\n",
      "Epoch 585/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 43.1398 - val_loss: 169.5981\n",
      "Epoch 586/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 40.4651 - val_loss: 157.8789\n",
      "Epoch 587/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 41.2855 - val_loss: 184.2086\n",
      "Epoch 588/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 37.1161 - val_loss: 160.3464\n",
      "Epoch 589/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 40.0345 - val_loss: 179.2828\n",
      "Epoch 590/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 42.9673 - val_loss: 178.9311\n",
      "Epoch 591/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 39.2153 - val_loss: 170.5042\n",
      "Epoch 592/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 39.4986 - val_loss: 160.2751\n",
      "Epoch 593/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 41.4074 - val_loss: 164.4402\n",
      "Epoch 594/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 43.6400 - val_loss: 185.8349\n",
      "Epoch 595/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 40.1917 - val_loss: 170.7901\n",
      "Epoch 596/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 38.4250 - val_loss: 171.9522\n",
      "Epoch 597/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 38.4951 - val_loss: 166.6559\n",
      "Epoch 598/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 39.2834 - val_loss: 168.6609\n",
      "Epoch 599/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 39.0436 - val_loss: 162.0329\n",
      "Epoch 600/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.9249 - val_loss: 175.1906\n",
      "Epoch 601/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 42.9862 - val_loss: 164.4726\n",
      "Epoch 602/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 39.0693 - val_loss: 157.3423\n",
      "Epoch 603/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 41.8399 - val_loss: 168.8133\n",
      "Epoch 604/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.1671 - val_loss: 174.1029\n",
      "Epoch 605/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 43.4475 - val_loss: 166.3480\n",
      "Epoch 606/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.4930 - val_loss: 172.3617\n",
      "Epoch 607/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.2887 - val_loss: 152.5923\n",
      "Epoch 608/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 43.0511 - val_loss: 170.3067\n",
      "Epoch 609/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 41.0754 - val_loss: 164.5715\n",
      "Epoch 610/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 41.5996 - val_loss: 157.7259\n",
      "Epoch 611/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 43.0522 - val_loss: 164.1438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 612/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.8124 - val_loss: 168.0242\n",
      "Epoch 613/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.8399 - val_loss: 168.9884\n",
      "Epoch 614/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 36.2159 - val_loss: 182.6835\n",
      "Epoch 615/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.4361 - val_loss: 186.8691\n",
      "Epoch 616/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 40.9869 - val_loss: 178.5209\n",
      "Epoch 617/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 39.0788 - val_loss: 169.1117\n",
      "Epoch 618/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 41.0210 - val_loss: 179.7528\n",
      "Epoch 619/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 42.4104 - val_loss: 170.9698\n",
      "Epoch 620/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 38.3674 - val_loss: 171.6672\n",
      "Epoch 621/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 36.7765 - val_loss: 168.7563\n",
      "Epoch 622/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 41.3373 - val_loss: 166.2792\n",
      "Epoch 623/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 37.1476 - val_loss: 170.8983\n",
      "Epoch 624/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 43.3115 - val_loss: 161.8584\n",
      "Epoch 625/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 39.8720 - val_loss: 178.3334\n",
      "Epoch 626/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.2668 - val_loss: 162.5492\n",
      "Epoch 627/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 36.1802 - val_loss: 164.1956\n",
      "Epoch 628/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.6483 - val_loss: 164.2391\n",
      "Epoch 629/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 37.8100 - val_loss: 177.6475\n",
      "Epoch 630/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 42.5810 - val_loss: 169.0061\n",
      "Epoch 631/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 45.1401 - val_loss: 162.7020\n",
      "Epoch 632/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 43.0171 - val_loss: 179.2707\n",
      "Epoch 633/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 46.9623 - val_loss: 158.7498\n",
      "Epoch 634/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 37.1027 - val_loss: 162.3353\n",
      "Epoch 635/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 41.8932 - val_loss: 157.6759\n",
      "Epoch 636/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 41.4081 - val_loss: 161.6925\n",
      "Epoch 637/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 36.7425 - val_loss: 170.3629\n",
      "Epoch 638/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.4249 - val_loss: 171.6816\n",
      "Epoch 639/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 40.2203 - val_loss: 165.5073\n",
      "Epoch 640/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 39.4927 - val_loss: 163.3314\n",
      "Epoch 641/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 39.3645 - val_loss: 165.2444\n",
      "Epoch 642/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 43.0100 - val_loss: 162.4922\n",
      "Epoch 643/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 42.2353 - val_loss: 163.0832\n",
      "Epoch 644/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 36.6879 - val_loss: 160.4723\n",
      "Epoch 645/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.1215 - val_loss: 164.1179\n",
      "Epoch 646/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 38.3602 - val_loss: 157.8849\n",
      "Epoch 647/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 40.0259 - val_loss: 167.2113\n",
      "Epoch 648/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 35.9418 - val_loss: 160.7316\n",
      "Epoch 649/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 40.0304 - val_loss: 164.6386\n",
      "Epoch 650/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 37.6677 - val_loss: 167.0382\n",
      "Epoch 651/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 37.2280 - val_loss: 176.3837\n",
      "Epoch 652/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 41.8424 - val_loss: 160.2440\n",
      "Epoch 653/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.1196 - val_loss: 163.9086\n",
      "Epoch 654/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 42.8049 - val_loss: 165.2789\n",
      "Epoch 655/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 39.8187 - val_loss: 161.8759\n",
      "Epoch 656/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 39.4727 - val_loss: 194.3140\n",
      "Epoch 657/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 40.3098 - val_loss: 168.2696\n",
      "Epoch 658/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 36.8575 - val_loss: 166.0251\n",
      "Epoch 659/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 38.4294 - val_loss: 167.3857\n",
      "Epoch 660/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 37.1863 - val_loss: 177.5892\n",
      "Epoch 661/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 40.5317 - val_loss: 168.7975\n",
      "Epoch 662/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 37.6081 - val_loss: 167.9304\n",
      "Epoch 663/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 37.4715 - val_loss: 164.5036\n",
      "Epoch 664/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 37.8898 - val_loss: 156.4400\n",
      "Epoch 665/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 37.6489 - val_loss: 169.5778\n",
      "Epoch 666/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 38.1859 - val_loss: 153.0392\n",
      "Epoch 667/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 35.7268 - val_loss: 151.4856\n",
      "Epoch 668/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 38.5920 - val_loss: 173.8980\n",
      "Epoch 669/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 39.8352 - val_loss: 155.9844\n",
      "Epoch 670/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 38.1237 - val_loss: 167.9677\n",
      "Epoch 671/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.8991 - val_loss: 167.6399\n",
      "Epoch 672/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 40.7224 - val_loss: 160.9567\n",
      "Epoch 673/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 38.1952 - val_loss: 167.8315\n",
      "Epoch 674/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.1482 - val_loss: 165.1488\n",
      "Epoch 675/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 39.4538 - val_loss: 170.8936\n",
      "Epoch 676/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 34.2211 - val_loss: 166.7875\n",
      "Epoch 677/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 37.5169 - val_loss: 162.6867\n",
      "Epoch 678/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.7822 - val_loss: 162.2690\n",
      "Epoch 679/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 44.1915 - val_loss: 158.6248\n",
      "Epoch 680/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 35.6794 - val_loss: 174.3559\n",
      "Epoch 681/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 38.9499 - val_loss: 160.3659\n",
      "Epoch 682/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 40.7368 - val_loss: 154.8239\n",
      "Epoch 683/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 39.9392 - val_loss: 159.5191\n",
      "Epoch 684/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 37.5609 - val_loss: 150.5036\n",
      "Epoch 685/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 35.2828 - val_loss: 161.1842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 686/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 36.5784 - val_loss: 175.8072\n",
      "Epoch 687/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 37.5512 - val_loss: 161.9891\n",
      "Epoch 688/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 35.9497 - val_loss: 161.3153\n",
      "Epoch 689/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 33.3925 - val_loss: 160.8042\n",
      "Epoch 690/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 37.3024 - val_loss: 159.7910\n",
      "Epoch 691/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 38.9113 - val_loss: 178.1187\n",
      "Epoch 692/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 39.3892 - val_loss: 167.4373\n",
      "Epoch 693/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.1242 - val_loss: 158.4303\n",
      "Epoch 694/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 33.7129 - val_loss: 190.1641\n",
      "Epoch 695/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 39.7293 - val_loss: 171.7367\n",
      "Epoch 696/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 39.6434 - val_loss: 171.8922\n",
      "Epoch 697/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 42.8101 - val_loss: 159.3928\n",
      "Epoch 698/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.9427 - val_loss: 157.3831\n",
      "Epoch 699/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 35.7164 - val_loss: 167.0757\n",
      "Epoch 700/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 32.8344 - val_loss: 178.2327\n",
      "Epoch 701/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 38.0262 - val_loss: 157.9059\n",
      "Epoch 702/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 39.8989 - val_loss: 162.7703\n",
      "Epoch 703/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 36.6161 - val_loss: 175.8249\n",
      "Epoch 704/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.5945 - val_loss: 167.4396\n",
      "Epoch 705/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 41.9118 - val_loss: 161.8467\n",
      "Epoch 706/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 44.3430 - val_loss: 166.4264\n",
      "Epoch 707/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 38.8620 - val_loss: 164.5764\n",
      "Epoch 708/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 36.7658 - val_loss: 173.9755\n",
      "Epoch 709/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 34.2604 - val_loss: 161.5468\n",
      "Epoch 710/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 36.4444 - val_loss: 164.0018\n",
      "Epoch 711/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 34.8550 - val_loss: 168.7832\n",
      "Epoch 712/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 35.8848 - val_loss: 165.1566\n",
      "Epoch 713/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 35.4214 - val_loss: 162.5722\n",
      "Epoch 714/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 36.2673 - val_loss: 174.0095\n",
      "Epoch 715/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 35.2309 - val_loss: 166.6572\n",
      "Epoch 716/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 36.7370 - val_loss: 152.9105\n",
      "Epoch 717/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 40.4470 - val_loss: 168.9912\n",
      "Epoch 718/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 35.7446 - val_loss: 171.2881\n",
      "Epoch 719/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 36.9865 - val_loss: 163.1741\n",
      "Epoch 720/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 36.7906 - val_loss: 158.1249\n",
      "Epoch 721/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 38.5428 - val_loss: 176.7912\n",
      "Epoch 722/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 36.5882 - val_loss: 167.8141\n",
      "Epoch 723/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 35.6570 - val_loss: 176.6218\n",
      "Epoch 724/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 39.4794 - val_loss: 181.2511\n",
      "Epoch 725/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 39.4056 - val_loss: 160.5081\n",
      "Epoch 726/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 37.3361 - val_loss: 160.4809\n",
      "Epoch 727/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 35.2832 - val_loss: 160.7983\n",
      "Epoch 728/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 34.3325 - val_loss: 164.5776\n",
      "Epoch 729/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 34.8418 - val_loss: 179.2318\n",
      "Epoch 730/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 37.7614 - val_loss: 166.7460\n",
      "Epoch 731/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 33.4062 - val_loss: 170.2587\n",
      "Epoch 732/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 35.5305 - val_loss: 179.8761\n",
      "Epoch 733/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 37.3326 - val_loss: 160.5902\n",
      "Epoch 734/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 36.7125 - val_loss: 173.5786\n",
      "Epoch 735/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 35.1518 - val_loss: 158.1956\n",
      "Epoch 736/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 34.3367 - val_loss: 174.1116\n",
      "Epoch 737/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 39.0815 - val_loss: 174.9832\n",
      "Epoch 738/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 41.5421 - val_loss: 163.1643\n",
      "Epoch 739/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 39.2520 - val_loss: 169.6821\n",
      "Epoch 740/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 42.3303 - val_loss: 185.8972\n",
      "Epoch 741/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 35.6764 - val_loss: 167.6262\n",
      "Epoch 742/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 33.7227 - val_loss: 177.0006\n",
      "Epoch 743/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 35.8576 - val_loss: 152.0034\n",
      "Epoch 744/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 32.8597 - val_loss: 157.5709\n",
      "Epoch 745/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 35.2226 - val_loss: 162.8838\n",
      "Epoch 746/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 33.3834 - val_loss: 162.1908\n",
      "Epoch 747/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 36.0081 - val_loss: 156.1617\n",
      "Epoch 748/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 36.3299 - val_loss: 156.4197\n",
      "Epoch 749/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 38.1568 - val_loss: 169.1566\n",
      "Epoch 750/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 37.5757 - val_loss: 154.4582\n",
      "Epoch 751/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.4692 - val_loss: 188.5373\n",
      "Epoch 752/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 34.2468 - val_loss: 169.8965\n",
      "Epoch 753/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 35.5531 - val_loss: 147.9471\n",
      "Epoch 754/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 36.7659 - val_loss: 170.1649\n",
      "Epoch 755/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 40.7999 - val_loss: 163.5183\n",
      "Epoch 756/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 36.3766 - val_loss: 165.4540\n",
      "Epoch 757/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 36.5504 - val_loss: 165.4485\n",
      "Epoch 758/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.0847 - val_loss: 170.3827\n",
      "Epoch 759/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 37.5893 - val_loss: 170.5102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 760/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 35.7928 - val_loss: 167.8960\n",
      "Epoch 761/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 38.5664 - val_loss: 181.1943\n",
      "Epoch 762/10000\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 36.9499 - val_loss: 159.5181\n",
      "Epoch 763/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 37.9595 - val_loss: 171.8524\n",
      "Epoch 764/10000\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 34.0389 - val_loss: 161.4040\n",
      "Epoch 765/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 35.9788 - val_loss: 166.3823\n",
      "Epoch 766/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 34.8473 - val_loss: 174.6579\n",
      "Epoch 767/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 35.0226 - val_loss: 168.3314\n",
      "Epoch 768/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 33.9057 - val_loss: 171.2748\n",
      "Epoch 769/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 34.4737 - val_loss: 160.1668\n",
      "Epoch 770/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 32.8073 - val_loss: 158.9688\n",
      "Epoch 771/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.6922 - val_loss: 172.2659\n",
      "Epoch 772/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.5245 - val_loss: 170.7797\n",
      "Epoch 773/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 36.0335 - val_loss: 157.1582\n",
      "Epoch 774/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 35.4675 - val_loss: 177.1559\n",
      "Epoch 775/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 39.0092 - val_loss: 167.5453\n",
      "Epoch 776/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 35.6758 - val_loss: 160.4016\n",
      "Epoch 777/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 37.6467 - val_loss: 162.5099\n",
      "Epoch 778/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 37.3370 - val_loss: 169.8482\n",
      "Epoch 779/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 35.4322 - val_loss: 163.0159\n",
      "Epoch 780/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 34.0646 - val_loss: 174.7375\n",
      "Epoch 781/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 41.6428 - val_loss: 166.3953\n",
      "Epoch 782/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 33.6875 - val_loss: 165.7754\n",
      "Epoch 783/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 36.7894 - val_loss: 171.9951\n",
      "Epoch 784/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 32.9210 - val_loss: 167.7975\n",
      "Epoch 785/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 36.6594 - val_loss: 178.1043\n",
      "Epoch 786/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 35.2379 - val_loss: 185.1241\n",
      "Epoch 787/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.9968 - val_loss: 154.6483\n",
      "Epoch 788/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 35.1031 - val_loss: 172.7589\n",
      "Epoch 789/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 37.5045 - val_loss: 164.2484\n",
      "Epoch 790/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 37.2589 - val_loss: 159.0432\n",
      "Epoch 791/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.5965 - val_loss: 163.4277\n",
      "Epoch 792/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 34.8929 - val_loss: 158.4858\n",
      "Epoch 793/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 33.7834 - val_loss: 169.8834\n",
      "Epoch 794/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 35.3285 - val_loss: 161.0292\n",
      "Epoch 795/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 36.2738 - val_loss: 176.7475\n",
      "Epoch 796/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 33.5001 - val_loss: 175.1435\n",
      "Epoch 797/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 34.6057 - val_loss: 180.4139\n",
      "Epoch 798/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 34.6944 - val_loss: 165.6151\n",
      "Epoch 799/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 35.4443 - val_loss: 163.2317\n",
      "Epoch 800/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 37.3423 - val_loss: 173.6839\n",
      "Epoch 801/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 39.6945 - val_loss: 176.7226\n",
      "Epoch 802/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 34.5252 - val_loss: 169.7145\n",
      "Epoch 803/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 34.7489 - val_loss: 166.4117\n",
      "Epoch 804/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 34.7435 - val_loss: 173.2325\n",
      "Epoch 805/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.6014 - val_loss: 166.6130\n",
      "Epoch 806/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.4882 - val_loss: 168.1236\n",
      "Epoch 807/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 36.1232 - val_loss: 169.2735\n",
      "Epoch 808/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 33.8574 - val_loss: 166.9365\n",
      "Epoch 809/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.2775 - val_loss: 160.3435\n",
      "Epoch 810/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 34.2973 - val_loss: 174.6258\n",
      "Epoch 811/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 34.0764 - val_loss: 170.2439\n",
      "Epoch 812/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 35.4028 - val_loss: 170.9881\n",
      "Epoch 813/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.1580 - val_loss: 177.1446\n",
      "Epoch 814/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 43.7190 - val_loss: 170.6450\n",
      "Epoch 815/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 37.1566 - val_loss: 164.1665\n",
      "Epoch 816/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 38.0490 - val_loss: 164.3869\n",
      "Epoch 817/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 31.3756 - val_loss: 179.1276\n",
      "Epoch 818/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 33.7305 - val_loss: 163.5304\n",
      "Epoch 819/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 34.4177 - val_loss: 165.0479\n",
      "Epoch 820/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 35.2333 - val_loss: 158.0215\n",
      "Epoch 821/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 34.2176 - val_loss: 167.9580\n",
      "Epoch 822/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 31.3759 - val_loss: 162.8712\n",
      "Epoch 823/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 32.0904 - val_loss: 165.7844\n",
      "Epoch 824/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 35.1545 - val_loss: 170.8199\n",
      "Epoch 825/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 35.1549 - val_loss: 165.7889\n",
      "Epoch 826/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 32.9720 - val_loss: 164.4603\n",
      "Epoch 827/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 31.7435 - val_loss: 156.1313\n",
      "Epoch 828/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 35.5491 - val_loss: 164.8531\n",
      "Epoch 829/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 32.9542 - val_loss: 153.0968\n",
      "Epoch 830/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 34.0884 - val_loss: 163.7925\n",
      "Epoch 831/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 37.2536 - val_loss: 179.3438\n",
      "Epoch 832/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 32.5166 - val_loss: 164.6518\n",
      "Epoch 833/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 33.5854 - val_loss: 175.7714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 834/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 34.2995 - val_loss: 167.0724\n",
      "Epoch 835/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.5232 - val_loss: 167.4841\n",
      "Epoch 836/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 36.7828 - val_loss: 161.5094\n",
      "Epoch 837/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 34.3843 - val_loss: 175.2041\n",
      "Epoch 838/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 34.8362 - val_loss: 165.4975\n",
      "Epoch 839/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 33.5811 - val_loss: 171.9071\n",
      "Epoch 840/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 35.4636 - val_loss: 154.8435\n",
      "Epoch 841/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 32.8271 - val_loss: 168.5453\n",
      "Epoch 842/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 35.0846 - val_loss: 163.8601\n",
      "Epoch 843/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 31.7105 - val_loss: 173.8414\n",
      "Epoch 844/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 34.2845 - val_loss: 195.3395\n",
      "Epoch 845/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.3548 - val_loss: 163.3199\n",
      "Epoch 846/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 32.1110 - val_loss: 160.5148\n",
      "Epoch 847/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 36.2930 - val_loss: 160.1780\n",
      "Epoch 848/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 34.5280 - val_loss: 170.4009\n",
      "Epoch 849/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 31.6447 - val_loss: 154.8903\n",
      "Epoch 850/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 38.7688 - val_loss: 161.3390\n",
      "Epoch 851/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 32.9737 - val_loss: 170.7901\n",
      "Epoch 852/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 36.2916 - val_loss: 171.5544\n",
      "Epoch 853/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 37.9906 - val_loss: 170.2568\n",
      "Epoch 854/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 32.1246 - val_loss: 171.9266\n",
      "Epoch 855/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 32.4268 - val_loss: 176.9891\n",
      "Epoch 856/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 31.0468 - val_loss: 173.3633\n",
      "Epoch 857/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 34.1200 - val_loss: 158.3865\n",
      "Epoch 858/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 33.6807 - val_loss: 168.4326\n",
      "Epoch 859/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 33.5640 - val_loss: 170.1014\n",
      "Epoch 860/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 32.4373 - val_loss: 168.0883\n",
      "Epoch 861/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 35.7573 - val_loss: 173.8386\n",
      "Epoch 862/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 37.0792 - val_loss: 155.0846\n",
      "Epoch 863/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 33.2997 - val_loss: 172.4172\n",
      "Epoch 864/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 32.5343 - val_loss: 167.5297\n",
      "Epoch 865/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 31.9694 - val_loss: 168.9623\n",
      "Epoch 866/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 34.1589 - val_loss: 172.9770\n",
      "Epoch 867/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 33.1667 - val_loss: 161.6147\n",
      "Epoch 868/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 35.4659 - val_loss: 171.0752\n",
      "Epoch 869/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 33.1801 - val_loss: 157.6304\n",
      "Epoch 870/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 30.9823 - val_loss: 155.3235\n",
      "Epoch 871/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.7916 - val_loss: 159.0088\n",
      "Epoch 872/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 37.3093 - val_loss: 152.8478\n",
      "Epoch 873/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.8196 - val_loss: 163.6048\n",
      "Epoch 874/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 32.6974 - val_loss: 168.5668\n",
      "Epoch 875/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 36.2755 - val_loss: 165.4712\n",
      "Epoch 876/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 35.6660 - val_loss: 162.0298\n",
      "Epoch 877/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 36.1227 - val_loss: 170.7668\n",
      "Epoch 878/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.2611 - val_loss: 159.8018\n",
      "Epoch 879/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.1350 - val_loss: 166.4609\n",
      "Epoch 880/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.3515 - val_loss: 159.3979\n",
      "Epoch 881/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.8179 - val_loss: 158.3769\n",
      "Epoch 882/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.4608 - val_loss: 162.5200\n",
      "Epoch 883/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 34.1963 - val_loss: 175.6681\n",
      "Epoch 884/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 35.8678 - val_loss: 152.5728\n",
      "Epoch 885/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.1649 - val_loss: 161.0430\n",
      "Epoch 886/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 35.3895 - val_loss: 159.2037\n",
      "Epoch 887/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 32.2309 - val_loss: 168.0954\n",
      "Epoch 888/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 34.4286 - val_loss: 169.4433\n",
      "Epoch 889/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.2558 - val_loss: 167.6221\n",
      "Epoch 890/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 34.1382 - val_loss: 165.3472\n",
      "Epoch 891/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.1871 - val_loss: 180.1364\n",
      "Epoch 892/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 35.1228 - val_loss: 165.0595\n",
      "Epoch 893/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 35.6671 - val_loss: 166.6188\n",
      "Epoch 894/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.2016 - val_loss: 165.6723\n",
      "Epoch 895/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.3108 - val_loss: 153.3988\n",
      "Epoch 896/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.9882 - val_loss: 167.0097\n",
      "Epoch 897/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 32.2373 - val_loss: 177.8049\n",
      "Epoch 898/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.1104 - val_loss: 163.9977\n",
      "Epoch 899/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.3723 - val_loss: 171.1704\n",
      "Epoch 900/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 32.6802 - val_loss: 156.4012\n",
      "Epoch 901/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.8438 - val_loss: 156.2388\n",
      "Epoch 902/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.1682 - val_loss: 160.2584\n",
      "Epoch 903/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 35.5542 - val_loss: 160.5733\n",
      "Epoch 904/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 33.2003 - val_loss: 165.2948\n",
      "Epoch 905/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 31.5035 - val_loss: 157.3003\n",
      "Epoch 906/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 29.8999 - val_loss: 157.9177\n",
      "Epoch 907/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 31.0489 - val_loss: 169.7945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 908/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 34.1484 - val_loss: 163.6958\n",
      "Epoch 909/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 29.7371 - val_loss: 158.3439\n",
      "Epoch 910/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 33.2369 - val_loss: 182.6240\n",
      "Epoch 911/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.0241 - val_loss: 169.5390\n",
      "Epoch 912/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.6895 - val_loss: 158.1154\n",
      "Epoch 913/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 34.4096 - val_loss: 173.3261\n",
      "Epoch 914/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 33.8928 - val_loss: 169.1139\n",
      "Epoch 915/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.5677 - val_loss: 159.1751\n",
      "Epoch 916/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.6035 - val_loss: 169.8787\n",
      "Epoch 917/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.8580 - val_loss: 170.2716\n",
      "Epoch 918/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 34.2352 - val_loss: 163.2246\n",
      "Epoch 919/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 34.2611 - val_loss: 174.8662\n",
      "Epoch 920/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 33.6005 - val_loss: 164.8224\n",
      "Epoch 921/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.7472 - val_loss: 168.6476\n",
      "Epoch 922/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 31.7580 - val_loss: 158.8278\n",
      "Epoch 923/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 29.3658 - val_loss: 153.4783\n",
      "Epoch 924/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 33.1429 - val_loss: 162.8784\n",
      "Epoch 925/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 31.8318 - val_loss: 162.0396\n",
      "Epoch 926/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 32.7770 - val_loss: 168.2575\n",
      "Epoch 927/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 32.2446 - val_loss: 157.5954\n",
      "Epoch 928/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 32.7864 - val_loss: 159.5073\n",
      "Epoch 929/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 31.3830 - val_loss: 146.3345\n",
      "Epoch 930/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 33.6520 - val_loss: 163.3043\n",
      "Epoch 931/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.7102 - val_loss: 162.0378\n",
      "Epoch 932/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 32.6337 - val_loss: 166.9698\n",
      "Epoch 933/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 34.0437 - val_loss: 171.5976\n",
      "Epoch 934/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 33.2905 - val_loss: 164.3796\n",
      "Epoch 935/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 32.4146 - val_loss: 161.8785\n",
      "Epoch 936/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 29.4094 - val_loss: 170.6066\n",
      "Epoch 937/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 33.5740 - val_loss: 161.1072\n",
      "Epoch 938/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 32.9262 - val_loss: 172.7025\n",
      "Epoch 939/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 29.8509 - val_loss: 174.6689\n",
      "Epoch 940/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 35.6453 - val_loss: 171.7762\n",
      "Epoch 941/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 28.7993 - val_loss: 174.1501\n",
      "Epoch 942/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 32.1868 - val_loss: 161.0761\n",
      "Epoch 943/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 31.3864 - val_loss: 168.3971\n",
      "Epoch 944/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 33.2950 - val_loss: 165.4195\n",
      "Epoch 945/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 30.2417 - val_loss: 166.9338\n",
      "Epoch 946/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 29.6407 - val_loss: 164.2637\n",
      "Epoch 947/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.7498 - val_loss: 168.4499\n",
      "Epoch 948/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 29.0542 - val_loss: 166.6864\n",
      "Epoch 949/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 33.2767 - val_loss: 155.9080\n",
      "Epoch 950/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.9863 - val_loss: 158.5287\n",
      "Epoch 951/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 31.1839 - val_loss: 161.6238\n",
      "Epoch 952/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.0775 - val_loss: 159.5464\n",
      "Epoch 953/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 31.1606 - val_loss: 161.9553\n",
      "Epoch 954/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 32.9700 - val_loss: 166.0802\n",
      "Epoch 955/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 31.4120 - val_loss: 165.8047\n",
      "Epoch 956/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.1713 - val_loss: 168.0673\n",
      "Epoch 957/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 32.1310 - val_loss: 161.7678\n",
      "Epoch 958/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 30.1059 - val_loss: 164.5005\n",
      "Epoch 959/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 31.3157 - val_loss: 179.2027\n",
      "Epoch 960/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 31.3926 - val_loss: 161.1459\n",
      "Epoch 961/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 33.5519 - val_loss: 164.8053\n",
      "Epoch 962/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 32.2061 - val_loss: 158.8371\n",
      "Epoch 963/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 29.4052 - val_loss: 185.2311\n",
      "Epoch 964/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 30.9282 - val_loss: 156.2908\n",
      "Epoch 965/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 32.2667 - val_loss: 166.9434\n",
      "Epoch 966/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 35.1061 - val_loss: 159.5862\n",
      "Epoch 967/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.7631 - val_loss: 173.9822\n",
      "Epoch 968/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.6503 - val_loss: 167.6704\n",
      "Epoch 969/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 31.3034 - val_loss: 163.6463\n",
      "Epoch 970/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 33.0103 - val_loss: 162.7578\n",
      "Epoch 971/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 30.7095 - val_loss: 173.4532\n",
      "Epoch 972/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 33.3366 - val_loss: 164.8564\n",
      "Epoch 973/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 28.2808 - val_loss: 164.8547\n",
      "Epoch 974/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.4491 - val_loss: 160.2555\n",
      "Epoch 975/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.3860 - val_loss: 169.5008\n",
      "Epoch 976/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.7991 - val_loss: 155.6328\n",
      "Epoch 977/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.8858 - val_loss: 157.9017\n",
      "Epoch 978/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 30.1847 - val_loss: 156.1720\n",
      "Epoch 979/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 28.4169 - val_loss: 178.6669\n",
      "Epoch 980/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.8442 - val_loss: 158.2697\n",
      "Epoch 981/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 32.5443 - val_loss: 169.7818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 982/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 29.0293 - val_loss: 161.5946\n",
      "Epoch 983/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 32.0499 - val_loss: 166.0415\n",
      "Epoch 984/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 33.6562 - val_loss: 171.2438\n",
      "Epoch 985/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.3369 - val_loss: 169.8837\n",
      "Epoch 986/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 32.5665 - val_loss: 155.2494\n",
      "Epoch 987/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 31.5395 - val_loss: 163.9839\n",
      "Epoch 988/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 32.7838 - val_loss: 164.8706\n",
      "Epoch 989/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.7001 - val_loss: 168.8207\n",
      "Epoch 990/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 29.8674 - val_loss: 164.0220\n",
      "Epoch 991/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 27.9825 - val_loss: 172.6216\n",
      "Epoch 992/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 29.9278 - val_loss: 164.1232\n",
      "Epoch 993/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 30.6978 - val_loss: 168.9909\n",
      "Epoch 994/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 31.1393 - val_loss: 161.6003\n",
      "Epoch 995/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 30.5315 - val_loss: 155.9970\n",
      "Epoch 996/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 31.1218 - val_loss: 153.9501\n",
      "Epoch 997/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 29.5914 - val_loss: 166.4576\n",
      "Epoch 998/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 31.1441 - val_loss: 160.5165\n",
      "Epoch 999/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 32.2166 - val_loss: 155.7116\n",
      "Epoch 1000/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 32.2575 - val_loss: 153.2943\n",
      "Epoch 1001/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 31.9934 - val_loss: 156.0374\n",
      "Epoch 1002/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 29.9440 - val_loss: 155.2485\n",
      "Epoch 1003/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 30.7742 - val_loss: 149.5093\n",
      "Epoch 1004/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.0055 - val_loss: 161.0176\n",
      "Epoch 1005/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 29.7874 - val_loss: 151.7066\n",
      "Epoch 1006/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 33.8983 - val_loss: 157.9774\n",
      "Epoch 1007/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 33.3821 - val_loss: 156.9752\n",
      "Epoch 1008/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.9121 - val_loss: 157.9727\n",
      "Epoch 1009/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 30.0235 - val_loss: 155.0913\n",
      "Epoch 1010/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 30.1782 - val_loss: 163.4150\n",
      "Epoch 1011/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 31.7848 - val_loss: 166.3246\n",
      "Epoch 1012/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.2221 - val_loss: 165.1564\n",
      "Epoch 1013/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 31.5919 - val_loss: 160.9295\n",
      "Epoch 1014/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 30.7533 - val_loss: 168.5102\n",
      "Epoch 1015/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 30.3577 - val_loss: 150.9985\n",
      "Epoch 1016/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 28.7313 - val_loss: 159.2765\n",
      "Epoch 1017/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 27.9873 - val_loss: 153.2847\n",
      "Epoch 1018/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.6773 - val_loss: 177.1477\n",
      "Epoch 1019/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 34.1681 - val_loss: 161.5761\n",
      "Epoch 1020/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 26.1237 - val_loss: 165.6719\n",
      "Epoch 1021/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.4222 - val_loss: 159.0990\n",
      "Epoch 1022/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.1874 - val_loss: 155.8298\n",
      "Epoch 1023/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 31.0712 - val_loss: 156.9694\n",
      "Epoch 1024/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 30.3839 - val_loss: 174.3381\n",
      "Epoch 1025/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 29.8844 - val_loss: 159.5108\n",
      "Epoch 1026/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 29.2222 - val_loss: 160.6792\n",
      "Epoch 1027/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 31.3196 - val_loss: 154.2430\n",
      "Epoch 1028/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 29.2919 - val_loss: 158.8490\n",
      "Epoch 1029/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 32.8366 - val_loss: 165.2449\n",
      "Epoch 1030/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 29.0029 - val_loss: 153.2223\n",
      "Epoch 1031/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 30.9660 - val_loss: 148.7642\n",
      "Epoch 1032/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 30.1706 - val_loss: 155.2647\n",
      "Epoch 1033/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 31.0340 - val_loss: 159.8139\n",
      "Epoch 1034/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 30.5731 - val_loss: 173.8494\n",
      "Epoch 1035/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.9575 - val_loss: 175.2478\n",
      "Epoch 1036/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.0757 - val_loss: 161.4358\n",
      "Epoch 1037/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.8844 - val_loss: 159.7023\n",
      "Epoch 1038/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 29.7159 - val_loss: 159.5042\n",
      "Epoch 1039/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 29.1559 - val_loss: 160.3383\n",
      "Epoch 1040/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.8858 - val_loss: 164.9369\n",
      "Epoch 1041/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.7206 - val_loss: 168.2727\n",
      "Epoch 1042/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 27.7771 - val_loss: 161.6015\n",
      "Epoch 1043/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 31.5068 - val_loss: 167.3369\n",
      "Epoch 1044/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 31.1982 - val_loss: 161.8598\n",
      "Epoch 1045/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 28.4742 - val_loss: 159.4207\n",
      "Epoch 1046/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 28.1514 - val_loss: 152.6117\n",
      "Epoch 1047/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 31.8111 - val_loss: 144.4300\n",
      "Epoch 1048/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.6601 - val_loss: 148.0568\n",
      "Epoch 1049/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.4756 - val_loss: 160.0767\n",
      "Epoch 1050/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 29.2974 - val_loss: 154.0394\n",
      "Epoch 1051/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 29.7838 - val_loss: 165.0826\n",
      "Epoch 1052/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.9398 - val_loss: 165.5226\n",
      "Epoch 1053/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 31.6334 - val_loss: 155.6680\n",
      "Epoch 1054/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 28.5098 - val_loss: 162.0365\n",
      "Epoch 1055/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 56us/step - loss: 29.6631 - val_loss: 175.5812\n",
      "Epoch 1056/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 31.1676 - val_loss: 161.9228\n",
      "Epoch 1057/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 29.1285 - val_loss: 155.9719\n",
      "Epoch 1058/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 30.6390 - val_loss: 150.1304\n",
      "Epoch 1059/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 31.8908 - val_loss: 157.2935\n",
      "Epoch 1060/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.4153 - val_loss: 159.9417\n",
      "Epoch 1061/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.5559 - val_loss: 154.6860\n",
      "Epoch 1062/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 28.7710 - val_loss: 160.0201\n",
      "Epoch 1063/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.7418 - val_loss: 159.0898\n",
      "Epoch 1064/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 26.0404 - val_loss: 162.4761\n",
      "Epoch 1065/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 29.9897 - val_loss: 157.7009\n",
      "Epoch 1066/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 28.6858 - val_loss: 157.5919\n",
      "Epoch 1067/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.4044 - val_loss: 166.7285\n",
      "Epoch 1068/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.4357 - val_loss: 168.1116\n",
      "Epoch 1069/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.5504 - val_loss: 159.9782\n",
      "Epoch 1070/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.7404 - val_loss: 167.2593\n",
      "Epoch 1071/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 32.1655 - val_loss: 165.9583\n",
      "Epoch 1072/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.9021 - val_loss: 167.1877\n",
      "Epoch 1073/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 28.0310 - val_loss: 164.2200\n",
      "Epoch 1074/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.7359 - val_loss: 159.8208\n",
      "Epoch 1075/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 33.9249 - val_loss: 171.0017\n",
      "Epoch 1076/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 28.9087 - val_loss: 150.6573\n",
      "Epoch 1077/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 29.6105 - val_loss: 162.5566\n",
      "Epoch 1078/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 27.2891 - val_loss: 160.3489\n",
      "Epoch 1079/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 31.2983 - val_loss: 145.7067\n",
      "Epoch 1080/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 29.4643 - val_loss: 168.9246\n",
      "Epoch 1081/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 31.5540 - val_loss: 151.6309\n",
      "Epoch 1082/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 28.7936 - val_loss: 161.9487\n",
      "Epoch 1083/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 27.9699 - val_loss: 156.6710\n",
      "Epoch 1084/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.5615 - val_loss: 167.3827\n",
      "Epoch 1085/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 27.1764 - val_loss: 171.6908\n",
      "Epoch 1086/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 28.8161 - val_loss: 164.3495\n",
      "Epoch 1087/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 27.9202 - val_loss: 161.1523\n",
      "Epoch 1088/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.7839 - val_loss: 170.2209\n",
      "Epoch 1089/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.4300 - val_loss: 159.4025\n",
      "Epoch 1090/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.9857 - val_loss: 174.5062\n",
      "Epoch 1091/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.3384 - val_loss: 152.6287\n",
      "Epoch 1092/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.9833 - val_loss: 165.5795\n",
      "Epoch 1093/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 27.8883 - val_loss: 162.0332\n",
      "Epoch 1094/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 28.7341 - val_loss: 160.5444\n",
      "Epoch 1095/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 29.0258 - val_loss: 160.6555\n",
      "Epoch 1096/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.9199 - val_loss: 157.3826\n",
      "Epoch 1097/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.7035 - val_loss: 158.2789\n",
      "Epoch 1098/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 28.2470 - val_loss: 158.8944\n",
      "Epoch 1099/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 29.2370 - val_loss: 148.9550\n",
      "Epoch 1100/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.0311 - val_loss: 166.0404\n",
      "Epoch 1101/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.8140 - val_loss: 173.9221\n",
      "Epoch 1102/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.1633 - val_loss: 149.5190\n",
      "Epoch 1103/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.3400 - val_loss: 160.1560\n",
      "Epoch 1104/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.7086 - val_loss: 160.8264\n",
      "Epoch 1105/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.0163 - val_loss: 166.3698\n",
      "Epoch 1106/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.4190 - val_loss: 169.7797\n",
      "Epoch 1107/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.8755 - val_loss: 176.9790\n",
      "Epoch 1108/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.5102 - val_loss: 162.9209\n",
      "Epoch 1109/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.2418 - val_loss: 163.0940\n",
      "Epoch 1110/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.7083 - val_loss: 159.6151\n",
      "Epoch 1111/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.3912 - val_loss: 172.1167\n",
      "Epoch 1112/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.5223 - val_loss: 160.7522\n",
      "Epoch 1113/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 27.7257 - val_loss: 165.0175\n",
      "Epoch 1114/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.6020 - val_loss: 173.5566\n",
      "Epoch 1115/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 32.6952 - val_loss: 176.4507\n",
      "Epoch 1116/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.3098 - val_loss: 164.1073\n",
      "Epoch 1117/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 31.6103 - val_loss: 180.6353\n",
      "Epoch 1118/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.8228 - val_loss: 166.8702\n",
      "Epoch 1119/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 30.4536 - val_loss: 171.1116\n",
      "Epoch 1120/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.9411 - val_loss: 169.0523\n",
      "Epoch 1121/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.4800 - val_loss: 169.0412\n",
      "Epoch 1122/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.6340 - val_loss: 169.0053\n",
      "Epoch 1123/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.9385 - val_loss: 161.8131\n",
      "Epoch 1124/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 28.9980 - val_loss: 159.5424\n",
      "Epoch 1125/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 28.3809 - val_loss: 157.1857\n",
      "Epoch 1126/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 28.9596 - val_loss: 162.8928\n",
      "Epoch 1127/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 27.6077 - val_loss: 153.7108\n",
      "Epoch 1128/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 54us/step - loss: 27.8423 - val_loss: 156.0410\n",
      "Epoch 1129/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 25.6029 - val_loss: 167.3893\n",
      "Epoch 1130/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 33.6337 - val_loss: 160.5131\n",
      "Epoch 1131/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 28.4555 - val_loss: 155.5225\n",
      "Epoch 1132/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.8968 - val_loss: 163.3882\n",
      "Epoch 1133/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.6887 - val_loss: 173.8842\n",
      "Epoch 1134/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.9482 - val_loss: 178.8019\n",
      "Epoch 1135/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.5017 - val_loss: 164.1406\n",
      "Epoch 1136/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 26.4828 - val_loss: 166.5105\n",
      "Epoch 1137/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.8916 - val_loss: 161.0445\n",
      "Epoch 1138/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 25.0042 - val_loss: 164.8050\n",
      "Epoch 1139/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.3162 - val_loss: 164.1507\n",
      "Epoch 1140/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.0234 - val_loss: 168.0366\n",
      "Epoch 1141/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 27.9270 - val_loss: 158.1704\n",
      "Epoch 1142/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.3145 - val_loss: 161.7031\n",
      "Epoch 1143/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.5972 - val_loss: 160.2084\n",
      "Epoch 1144/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.8232 - val_loss: 168.1848\n",
      "Epoch 1145/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.8614 - val_loss: 164.2043\n",
      "Epoch 1146/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.2043 - val_loss: 160.8962\n",
      "Epoch 1147/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 26.0026 - val_loss: 159.6368\n",
      "Epoch 1148/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.1075 - val_loss: 168.6967\n",
      "Epoch 1149/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 24.3737 - val_loss: 164.1470\n",
      "Epoch 1150/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 26.5352 - val_loss: 166.1365\n",
      "Epoch 1151/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.7335 - val_loss: 175.5871\n",
      "Epoch 1152/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.7917 - val_loss: 166.2450\n",
      "Epoch 1153/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.0764 - val_loss: 165.4220\n",
      "Epoch 1154/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 27.5808 - val_loss: 163.1600\n",
      "Epoch 1155/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 27.2834 - val_loss: 166.0732\n",
      "Epoch 1156/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 27.1723 - val_loss: 166.2303\n",
      "Epoch 1157/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.6845 - val_loss: 159.3071\n",
      "Epoch 1158/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 28.5123 - val_loss: 160.4994\n",
      "Epoch 1159/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 25.5584 - val_loss: 161.9955\n",
      "Epoch 1160/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 27.3535 - val_loss: 163.8179\n",
      "Epoch 1161/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 26.7249 - val_loss: 156.1676\n",
      "Epoch 1162/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 26.6148 - val_loss: 160.5868\n",
      "Epoch 1163/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.7628 - val_loss: 161.6291\n",
      "Epoch 1164/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 30.9939 - val_loss: 171.1256\n",
      "Epoch 1165/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 26.2334 - val_loss: 155.1425\n",
      "Epoch 1166/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 28.3243 - val_loss: 162.1437\n",
      "Epoch 1167/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 27.6933 - val_loss: 165.3006\n",
      "Epoch 1168/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 27.4840 - val_loss: 163.8552\n",
      "Epoch 1169/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 26.0790 - val_loss: 171.8686\n",
      "Epoch 1170/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 30.5207 - val_loss: 166.0121\n",
      "Epoch 1171/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 33.1929 - val_loss: 154.1085\n",
      "Epoch 1172/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 28.5049 - val_loss: 163.2437\n",
      "Epoch 1173/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 26.0953 - val_loss: 161.8484\n",
      "Epoch 1174/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 28.1618 - val_loss: 154.3788\n",
      "Epoch 1175/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.8915 - val_loss: 154.1191\n",
      "Epoch 1176/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 28.0601 - val_loss: 158.5197\n",
      "Epoch 1177/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 25.2994 - val_loss: 153.8041\n",
      "Epoch 1178/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 27.8220 - val_loss: 162.4512\n",
      "Epoch 1179/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.7138 - val_loss: 166.7633\n",
      "Epoch 1180/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 26.8070 - val_loss: 159.0001\n",
      "Epoch 1181/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.2320 - val_loss: 158.5818\n",
      "Epoch 1182/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.4155 - val_loss: 163.5511\n",
      "Epoch 1183/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 26.9885 - val_loss: 167.5823\n",
      "Epoch 1184/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 26.8972 - val_loss: 173.6248\n",
      "Epoch 1185/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.0342 - val_loss: 167.1081\n",
      "Epoch 1186/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.0627 - val_loss: 170.0785\n",
      "Epoch 1187/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 26.2898 - val_loss: 160.8246\n",
      "Epoch 1188/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 29.7375 - val_loss: 172.4587\n",
      "Epoch 1189/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 27.4323 - val_loss: 157.4887\n",
      "Epoch 1190/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.8679 - val_loss: 159.9043\n",
      "Epoch 1191/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 29.3995 - val_loss: 163.3105\n",
      "Epoch 1192/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 27.2815 - val_loss: 170.7044\n",
      "Epoch 1193/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 26.6357 - val_loss: 154.5668\n",
      "Epoch 1194/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 27.8322 - val_loss: 159.0799\n",
      "Epoch 1195/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 26.4897 - val_loss: 155.6764\n",
      "Epoch 1196/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.2651 - val_loss: 154.6502\n",
      "Epoch 1197/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 25.8609 - val_loss: 167.3366\n",
      "Epoch 1198/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 26.1669 - val_loss: 160.7981\n",
      "Epoch 1199/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 28.0015 - val_loss: 167.6970\n",
      "Epoch 1200/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 29.5386 - val_loss: 167.3676\n",
      "Epoch 1201/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 56us/step - loss: 26.3804 - val_loss: 166.6947\n",
      "Epoch 1202/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 26.5542 - val_loss: 164.5873\n",
      "Epoch 1203/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 25.3218 - val_loss: 161.2953\n",
      "Epoch 1204/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 25.7123 - val_loss: 162.5221\n",
      "Epoch 1205/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 25.1855 - val_loss: 166.4965\n",
      "Epoch 1206/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 24.7062 - val_loss: 177.1660\n",
      "Epoch 1207/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 30.9179 - val_loss: 159.3061\n",
      "Epoch 1208/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 28.3333 - val_loss: 167.1760\n",
      "Epoch 1209/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 26.6087 - val_loss: 160.9786\n",
      "Epoch 1210/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 26.5289 - val_loss: 164.8800\n",
      "Epoch 1211/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 25.9161 - val_loss: 169.7262\n",
      "Epoch 1212/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 28.5069 - val_loss: 163.4265\n",
      "Epoch 1213/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 29.6380 - val_loss: 159.6447\n",
      "Epoch 1214/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.8372 - val_loss: 166.3571\n",
      "Epoch 1215/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 25.9251 - val_loss: 160.9321\n",
      "Epoch 1216/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.9961 - val_loss: 167.8479\n",
      "Epoch 1217/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 24.4641 - val_loss: 180.9101\n",
      "Epoch 1218/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 25.2905 - val_loss: 178.9752\n",
      "Epoch 1219/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 27.9551 - val_loss: 168.2492\n",
      "Epoch 1220/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 25.2971 - val_loss: 155.0857\n",
      "Epoch 1221/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 27.9887 - val_loss: 180.8264\n",
      "Epoch 1222/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 25.9679 - val_loss: 172.0556\n",
      "Epoch 1223/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.1025 - val_loss: 174.7586\n",
      "Epoch 1224/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 27.9574 - val_loss: 163.6076\n",
      "Epoch 1225/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 25.4885 - val_loss: 168.0951\n",
      "Epoch 1226/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 26.6806 - val_loss: 170.4062\n",
      "Epoch 1227/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 26.0736 - val_loss: 154.0390\n",
      "Epoch 1228/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 26.4716 - val_loss: 169.6165\n",
      "Epoch 1229/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 27.4653 - val_loss: 168.4049\n",
      "Epoch 1230/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 29.2640 - val_loss: 159.9829\n",
      "Epoch 1231/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 27.3306 - val_loss: 168.2717\n",
      "Epoch 1232/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 25.3761 - val_loss: 164.1099\n",
      "Epoch 1233/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 27.2403 - val_loss: 169.6963\n",
      "Epoch 1234/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 27.4097 - val_loss: 166.0713\n",
      "Epoch 1235/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.0521 - val_loss: 168.4059\n",
      "Epoch 1236/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.6550 - val_loss: 163.3323\n",
      "Epoch 1237/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 25.8427 - val_loss: 160.1541\n",
      "Epoch 1238/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.5739 - val_loss: 171.5592\n",
      "Epoch 1239/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.8622 - val_loss: 160.9702\n",
      "Epoch 1240/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.9886 - val_loss: 161.0647\n",
      "Epoch 1241/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 27.8494 - val_loss: 169.5886\n",
      "Epoch 1242/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 26.8490 - val_loss: 163.3217\n",
      "Epoch 1243/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 24.9854 - val_loss: 171.2062\n",
      "Epoch 1244/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.5211 - val_loss: 166.7646\n",
      "Epoch 1245/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 26.3654 - val_loss: 164.1922\n",
      "Epoch 1246/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 26.0815 - val_loss: 168.7566\n",
      "Epoch 1247/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 27.2350 - val_loss: 156.4267\n",
      "Epoch 1248/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.6225 - val_loss: 174.8829\n",
      "Epoch 1249/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 29.8260 - val_loss: 164.7534\n",
      "Epoch 1250/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 25.1905 - val_loss: 157.8892\n",
      "Epoch 1251/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 26.7626 - val_loss: 158.3146\n",
      "Epoch 1252/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 27.6706 - val_loss: 159.7790\n",
      "Epoch 1253/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 24.6674 - val_loss: 159.6650\n",
      "Epoch 1254/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 26.0432 - val_loss: 162.3027\n",
      "Epoch 1255/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 28.2895 - val_loss: 159.9886\n",
      "Epoch 1256/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 25.6783 - val_loss: 163.2464\n",
      "Epoch 1257/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 25.9280 - val_loss: 158.5950\n",
      "Epoch 1258/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 26.9795 - val_loss: 165.2817\n",
      "Epoch 1259/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 27.3771 - val_loss: 161.2543\n",
      "Epoch 1260/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 25.1558 - val_loss: 158.1414\n",
      "Epoch 1261/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 25.6013 - val_loss: 164.7218\n",
      "Epoch 1262/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 25.3180 - val_loss: 169.2203\n",
      "Epoch 1263/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 25.9561 - val_loss: 188.6887\n",
      "Epoch 1264/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 28.7968 - val_loss: 176.7401\n",
      "Epoch 1265/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 27.7185 - val_loss: 162.6419\n",
      "Epoch 1266/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 26.3924 - val_loss: 174.2605\n",
      "Epoch 1267/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 27.8342 - val_loss: 177.1954\n",
      "Epoch 1268/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.5175 - val_loss: 155.4825\n",
      "Epoch 1269/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 25.2703 - val_loss: 171.2796\n",
      "Epoch 1270/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 26.7017 - val_loss: 168.3607\n",
      "Epoch 1271/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 26.4454 - val_loss: 175.2863\n",
      "Epoch 1272/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 28.3634 - val_loss: 175.4670\n",
      "Epoch 1273/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 26.0652 - val_loss: 162.7371\n",
      "Epoch 1274/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 65us/step - loss: 29.1293 - val_loss: 163.1379\n",
      "Epoch 1275/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 27.0438 - val_loss: 175.1028\n",
      "Epoch 1276/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 25.9648 - val_loss: 154.3609\n",
      "Epoch 1277/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 26.9334 - val_loss: 157.5178\n",
      "Epoch 1278/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 24.5985 - val_loss: 162.9792\n",
      "Epoch 1279/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 23.9441 - val_loss: 169.9210\n",
      "Epoch 1280/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 26.9838 - val_loss: 163.0979\n",
      "Epoch 1281/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 26.5713 - val_loss: 171.0290\n",
      "Epoch 1282/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 25.8997 - val_loss: 156.5781\n",
      "Epoch 1283/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 26.9811 - val_loss: 153.0344\n",
      "Epoch 1284/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 29.4362 - val_loss: 161.6461\n",
      "Epoch 1285/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 27.7137 - val_loss: 160.5419\n",
      "Epoch 1286/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 24.6773 - val_loss: 160.5724\n",
      "Epoch 1287/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 27.2183 - val_loss: 192.1979\n",
      "Epoch 1288/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 26.6970 - val_loss: 155.6305\n",
      "Epoch 1289/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 26.9524 - val_loss: 166.4316\n",
      "Epoch 1290/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 25.0064 - val_loss: 173.9812\n",
      "Epoch 1291/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 27.3179 - val_loss: 162.1715\n",
      "Epoch 1292/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 28.2693 - val_loss: 164.9902\n",
      "Epoch 1293/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 26.0394 - val_loss: 157.3388\n",
      "Epoch 1294/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 26.3385 - val_loss: 161.8071\n",
      "Epoch 1295/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 26.7149 - val_loss: 173.4063\n",
      "Epoch 1296/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 25.8390 - val_loss: 163.7980\n",
      "Epoch 1297/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 25.2071 - val_loss: 159.7227\n",
      "Epoch 1298/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 26.1253 - val_loss: 163.0449\n",
      "Epoch 1299/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 30.4735 - val_loss: 157.7526\n",
      "Epoch 1300/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.7760 - val_loss: 171.8380\n",
      "Epoch 1301/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.6512 - val_loss: 161.4381\n",
      "Epoch 1302/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.0829 - val_loss: 163.8411\n",
      "Epoch 1303/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 23.4053 - val_loss: 172.3151\n",
      "Epoch 1304/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 25.3046 - val_loss: 157.9781\n",
      "Epoch 1305/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 23.9960 - val_loss: 163.1090\n",
      "Epoch 1306/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.1961 - val_loss: 166.7895\n",
      "Epoch 1307/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.1650 - val_loss: 170.4850\n",
      "Epoch 1308/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 27.3729 - val_loss: 165.1017\n",
      "Epoch 1309/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 24.2573 - val_loss: 181.2649\n",
      "Epoch 1310/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 27.4605 - val_loss: 156.1599\n",
      "Epoch 1311/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 26.2763 - val_loss: 173.7943\n",
      "Epoch 1312/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 26.0953 - val_loss: 169.0012\n",
      "Epoch 1313/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 24.4671 - val_loss: 179.1852\n",
      "Epoch 1314/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 25.4606 - val_loss: 158.7405\n",
      "Epoch 1315/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 26.3543 - val_loss: 174.2142\n",
      "Epoch 1316/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.5047 - val_loss: 155.3174\n",
      "Epoch 1317/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 24.6871 - val_loss: 175.6314\n",
      "Epoch 1318/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 30.1410 - val_loss: 161.2855\n",
      "Epoch 1319/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.0579 - val_loss: 158.1265\n",
      "Epoch 1320/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.5746 - val_loss: 174.2384\n",
      "Epoch 1321/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 27.9827 - val_loss: 164.3935\n",
      "Epoch 1322/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 24.8320 - val_loss: 159.3639\n",
      "Epoch 1323/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 26.7027 - val_loss: 165.9845\n",
      "Epoch 1324/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 24.0557 - val_loss: 159.6383\n",
      "Epoch 1325/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 27.4077 - val_loss: 183.2091\n",
      "Epoch 1326/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 23.7729 - val_loss: 165.1086\n",
      "Epoch 1327/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.1777 - val_loss: 175.1874\n",
      "Epoch 1328/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.9167 - val_loss: 171.8200\n",
      "Epoch 1329/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 25.0261 - val_loss: 156.6586\n",
      "Epoch 1330/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 26.6947 - val_loss: 165.8923\n",
      "Epoch 1331/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 27.8144 - val_loss: 157.5903\n",
      "Epoch 1332/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.0598 - val_loss: 165.3678\n",
      "Epoch 1333/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.0376 - val_loss: 170.4071\n",
      "Epoch 1334/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 23.6926 - val_loss: 173.1869\n",
      "Epoch 1335/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 25.5470 - val_loss: 159.8771\n",
      "Epoch 1336/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.6319 - val_loss: 153.5152\n",
      "Epoch 1337/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.0439 - val_loss: 168.1038\n",
      "Epoch 1338/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.6819 - val_loss: 170.3648\n",
      "Epoch 1339/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 28.1610 - val_loss: 166.5699\n",
      "Epoch 1340/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 27.1613 - val_loss: 165.4943\n",
      "Epoch 1341/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 27.0057 - val_loss: 149.2596\n",
      "Epoch 1342/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 24.8337 - val_loss: 164.8938\n",
      "Epoch 1343/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.9780 - val_loss: 169.4455\n",
      "Epoch 1344/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 26.4067 - val_loss: 177.4605\n",
      "Epoch 1345/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.7469 - val_loss: 165.3680\n",
      "Epoch 1346/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.2217 - val_loss: 162.9504\n",
      "Epoch 1347/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.9319 - val_loss: 166.3684\n",
      "Epoch 1348/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.4731 - val_loss: 168.1251\n",
      "Epoch 1349/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 24.9431 - val_loss: 169.1067\n",
      "Epoch 1350/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.2818 - val_loss: 170.9287\n",
      "Epoch 1351/10000\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 26.4171 - val_loss: 163.2422\n",
      "Epoch 1352/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.7630 - val_loss: 164.6159\n",
      "Epoch 1353/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.6050 - val_loss: 160.4768\n",
      "Epoch 1354/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 23.9330 - val_loss: 172.7292\n",
      "Epoch 1355/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.1631 - val_loss: 159.9767\n",
      "Epoch 1356/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 28.3563 - val_loss: 165.2845\n",
      "Epoch 1357/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 27.2896 - val_loss: 159.2603\n",
      "Epoch 1358/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 28.3026 - val_loss: 160.1096\n",
      "Epoch 1359/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 27.1755 - val_loss: 168.3883\n",
      "Epoch 1360/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 23.3306 - val_loss: 167.3317\n",
      "Epoch 1361/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 23.8276 - val_loss: 163.8487\n",
      "Epoch 1362/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 24.7653 - val_loss: 164.3786\n",
      "Epoch 1363/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 24.3835 - val_loss: 163.6502\n",
      "Epoch 1364/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 24.5211 - val_loss: 181.4850\n",
      "Epoch 1365/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 23.0837 - val_loss: 163.3159\n",
      "Epoch 1366/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.3597 - val_loss: 173.8911\n",
      "Epoch 1367/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 26.6271 - val_loss: 164.0218\n",
      "Epoch 1368/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.4371 - val_loss: 161.4849\n",
      "Epoch 1369/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.5677 - val_loss: 160.1292\n",
      "Epoch 1370/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 27.8421 - val_loss: 161.6012\n",
      "Epoch 1371/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 24.4663 - val_loss: 165.8007\n",
      "Epoch 1372/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 26.4416 - val_loss: 173.5511\n",
      "Epoch 1373/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 23.6013 - val_loss: 157.1078\n",
      "Epoch 1374/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 26.0328 - val_loss: 174.0488\n",
      "Epoch 1375/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 23.1524 - val_loss: 161.9175\n",
      "Epoch 1376/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.0768 - val_loss: 170.1245\n",
      "Epoch 1377/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.0456 - val_loss: 170.7949\n",
      "Epoch 1378/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 29.0478 - val_loss: 169.7142\n",
      "Epoch 1379/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.2185 - val_loss: 162.1464\n",
      "Epoch 1380/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 26.3758 - val_loss: 162.1839\n",
      "Epoch 1381/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 25.3605 - val_loss: 162.1466\n",
      "Epoch 1382/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 24.0182 - val_loss: 156.3465\n",
      "Epoch 1383/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 25.2544 - val_loss: 157.3940\n",
      "Epoch 1384/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 24.4491 - val_loss: 172.3006\n",
      "Epoch 1385/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.6958 - val_loss: 163.4968\n",
      "Epoch 1386/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.6248 - val_loss: 162.2796\n",
      "Epoch 1387/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 26.1270 - val_loss: 157.2827\n",
      "Epoch 1388/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.3822 - val_loss: 176.6842\n",
      "Epoch 1389/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.0968 - val_loss: 168.0944\n",
      "Epoch 1390/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.1717 - val_loss: 163.5879\n",
      "Epoch 1391/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 23.5910 - val_loss: 162.1869\n",
      "Epoch 1392/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.3758 - val_loss: 163.8528\n",
      "Epoch 1393/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 24.8466 - val_loss: 170.7707\n",
      "Epoch 1394/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 28.1933 - val_loss: 163.4419\n",
      "Epoch 1395/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 23.7263 - val_loss: 166.5427\n",
      "Epoch 1396/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.4437 - val_loss: 167.1101\n",
      "Epoch 1397/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 25.6692 - val_loss: 158.7016\n",
      "Epoch 1398/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 22.8503 - val_loss: 160.3033\n",
      "Epoch 1399/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.5082 - val_loss: 170.9815\n",
      "Epoch 1400/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 24.5180 - val_loss: 165.6915\n",
      "Epoch 1401/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 26.7762 - val_loss: 158.2839\n",
      "Epoch 1402/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 26.2236 - val_loss: 162.3555\n",
      "Epoch 1403/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 26.5580 - val_loss: 161.0446\n",
      "Epoch 1404/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 24.4688 - val_loss: 167.5011\n",
      "Epoch 1405/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 28.3284 - val_loss: 172.7327\n",
      "Epoch 1406/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 24.9534 - val_loss: 174.3986\n",
      "Epoch 1407/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 24.2265 - val_loss: 164.3634\n",
      "Epoch 1408/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 25.8808 - val_loss: 167.5854\n",
      "Epoch 1409/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 22.8390 - val_loss: 162.7981\n",
      "Epoch 1410/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.5017 - val_loss: 171.9799\n",
      "Epoch 1411/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 29.6618 - val_loss: 165.8483\n",
      "Epoch 1412/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 24.7654 - val_loss: 170.0577\n",
      "Epoch 1413/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 23.81 - 1s 97us/step - loss: 24.1372 - val_loss: 167.3807\n",
      "Epoch 1414/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 24.2870 - val_loss: 161.6996\n",
      "Epoch 1415/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 23.6057 - val_loss: 160.8542\n",
      "Epoch 1416/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 27.2951 - val_loss: 172.5728\n",
      "Epoch 1417/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 23.1442 - val_loss: 175.1310\n",
      "Epoch 1418/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 23.3031 - val_loss: 167.4383\n",
      "Epoch 1419/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 25.4193 - val_loss: 166.5582\n",
      "Epoch 1420/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 67us/step - loss: 29.1385 - val_loss: 162.2636\n",
      "Epoch 1421/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 25.1452 - val_loss: 165.4252\n",
      "Epoch 1422/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 26.7177 - val_loss: 157.7417\n",
      "Epoch 1423/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 24.7176 - val_loss: 169.2529\n",
      "Epoch 1424/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 27.3921 - val_loss: 162.7672\n",
      "Epoch 1425/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 25.3037 - val_loss: 160.9278\n",
      "Epoch 1426/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 25.1838 - val_loss: 161.6243\n",
      "Epoch 1427/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 22.4094 - val_loss: 176.4453\n",
      "Epoch 1428/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 24.4795 - val_loss: 167.6637\n",
      "Epoch 1429/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 26.0686 - val_loss: 163.2434\n",
      "Epoch 1430/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 25.7040 - val_loss: 179.1187\n",
      "Epoch 1431/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 24.6841 - val_loss: 161.7165\n",
      "Epoch 1432/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 25.1756 - val_loss: 175.8338\n",
      "Epoch 1433/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 25.8739 - val_loss: 162.9927\n",
      "Epoch 1434/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 25.7930 - val_loss: 162.2030\n",
      "Epoch 1435/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 24.2350 - val_loss: 168.3661\n",
      "Epoch 1436/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 23.3234 - val_loss: 171.5587\n",
      "Epoch 1437/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 23.2330 - val_loss: 160.5202\n",
      "Epoch 1438/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 24.1464 - val_loss: 161.5686\n",
      "Epoch 1439/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 22.4683 - val_loss: 163.3821\n",
      "Epoch 1440/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 26.6567 - val_loss: 190.0831\n",
      "Epoch 1441/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 24.8268 - val_loss: 161.1488\n",
      "Epoch 1442/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 23.8028 - val_loss: 162.7389\n",
      "Epoch 1443/10000\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 27.0104 - val_loss: 177.5956\n",
      "Epoch 1444/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 23.9593 - val_loss: 168.7253\n",
      "Epoch 1445/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 26.2373 - val_loss: 175.7904\n",
      "Epoch 1446/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 23.3377 - val_loss: 163.1280\n",
      "Epoch 1447/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 27.7713 - val_loss: 159.3787\n",
      "Epoch 1448/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 24.9469 - val_loss: 157.1384\n",
      "Epoch 1449/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 23.7015 - val_loss: 171.5049\n",
      "Epoch 1450/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 24.0186 - val_loss: 158.3773\n",
      "Epoch 1451/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 36.0907 - val_loss: 174.5750\n",
      "Epoch 1452/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 24.1802 - val_loss: 162.1260\n",
      "Epoch 1453/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 24.6963 - val_loss: 167.6799\n",
      "Epoch 1454/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 25.3471 - val_loss: 167.6472\n",
      "Epoch 1455/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 24.6791 - val_loss: 161.0600\n",
      "Epoch 1456/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 27.7424 - val_loss: 176.2934\n",
      "Epoch 1457/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 23.4400 - val_loss: 154.1801\n",
      "Epoch 1458/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 24.4491 - val_loss: 157.7938\n",
      "Epoch 1459/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 24.9652 - val_loss: 169.2000\n",
      "Epoch 1460/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 27.4979 - val_loss: 168.3002\n",
      "Epoch 1461/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 26.1809 - val_loss: 169.1539\n",
      "Epoch 1462/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 25.4380 - val_loss: 164.8583\n",
      "Epoch 1463/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 24.5264 - val_loss: 167.2623\n",
      "Epoch 1464/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 22.6064 - val_loss: 182.7449\n",
      "Epoch 1465/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 23.4109 - val_loss: 170.2899\n",
      "Epoch 1466/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 26.5142 - val_loss: 164.6683\n",
      "Epoch 1467/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 24.5995 - val_loss: 177.6125\n",
      "Epoch 1468/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 22.8325 - val_loss: 171.1703\n",
      "Epoch 1469/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 23.2420 - val_loss: 172.8876\n",
      "Epoch 1470/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 27.8261 - val_loss: 164.1059\n",
      "Epoch 1471/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 25.4929 - val_loss: 164.2513\n",
      "Epoch 1472/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 24.9165 - val_loss: 164.0744\n",
      "Epoch 1473/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 24.5017 - val_loss: 158.1143\n",
      "Epoch 1474/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 22.1867 - val_loss: 166.3148\n",
      "Epoch 1475/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 28.8664 - val_loss: 158.9771\n",
      "Epoch 1476/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 25.6598 - val_loss: 163.4739\n",
      "Epoch 1477/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 23.2243 - val_loss: 162.6475\n",
      "Epoch 1478/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 23.6578 - val_loss: 172.6858\n",
      "Epoch 1479/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 23.9106 - val_loss: 181.4179\n",
      "Epoch 1480/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 23.3613 - val_loss: 182.3116\n",
      "Epoch 1481/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 30.0128 - val_loss: 156.7128\n",
      "Epoch 1482/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 29.9319 - val_loss: 169.9849\n",
      "Epoch 1483/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 26.5782 - val_loss: 166.7535\n",
      "Epoch 1484/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 24.9820 - val_loss: 174.6280\n",
      "Epoch 1485/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 25.4360 - val_loss: 175.8052\n",
      "Epoch 1486/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 25.0566 - val_loss: 177.2553\n",
      "Epoch 1487/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 22.5423 - val_loss: 166.1092\n",
      "Epoch 1488/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 24.9314 - val_loss: 170.2111\n",
      "Epoch 1489/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 21.8046 - val_loss: 159.8549\n",
      "Epoch 1490/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 22.6976 - val_loss: 162.6109\n",
      "Epoch 1491/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 23.1333 - val_loss: 172.1241\n",
      "Epoch 1492/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 22.6595 - val_loss: 162.9900\n",
      "Epoch 1493/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 62us/step - loss: 27.0842 - val_loss: 164.0759\n",
      "Epoch 1494/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 25.6667 - val_loss: 173.7475\n",
      "Epoch 1495/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 25.8365 - val_loss: 176.8815\n",
      "Epoch 1496/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 26.1336 - val_loss: 166.3370\n",
      "Epoch 1497/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 23.8019 - val_loss: 162.6596\n",
      "Epoch 1498/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 23.4443 - val_loss: 165.2957\n",
      "Epoch 1499/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 23.3339 - val_loss: 176.5409\n",
      "Epoch 1500/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 23.4890 - val_loss: 164.3400\n",
      "Epoch 1501/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 23.0461 - val_loss: 166.3641\n",
      "Epoch 1502/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 26.0857 - val_loss: 166.8018\n",
      "Epoch 1503/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 25.0194 - val_loss: 166.0446\n",
      "Epoch 1504/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 23.9023 - val_loss: 154.4166\n",
      "Epoch 1505/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 24.8508 - val_loss: 159.0657\n",
      "Epoch 1506/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 26.4015 - val_loss: 160.1716\n",
      "Epoch 1507/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 26.0581 - val_loss: 166.3041\n",
      "Epoch 1508/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 24.7049 - val_loss: 171.4927\n",
      "Epoch 1509/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 22.7490 - val_loss: 160.4427\n",
      "Epoch 1510/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 22.9775 - val_loss: 163.5090\n",
      "Epoch 1511/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 21.6536 - val_loss: 164.3897\n",
      "Epoch 1512/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 25.4596 - val_loss: 156.8270\n",
      "Epoch 1513/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 25.3109 - val_loss: 171.8811\n",
      "Epoch 1514/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 25.2193 - val_loss: 163.7528\n",
      "Epoch 1515/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 23.7075 - val_loss: 166.9222\n",
      "Epoch 1516/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 27.4934 - val_loss: 179.9744\n",
      "Epoch 1517/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 23.1949 - val_loss: 162.0574\n",
      "Epoch 1518/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 24.8147 - val_loss: 175.2214\n",
      "Epoch 1519/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 25.6079 - val_loss: 158.2211\n",
      "Epoch 1520/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 24.3823 - val_loss: 164.2758\n",
      "Epoch 1521/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 22.4160 - val_loss: 166.8459\n",
      "Epoch 1522/10000\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 24.0128 - val_loss: 175.1213\n",
      "Epoch 1523/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 23.3635 - val_loss: 169.4652\n",
      "Epoch 1524/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 26.6565 - val_loss: 183.5054\n",
      "Epoch 1525/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 25.8905 - val_loss: 165.6764\n",
      "Epoch 1526/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 24.2834 - val_loss: 162.6864\n",
      "Epoch 1527/10000\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 21.4716 - val_loss: 168.1282\n",
      "Epoch 1528/10000\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 22.4817 - val_loss: 162.9288\n",
      "Epoch 1529/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 23.9261 - val_loss: 156.6183\n",
      "Epoch 1530/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 25.6942 - val_loss: 165.1007\n",
      "Epoch 1531/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 25.1906 - val_loss: 151.7842\n",
      "Epoch 1532/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 21.9457 - val_loss: 159.8251\n",
      "Epoch 1533/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 23.6624 - val_loss: 155.5366\n",
      "Epoch 1534/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 23.1803 - val_loss: 158.4496\n",
      "Epoch 1535/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 28.5067 - val_loss: 169.2504\n",
      "Epoch 1536/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 23.2333 - val_loss: 168.8674\n",
      "Epoch 1537/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 21.3635 - val_loss: 155.8690\n",
      "Epoch 1538/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 25.3439 - val_loss: 167.4402\n",
      "Epoch 1539/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 24.1086 - val_loss: 174.9170\n",
      "Epoch 1540/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 25.7728 - val_loss: 171.7697\n",
      "Epoch 1541/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 23.9603 - val_loss: 164.9607\n",
      "Epoch 1542/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 25.2462 - val_loss: 171.1517\n",
      "Epoch 1543/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 23.7851 - val_loss: 162.4246\n",
      "Epoch 1544/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 23.2807 - val_loss: 170.0691\n",
      "Epoch 1545/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 23.1420 - val_loss: 163.8728\n",
      "Epoch 1546/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 24.9846 - val_loss: 155.2349\n",
      "Epoch 1547/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 23.2971 - val_loss: 169.0166\n",
      "Epoch 1548/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 23.0518 - val_loss: 158.5193\n",
      "Epoch 1549/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 24.1373 - val_loss: 161.4653\n",
      "Epoch 1550/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 24.7047 - val_loss: 160.9079\n",
      "Epoch 1551/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 24.4886 - val_loss: 161.7350\n",
      "Epoch 1552/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 23.1927 - val_loss: 170.4184\n",
      "Epoch 1553/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 23.2121 - val_loss: 162.9151\n",
      "Epoch 1554/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 23.6261 - val_loss: 166.7859\n",
      "Epoch 1555/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 26.7675 - val_loss: 163.0397\n",
      "Epoch 1556/10000\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 24.4437 - val_loss: 161.6660\n",
      "Epoch 1557/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 22.0381 - val_loss: 161.7828\n",
      "Epoch 1558/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 25.2256 - val_loss: 160.1249\n",
      "Epoch 1559/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 21.9879 - val_loss: 162.1368\n",
      "Epoch 1560/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 24.3515 - val_loss: 168.2964\n",
      "Epoch 1561/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 25.1547 - val_loss: 161.5333\n",
      "Epoch 1562/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 24.7802 - val_loss: 160.1591\n",
      "Epoch 1563/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 24.4620 - val_loss: 154.8524\n",
      "Epoch 1564/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 23.2559 - val_loss: 170.6582\n",
      "Epoch 1565/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 22.8556 - val_loss: 163.0615\n",
      "Epoch 1566/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 65us/step - loss: 23.8995 - val_loss: 168.3013\n",
      "Epoch 1567/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 22.0723 - val_loss: 169.6437\n",
      "Epoch 1568/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 22.7140 - val_loss: 165.2185\n",
      "Epoch 1569/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 23.2491 - val_loss: 172.4323\n",
      "Epoch 1570/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 24.1079 - val_loss: 170.1485\n",
      "Epoch 1571/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 25.6216 - val_loss: 160.7460\n",
      "Epoch 1572/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 22.2619 - val_loss: 166.2756\n",
      "Epoch 1573/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 25.1084 - val_loss: 172.4615\n",
      "Epoch 1574/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 28.8651 - val_loss: 171.0553\n",
      "Epoch 1575/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 26.7030 - val_loss: 166.7977\n",
      "Epoch 1576/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 28.6482 - val_loss: 168.2791\n",
      "Epoch 1577/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 23.1091 - val_loss: 160.8133\n",
      "Epoch 1578/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 23.0779 - val_loss: 166.7113\n",
      "Epoch 1579/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 22.4986 - val_loss: 167.7642\n",
      "Epoch 1580/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 24.3381 - val_loss: 172.5851\n",
      "Epoch 1581/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 22.4455 - val_loss: 155.3588\n",
      "Epoch 1582/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 23.3702 - val_loss: 164.6857\n",
      "Epoch 1583/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 24.1114 - val_loss: 160.5521\n",
      "Epoch 1584/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 23.4816 - val_loss: 169.3496\n",
      "Epoch 1585/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 22.8030 - val_loss: 166.6758\n",
      "Epoch 1586/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 23.1538 - val_loss: 163.4402\n",
      "Epoch 1587/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 22.70 - 1s 63us/step - loss: 22.1309 - val_loss: 182.4859\n",
      "Epoch 1588/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 22.2502 - val_loss: 170.3379\n",
      "Epoch 1589/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 21.5247 - val_loss: 174.9717\n",
      "Epoch 1590/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 26.5036 - val_loss: 169.1151\n",
      "Epoch 1591/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 24.5015 - val_loss: 160.7986\n",
      "Epoch 1592/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 24.4625 - val_loss: 159.3543\n",
      "Epoch 1593/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 23.9824 - val_loss: 161.0561\n",
      "Epoch 1594/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 22.8351 - val_loss: 156.3974\n",
      "Epoch 1595/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 22.7453 - val_loss: 166.3702\n",
      "Epoch 1596/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 24.2003 - val_loss: 158.9083\n",
      "Epoch 1597/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 22.2433 - val_loss: 154.6282\n",
      "Epoch 1598/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 23.7063 - val_loss: 158.5289\n",
      "Epoch 1599/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 26.3265 - val_loss: 168.8597\n",
      "Epoch 1600/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 22.2752 - val_loss: 164.6483\n",
      "Epoch 1601/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 21.3135 - val_loss: 162.2306\n",
      "Epoch 1602/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 23.0014 - val_loss: 155.5568\n",
      "Epoch 1603/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 21.7333 - val_loss: 164.8580\n",
      "Epoch 1604/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 25.4791 - val_loss: 155.5037\n",
      "Epoch 1605/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 22.2276 - val_loss: 158.7228\n",
      "Epoch 1606/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 21.6987 - val_loss: 159.7944\n",
      "Epoch 1607/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 23.9367 - val_loss: 159.4512\n",
      "Epoch 1608/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 25.1382 - val_loss: 149.3508\n",
      "Epoch 1609/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 24.3611 - val_loss: 156.7253\n",
      "Epoch 1610/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 24.9058 - val_loss: 148.6482\n",
      "Epoch 1611/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 23.5942 - val_loss: 160.4619\n",
      "Epoch 1612/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 23.6481 - val_loss: 170.5683\n",
      "Epoch 1613/10000\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 23.5363 - val_loss: 151.0016\n",
      "Epoch 1614/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 23.8244 - val_loss: 168.5968\n",
      "Epoch 1615/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 23.0910 - val_loss: 157.8817\n",
      "Epoch 1616/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 22.6548 - val_loss: 156.6812\n",
      "Epoch 1617/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 23.2586 - val_loss: 173.6266\n",
      "Epoch 1618/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 22.1572 - val_loss: 167.0975\n",
      "Epoch 1619/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 21.8715 - val_loss: 158.3339\n",
      "Epoch 1620/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 21.7205 - val_loss: 160.4859\n",
      "Epoch 1621/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 22.2382 - val_loss: 159.3468\n",
      "Epoch 1622/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 21.7882 - val_loss: 170.0098\n",
      "Epoch 1623/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 22.6962 - val_loss: 156.2587\n",
      "Epoch 1624/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 22.9950 - val_loss: 177.3630\n",
      "Epoch 1625/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 25.7994 - val_loss: 154.6447\n",
      "Epoch 1626/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 22.6046 - val_loss: 162.5334\n",
      "Epoch 1627/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 24.3922 - val_loss: 166.5207\n",
      "Epoch 1628/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 23.1209 - val_loss: 152.5386\n",
      "Epoch 1629/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 21.7021 - val_loss: 166.5133\n",
      "Epoch 1630/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 23.5842 - val_loss: 159.8868\n",
      "Epoch 1631/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 22.3088 - val_loss: 154.5081\n",
      "Epoch 1632/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 24.3951 - val_loss: 166.6546\n",
      "Epoch 1633/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 24.8492 - val_loss: 154.0377\n",
      "Epoch 1634/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 24.3720 - val_loss: 161.6264\n",
      "Epoch 1635/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 23.6461 - val_loss: 170.4712\n",
      "Epoch 1636/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 28.2182 - val_loss: 152.5305\n",
      "Epoch 1637/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 26.4228 - val_loss: 161.7989\n",
      "Epoch 1638/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 20.5958 - val_loss: 158.7720\n",
      "Epoch 1639/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 22.4785 - val_loss: 163.5221\n",
      "Epoch 1640/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 22.9319 - val_loss: 162.1937\n",
      "Epoch 1641/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 23.3193 - val_loss: 157.9885\n",
      "Epoch 1642/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 21.5654 - val_loss: 150.3099\n",
      "Epoch 1643/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 23.9513 - val_loss: 163.0491\n",
      "Epoch 1644/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 25.7217 - val_loss: 161.4303\n",
      "Epoch 1645/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 21.8331 - val_loss: 173.9929\n",
      "Epoch 1646/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 24.1563 - val_loss: 160.0307\n",
      "Epoch 1647/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 24.2078 - val_loss: 161.6543\n",
      "Epoch 1648/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 23.0813 - val_loss: 166.4076\n",
      "Epoch 1649/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 22.4891 - val_loss: 162.4421\n",
      "Epoch 1650/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 21.9786 - val_loss: 164.7993\n",
      "Epoch 1651/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 22.9913 - val_loss: 158.4755\n",
      "Epoch 1652/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 24.3467 - val_loss: 164.8990\n",
      "Epoch 1653/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 24.0190 - val_loss: 167.2365\n",
      "Epoch 1654/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 23.3955 - val_loss: 159.0851\n",
      "Epoch 1655/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 20.8330 - val_loss: 160.6003\n",
      "Epoch 1656/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 20.4366 - val_loss: 170.0437\n",
      "Epoch 1657/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 22.7289 - val_loss: 176.2743\n",
      "Epoch 1658/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 24.0965 - val_loss: 152.0067\n",
      "Epoch 1659/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 22.6479 - val_loss: 169.4440\n",
      "Epoch 1660/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 23.7964 - val_loss: 156.2506\n",
      "Epoch 1661/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 24.4671 - val_loss: 155.7487\n",
      "Epoch 1662/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 25.0461 - val_loss: 160.3509\n",
      "Epoch 1663/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 21.5645 - val_loss: 161.1605\n",
      "Epoch 1664/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 23.2596 - val_loss: 157.3107\n",
      "Epoch 1665/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 22.7112 - val_loss: 163.6924\n",
      "Epoch 1666/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 20.4865 - val_loss: 154.8992\n",
      "Epoch 1667/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 23.6445 - val_loss: 164.1408\n",
      "Epoch 1668/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 24.0741 - val_loss: 156.8794\n",
      "Epoch 1669/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 24.6269 - val_loss: 163.8141\n",
      "Epoch 1670/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 21.3550 - val_loss: 157.3656\n",
      "Epoch 1671/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 23.3311 - val_loss: 159.3182\n",
      "Epoch 1672/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 22.1479 - val_loss: 156.9108\n",
      "Epoch 1673/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 24.5122 - val_loss: 164.4071\n",
      "Epoch 1674/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 21.9111 - val_loss: 164.4093\n",
      "Epoch 1675/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 25.3503 - val_loss: 161.0648\n",
      "Epoch 1676/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 22.8811 - val_loss: 162.5227\n",
      "Epoch 1677/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 21.3353 - val_loss: 158.3530\n",
      "Epoch 1678/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 20.5991 - val_loss: 156.2214\n",
      "Epoch 1679/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 26.0080 - val_loss: 164.6336\n",
      "Epoch 1680/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 22.5332 - val_loss: 171.0772\n",
      "Epoch 1681/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 22.9089 - val_loss: 165.2065\n",
      "Epoch 1682/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 24.6216 - val_loss: 164.2932\n",
      "Epoch 1683/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 21.0888 - val_loss: 151.9198\n",
      "Epoch 1684/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 20.6762 - val_loss: 161.7765\n",
      "Epoch 1685/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 26.2588 - val_loss: 166.5290\n",
      "Epoch 1686/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 22.9035 - val_loss: 157.0065\n",
      "Epoch 1687/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 22.4704 - val_loss: 156.5478\n",
      "Epoch 1688/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 21.8562 - val_loss: 174.2322\n",
      "Epoch 1689/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 21.3927 - val_loss: 156.1818\n",
      "Epoch 1690/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 21.6625 - val_loss: 159.4183\n",
      "Epoch 1691/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 20.6178 - val_loss: 153.9966\n",
      "Epoch 1692/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 23.7542 - val_loss: 156.9508\n",
      "Epoch 1693/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 21.5020 - val_loss: 164.3560\n",
      "Epoch 1694/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 24.6475 - val_loss: 159.0664\n",
      "Epoch 1695/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 23.6878 - val_loss: 162.7114\n",
      "Epoch 1696/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 23.8856 - val_loss: 169.5970\n",
      "Epoch 1697/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 24.8433 - val_loss: 153.3124\n",
      "Epoch 1698/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 22.6966 - val_loss: 162.9337\n",
      "Epoch 1699/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 25.2806 - val_loss: 168.4687\n",
      "Epoch 1700/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 25.6822 - val_loss: 152.0823\n",
      "Epoch 1701/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 21.8506 - val_loss: 164.9678\n",
      "Epoch 1702/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 21.9198 - val_loss: 152.9192\n",
      "Epoch 1703/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 25.1021 - val_loss: 149.3060\n",
      "Epoch 1704/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 21.7721 - val_loss: 160.1966\n",
      "Epoch 1705/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 21.6171 - val_loss: 164.7429\n",
      "Epoch 1706/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 22.7684 - val_loss: 174.4435\n",
      "Epoch 1707/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 22.8901 - val_loss: 150.0719\n",
      "Epoch 1708/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 22.1213 - val_loss: 153.8465\n",
      "Epoch 1709/10000\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 19.7245 - val_loss: 161.4711\n",
      "Epoch 1710/10000\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 24.7633 - val_loss: 151.1969\n",
      "Epoch 1711/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 22.0965 - val_loss: 165.3358\n",
      "Epoch 1712/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 68us/step - loss: 20.1135 - val_loss: 160.4045\n",
      "Epoch 1713/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 23.4431 - val_loss: 149.7602\n",
      "Epoch 1714/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 21.4468 - val_loss: 166.2469\n",
      "Epoch 1715/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 23.5226 - val_loss: 156.7870\n",
      "Epoch 1716/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 26.5778 - val_loss: 153.5856\n",
      "Epoch 1717/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 21.4198 - val_loss: 161.9381\n",
      "Epoch 1718/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 21.9831 - val_loss: 157.8596\n",
      "Epoch 1719/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 21.4042 - val_loss: 155.9005\n",
      "Epoch 1720/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 25.4337 - val_loss: 150.7822\n",
      "Epoch 1721/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 22.8030 - val_loss: 154.7395\n",
      "Epoch 1722/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 20.2186 - val_loss: 161.6966\n",
      "Epoch 1723/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 21.4975 - val_loss: 161.4398\n",
      "Epoch 1724/10000\n",
      "8000/8000 [==============================] - 1s 146us/step - loss: 21.7082 - val_loss: 172.5422\n",
      "Epoch 1725/10000\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 20.0799 - val_loss: 150.3348\n",
      "Epoch 1726/10000\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 22.6061 - val_loss: 161.5640\n",
      "Epoch 1727/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 24.0585 - val_loss: 172.7719\n",
      "Epoch 1728/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 21.3662 - val_loss: 162.6982\n",
      "Epoch 1729/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 22.7006 - val_loss: 169.8295\n",
      "Epoch 1730/10000\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 20.3486 - val_loss: 166.5453\n",
      "Epoch 1731/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 25.1708 - val_loss: 155.0902\n",
      "Epoch 1732/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 22.3400 - val_loss: 168.4964\n",
      "Epoch 1733/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 21.3670 - val_loss: 160.6309\n",
      "Epoch 1734/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 22.0169 - val_loss: 169.3529\n",
      "Epoch 1735/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 22.0560 - val_loss: 160.1555\n",
      "Epoch 1736/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 22.9676 - val_loss: 160.2596\n",
      "Epoch 1737/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 22.3841 - val_loss: 159.2312\n",
      "Epoch 1738/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 24.7933 - val_loss: 164.3786\n",
      "Epoch 1739/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 23.8325 - val_loss: 164.4634\n",
      "Epoch 1740/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 23.2807 - val_loss: 160.8585\n",
      "Epoch 1741/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 20.5117 - val_loss: 149.0382\n",
      "Epoch 1742/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 21.1807 - val_loss: 152.7762\n",
      "Epoch 1743/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 21.7452 - val_loss: 161.0936\n",
      "Epoch 1744/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 20.7446 - val_loss: 167.9084\n",
      "Epoch 1745/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 23.6755 - val_loss: 160.7251\n",
      "Epoch 1746/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 21.9464 - val_loss: 168.5160\n",
      "Epoch 1747/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 21.2596 - val_loss: 158.0744\n",
      "Epoch 1748/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 21.5934 - val_loss: 165.3678\n",
      "Epoch 1749/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 23.0141 - val_loss: 171.8172\n",
      "Epoch 1750/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 22.4462 - val_loss: 160.6421\n",
      "Epoch 1751/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 21.6238 - val_loss: 166.7521\n",
      "Epoch 1752/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 22.2667 - val_loss: 170.4164\n",
      "Epoch 1753/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 21.3558 - val_loss: 176.3225\n",
      "Epoch 1754/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 22.0950 - val_loss: 165.2305\n",
      "Epoch 1755/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 18.9230 - val_loss: 157.9048\n",
      "Epoch 1756/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 20.9829 - val_loss: 156.3201\n",
      "Epoch 1757/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 23.8317 - val_loss: 153.2656\n",
      "Epoch 1758/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 21.7761 - val_loss: 158.3738\n",
      "Epoch 1759/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 20.9894 - val_loss: 175.6131\n",
      "Epoch 1760/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 22.9846 - val_loss: 172.7909\n",
      "Epoch 1761/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 27.6902 - val_loss: 165.9600\n",
      "Epoch 1762/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 22.2521 - val_loss: 158.1372\n",
      "Epoch 1763/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 22.8611 - val_loss: 161.3019\n",
      "Epoch 1764/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 22.7914 - val_loss: 155.1083\n",
      "Epoch 1765/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 25.2991 - val_loss: 158.9774\n",
      "Epoch 1766/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 22.8711 - val_loss: 160.6753\n",
      "Epoch 1767/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 23.7748 - val_loss: 162.6795\n",
      "Epoch 1768/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 20.1034 - val_loss: 166.1631\n",
      "Epoch 1769/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 21.4580 - val_loss: 157.7248\n",
      "Epoch 1770/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 21.1923 - val_loss: 159.8401\n",
      "Epoch 1771/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 21.6800 - val_loss: 163.7430\n",
      "Epoch 1772/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 19.3753 - val_loss: 170.2694\n",
      "Epoch 1773/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 22.7493 - val_loss: 166.5745\n",
      "Epoch 1774/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 21.5961 - val_loss: 175.0689\n",
      "Epoch 1775/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 20.5445 - val_loss: 168.0974\n",
      "Epoch 1776/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 24.5117 - val_loss: 162.3064\n",
      "Epoch 1777/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 26.2569 - val_loss: 167.8471\n",
      "Epoch 1778/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 20.6703 - val_loss: 156.9026\n",
      "Epoch 1779/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 23.3301 - val_loss: 167.7658\n",
      "Epoch 1780/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 21.1383 - val_loss: 173.0295\n",
      "Epoch 1781/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 20.4956 - val_loss: 167.1736\n",
      "Epoch 1782/10000\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 24.6500 - val_loss: 171.9504\n",
      "Epoch 1783/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 21.2811 - val_loss: 185.7289\n",
      "Epoch 1784/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 22.9484 - val_loss: 166.3689\n",
      "Epoch 1785/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 58us/step - loss: 21.2540 - val_loss: 160.8519\n",
      "Epoch 1786/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 23.4739 - val_loss: 164.4121\n",
      "Epoch 1787/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 22.9607 - val_loss: 156.2446\n",
      "Epoch 1788/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 21.9930 - val_loss: 153.7331\n",
      "Epoch 1789/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 24.7964 - val_loss: 171.6898\n",
      "Epoch 1790/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 20.0690 - val_loss: 162.2542\n",
      "Epoch 1791/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 19.8716 - val_loss: 156.4979\n",
      "Epoch 1792/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 20.9357 - val_loss: 162.1625\n",
      "Epoch 1793/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 24.8512 - val_loss: 156.5643\n",
      "Epoch 1794/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 21.3913 - val_loss: 159.7011\n",
      "Epoch 1795/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 20.5495 - val_loss: 159.7062\n",
      "Epoch 1796/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 21.9285 - val_loss: 161.9258\n",
      "Epoch 1797/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 24.6944 - val_loss: 177.5728\n",
      "Epoch 1798/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 26.3323 - val_loss: 168.1227\n",
      "Epoch 1799/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 21.5985 - val_loss: 170.9527\n",
      "Epoch 1800/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 20.4738 - val_loss: 162.5557\n",
      "Epoch 1801/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 20.0753 - val_loss: 165.2502\n",
      "Epoch 1802/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 25.5296 - val_loss: 154.1326\n",
      "Epoch 1803/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 21.5796 - val_loss: 153.6082\n",
      "Epoch 1804/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 21.8639 - val_loss: 172.7513\n",
      "Epoch 1805/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 18.6190 - val_loss: 160.8435\n",
      "Epoch 1806/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 21.0713 - val_loss: 163.8743\n",
      "Epoch 1807/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 23.0000 - val_loss: 165.0169\n",
      "Epoch 1808/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 21.1943 - val_loss: 170.7638\n",
      "Epoch 1809/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 22.5431 - val_loss: 171.3031\n",
      "Epoch 1810/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 22.4326 - val_loss: 172.2988\n",
      "Epoch 1811/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 21.3809 - val_loss: 155.4147\n",
      "Epoch 1812/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 19.9864 - val_loss: 176.5051\n",
      "Epoch 1813/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 21.5789 - val_loss: 166.5163\n",
      "Epoch 1814/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 24.9167 - val_loss: 156.5440\n",
      "Epoch 1815/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 21.0229 - val_loss: 180.5614\n",
      "Epoch 1816/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 21.4275 - val_loss: 176.1580\n",
      "Epoch 1817/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 20.0932 - val_loss: 178.8082\n",
      "Epoch 1818/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 23.3595 - val_loss: 170.8702\n",
      "Epoch 1819/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 24.6713 - val_loss: 173.2756\n",
      "Epoch 1820/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 20.0687 - val_loss: 169.5291\n",
      "Epoch 1821/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 18.9392 - val_loss: 163.4696\n",
      "Epoch 1822/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 21.6747 - val_loss: 171.5866\n",
      "Epoch 1823/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 21.9530 - val_loss: 169.2409\n",
      "Epoch 1824/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 21.0232 - val_loss: 167.2896\n",
      "Epoch 1825/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 19.6842 - val_loss: 172.6150\n",
      "Epoch 1826/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 20.4280 - val_loss: 168.8491\n",
      "Epoch 1827/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 23.5423 - val_loss: 172.3931\n",
      "Epoch 1828/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 20.8288 - val_loss: 184.2394\n",
      "Epoch 1829/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 21.2217 - val_loss: 169.7944\n",
      "Epoch 1830/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 20.2657 - val_loss: 154.9985\n",
      "Epoch 1831/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 23.8147 - val_loss: 170.7518\n",
      "Epoch 1832/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 19.2111 - val_loss: 167.6877\n",
      "Epoch 1833/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 23.3665 - val_loss: 176.2997\n",
      "Epoch 1834/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 19.6779 - val_loss: 185.9771\n",
      "Epoch 1835/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 22.7050 - val_loss: 181.2773\n",
      "Epoch 1836/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 21.9943 - val_loss: 176.7237\n",
      "Epoch 1837/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 21.5812 - val_loss: 186.6575\n",
      "Epoch 1838/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 21.5056 - val_loss: 165.0235\n",
      "Epoch 1839/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 21.1172 - val_loss: 180.3461\n",
      "Epoch 1840/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 25.2785 - val_loss: 172.0951\n",
      "Epoch 1841/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 21.7623 - val_loss: 172.6502\n",
      "Epoch 1842/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 20.8220 - val_loss: 170.4737\n",
      "Epoch 1843/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 22.5896 - val_loss: 161.2183\n",
      "Epoch 1844/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 22.7036 - val_loss: 174.0156\n",
      "Epoch 1845/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 21.4511 - val_loss: 175.9218\n",
      "Epoch 1846/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 22.0377 - val_loss: 163.6401\n",
      "Epoch 1847/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 22.5528 - val_loss: 161.0647\n",
      "Epoch 1848/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 24.6673 - val_loss: 160.6428\n",
      "Epoch 1849/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 22.1487 - val_loss: 169.5607\n",
      "Epoch 1850/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 20.9465 - val_loss: 174.3476\n",
      "Epoch 1851/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 20.4125 - val_loss: 179.0742\n",
      "Epoch 1852/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 23.5439 - val_loss: 164.4100\n",
      "Epoch 1853/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 21.3968 - val_loss: 166.3710\n",
      "Epoch 1854/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 20.6206 - val_loss: 184.0109\n",
      "Epoch 1855/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 19.7254 - val_loss: 166.1820\n",
      "Epoch 1856/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 20.5676 - val_loss: 173.9702\n",
      "Epoch 1857/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 22.0742 - val_loss: 167.5527\n",
      "Epoch 1858/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 26.0580 - val_loss: 167.3121\n",
      "Epoch 1859/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 20.0988 - val_loss: 158.1790\n",
      "Epoch 1860/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 22.9437 - val_loss: 162.4553\n",
      "Epoch 1861/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 23.6143 - val_loss: 164.5085\n",
      "Epoch 1862/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 21.7334 - val_loss: 163.7923\n",
      "Epoch 1863/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 20.8518 - val_loss: 176.3488\n",
      "Epoch 1864/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 21.4826 - val_loss: 158.8402\n",
      "Epoch 1865/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 21.2346 - val_loss: 170.0278\n",
      "Epoch 1866/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 19.4335 - val_loss: 160.1155\n",
      "Epoch 1867/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 19.0534 - val_loss: 158.7836\n",
      "Epoch 1868/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 22.4213 - val_loss: 167.3770\n",
      "Epoch 1869/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 23.4706 - val_loss: 156.1629\n",
      "Epoch 1870/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 18.9105 - val_loss: 168.3588\n",
      "Epoch 1871/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 19.3976 - val_loss: 163.2426\n",
      "Epoch 1872/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 20.9503 - val_loss: 168.3995\n",
      "Epoch 1873/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 20.0834 - val_loss: 171.2530\n",
      "Epoch 1874/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 21.9834 - val_loss: 170.3395\n",
      "Epoch 1875/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 23.3780 - val_loss: 171.9584\n",
      "Epoch 1876/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 22.5623 - val_loss: 165.4902\n",
      "Epoch 1877/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 20.1838 - val_loss: 165.8036\n",
      "Epoch 1878/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 21.0198 - val_loss: 165.7403\n",
      "Epoch 1879/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 25.2159 - val_loss: 167.9050\n",
      "Epoch 1880/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 18.1622 - val_loss: 161.9155\n",
      "Epoch 1881/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 19.1844 - val_loss: 173.0796\n",
      "Epoch 1882/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 22.7805 - val_loss: 171.8514\n",
      "Epoch 1883/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 22.2667 - val_loss: 172.9460\n",
      "Epoch 1884/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 19.6323 - val_loss: 167.8954\n",
      "Epoch 1885/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 21.2028 - val_loss: 169.0501\n",
      "Epoch 1886/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 21.9621 - val_loss: 173.9947\n",
      "Epoch 1887/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 21.3112 - val_loss: 171.1628\n",
      "Epoch 1888/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 19.5272 - val_loss: 169.1089\n",
      "Epoch 1889/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 18.4759 - val_loss: 166.3016\n",
      "Epoch 1890/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 20.9452 - val_loss: 165.6950\n",
      "Epoch 1891/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 19.9660 - val_loss: 155.4259\n",
      "Epoch 1892/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 24.1721 - val_loss: 164.2167\n",
      "Epoch 1893/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 25.7025 - val_loss: 162.4125\n",
      "Epoch 1894/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 21.9256 - val_loss: 173.6632\n",
      "Epoch 1895/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 18.9093 - val_loss: 170.2831\n",
      "Epoch 1896/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 20.6276 - val_loss: 169.5123\n",
      "Epoch 1897/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 22.5733 - val_loss: 163.4910\n",
      "Epoch 1898/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.3981 - val_loss: 169.4692\n",
      "Epoch 1899/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 22.1226 - val_loss: 159.5602\n",
      "Epoch 1900/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 20.4969 - val_loss: 164.5783\n",
      "Epoch 1901/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 20.4401 - val_loss: 167.6019\n",
      "Epoch 1902/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 22.5071 - val_loss: 159.6334\n",
      "Epoch 1903/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 18.6801 - val_loss: 163.1046\n",
      "Epoch 1904/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 19.5401 - val_loss: 164.8080\n",
      "Epoch 1905/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 19.0395 - val_loss: 162.9287\n",
      "Epoch 1906/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 21.9909 - val_loss: 157.9209\n",
      "Epoch 1907/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 21.3165 - val_loss: 164.3173\n",
      "Epoch 1908/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 20.5927 - val_loss: 167.1934\n",
      "Epoch 1909/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 20.5857 - val_loss: 163.8905\n",
      "Epoch 1910/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 20.6104 - val_loss: 170.9718\n",
      "Epoch 1911/10000\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 19.7958 - val_loss: 170.6340\n",
      "Epoch 1912/10000\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 21.5978 - val_loss: 166.6770\n",
      "Epoch 1913/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 20.9996 - val_loss: 175.6478\n",
      "Epoch 1914/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 21.0211 - val_loss: 164.9111\n",
      "Epoch 1915/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 21.0946 - val_loss: 165.7663\n",
      "Epoch 1916/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 19.8384 - val_loss: 172.2389\n",
      "Epoch 1917/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 19.7969 - val_loss: 170.8129\n",
      "Epoch 1918/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 24.3687 - val_loss: 166.7786\n",
      "Epoch 1919/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 21.1042 - val_loss: 171.2138\n",
      "Epoch 1920/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 21.0198 - val_loss: 169.3711\n",
      "Epoch 1921/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 19.6876 - val_loss: 160.4624\n",
      "Epoch 1922/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 21.0771 - val_loss: 165.8528\n",
      "Epoch 1923/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 19.4665 - val_loss: 164.0044\n",
      "Epoch 1924/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 22.1259 - val_loss: 167.2888\n",
      "Epoch 1925/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 22.3977 - val_loss: 167.8015\n",
      "Epoch 1926/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.1191 - val_loss: 171.5393\n",
      "Epoch 1927/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 21.1825 - val_loss: 159.3903\n",
      "Epoch 1928/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 19.7444 - val_loss: 169.8926\n",
      "Epoch 1929/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 22.0667 - val_loss: 162.5869\n",
      "Epoch 1930/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 20.1542 - val_loss: 166.0938\n",
      "Epoch 1931/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 56us/step - loss: 19.8501 - val_loss: 162.8912\n",
      "Epoch 1932/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 23.2386 - val_loss: 153.5703\n",
      "Epoch 1933/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 21.7138 - val_loss: 162.5021\n",
      "Epoch 1934/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 21.0080 - val_loss: 168.9533\n",
      "Epoch 1935/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 20.5891 - val_loss: 166.6029\n",
      "Epoch 1936/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 20.2546 - val_loss: 185.5085\n",
      "Epoch 1937/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 19.3836 - val_loss: 159.2571\n",
      "Epoch 1938/10000\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 21.7154 - val_loss: 159.4857\n",
      "Epoch 1939/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 19.4371 - val_loss: 187.8998\n",
      "Epoch 1940/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 23.0193 - val_loss: 157.8607\n",
      "Epoch 1941/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 20.6965 - val_loss: 181.8993\n",
      "Epoch 1942/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 21.8420 - val_loss: 168.5607\n",
      "Epoch 1943/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 22.1425 - val_loss: 166.7804\n",
      "Epoch 1944/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 21.9663 - val_loss: 169.7388\n",
      "Epoch 1945/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 18.7165 - val_loss: 157.0234\n",
      "Epoch 1946/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 21.3073 - val_loss: 172.7111\n",
      "Epoch 1947/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 19.4108 - val_loss: 163.6418\n",
      "Epoch 1948/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 22.7402 - val_loss: 169.5192\n",
      "Epoch 1949/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 19.9584 - val_loss: 177.4185\n",
      "Epoch 1950/10000\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 24.5743 - val_loss: 175.1444\n",
      "Epoch 1951/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 21.9842 - val_loss: 178.6082\n",
      "Epoch 1952/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 22.5174 - val_loss: 174.9722\n",
      "Epoch 1953/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 22.2656 - val_loss: 168.6324\n",
      "Epoch 1954/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 19.2018 - val_loss: 168.4815\n",
      "Epoch 1955/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 19.1012 - val_loss: 175.9168\n",
      "Epoch 1956/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 19.2281 - val_loss: 174.6375\n",
      "Epoch 1957/10000\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 18.3798 - val_loss: 163.3210\n",
      "Epoch 1958/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 23.0280 - val_loss: 167.9559\n",
      "Epoch 1959/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 21.8724 - val_loss: 177.8007\n",
      "Epoch 1960/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 18.5933 - val_loss: 169.8185\n",
      "Epoch 1961/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 20.2262 - val_loss: 163.1815\n",
      "Epoch 1962/10000\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 21.2200 - val_loss: 169.5333\n",
      "Epoch 1963/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 21.4732 - val_loss: 157.7245\n",
      "Epoch 1964/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 19.8895 - val_loss: 171.3378\n",
      "Epoch 1965/10000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 20.37 - 0s 57us/step - loss: 20.3065 - val_loss: 169.8543\n",
      "Epoch 1966/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 21.4474 - val_loss: 166.7590\n",
      "Epoch 1967/10000\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 20.4636 - val_loss: 162.2647\n",
      "Epoch 1968/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 22.5468 - val_loss: 161.1985\n",
      "Epoch 1969/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 22.1552 - val_loss: 164.0130\n",
      "Epoch 1970/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 23.2925 - val_loss: 163.4305\n",
      "Epoch 1971/10000\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 18.1603 - val_loss: 165.2958\n",
      "Epoch 1972/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 19.8435 - val_loss: 159.8660\n",
      "Epoch 1973/10000\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 18.2867 - val_loss: 160.6084\n",
      "Epoch 1974/10000\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 19.1991 - val_loss: 173.2217\n",
      "Epoch 1975/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 19.3153 - val_loss: 165.2590\n",
      "Epoch 1976/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 23.5720 - val_loss: 164.8671\n",
      "Epoch 1977/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 18.6242 - val_loss: 164.8807\n",
      "Epoch 1978/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 18.2431 - val_loss: 162.7422\n",
      "Epoch 1979/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 17.6488 - val_loss: 163.3992\n",
      "Epoch 1980/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 17.8868 - val_loss: 162.5956\n",
      "Epoch 1981/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 19.9854 - val_loss: 156.4979\n",
      "Epoch 1982/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 21.0183 - val_loss: 168.5982\n",
      "Epoch 1983/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 21.0461 - val_loss: 170.4727\n",
      "Epoch 1984/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 19.9055 - val_loss: 166.8952\n",
      "Epoch 1985/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 19.1768 - val_loss: 159.1827\n",
      "Epoch 1986/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 23.9516 - val_loss: 169.5022\n",
      "Epoch 1987/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 21.3592 - val_loss: 172.0741\n",
      "Epoch 1988/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 20.9212 - val_loss: 162.1136\n",
      "Epoch 1989/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 19.3641 - val_loss: 166.8965\n",
      "Epoch 1990/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 19.4624 - val_loss: 165.0068\n",
      "Epoch 1991/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 18.6325 - val_loss: 159.1985\n",
      "Epoch 1992/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 20.5903 - val_loss: 157.5450\n",
      "Epoch 1993/10000\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 24.7045 - val_loss: 169.5682\n",
      "Epoch 1994/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 21.8764 - val_loss: 157.2973\n",
      "Epoch 1995/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 19.7623 - val_loss: 174.9233\n",
      "Epoch 1996/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 19.3453 - val_loss: 172.6677\n",
      "Epoch 1997/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 19.2390 - val_loss: 168.4219\n",
      "Epoch 1998/10000\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 16.7792 - val_loss: 166.8710\n",
      "Epoch 1999/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 19.0960 - val_loss: 169.4004\n",
      "Epoch 2000/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 22.3154 - val_loss: 166.3002\n",
      "Epoch 2001/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 21.0403 - val_loss: 167.8483\n",
      "Epoch 2002/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 19.8704 - val_loss: 167.2468\n",
      "Epoch 2003/10000\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 19.9036 - val_loss: 175.3943\n",
      "Epoch 2004/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 59us/step - loss: 18.2643 - val_loss: 172.8353\n",
      "Epoch 2005/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 20.4361 - val_loss: 174.0581\n",
      "Epoch 2006/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 20.4999 - val_loss: 167.6614\n",
      "Epoch 2007/10000\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 21.2090 - val_loss: 163.3882\n",
      "Epoch 2008/10000\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 20.8409 - val_loss: 159.7738\n",
      "Epoch 2009/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 19.1691 - val_loss: 165.7948\n",
      "Epoch 2010/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 18.9161 - val_loss: 164.4351\n",
      "Epoch 2011/10000\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 22.1884 - val_loss: 167.1031\n",
      "Epoch 2012/10000\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 21.3928 - val_loss: 185.6862\n",
      "Epoch 2013/10000\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 19.7912 - val_loss: 159.4169\n",
      "Epoch 2014/10000\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 19.6051 - val_loss: 161.3349\n",
      "Epoch 2015/10000\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 21.3032 - val_loss: 164.8503\n",
      "Epoch 2016/10000\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 21.0879 - val_loss: 173.5696\n",
      "Epoch 2017/10000\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 23.2664 - val_loss: 151.1025\n",
      "Epoch 2018/10000\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 23.0809 - val_loss: 172.6378\n",
      "Epoch 2019/10000\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 20.4305 - val_loss: 150.8931\n",
      "Epoch 2020/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 22.5823 - val_loss: 161.7636\n",
      "Epoch 2021/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 18.1219 - val_loss: 170.5032\n",
      "Epoch 2022/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 20.9675 - val_loss: 169.4463\n",
      "Epoch 2023/10000\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 20.2973 - val_loss: 169.4793\n",
      "Epoch 2024/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 20.3145 - val_loss: 160.4332\n",
      "Epoch 2025/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 17.6269 - val_loss: 165.2771\n",
      "Epoch 2026/10000\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 18.4258 - val_loss: 163.2739\n",
      "Epoch 2027/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 19.9191 - val_loss: 169.8131\n",
      "Epoch 2028/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 18.2430 - val_loss: 168.3302\n",
      "Epoch 2029/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 17.0463 - val_loss: 165.9224\n",
      "Epoch 2030/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 19.6154 - val_loss: 172.6381\n",
      "Epoch 2031/10000\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 22.2636 - val_loss: 162.7250\n",
      "Epoch 2032/10000\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 18.6659 - val_loss: 160.5038\n",
      "Epoch 2033/10000\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 18.5857 - val_loss: 166.5381\n",
      "Epoch 2034/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 19.4112 - val_loss: 165.5039\n",
      "Epoch 2035/10000\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 19.4184 - val_loss: 170.3570\n",
      "Epoch 2036/10000\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 20.4117 - val_loss: 162.6951\n",
      "Epoch 2037/10000\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 21.1608 - val_loss: 176.2226\n",
      "Epoch 2038/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 23.1415 - val_loss: 172.0104\n",
      "Epoch 2039/10000\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 19.8563 - val_loss: 159.8883\n",
      "Epoch 2040/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 17.7947 - val_loss: 164.7470\n",
      "Epoch 2041/10000\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 18.7263 - val_loss: 170.8666\n",
      "Epoch 2042/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 19.4407 - val_loss: 162.3819\n",
      "Epoch 2043/10000\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 19.8676 - val_loss: 161.6304\n",
      "Epoch 2044/10000\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 21.4921 - val_loss: 168.3323\n",
      "Epoch 2045/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 19.8826 - val_loss: 164.3506\n",
      "Epoch 2046/10000\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 18.8609 - val_loss: 170.9442\n",
      "Epoch 2047/10000\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 18.4819 - val_loss: 168.4679\n",
      "Epoch 02047: early stopping\n",
      "Fold score (RMSE): 12.885672569274902\n"
     ]
    }
   ],
   "source": [
    "# Cross-Validate\n",
    "kf = KFold(5)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=filename_checkpoint, verbose=0, save_best_only=True)\n",
    "\n",
    "# Turn off KFold\n",
    "#if (0):\n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "    \n",
    "for train, test in kf.split(x):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "    model.add(Dropout(0.01)) # Dropout Layer 1\n",
    "    model.add(Dense(50, activation='relu')) # Hidden 2\n",
    "    #model.add(Dropout(0.01)) # Dropout Layer 2\n",
    "    model.add(Dense(25, \n",
    "                    kernel_regularizer=regularizers.l2(0.01), #L2 regularization\n",
    "                    activity_regularizer=regularizers.l1(0.01), #L1 Lasso regularization\n",
    "                    activation='relu')) # Hidden 3 \n",
    "    model.add(Dense(10, activation='relu')) # Hidden 4\n",
    "    model.add(Dense(1)) # Output\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=1000, verbose=1, mode='auto')\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpoint],verbose=1,epochs=10000)\n",
    "    \n",
    "    pred = model.predict(x_test)\n",
    "    \n",
    "    oos_y.append(y_test)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure this fold's RMSE\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "    print(\"Fold score (RMSE): {}\".format(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-sample RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))\n",
    "print(\"Final, out of sample score (RMSE): {}\".format(score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#pred = model.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "#score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "#print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "#chart_regression(pred.flatten(),y_test)\n",
    "#chart_regression(pred.flatten(),y_test,sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Best Weights and Predict on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(filename_checkpoint)\n",
    "\n",
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "extract_and_encode_features(df_test)\n",
    "\n",
    "ids_test = df_test['id']\n",
    "df_test.drop('id',1,inplace=True)\n",
    "\n",
    "names_test = df_test['name']\n",
    "df_test.drop('name',1,inplace=True)\n",
    "\n",
    "x_submit = df_test.as_matrix().astype(np.float32)\n",
    "pred_submit = model.predict(x_submit)\n",
    "\n",
    "# Handles negative cost values. Use inverse\n",
    "cost = [n if n > 0 else n * -1 for n in pred_submit[:,0]]\n",
    "\n",
    "df_submit = pd.DataFrame({'id': ids_test,'cost': cost})\n",
    "df_submit = df_submit[['id', 'cost']]\n",
    "df_submit.to_csv(filename_submit, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
