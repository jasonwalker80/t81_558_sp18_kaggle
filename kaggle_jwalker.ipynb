{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n",
    "\n",
    "**Kaggle Assignment: **\n",
    "\n",
    "**Student Name: Jason Walker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## Description\n",
    "This is one of the projects from the course T81-855: Applications of Deep Learning at Washington University in St. Louis. All students must create a Kaggle account and submit a solution. Once you have submitted your solution entry log into Blackboard (at WUSTL) and submit a single file telling me your Kaggle name on the leaderboard (you do not need to register to Kaggle with your real name). This competition will be visible to the public, so there may be non-student submissions as well as student.\n",
    "\n",
    "The data set for this competition consists of a number of input columns that should be used to predict a stores sales. This is a regression problem. The inputs are a mixture of discrete and category values. The data set is from a simulation.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The evaluation pages describes how submissions will be scored and how students should format their submissions. The scores are in RMSE.\n",
    "Submission Format\n",
    "\n",
    "For every store in the dataset, submission files should contain a sales volume.\n",
    "\n",
    "The file should contain a header and have the following format:\n",
    "\n",
    "```\n",
    "100000,1.23\n",
    "100001,1.123\n",
    "100002,3.332\n",
    "100003,1.53\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The data contains data and costs for various office supplies. The data came from a simulation and do not directly correspond to any real-world items. See how well you can predict the cost of an item using the provided data. Feature engineering will likely help you. The *name* column may seem useless at first glance; however, it contains information that you can parse to help your predictions.\n",
    "File descriptions\n",
    "```\n",
    "    id - The identifier/primary key.\n",
    "    name - The name of this item.\n",
    "    manufacturer - The manufacturer.\n",
    "    pack - The number of items in this pack.\n",
    "    weight - The weight of a pack of these items.\n",
    "    height - The height of a pack of these items.\n",
    "    width - The width of a pack of these items.\n",
    "    length - The length of a pack of these items.\n",
    "    cost - The cost for this item pack. This is what you are to predict (the target). \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions\n",
    "\n",
    "You will see these at the top of every module and assignment.  These are simply a set of reusable functions that we will make use of.  Each of them will be explained as the semester progresses.  They are explained in greater detail as the course progresses.  Class 4 contains a complete overview of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n",
    "        \n",
    "# This function submits an assignment.  You can submit an assignment as much as you like, only the final\n",
    "# submission counts.  The paramaters are as follows:\n",
    "# data - Pandas dataframe output.\n",
    "# key - Your student key that was emailed to you.\n",
    "# no - The assignment class number, should be 1 through 1.\n",
    "# source_file - The full path to your Python or IPYNB file.  This must have \"_class1\" as part of its name.  \n",
    "# .             The number must match your assignment number.  For example \"_class2\" for class assignment #2.\n",
    "def submit(data,key,no,source_file=None):\n",
    "    if source_file is None and '__file__' not in globals(): raise Exception('Must specify a filename when a Jupyter notebook.')\n",
    "    if source_file is None: source_file = __file__\n",
    "    suffix = '_class{}'.format(no)\n",
    "    if suffix not in source_file: raise Exception('{} must be part of the filename.'.format(suffix))\n",
    "    with open(source_file, \"rb\") as image_file:\n",
    "        encoded_python = base64.b64encode(image_file.read()).decode('ascii')\n",
    "    ext = os.path.splitext(source_file)[-1].lower()\n",
    "    if ext not in ['.ipynb','.py']: raise Exception(\"Source file is {} must be .py or .ipynb\".format(ext))\n",
    "    r = requests.post(\"https://api.heatonresearch.com/assignment-submit\",\n",
    "        headers={'x-api-key':key}, json={'csv':base64.b64encode(data.to_csv(index=False).encode('ascii')).decode(\"ascii\"),\n",
    "        'assignment': no, 'ext':ext, 'py':encoded_python})\n",
    "    if r.status_code == 200:\n",
    "        print(\"Success: {}\".format(r.text))\n",
    "    else: print(\"Failure: {}\".format(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kaggle Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers\n",
    "\n",
    "path = './data'\n",
    "\n",
    "filename_test = os.path.join(path,\"test.csv\")\n",
    "filename_train = os.path.join(path,\"train.csv\")\n",
    "filename_sample = os.path.join(path,\"sample.csv\")\n",
    "filename_submit = os.path.join(path,\"submit.csv\")\n",
    "filename_checkpoint = os.path.join(path,\"checkpoint.hdf5\")\n",
    "\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "\n",
    "np.random.seed(42) # Uncomment this line to get the same shuffle each time\n",
    "df_train = df_train.reindex(np.random.permutation(df_train.index))\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Encode Features\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def extract_and_encode_features(df):\n",
    "    color_regex='(?P<color>red|blue|green|yellow|orange|pink|black|brown|white)'\n",
    "    df['color'] = df.name.str.extract(color_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    quality_regex='(?P<quality>generic|medium\\shigh\\squality|high\\squality)'\n",
    "    df['quality'] = df.name.str.extract(quality_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    size_regex='(?P<size>tiny|small|medium|large)'\n",
    "    df['size'] = df.name.str.extract(size_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    item_regex='(?P<item>paperclips|paperweights|ink\\spens|pencils|stapler|tablets|thumbtacks|post\\sit\\snotes)'\n",
    "    df['item'] = df.name.str.extract(item_regex, flags=re.IGNORECASE, expand=False)\n",
    "    \n",
    "    for column in ['pack','weight','height','width','length']:\n",
    "        missing_median(df,column)\n",
    "    \n",
    "    #df.insert(1,'surface_area',(df['height']*df['width']*df['length']).astype(int))\n",
    "    \n",
    "    ## encode numeric features\n",
    "    #for column in ['pack','weight','height','width','length','surface_area']:\n",
    "    #    encode_numeric_zscore(df,column)\n",
    "\n",
    "    # encode text/categorical features\n",
    "    for column in ['manufacturer','color','quality','size','item']:\n",
    "        encode_text_dummy(df,column)\n",
    "  \n",
    "extract_and_encode_features(df_train)\n",
    "\n",
    "ids_train = df_train['id']\n",
    "df_train.drop('id',1,inplace=True)\n",
    "\n",
    "names_train = df_train['name']\n",
    "df_train.drop('name',1,inplace=True)\n",
    "\n",
    "x,y = to_xy(df_train,'cost')\n",
    "\n",
    "# Cross-Validate\n",
    "#kf = KFold(5)\n",
    "\n",
    "# Used before KFold\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 32.560638427734375\n",
      "['pack', 'weight', 'height', 'width', 'length', 'manufacturer-6% Solution', 'manufacturer-Deep Office Supplies', 'manufacturer-Duck Lake', 'manufacturer-Offices-R-Us', 'manufacturer-WizBang', 'color-Black', 'color-Blue', 'color-Brown', 'color-Green', 'color-Pink', 'color-Red', 'color-White', 'quality-Generic', 'quality-High Quality', 'quality-Medium High Quality', 'size-Large', 'size-Medium', 'size-Small', 'size-Tiny', 'item-Ink Pens', 'item-Paperclips', 'item-Paperweights', 'item-Pencils', 'item-Post It Notes', 'item-Stapler', 'item-Tablets', 'item-Thumbtacks']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item-Post It Notes</th>\n",
       "      <td>-59.312168</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Thumbtacks</th>\n",
       "      <td>-56.623314</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Pencils</th>\n",
       "      <td>-50.400471</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperclips</th>\n",
       "      <td>-43.599960</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Red</th>\n",
       "      <td>-35.857452</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Green</th>\n",
       "      <td>-24.303213</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>-10.113589</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Blue</th>\n",
       "      <td>-9.778607</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>-8.088337</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>width</th>\n",
       "      <td>-6.066093</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Generic</th>\n",
       "      <td>-4.613498</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Ink Pens</th>\n",
       "      <td>-4.261536</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Offices-R-Us</th>\n",
       "      <td>-1.761803</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Tiny</th>\n",
       "      <td>-1.449806</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Duck Lake</th>\n",
       "      <td>-0.532333</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack</th>\n",
       "      <td>0.019380</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>0.034990</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Deep Office Supplies</th>\n",
       "      <td>0.109180</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-6% Solution</th>\n",
       "      <td>0.238405</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Small</th>\n",
       "      <td>0.369865</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Brown</th>\n",
       "      <td>1.451031</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-WizBang</th>\n",
       "      <td>1.946654</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Medium</th>\n",
       "      <td>3.852479</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Medium High Quality</th>\n",
       "      <td>5.394495</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperweights</th>\n",
       "      <td>6.588786</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Large</th>\n",
       "      <td>6.606421</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-High Quality</th>\n",
       "      <td>7.698106</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Black</th>\n",
       "      <td>15.785252</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-White</th>\n",
       "      <td>29.429951</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Pink</th>\n",
       "      <td>40.224407</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Stapler</th>\n",
       "      <td>48.437527</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Tablets</th>\n",
       "      <td>159.170486</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         coef  positive\n",
       "item-Post It Notes                 -59.312168     False\n",
       "item-Thumbtacks                    -56.623314     False\n",
       "item-Pencils                       -50.400471     False\n",
       "item-Paperclips                    -43.599960     False\n",
       "color-Red                          -35.857452     False\n",
       "color-Green                        -24.303213     False\n",
       "height                             -10.113589     False\n",
       "color-Blue                          -9.778607     False\n",
       "length                              -8.088337     False\n",
       "width                               -6.066093     False\n",
       "quality-Generic                     -4.613498     False\n",
       "item-Ink Pens                       -4.261536     False\n",
       "manufacturer-Offices-R-Us           -1.761803     False\n",
       "size-Tiny                           -1.449806     False\n",
       "manufacturer-Duck Lake              -0.532333     False\n",
       "pack                                 0.019380      True\n",
       "weight                               0.034990      True\n",
       "manufacturer-Deep Office Supplies    0.109180      True\n",
       "manufacturer-6% Solution             0.238405      True\n",
       "size-Small                           0.369865      True\n",
       "color-Brown                          1.451031      True\n",
       "manufacturer-WizBang                 1.946654      True\n",
       "size-Medium                          3.852479      True\n",
       "quality-Medium High Quality          5.394495      True\n",
       "item-Paperweights                    6.588786      True\n",
       "size-Large                           6.606421      True\n",
       "quality-High Quality                 7.698106      True\n",
       "color-Black                         15.785252      True\n",
       "color-White                         29.429951      True\n",
       "color-Pink                          40.224407      True\n",
       "item-Stapler                        48.437527      True\n",
       "item-Tablets                       159.170486      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [ 80.02005768]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAD8CAYAAADExYYgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nF2ZmZtZQ7lyYmZlZQ7lzYWZmZg31/6lPpHWSQn7sAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1ca47ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple function to evaluate the coefficients of a regression\n",
    "%matplotlib inline    \n",
    "from IPython.display import display, HTML    \n",
    "\n",
    "def report_coef(names,coef,intercept):\n",
    "    r = pd.DataFrame( { 'coef': coef, 'positive': coef>=0  }, index = names )\n",
    "    r = r.sort_values(by=['coef'])\n",
    "    display(r)\n",
    "    print(\"Intercept: {}\".format(intercept))\n",
    "    r['coef'].plot(kind='barh', color=r['positive'].map({True: 'b', False: 'r'}))\n",
    "    \n",
    "# Create linear regression\n",
    "regressor = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# Fit/train linear regression\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "print(names)\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_[0,:],\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/10000\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 9389.8657 - val_loss: 5636.6957\n",
      "Epoch 2/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 5270.0127 - val_loss: 4677.4970\n",
      "Epoch 3/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 5024.8859 - val_loss: 4379.7816\n",
      "Epoch 4/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 4684.1535 - val_loss: 4182.2580\n",
      "Epoch 5/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 4382.2883 - val_loss: 4036.0152\n",
      "Epoch 6/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 4277.8622 - val_loss: 4378.1592\n",
      "Epoch 7/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 4148.8899 - val_loss: 3725.2570\n",
      "Epoch 8/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 4108.3764 - val_loss: 4111.1568\n",
      "Epoch 9/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 3963.4836 - val_loss: 3641.3059\n",
      "Epoch 10/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 3902.2246 - val_loss: 3741.2678\n",
      "Epoch 11/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 3970.3332 - val_loss: 3599.9394\n",
      "Epoch 12/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 3664.7321 - val_loss: 3871.8492\n",
      "Epoch 13/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 3538.1787 - val_loss: 3119.8049\n",
      "Epoch 14/10000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 3386.8987 - val_loss: 2923.0489\n",
      "Epoch 15/10000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 3326.0750 - val_loss: 2858.2498\n",
      "Epoch 16/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 3160.8679 - val_loss: 3837.6300\n",
      "Epoch 17/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 3156.0268 - val_loss: 3107.1647\n",
      "Epoch 18/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 2706.3387 - val_loss: 2156.1459\n",
      "Epoch 19/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 2452.1052 - val_loss: 4030.3655\n",
      "Epoch 20/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 2334.9963 - val_loss: 1855.7150\n",
      "Epoch 21/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1931.3209 - val_loss: 1286.4831\n",
      "Epoch 22/10000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1778.4508 - val_loss: 1198.5480\n",
      "Epoch 23/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1560.2051 - val_loss: 941.0848\n",
      "Epoch 24/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1599.7320 - val_loss: 901.1353\n",
      "Epoch 25/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1305.7571 - val_loss: 1498.4842\n",
      "Epoch 26/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1065.5396 - val_loss: 668.4415\n",
      "Epoch 27/10000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1148.2827 - val_loss: 1101.0464\n",
      "Epoch 28/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 803.4051 - val_loss: 493.0791\n",
      "Epoch 29/10000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 869.1526 - val_loss: 1004.5680\n",
      "Epoch 30/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 873.2073 - val_loss: 452.3467\n",
      "Epoch 31/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 932.5438 - val_loss: 1117.6494\n",
      "Epoch 32/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 883.5891 - val_loss: 720.9663\n",
      "Epoch 33/10000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 719.9175 - val_loss: 995.8190\n",
      "Epoch 34/10000\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 651.4309 - val_loss: 600.7629\n",
      "Epoch 35/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 705.3957 - val_loss: 541.9703\n",
      "Epoch 36/10000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 579.6961 - val_loss: 385.9655\n",
      "Epoch 37/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 653.6276 - val_loss: 386.2982\n",
      "Epoch 38/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 627.3180 - val_loss: 574.1694\n",
      "Epoch 39/10000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 556.9472 - val_loss: 403.5469\n",
      "Epoch 40/10000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 603.8911 - val_loss: 460.6193\n",
      "Epoch 41/10000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 508.8374 - val_loss: 350.4319\n",
      "Epoch 42/10000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 654.8603 - val_loss: 358.8790\n",
      "Epoch 43/10000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 527.0045 - val_loss: 366.6661\n",
      "Epoch 44/10000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 549.0173 - val_loss: 316.5177\n",
      "Epoch 45/10000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 486.7308 - val_loss: 373.4452\n",
      "Epoch 46/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 593.0596 - val_loss: 380.3088\n",
      "Epoch 47/10000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 547.0688 - val_loss: 377.6888\n",
      "Epoch 48/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 481.6733 - val_loss: 660.6745\n",
      "Epoch 49/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 577.0533 - val_loss: 293.5409\n",
      "Epoch 50/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 514.8994 - val_loss: 334.5816\n",
      "Epoch 51/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 549.0599 - val_loss: 362.9216\n",
      "Epoch 52/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 477.3517 - val_loss: 500.9883\n",
      "Epoch 53/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 461.9466 - val_loss: 320.1700\n",
      "Epoch 54/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 475.9624 - val_loss: 295.6988\n",
      "Epoch 55/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 586.4176 - val_loss: 304.0990\n",
      "Epoch 56/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 433.0668 - val_loss: 290.9615\n",
      "Epoch 57/10000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 471.7764 - val_loss: 582.5900\n",
      "Epoch 58/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 615.8509 - val_loss: 328.5814\n",
      "Epoch 59/10000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 423.8243 - val_loss: 853.3434\n",
      "Epoch 60/10000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 472.6577 - val_loss: 276.0281\n",
      "Epoch 61/10000\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 347.8088 - val_loss: 264.3886\n",
      "Epoch 62/10000\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 447.8811 - val_loss: 341.7417\n",
      "Epoch 63/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 417.9732 - val_loss: 438.2351\n",
      "Epoch 64/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 522.6219 - val_loss: 277.5051\n",
      "Epoch 65/10000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 531.0675 - val_loss: 370.3223\n",
      "Epoch 66/10000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 479.7416 - val_loss: 267.5514\n",
      "Epoch 67/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 453.3714 - val_loss: 306.7724\n",
      "Epoch 68/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 407.2002 - val_loss: 264.7048\n",
      "Epoch 69/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 429.1588 - val_loss: 676.5415\n",
      "Epoch 70/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 390.4017 - val_loss: 292.6481\n",
      "Epoch 71/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 402.5390 - val_loss: 855.6326\n",
      "Epoch 72/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 379.0628 - val_loss: 266.6686\n",
      "Epoch 73/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 408.7045 - val_loss: 272.5355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 457.5109 - val_loss: 279.0905\n",
      "Epoch 75/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 376.4175 - val_loss: 356.0011\n",
      "Epoch 76/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 367.1629 - val_loss: 463.1413\n",
      "Epoch 77/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 358.3432 - val_loss: 279.9959\n",
      "Epoch 78/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 385.3343 - val_loss: 587.7999\n",
      "Epoch 79/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 759.2222 - val_loss: 256.1992\n",
      "Epoch 80/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 339.1768 - val_loss: 238.5203\n",
      "Epoch 81/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 388.9839 - val_loss: 237.5248\n",
      "Epoch 82/10000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 582.2896 - val_loss: 286.4097\n",
      "Epoch 83/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 427.1039 - val_loss: 276.6115\n",
      "Epoch 84/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 328.1012 - val_loss: 270.4105\n",
      "Epoch 85/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 340.7524 - val_loss: 355.1638\n",
      "Epoch 86/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 493.0721 - val_loss: 259.7023\n",
      "Epoch 87/10000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 332.3516 - val_loss: 332.1352\n",
      "Epoch 88/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 402.0051 - val_loss: 298.2041\n",
      "Epoch 89/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 371.6787 - val_loss: 552.4990\n",
      "Epoch 90/10000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 340.0980 - val_loss: 260.3969\n",
      "Epoch 91/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 416.4200 - val_loss: 262.1260\n",
      "Epoch 92/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 389.2701 - val_loss: 305.0148\n",
      "Epoch 93/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 359.3524 - val_loss: 417.7973\n",
      "Epoch 94/10000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 367.4762 - val_loss: 238.4479\n",
      "Epoch 95/10000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 377.4707 - val_loss: 432.4140\n",
      "Epoch 96/10000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 354.9454 - val_loss: 294.8383\n",
      "Epoch 97/10000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 312.7125 - val_loss: 440.0619\n",
      "Epoch 98/10000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 353.8259 - val_loss: 246.1430\n",
      "Epoch 99/10000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 357.9632 - val_loss: 328.7903\n",
      "Epoch 100/10000\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 321.7196 - val_loss: 216.0699\n",
      "Epoch 101/10000\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 388.5311 - val_loss: 236.6143\n",
      "Epoch 102/10000\n",
      "7500/7500 [==============================] - 1s 107us/step - loss: 366.2777 - val_loss: 265.3339\n",
      "Epoch 103/10000\n",
      "7500/7500 [==============================] - 1s 115us/step - loss: 346.0119 - val_loss: 256.7454\n",
      "Epoch 104/10000\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 407.8958 - val_loss: 221.5624\n",
      "Epoch 105/10000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 360.5456 - val_loss: 352.0594\n",
      "Epoch 106/10000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 349.9213 - val_loss: 334.9368\n",
      "Epoch 107/10000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 361.1793 - val_loss: 231.3595\n",
      "Epoch 108/10000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 319.6772 - val_loss: 241.7640\n",
      "Epoch 109/10000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 378.3043 - val_loss: 641.0812\n",
      "Epoch 110/10000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 329.9184 - val_loss: 249.5632\n",
      "Epoch 111/10000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 340.5915 - val_loss: 233.8396\n",
      "Epoch 112/10000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 335.7622 - val_loss: 217.8930\n",
      "Epoch 113/10000\n",
      "7500/7500 [==============================] - 1s 112us/step - loss: 343.6798 - val_loss: 285.2898\n",
      "Epoch 114/10000\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 328.5523 - val_loss: 232.6810\n",
      "Epoch 115/10000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 311.5843 - val_loss: 548.5544\n",
      "Epoch 116/10000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 315.7315 - val_loss: 229.4577\n",
      "Epoch 117/10000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 309.4804 - val_loss: 233.7677\n",
      "Epoch 118/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 441.9910 - val_loss: 296.5844\n",
      "Epoch 119/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 338.7884 - val_loss: 218.1767\n",
      "Epoch 120/10000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 369.3294 - val_loss: 439.9393\n",
      "Epoch 121/10000\n",
      "7500/7500 [==============================] - 1s 101us/step - loss: 325.8724 - val_loss: 226.9568\n",
      "Epoch 122/10000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 333.4821 - val_loss: 225.5735\n",
      "Epoch 123/10000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 298.1294 - val_loss: 248.2868\n",
      "Epoch 124/10000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 355.0768 - val_loss: 229.0571\n",
      "Epoch 125/10000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 300.3373 - val_loss: 281.0620\n",
      "Epoch 126/10000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 315.3297 - val_loss: 214.4742\n",
      "Epoch 127/10000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 296.8643 - val_loss: 193.8179\n",
      "Epoch 128/10000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 265.0279 - val_loss: 222.1049\n",
      "Epoch 129/10000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 320.7902 - val_loss: 216.6524\n",
      "Epoch 130/10000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 345.3312 - val_loss: 258.1329\n",
      "Epoch 131/10000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 353.9855 - val_loss: 227.2774\n",
      "Epoch 132/10000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 409.6680 - val_loss: 293.5791\n",
      "Epoch 133/10000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 288.6162 - val_loss: 205.9669\n",
      "Epoch 134/10000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 370.7806 - val_loss: 212.5176\n",
      "Epoch 135/10000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 267.3187 - val_loss: 228.8546\n",
      "Epoch 136/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 279.4358 - val_loss: 244.3965\n",
      "Epoch 137/10000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 321.6710 - val_loss: 197.8214\n",
      "Epoch 138/10000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 287.4486 - val_loss: 286.0101\n",
      "Epoch 139/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 336.0490 - val_loss: 204.9000\n",
      "Epoch 140/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 272.8754 - val_loss: 685.4055\n",
      "Epoch 141/10000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 341.7060 - val_loss: 211.0541\n",
      "Epoch 142/10000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 294.4881 - val_loss: 390.4531\n",
      "Epoch 143/10000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 277.7238 - val_loss: 349.0349\n",
      "Epoch 144/10000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 283.6155 - val_loss: 207.2737\n",
      "Epoch 145/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 284.9725 - val_loss: 204.7638\n",
      "Epoch 146/10000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 310.9631 - val_loss: 204.0739\n",
      "Epoch 147/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 74us/step - loss: 281.6104 - val_loss: 275.1353\n",
      "Epoch 148/10000\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 303.0128 - val_loss: 896.6820\n",
      "Epoch 149/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 382.1927 - val_loss: 534.3156\n",
      "Epoch 150/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 274.9758 - val_loss: 222.9576\n",
      "Epoch 151/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 268.1526 - val_loss: 247.1741\n",
      "Epoch 152/10000\n",
      "7500/7500 [==============================] - 1s 104us/step - loss: 343.1787 - val_loss: 231.8839\n",
      "Epoch 153/10000\n",
      "7500/7500 [==============================] - 1s 139us/step - loss: 326.4865 - val_loss: 265.8332\n",
      "Epoch 154/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 317.9620 - val_loss: 199.1732\n",
      "Epoch 155/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 326.3424 - val_loss: 392.2970\n",
      "Epoch 156/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 286.8134 - val_loss: 222.4477\n",
      "Epoch 157/10000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 317.2728 - val_loss: 202.2053\n",
      "Epoch 158/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 273.1665 - val_loss: 205.5971\n",
      "Epoch 159/10000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 289.6313 - val_loss: 244.0376\n",
      "Epoch 160/10000\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 276.4735 - val_loss: 241.0491\n",
      "Epoch 161/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 276.3290 - val_loss: 394.2154\n",
      "Epoch 162/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 279.9385 - val_loss: 277.9412\n",
      "Epoch 163/10000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 338.1578 - val_loss: 208.9933\n",
      "Epoch 164/10000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 256.6047 - val_loss: 184.4249\n",
      "Epoch 165/10000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 280.8383 - val_loss: 203.8370\n",
      "Epoch 166/10000\n",
      "7500/7500 [==============================] - 1s 105us/step - loss: 280.0329 - val_loss: 243.7470\n",
      "Epoch 167/10000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 390.0261 - val_loss: 467.8294\n",
      "Epoch 168/10000\n",
      "7500/7500 [==============================] - 1s 122us/step - loss: 323.6806 - val_loss: 279.2081\n",
      "Epoch 169/10000\n",
      "7500/7500 [==============================] - 1s 116us/step - loss: 289.2575 - val_loss: 237.8559\n",
      "Epoch 170/10000\n",
      "7500/7500 [==============================] - 1s 108us/step - loss: 252.2560 - val_loss: 411.5979\n",
      "Epoch 171/10000\n",
      "7500/7500 [==============================] - 1s 100us/step - loss: 281.0704 - val_loss: 204.5390\n",
      "Epoch 172/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 227.9267 - val_loss: 196.5651\n",
      "Epoch 173/10000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 263.5499 - val_loss: 222.4197\n",
      "Epoch 174/10000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 273.5391 - val_loss: 285.1556\n",
      "Epoch 175/10000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 284.1122 - val_loss: 331.3998\n",
      "Epoch 176/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 296.6130 - val_loss: 213.4238\n",
      "Epoch 177/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 230.8779 - val_loss: 197.3847\n",
      "Epoch 178/10000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 338.3152 - val_loss: 442.1342\n",
      "Epoch 179/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 258.7367 - val_loss: 266.1138\n",
      "Epoch 180/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 263.6010 - val_loss: 349.2080\n",
      "Epoch 181/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 313.2755 - val_loss: 261.3714\n",
      "Epoch 182/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 283.7352 - val_loss: 198.1751\n",
      "Epoch 183/10000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 261.6134 - val_loss: 228.1344\n",
      "Epoch 184/10000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 227.9638 - val_loss: 314.8937\n",
      "Epoch 185/10000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 331.6431 - val_loss: 332.0030\n",
      "Epoch 186/10000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 332.7014 - val_loss: 195.1852\n",
      "Epoch 187/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 236.4952 - val_loss: 211.0332\n",
      "Epoch 188/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 270.1356 - val_loss: 347.3678\n",
      "Epoch 189/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 276.9588 - val_loss: 184.8879\n",
      "Epoch 190/10000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 260.2496 - val_loss: 494.8934\n",
      "Epoch 191/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 364.6126 - val_loss: 187.4256\n",
      "Epoch 192/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 263.2081 - val_loss: 182.4009\n",
      "Epoch 193/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 248.2964 - val_loss: 200.3368\n",
      "Epoch 194/10000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 245.4136 - val_loss: 220.2979\n",
      "Epoch 195/10000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 283.3488 - val_loss: 176.1044\n",
      "Epoch 196/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 244.4768 - val_loss: 283.6263\n",
      "Epoch 197/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 232.2562 - val_loss: 184.9597\n",
      "Epoch 198/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 233.3642 - val_loss: 254.3067\n",
      "Epoch 199/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 366.4718 - val_loss: 176.3053\n",
      "Epoch 200/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 219.8761 - val_loss: 176.1379\n",
      "Epoch 201/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 237.7905 - val_loss: 570.8396\n",
      "Epoch 202/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 244.3538 - val_loss: 331.6939\n",
      "Epoch 203/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 275.0006 - val_loss: 302.7311\n",
      "Epoch 204/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 252.6876 - val_loss: 176.2739\n",
      "Epoch 205/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 228.1200 - val_loss: 178.3541\n",
      "Epoch 206/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 276.8410 - val_loss: 179.2449\n",
      "Epoch 207/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 292.8580 - val_loss: 938.1531\n",
      "Epoch 208/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 269.8708 - val_loss: 215.5510\n",
      "Epoch 209/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 247.5562 - val_loss: 184.0675\n",
      "Epoch 210/10000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 331.8795 - val_loss: 274.1704\n",
      "Epoch 211/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 222.9967 - val_loss: 292.5836\n",
      "Epoch 212/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 236.2095 - val_loss: 183.6379\n",
      "Epoch 213/10000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 308.3301 - val_loss: 183.6337\n",
      "Epoch 214/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 219.7358 - val_loss: 206.8067\n",
      "Epoch 215/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 215.7324 - val_loss: 180.8398\n",
      "Epoch 216/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 296.6639 - val_loss: 216.5904\n",
      "Epoch 217/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 239.3722 - val_loss: 370.8064\n",
      "Epoch 218/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 226.5602 - val_loss: 222.9464\n",
      "Epoch 219/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 241.8759 - val_loss: 194.3720\n",
      "Epoch 220/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 60us/step - loss: 227.2464 - val_loss: 183.1685\n",
      "Epoch 221/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 264.4796 - val_loss: 231.7849\n",
      "Epoch 222/10000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 312.6618 - val_loss: 209.0924\n",
      "Epoch 223/10000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 241.4641 - val_loss: 193.5842\n",
      "Epoch 224/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 266.9206 - val_loss: 195.7032\n",
      "Epoch 225/10000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 217.4106 - val_loss: 600.1366\n",
      "Epoch 226/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 276.8777 - val_loss: 217.2198\n",
      "Epoch 227/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 257.0743 - val_loss: 181.0735\n",
      "Epoch 228/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 227.6841 - val_loss: 256.4215\n",
      "Epoch 229/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 215.1074 - val_loss: 237.0276\n",
      "Epoch 230/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 351.9935 - val_loss: 224.7604\n",
      "Epoch 231/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 262.7352 - val_loss: 373.3519\n",
      "Epoch 232/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 234.4553 - val_loss: 215.7884\n",
      "Epoch 233/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 215.5403 - val_loss: 170.8108\n",
      "Epoch 234/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 245.4433 - val_loss: 331.0797\n",
      "Epoch 235/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 236.6979 - val_loss: 311.1562\n",
      "Epoch 236/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 228.2666 - val_loss: 178.8730\n",
      "Epoch 237/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 219.5102 - val_loss: 171.1100\n",
      "Epoch 238/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 212.2776 - val_loss: 188.7457\n",
      "Epoch 239/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 259.0884 - val_loss: 172.7391\n",
      "Epoch 240/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 352.6791 - val_loss: 184.1158\n",
      "Epoch 241/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 238.8591 - val_loss: 282.6487\n",
      "Epoch 242/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 213.8847 - val_loss: 169.3327\n",
      "Epoch 243/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 216.7287 - val_loss: 328.2832\n",
      "Epoch 244/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 246.4422 - val_loss: 164.7851\n",
      "Epoch 245/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 222.9474 - val_loss: 200.8462\n",
      "Epoch 246/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 264.7733 - val_loss: 239.2563\n",
      "Epoch 247/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 216.0124 - val_loss: 201.9261\n",
      "Epoch 248/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 569.1210 - val_loss: 176.8021\n",
      "Epoch 249/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 192.4384 - val_loss: 163.7217\n",
      "Epoch 250/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 211.3477 - val_loss: 221.7211\n",
      "Epoch 251/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 225.3889 - val_loss: 165.6401\n",
      "Epoch 252/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 250.3506 - val_loss: 174.0598\n",
      "Epoch 253/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 272.9699 - val_loss: 179.1186\n",
      "Epoch 254/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 268.6015 - val_loss: 206.8064\n",
      "Epoch 255/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 209.9373 - val_loss: 169.2651\n",
      "Epoch 256/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 213.4758 - val_loss: 171.5362\n",
      "Epoch 257/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 237.6097 - val_loss: 450.0562\n",
      "Epoch 258/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 372.7576 - val_loss: 232.5677\n",
      "Epoch 259/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 217.7624 - val_loss: 210.1338\n",
      "Epoch 260/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 221.3625 - val_loss: 163.4650\n",
      "Epoch 261/10000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 237.0170 - val_loss: 164.3003\n",
      "Epoch 262/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 234.3507 - val_loss: 205.3786\n",
      "Epoch 263/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 242.3974 - val_loss: 281.4371\n",
      "Epoch 264/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 276.1099 - val_loss: 175.0725\n",
      "Epoch 265/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 219.9477 - val_loss: 177.5019\n",
      "Epoch 266/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 277.1033 - val_loss: 163.2336\n",
      "Epoch 267/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 210.5922 - val_loss: 272.1695\n",
      "Epoch 268/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 474.9808 - val_loss: 203.1817\n",
      "Epoch 269/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 320.2120 - val_loss: 184.6377\n",
      "Epoch 270/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 270.8697 - val_loss: 178.4066\n",
      "Epoch 271/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 250.0876 - val_loss: 172.0949\n",
      "Epoch 272/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 256.9196 - val_loss: 199.5644\n",
      "Epoch 273/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 210.3660 - val_loss: 186.2750\n",
      "Epoch 274/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 257.3388 - val_loss: 180.8090\n",
      "Epoch 275/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 216.3684 - val_loss: 194.9187\n",
      "Epoch 276/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 222.2400 - val_loss: 246.3629\n",
      "Epoch 277/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 237.1806 - val_loss: 186.2737\n",
      "Epoch 278/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 224.4609 - val_loss: 179.7068\n",
      "Epoch 279/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 207.7955 - val_loss: 165.6256\n",
      "Epoch 280/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 204.7324 - val_loss: 161.7622\n",
      "Epoch 281/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 247.1897 - val_loss: 174.7118\n",
      "Epoch 282/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 193.5087 - val_loss: 188.2080\n",
      "Epoch 283/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 216.4183 - val_loss: 182.7729\n",
      "Epoch 284/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 214.2374 - val_loss: 205.7244\n",
      "Epoch 285/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 208.6921 - val_loss: 164.3834\n",
      "Epoch 286/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 309.0919 - val_loss: 164.2154\n",
      "Epoch 287/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 211.5733 - val_loss: 187.7680\n",
      "Epoch 288/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 221.1356 - val_loss: 242.6041\n",
      "Epoch 289/10000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 189.5914 - val_loss: 222.8803\n",
      "Epoch 290/10000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 204.2639 - val_loss: 160.7569\n",
      "Epoch 291/10000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 209.6268 - val_loss: 171.4030\n",
      "Epoch 292/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 269.7298 - val_loss: 170.2621\n",
      "Epoch 293/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 63us/step - loss: 218.5678 - val_loss: 157.0504\n",
      "Epoch 294/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 203.8747 - val_loss: 198.0651\n",
      "Epoch 295/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 342.6318 - val_loss: 455.7004\n",
      "Epoch 296/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 245.7855 - val_loss: 164.3319\n",
      "Epoch 297/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 314.9827 - val_loss: 171.2228\n",
      "Epoch 298/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 225.7242 - val_loss: 162.7381\n",
      "Epoch 299/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 228.9916 - val_loss: 186.8771\n",
      "Epoch 300/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 209.3761 - val_loss: 234.1934\n",
      "Epoch 301/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 196.2225 - val_loss: 170.3041\n",
      "Epoch 302/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 207.3094 - val_loss: 243.9485\n",
      "Epoch 303/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 197.7936 - val_loss: 164.5468\n",
      "Epoch 304/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 408.5982 - val_loss: 214.5836\n",
      "Epoch 305/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 292.2335 - val_loss: 184.9275\n",
      "Epoch 306/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 195.6043 - val_loss: 157.7992\n",
      "Epoch 307/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 224.3646 - val_loss: 182.9201\n",
      "Epoch 308/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 251.5124 - val_loss: 197.6188\n",
      "Epoch 309/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 216.3640 - val_loss: 172.5035\n",
      "Epoch 310/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 341.9306 - val_loss: 243.0060\n",
      "Epoch 311/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 208.0426 - val_loss: 170.4640\n",
      "Epoch 312/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 200.8243 - val_loss: 235.4801\n",
      "Epoch 313/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 300.7940 - val_loss: 187.3686\n",
      "Epoch 314/10000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 305.4545 - val_loss: 204.7365\n",
      "Epoch 315/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 193.6208 - val_loss: 174.3411\n",
      "Epoch 316/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 243.3371 - val_loss: 220.5761\n",
      "Epoch 317/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 233.7185 - val_loss: 369.0826\n",
      "Epoch 318/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 249.0803 - val_loss: 190.9658\n",
      "Epoch 319/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 275.5943 - val_loss: 174.8080\n",
      "Epoch 320/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 221.7368 - val_loss: 195.8611\n",
      "Epoch 321/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 208.9151 - val_loss: 185.2871\n",
      "Epoch 322/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 211.2040 - val_loss: 159.1282\n",
      "Epoch 323/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 291.2402 - val_loss: 183.0074\n",
      "Epoch 324/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 246.9489 - val_loss: 166.2997\n",
      "Epoch 325/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 192.0053 - val_loss: 165.1592\n",
      "Epoch 326/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 198.5301 - val_loss: 158.8688\n",
      "Epoch 327/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 210.9083 - val_loss: 194.5927\n",
      "Epoch 328/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 194.2334 - val_loss: 219.7295\n",
      "Epoch 329/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 594.7942 - val_loss: 681.9810\n",
      "Epoch 330/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 516.5177 - val_loss: 274.4672\n",
      "Epoch 331/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 325.9202 - val_loss: 330.8762\n",
      "Epoch 332/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 297.9743 - val_loss: 196.0242\n",
      "Epoch 333/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 272.7076 - val_loss: 384.4213\n",
      "Epoch 334/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 261.9360 - val_loss: 201.7951\n",
      "Epoch 335/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 251.7980 - val_loss: 256.3144\n",
      "Epoch 336/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 230.9868 - val_loss: 184.4027\n",
      "Epoch 337/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 257.9545 - val_loss: 295.4127\n",
      "Epoch 338/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 269.0526 - val_loss: 185.8106\n",
      "Epoch 339/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 235.4037 - val_loss: 247.3176\n",
      "Epoch 340/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 250.0046 - val_loss: 188.1061\n",
      "Epoch 341/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 271.0078 - val_loss: 167.3353\n",
      "Epoch 342/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 207.2069 - val_loss: 159.4799\n",
      "Epoch 343/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 200.9286 - val_loss: 173.9951\n",
      "Epoch 344/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 226.5015 - val_loss: 219.1682\n",
      "Epoch 345/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 245.0926 - val_loss: 170.8118\n",
      "Epoch 346/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 221.9951 - val_loss: 376.0234\n",
      "Epoch 347/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 219.4343 - val_loss: 155.6839\n",
      "Epoch 348/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 192.9997 - val_loss: 167.8333\n",
      "Epoch 349/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 228.6991 - val_loss: 215.2179\n",
      "Epoch 350/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 349.9017 - val_loss: 193.2465\n",
      "Epoch 351/10000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 199.7940 - val_loss: 166.5522\n",
      "Epoch 352/10000\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 202.8735 - val_loss: 187.0352\n",
      "Epoch 353/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 232.6974 - val_loss: 391.6346\n",
      "Epoch 354/10000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 207.3164 - val_loss: 190.4137\n",
      "Epoch 355/10000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 227.3114 - val_loss: 170.6738\n",
      "Epoch 356/10000\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 232.5445 - val_loss: 168.7235\n",
      "Epoch 357/10000\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 201.2967 - val_loss: 208.9789\n",
      "Epoch 358/10000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 223.2768 - val_loss: 174.2504\n",
      "Epoch 359/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 214.6335 - val_loss: 207.4963\n",
      "Epoch 360/10000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 196.5473 - val_loss: 225.8652\n",
      "Epoch 361/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 203.6789 - val_loss: 199.7882\n",
      "Epoch 362/10000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 264.4383 - val_loss: 156.8337\n",
      "Epoch 363/10000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 232.3950 - val_loss: 167.0551\n",
      "Epoch 364/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 199.7759 - val_loss: 170.0642\n",
      "Epoch 365/10000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 205.6531 - val_loss: 155.8729\n",
      "Epoch 366/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 68us/step - loss: 197.2766 - val_loss: 169.9963\n",
      "Epoch 367/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 232.0088 - val_loss: 216.2478\n",
      "Epoch 368/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 218.3748 - val_loss: 175.4967\n",
      "Epoch 369/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 250.3275 - val_loss: 158.5181\n",
      "Epoch 370/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 201.3271 - val_loss: 185.4821\n",
      "Epoch 371/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 190.2593 - val_loss: 208.1615\n",
      "Epoch 372/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 215.2478 - val_loss: 182.4291\n",
      "Epoch 373/10000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 361.0135 - val_loss: 200.7411\n",
      "Epoch 374/10000\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 197.2189 - val_loss: 274.1982\n",
      "Epoch 375/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 215.1092 - val_loss: 182.0819\n",
      "Epoch 376/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 197.2269 - val_loss: 230.3979\n",
      "Epoch 377/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 213.3957 - val_loss: 155.4538\n",
      "Epoch 378/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 200.6108 - val_loss: 161.2611\n",
      "Epoch 379/10000\n",
      "  32/7500 [..............................] - ETA: 0s - loss: 108.5908"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=filename_checkpoint, verbose=0, save_best_only=True)\n",
    "\n",
    "# Turn off KFold\n",
    "if (0):\n",
    "    oos_y = []\n",
    "    oos_pred = []\n",
    "    fold = 0\n",
    "    \n",
    "    for train, test in kf.split(x):\n",
    "        fold+=1\n",
    "        print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "        x_train = x[train]\n",
    "        y_train = y[train]\n",
    "        x_test = x[test]\n",
    "        y_test = y[test]\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "model.add(Dropout(0.01)) # Dropout Layer\n",
    "model.add(Dense(25, activation='relu')) # Hidden 2\n",
    "model.add(Dense(10, \n",
    "                kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01),activation='relu')) # Hidden 3 w/regularization\n",
    "model.add(Dense(1)) # Output\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=100, verbose=1, mode='auto')\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpoint],verbose=1,epochs=10000)\n",
    "    \n",
    "pred = model.predict(x_test)\n",
    "    \n",
    "#    oos_y.append(y_test)\n",
    "#    oos_pred.append(pred)        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold score (RMSE): 11.68252182006836\n"
     ]
    }
   ],
   "source": [
    "# Measure this fold's RMSE\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Fold score (RMSE): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final, out of sample score (RMSE): 12.102914810180664\n"
     ]
    }
   ],
   "source": [
    "# Build the oos prediction list and calculate the error.\n",
    "#oos_y = np.concatenate(oos_y)\n",
    "#oos_pred = np.concatenate(oos_pred)\n",
    "#score = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))\n",
    "#print(\"Final, out of sample score (RMSE): {}\".format(score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#pred = model.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "#score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "#print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "#chart_regression(pred.flatten(),y_test)\n",
    "#chart_regression(pred.flatten(),y_test,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(filename_checkpoint)\n",
    "\n",
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "extract_and_encode_features(df_test)\n",
    "\n",
    "ids_test = df_test['id']\n",
    "df_test.drop('id',1,inplace=True)\n",
    "\n",
    "names_test = df_test['name']\n",
    "df_test.drop('name',1,inplace=True)\n",
    "\n",
    "x_submit = df_test.as_matrix().astype(np.float32)\n",
    "pred_submit = model.predict(x_submit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = [n if n > 0 else 0 for n in pred_submit[:,0]]\n",
    "df_submit = pd.DataFrame({'id': ids_test,'cost': cost})\n",
    "df_submit = df_submit[['id', 'cost']]\n",
    "df_submit.to_csv(filename_submit, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
