{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n",
    "\n",
    "**Kaggle Assignment: **\n",
    "\n",
    "**Student Name: Jason Walker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## Description\n",
    "This is one of the projects from the course T81-855: Applications of Deep Learning at Washington University in St. Louis. All students must create a Kaggle account and submit a solution. Once you have submitted your solution entry log into Blackboard (at WUSTL) and submit a single file telling me your Kaggle name on the leaderboard (you do not need to register to Kaggle with your real name). This competition will be visible to the public, so there may be non-student submissions as well as student.\n",
    "\n",
    "The data set for this competition consists of a number of input columns that should be used to predict a stores sales. This is a regression problem. The inputs are a mixture of discrete and category values. The data set is from a simulation.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The evaluation pages describes how submissions will be scored and how students should format their submissions. The scores are in RMSE.\n",
    "Submission Format\n",
    "\n",
    "For every store in the dataset, submission files should contain a sales volume.\n",
    "\n",
    "The file should contain a header and have the following format:\n",
    "\n",
    "```\n",
    "100000,1.23\n",
    "100001,1.123\n",
    "100002,3.332\n",
    "100003,1.53\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The data contains data and costs for various office supplies. The data came from a simulation and do not directly correspond to any real-world items. See how well you can predict the cost of an item using the provided data. Feature engineering will likely help you. The *name* column may seem useless at first glance; however, it contains information that you can parse to help your predictions.\n",
    "File descriptions\n",
    "```\n",
    "    id - The identifier/primary key.\n",
    "    name - The name of this item.\n",
    "    manufacturer - The manufacturer.\n",
    "    pack - The number of items in this pack.\n",
    "    weight - The weight of a pack of these items.\n",
    "    height - The height of a pack of these items.\n",
    "    width - The width of a pack of these items.\n",
    "    length - The length of a pack of these items.\n",
    "    cost - The cost for this item pack. This is what you are to predict (the target). \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions\n",
    "\n",
    "You will see these at the top of every module and assignment.  These are simply a set of reusable functions that we will make use of.  Each of them will be explained as the semester progresses.  They are explained in greater detail as the course progresses.  Class 4 contains a complete overview of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n",
    "        \n",
    "# This function submits an assignment.  You can submit an assignment as much as you like, only the final\n",
    "# submission counts.  The paramaters are as follows:\n",
    "# data - Pandas dataframe output.\n",
    "# key - Your student key that was emailed to you.\n",
    "# no - The assignment class number, should be 1 through 1.\n",
    "# source_file - The full path to your Python or IPYNB file.  This must have \"_class1\" as part of its name.  \n",
    "# .             The number must match your assignment number.  For example \"_class2\" for class assignment #2.\n",
    "def submit(data,key,no,source_file=None):\n",
    "    if source_file is None and '__file__' not in globals(): raise Exception('Must specify a filename when a Jupyter notebook.')\n",
    "    if source_file is None: source_file = __file__\n",
    "    suffix = '_class{}'.format(no)\n",
    "    if suffix not in source_file: raise Exception('{} must be part of the filename.'.format(suffix))\n",
    "    with open(source_file, \"rb\") as image_file:\n",
    "        encoded_python = base64.b64encode(image_file.read()).decode('ascii')\n",
    "    ext = os.path.splitext(source_file)[-1].lower()\n",
    "    if ext not in ['.ipynb','.py']: raise Exception(\"Source file is {} must be .py or .ipynb\".format(ext))\n",
    "    r = requests.post(\"https://api.heatonresearch.com/assignment-submit\",\n",
    "        headers={'x-api-key':key}, json={'csv':base64.b64encode(data.to_csv(index=False).encode('ascii')).decode(\"ascii\"),\n",
    "        'assignment': no, 'ext':ext, 'py':encoded_python})\n",
    "    if r.status_code == 200:\n",
    "        print(\"Success: {}\".format(r.text))\n",
    "    else: print(\"Failure: {}\".format(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kaggle Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwalker/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers\n",
    "\n",
    "path = './data'\n",
    "\n",
    "filename_test = os.path.join(path,\"test.csv\")\n",
    "filename_train = os.path.join(path,\"train.csv\")\n",
    "filename_sample = os.path.join(path,\"sample.csv\")\n",
    "filename_submit = os.path.join(path,\"submit.csv\")\n",
    "filename_checkpoint = os.path.join(path,\"checkpoint.hdf5\")\n",
    "\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "\n",
    "np.random.seed(42) # Uncomment this line to get the same shuffle each time\n",
    "df_train = df_train.reindex(np.random.permutation(df_train.index))\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Encode Features\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def extract_and_encode_features(df):\n",
    "    color_regex='(?P<color>red|blue|green|yellow|orange|pink|black|brown|white)'\n",
    "    df['color'] = df.name.str.extract(color_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    quality_regex='(?P<quality>generic|medium\\shigh\\squality|high\\squality)'\n",
    "    df['quality'] = df.name.str.extract(quality_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    size_regex='(?P<size>tiny|small|medium|large)'\n",
    "    df['size'] = df.name.str.extract(size_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    item_regex='(?P<item>paperclips|paperweights|ink\\spens|pencils|stapler|tablets|thumbtacks|post\\sit\\snotes)'\n",
    "    df['item'] = df.name.str.extract(item_regex, flags=re.IGNORECASE, expand=False)\n",
    "    \n",
    "    for column in ['pack','weight','height','width','length']:\n",
    "        missing_median(df_train,column)\n",
    "    \n",
    "    df.insert(1,'surface_area',(df['height']*df['width']*df['length']).astype(int))\n",
    "    \n",
    "    ## encode numeric features\n",
    "    for column in ['pack','weight','height','width','length','surface_area']:\n",
    "        encode_numeric_zscore(df,column)\n",
    "     \n",
    "    # encode text/categorical features\n",
    "    for column in ['manufacturer','color','quality','size','item']:\n",
    "        encode_text_dummy(df,column)\n",
    "  \n",
    "\n",
    "extract_and_encode_features(df_train)\n",
    "ids_train = df_train['id']\n",
    "df_train.drop('id',1,inplace=True)\n",
    "\n",
    "names_train = df_train['name']\n",
    "df_train.drop('name',1,inplace=True)\n",
    "\n",
    "x,y = to_xy(df_train,'cost')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=45)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 32.211875915527344\n",
      "['surface_area', 'pack', 'weight', 'height', 'width', 'length', 'manufacturer-6% Solution', 'manufacturer-Deep Office Supplies', 'manufacturer-Duck Lake', 'manufacturer-Offices-R-Us', 'manufacturer-WizBang', 'color-Black', 'color-Blue', 'color-Brown', 'color-Green', 'color-Pink', 'color-Red', 'color-White', 'quality-Generic', 'quality-High Quality', 'quality-Medium High Quality', 'size-Large', 'size-Medium', 'size-Small', 'size-Tiny', 'item-Ink Pens', 'item-Paperclips', 'item-Paperweights', 'item-Pencils', 'item-Post It Notes', 'item-Stapler', 'item-Tablets', 'item-Thumbtacks']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item-Post It Notes</th>\n",
       "      <td>-76.430450</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Thumbtacks</th>\n",
       "      <td>-57.680199</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperclips</th>\n",
       "      <td>-49.292679</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Pencils</th>\n",
       "      <td>-46.917885</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Red</th>\n",
       "      <td>-38.541794</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Green</th>\n",
       "      <td>-26.209276</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>-16.682915</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>width</th>\n",
       "      <td>-16.682903</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>-16.682266</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Blue</th>\n",
       "      <td>-12.348345</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Tiny</th>\n",
       "      <td>-4.614498</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Generic</th>\n",
       "      <td>-4.514582</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Offices-R-Us</th>\n",
       "      <td>-0.851445</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Deep Office Supplies</th>\n",
       "      <td>-0.733551</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Brown</th>\n",
       "      <td>-0.552734</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack</th>\n",
       "      <td>-0.446889</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Small</th>\n",
       "      <td>-0.333237</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-6% Solution</th>\n",
       "      <td>-0.189339</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Duck Lake</th>\n",
       "      <td>0.473938</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Ink Pens</th>\n",
       "      <td>0.496303</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-WizBang</th>\n",
       "      <td>1.299829</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Medium</th>\n",
       "      <td>4.834967</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Medium High Quality</th>\n",
       "      <td>5.669786</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Large</th>\n",
       "      <td>6.766665</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-High Quality</th>\n",
       "      <td>7.788834</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Black</th>\n",
       "      <td>13.706039</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperweights</th>\n",
       "      <td>14.917278</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surface_area</th>\n",
       "      <td>18.134123</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-White</th>\n",
       "      <td>26.833145</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Pink</th>\n",
       "      <td>38.844139</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Stapler</th>\n",
       "      <td>51.922276</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>105.468979</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Tablets</th>\n",
       "      <td>162.987289</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         coef  positive\n",
       "item-Post It Notes                 -76.430450     False\n",
       "item-Thumbtacks                    -57.680199     False\n",
       "item-Paperclips                    -49.292679     False\n",
       "item-Pencils                       -46.917885     False\n",
       "color-Red                          -38.541794     False\n",
       "color-Green                        -26.209276     False\n",
       "length                             -16.682915     False\n",
       "width                              -16.682903     False\n",
       "height                             -16.682266     False\n",
       "color-Blue                         -12.348345     False\n",
       "size-Tiny                           -4.614498     False\n",
       "quality-Generic                     -4.514582     False\n",
       "manufacturer-Offices-R-Us           -0.851445     False\n",
       "manufacturer-Deep Office Supplies   -0.733551     False\n",
       "color-Brown                         -0.552734     False\n",
       "pack                                -0.446889     False\n",
       "size-Small                          -0.333237     False\n",
       "manufacturer-6% Solution            -0.189339     False\n",
       "manufacturer-Duck Lake               0.473938      True\n",
       "item-Ink Pens                        0.496303      True\n",
       "manufacturer-WizBang                 1.299829      True\n",
       "size-Medium                          4.834967      True\n",
       "quality-Medium High Quality          5.669786      True\n",
       "size-Large                           6.766665      True\n",
       "quality-High Quality                 7.788834      True\n",
       "color-Black                         13.706039      True\n",
       "item-Paperweights                   14.917278      True\n",
       "surface_area                        18.134123      True\n",
       "color-White                         26.833145      True\n",
       "color-Pink                          38.844139      True\n",
       "item-Stapler                        51.922276      True\n",
       "weight                             105.468979      True\n",
       "item-Tablets                       162.987289      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [ 126.38175964]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAD8CAYAAADExYYgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d0ba240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple function to evaluate the coefficients of a regression\n",
    "%matplotlib inline    \n",
    "from IPython.display import display, HTML    \n",
    "\n",
    "def report_coef(names,coef,intercept):\n",
    "    r = pd.DataFrame( { 'coef': coef, 'positive': coef>=0  }, index = names )\n",
    "    r = r.sort_values(by=['coef'])\n",
    "    display(r)\n",
    "    print(\"Intercept: {}\".format(intercept))\n",
    "    r['coef'].plot(kind='barh', color=r['positive'].map({True: 'b', False: 'r'}))\n",
    "    \n",
    "# Create linear regression\n",
    "regressor = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# Fit/train linear regression\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "print(names)\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_[0,:],\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 19177.3847 - val_loss: 3324.0908\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1828.9177 - val_loss: 1005.8463\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 854.6915 - val_loss: 661.3692\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 634.4721 - val_loss: 514.2066\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 503.7967 - val_loss: 419.8932\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 421.5738 - val_loss: 367.4138\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 379.5941 - val_loss: 333.1901\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 356.6737 - val_loss: 317.2318\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 346.1430 - val_loss: 307.0019\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 332.5588 - val_loss: 303.6109\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 323.7663 - val_loss: 303.3367\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 323.3723 - val_loss: 297.1005\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 313.0607 - val_loss: 297.1665\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 316.8191 - val_loss: 288.6112\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 307.5308 - val_loss: 288.1161\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 313.6897 - val_loss: 282.5090\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 303.6229 - val_loss: 285.7539\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 306.5680 - val_loss: 278.6052\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 300.3322 - val_loss: 280.1271\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 298.3427 - val_loss: 276.1864\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 300.7870 - val_loss: 277.9214\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 298.6771 - val_loss: 281.9801\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 286.3003 - val_loss: 275.0902\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 288.7432 - val_loss: 280.7017\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 284.9640 - val_loss: 276.7649\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 283.3761 - val_loss: 268.1440\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 283.0170 - val_loss: 275.5398\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 1s 104us/step - loss: 283.8027 - val_loss: 275.6967\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 278.8716 - val_loss: 277.7215\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 1s 106us/step - loss: 285.8973 - val_loss: 265.5024\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 275.8552 - val_loss: 264.2407\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 277.2471 - val_loss: 262.6699\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 273.4119 - val_loss: 277.1110\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 1s 110us/step - loss: 272.7426 - val_loss: 266.6759\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 272.3453 - val_loss: 264.4512\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 273.2452 - val_loss: 286.3139\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 268.9944 - val_loss: 264.0201\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 270.7630 - val_loss: 264.7357\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 264.8370 - val_loss: 256.2089\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 1s 174us/step - loss: 268.6348 - val_loss: 266.1414\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 1s 112us/step - loss: 265.2856 - val_loss: 256.1711\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 261.4569 - val_loss: 252.3745\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 260.1739 - val_loss: 251.0669\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 265.4959 - val_loss: 249.4016\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 258.0432 - val_loss: 252.7264\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 261.8453 - val_loss: 259.0749\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 1s 113us/step - loss: 259.2913 - val_loss: 251.0820\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 258.1465 - val_loss: 249.9515\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 255.6972 - val_loss: 256.0971\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 255.4801 - val_loss: 262.3042\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 255.9482 - val_loss: 264.9464\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 254.3031 - val_loss: 255.0733\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 251.9845 - val_loss: 245.2686\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 255.4083 - val_loss: 247.1905\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 250.4338 - val_loss: 249.9638\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 248.4380 - val_loss: 254.5555\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 252.2337 - val_loss: 249.7106\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 251.323 - 1s 153us/step - loss: 250.0412 - val_loss: 249.3625\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 1s 106us/step - loss: 253.0613 - val_loss: 245.4814\n",
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 1s 97us/step - loss: 244.1994 - val_loss: 253.6375\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 250.4455 - val_loss: 245.7296\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 1s 99us/step - loss: 243.8970 - val_loss: 249.5550\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 1s 169us/step - loss: 246.5630 - val_loss: 240.8864\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 1s 127us/step - loss: 242.8350 - val_loss: 246.1817\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 1s 117us/step - loss: 244.9541 - val_loss: 241.5246\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 245.8381 - val_loss: 256.4188\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 1s 128us/step - loss: 243.0721 - val_loss: 241.5203\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 1s 131us/step - loss: 244.3178 - val_loss: 242.3092\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 1s 162us/step - loss: 246.0388 - val_loss: 240.5183\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 243.4693 - val_loss: 241.5539\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 1s 139us/step - loss: 243.0276 - val_loss: 239.1946\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 240.3365 - val_loss: 240.5364\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 239.9569 - val_loss: 245.2352\n",
      "Epoch 74/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 85us/step - loss: 239.1907 - val_loss: 243.2305\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 241.8940 - val_loss: 251.0011\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 237.2910 - val_loss: 243.0167\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 238.1536 - val_loss: 244.9358\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 242.4332 - val_loss: 252.3323\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 239.4404 - val_loss: 248.4904\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 239.1040 - val_loss: 252.7971\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 236.6915 - val_loss: 251.3033\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 235.4914 - val_loss: 238.9734\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 236.8231 - val_loss: 240.4371\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 233.2737 - val_loss: 248.6667\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 243.1538 - val_loss: 240.0534\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 233.0320 - val_loss: 242.3460\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 234.6724 - val_loss: 255.1408\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 234.8982 - val_loss: 241.2768\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 231.3287 - val_loss: 246.6897\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 234.5378 - val_loss: 238.8207\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 238.1768 - val_loss: 234.0188\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 235.0004 - val_loss: 247.1411\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 232.5329 - val_loss: 237.5942\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 233.5202 - val_loss: 239.8306\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 233.1448 - val_loss: 256.6365\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 231.5295 - val_loss: 233.1479\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 230.0758 - val_loss: 233.3186\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 231.2467 - val_loss: 233.6258\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 231.4294 - val_loss: 231.2010\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 230.5758 - val_loss: 242.5564\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 229.6225 - val_loss: 237.8620\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 230.6911 - val_loss: 235.7129\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 230.1296 - val_loss: 239.4361\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 230.0286 - val_loss: 236.7511\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 228.3373 - val_loss: 232.0725\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 229.2133 - val_loss: 235.2278\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 224.6220 - val_loss: 233.5231\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 231.6769 - val_loss: 250.3071\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 232.8748 - val_loss: 249.5720\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 230.2714 - val_loss: 234.0951\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 229.9259 - val_loss: 232.3940\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 227.9654 - val_loss: 248.8686\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 230.1935 - val_loss: 232.9734\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 226.6051 - val_loss: 234.7765\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 224.6788 - val_loss: 232.8968\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 229.5079 - val_loss: 232.7458\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 226.6843 - val_loss: 235.6458\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 228.5401 - val_loss: 234.7806\n",
      "Epoch 119/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 227.4402 - val_loss: 233.9348\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 226.6237 - val_loss: 232.5544\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 221.9471 - val_loss: 234.2008\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 223.8760 - val_loss: 236.7641\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 224.3012 - val_loss: 230.8408\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 222.8287 - val_loss: 231.9653\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 225.5849 - val_loss: 238.8681\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 224.4063 - val_loss: 236.9471\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 223.3447 - val_loss: 233.1555\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 221.5057 - val_loss: 236.1856\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 218.3431 - val_loss: 235.2197\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 219.1615 - val_loss: 233.0366\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 222.4258 - val_loss: 238.2547\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 223.7707 - val_loss: 238.3528\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 221.1135 - val_loss: 244.9999\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 219.7149 - val_loss: 228.5907\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 219.5168 - val_loss: 231.5814\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 220.5358 - val_loss: 234.7042\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 224.0797 - val_loss: 230.2468\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 222.5431 - val_loss: 229.1532\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 219.4635 - val_loss: 232.4376\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 218.7175 - val_loss: 240.1232\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 225.8736 - val_loss: 241.4186\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 219.6656 - val_loss: 229.7205\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 222.1684 - val_loss: 230.1161\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 217.7056 - val_loss: 231.8288\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 216.7408 - val_loss: 242.4957\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 223.8856 - val_loss: 237.9307\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 1s 118us/step - loss: 217.2543 - val_loss: 231.3048\n",
      "Epoch 148/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 84us/step - loss: 218.9207 - val_loss: 244.0079\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 217.7360 - val_loss: 234.0339\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 219.9330 - val_loss: 232.2217\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 220.4232 - val_loss: 233.2286\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 218.9615 - val_loss: 229.3980\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 216.8817 - val_loss: 232.0293\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 213.7329 - val_loss: 229.8084\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 214.011 - 0s 61us/step - loss: 215.8284 - val_loss: 229.2359\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 219.1778 - val_loss: 243.3164\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 214.9557 - val_loss: 228.9171\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 219.9091 - val_loss: 231.5546\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 1s 119us/step - loss: 214.8351 - val_loss: 229.0243\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 217.1314 - val_loss: 237.7579\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 219.3718 - val_loss: 240.6825\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 215.4602 - val_loss: 234.8478\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 217.5900 - val_loss: 229.7011\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 215.2754 - val_loss: 230.6806\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 217.6711 - val_loss: 229.6420\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 214.5592 - val_loss: 227.3863\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 215.1871 - val_loss: 228.6707\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 214.2321 - val_loss: 235.7733\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 219.3317 - val_loss: 228.3572\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 215.0995 - val_loss: 240.3751\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 217.7715 - val_loss: 231.9657\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 215.4823 - val_loss: 246.2580\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 213.3107 - val_loss: 230.9068\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 210.7888 - val_loss: 230.0143\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 213.9755 - val_loss: 240.3639\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 217.0389 - val_loss: 231.3716\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 213.2221 - val_loss: 230.4586\n",
      "Epoch 178/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 213.6910 - val_loss: 228.9455\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 213.1925 - val_loss: 252.3430\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 212.3802 - val_loss: 225.1829\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 214.5129 - val_loss: 238.8106\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 213.9971 - val_loss: 230.1764\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 213.4518 - val_loss: 232.2363\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 210.8981 - val_loss: 230.6880\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 210.3518 - val_loss: 226.2496\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 212.0866 - val_loss: 244.0628\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 210.0836 - val_loss: 228.8089\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 209.8235 - val_loss: 229.3651\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 210.3638 - val_loss: 235.4673\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 210.7141 - val_loss: 229.6085\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 210.5935 - val_loss: 232.1709\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 215.4028 - val_loss: 227.0448\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 216.3236 - val_loss: 225.8077\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 212.6212 - val_loss: 228.6458\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 1s 160us/step - loss: 211.7825 - val_loss: 225.0088\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 1s 131us/step - loss: 208.0272 - val_loss: 257.7420\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 1s 125us/step - loss: 211.1677 - val_loss: 227.6677\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 1s 132us/step - loss: 208.9571 - val_loss: 225.5611\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 1s 116us/step - loss: 207.4634 - val_loss: 226.2980\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 211.0545 - val_loss: 227.0633\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 213.2971 - val_loss: 241.2107\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 212.1624 - val_loss: 227.8366\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 213.1066 - val_loss: 228.7379\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 207.8081 - val_loss: 236.4752\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 211.5289 - val_loss: 224.7702\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 209.1789 - val_loss: 237.5027\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 209.6363 - val_loss: 224.7473\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 1s 101us/step - loss: 209.0000 - val_loss: 226.4362\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 208.6106 - val_loss: 233.5448\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 1s 112us/step - loss: 210.2919 - val_loss: 233.1106\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 208.6551 - val_loss: 231.5265\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 208.0180 - val_loss: 224.7730\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 208.9414 - val_loss: 228.3087\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 209.9915 - val_loss: 228.7839\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 209.1887 - val_loss: 225.8351\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 208.9081 - val_loss: 228.2123\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 1s 104us/step - loss: 205.2028 - val_loss: 227.0314\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 1s 107us/step - loss: 207.4467 - val_loss: 230.8117\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 213.1979 - val_loss: 260.4373\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 207.1451 - val_loss: 228.7042\n",
      "Epoch 221/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 82us/step - loss: 206.9552 - val_loss: 257.8719\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 207.5077 - val_loss: 226.9177\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 206.9014 - val_loss: 236.9512\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 205.9155 - val_loss: 224.5848\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 207.2806 - val_loss: 225.9834\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 208.9577 - val_loss: 238.3925\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 207.1184 - val_loss: 229.0755\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 201.7835 - val_loss: 227.3573\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 208.0584 - val_loss: 238.7160\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 204.6353 - val_loss: 225.3105\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 204.2708 - val_loss: 243.1045\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 207.6965 - val_loss: 226.3361\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 203.4405 - val_loss: 224.7583\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 201.7283 - val_loss: 224.2333\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 203.9468 - val_loss: 226.7535\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 202.1469 - val_loss: 223.9462\n",
      "Epoch 237/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 204.5690 - val_loss: 233.1325\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 201.1515 - val_loss: 224.6448\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 202.0561 - val_loss: 260.4160\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 206.0640 - val_loss: 239.2215\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 203.1093 - val_loss: 229.1738\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 198.9269 - val_loss: 227.4552\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 204.4358 - val_loss: 223.8054\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 201.3550 - val_loss: 228.0121\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 201.3240 - val_loss: 229.0298\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 201.9032 - val_loss: 226.8171\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 204.1587 - val_loss: 230.9535\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 201.3279 - val_loss: 225.1045\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 201.3778 - val_loss: 223.2795\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 203.0666 - val_loss: 224.0112\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 198.5873 - val_loss: 223.4665\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 200.7021 - val_loss: 222.9788\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 203.7516 - val_loss: 223.8962\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 201.8772 - val_loss: 219.8278\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 200.3445 - val_loss: 222.4543\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 201.6526 - val_loss: 229.2784\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 204.1479 - val_loss: 243.9278\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 203.5606 - val_loss: 223.4801\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 202.1257 - val_loss: 230.1987\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 200.2495 - val_loss: 231.0324\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 196.5123 - val_loss: 229.6042\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 200.7936 - val_loss: 230.4453\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 195.6899 - val_loss: 224.1040\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 201.2317 - val_loss: 221.2633\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 195.6768 - val_loss: 228.9739\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 196.7138 - val_loss: 231.3850\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 197.3366 - val_loss: 241.5982\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 202.1846 - val_loss: 231.1471\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 201.8017 - val_loss: 226.7782\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 203.6460 - val_loss: 231.3475\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 195.2946 - val_loss: 224.8809\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 196.8011 - val_loss: 219.6667\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 205.1104 - val_loss: 230.5262\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 197.5788 - val_loss: 229.0505\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 196.4878 - val_loss: 224.2035\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 200.0962 - val_loss: 250.1854\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 194.6204 - val_loss: 227.3483\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 194.5166 - val_loss: 220.1393\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 195.3773 - val_loss: 220.5735\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 194.4622 - val_loss: 218.9938\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 194.9606 - val_loss: 221.3671\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 191.9864 - val_loss: 232.6373\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 191.1469 - val_loss: 220.5608\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 190.7270 - val_loss: 221.2334\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 193.2428 - val_loss: 219.3655\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 191.2897 - val_loss: 215.2353\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 194.3701 - val_loss: 252.2347\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 198.4503 - val_loss: 224.6646\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 188.0745 - val_loss: 224.3113\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 188.7058 - val_loss: 217.2455\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 190.8273 - val_loss: 223.5908\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 195.6736 - val_loss: 224.5508\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 188.1747 - val_loss: 220.8060\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 196.2918 - val_loss: 234.4832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 185.8912 - val_loss: 219.6087\n",
      "Epoch 296/1000\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 190.2542 - val_loss: 216.4992\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 186.9204 - val_loss: 226.5582\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 191.5927 - val_loss: 219.1107\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 188.0389 - val_loss: 217.6215\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 188.4084 - val_loss: 225.8117\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 185.9412 - val_loss: 217.2970\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 184.0874 - val_loss: 218.4717\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 182.8789 - val_loss: 215.5409\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 183.9719 - val_loss: 228.5883\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 182.5525 - val_loss: 211.6080\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 185.1583 - val_loss: 210.6694\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 181.8054 - val_loss: 213.5345\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 180.8193 - val_loss: 243.8728\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 180.5423 - val_loss: 226.8779\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 181.9571 - val_loss: 220.3210\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 177.1846 - val_loss: 208.6487\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 181.2520 - val_loss: 264.4851\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 178.4312 - val_loss: 213.6614\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 175.0347 - val_loss: 229.7738\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 174.1955 - val_loss: 207.8225\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 173.8862 - val_loss: 215.5449\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 173.3335 - val_loss: 206.0631\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 171.1984 - val_loss: 206.7759\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 172.2790 - val_loss: 206.1004\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 167.2127 - val_loss: 200.5305\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 170.7832 - val_loss: 202.8088\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 172.3258 - val_loss: 207.7908\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 171.4050 - val_loss: 197.9137\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 170.8762 - val_loss: 205.7045\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 162.5181 - val_loss: 199.5899\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 167.1901 - val_loss: 198.7778\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 166.4026 - val_loss: 206.7775\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 165.7581 - val_loss: 209.4357\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 165.2341 - val_loss: 204.2904\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 165.1067 - val_loss: 196.8587\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 159.1538 - val_loss: 197.1489\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 163.6196 - val_loss: 197.2829\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 159.1235 - val_loss: 195.0288\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 157.7020 - val_loss: 223.4241\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 155.3664 - val_loss: 200.0766\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 156.8498 - val_loss: 194.6217\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 165.8958 - val_loss: 200.2372\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 156.7665 - val_loss: 197.7679\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 157.5042 - val_loss: 201.3035\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 153.8670 - val_loss: 183.8816\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 151.4393 - val_loss: 194.7666\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 156.0675 - val_loss: 190.4522\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 151.9889 - val_loss: 180.4408\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 149.6796 - val_loss: 190.7399\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 149.0308 - val_loss: 185.2404\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 149.3226 - val_loss: 185.3990\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 150.5358 - val_loss: 181.2887\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 149.2012 - val_loss: 206.8578\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 148.5160 - val_loss: 189.1943\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 148.3837 - val_loss: 175.9598\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 144.9390 - val_loss: 183.1501\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 144.6048 - val_loss: 180.8219\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 147.9547 - val_loss: 172.8677\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 143.9525 - val_loss: 173.1016\n",
      "Epoch 355/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 148.6872 - val_loss: 184.4159\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 139.0107 - val_loss: 170.0765\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 138.1129 - val_loss: 175.6288\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 141.7765 - val_loss: 171.6768\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 141.5718 - val_loss: 176.5962\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 135.6001 - val_loss: 180.0729\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 133.9759 - val_loss: 175.4658\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 137.2527 - val_loss: 165.5125\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 135.6781 - val_loss: 169.6541\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 139.9241 - val_loss: 171.2893\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 135.2797 - val_loss: 170.0716\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 138.7536 - val_loss: 178.1306\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 136.7705 - val_loss: 175.0492\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 139.3354 - val_loss: 174.8144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 138.9395 - val_loss: 189.9395\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 136.5730 - val_loss: 175.5779\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 141.5863 - val_loss: 169.3967\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 131.8002 - val_loss: 163.9787\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 134.6623 - val_loss: 159.5572\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 131.4853 - val_loss: 188.4814\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 135.6067 - val_loss: 173.5102\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 130.6660 - val_loss: 164.5232\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 134.8903 - val_loss: 171.8164\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 133.0735 - val_loss: 161.1924\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 136.5927 - val_loss: 166.5181\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 126.5163 - val_loss: 164.3203\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 126.2864 - val_loss: 169.6575\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 137.7666 - val_loss: 173.3970\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 124.1333 - val_loss: 165.3146\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 129.7134 - val_loss: 174.7635\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 129.2107 - val_loss: 168.4695\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 129.9757 - val_loss: 169.2607\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 127.5089 - val_loss: 170.4305\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 1s 101us/step - loss: 125.7285 - val_loss: 153.4341\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 131.8397 - val_loss: 169.8782\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 127.7855 - val_loss: 165.7196\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 129.5837 - val_loss: 175.3667\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 132.1479 - val_loss: 159.9616\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 135.3886 - val_loss: 162.2369\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 120.3069 - val_loss: 161.1954\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 123.8800 - val_loss: 169.1264\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 123.0271 - val_loss: 177.3277\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 129.3537 - val_loss: 169.9202\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 124.3807 - val_loss: 163.6910\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 132.6130 - val_loss: 164.9016\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 125.3075 - val_loss: 176.3196\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 122.9633 - val_loss: 164.3890\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 126.6049 - val_loss: 216.3286\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 127.5579 - val_loss: 153.7537\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 125.0134 - val_loss: 172.5223\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 125.3961 - val_loss: 157.7022\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 118.0624 - val_loss: 172.1909\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 120.9269 - val_loss: 164.1469\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 127.5248 - val_loss: 165.7189\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 129.1363 - val_loss: 175.4546\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 123.2877 - val_loss: 175.1309\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 119.7078 - val_loss: 170.2781\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 119.8170 - val_loss: 154.0048\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 118.2118 - val_loss: 170.8397\n",
      "Epoch 414/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 121.9001 - val_loss: 175.3141\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 122.8497 - val_loss: 164.3819\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 124.3117 - val_loss: 162.9358\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 123.7562 - val_loss: 159.0445\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 124.5408 - val_loss: 170.4642\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 121.4011 - val_loss: 161.2742\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 123.3011 - val_loss: 160.5277\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 120.7062 - val_loss: 158.4613\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 124.6442 - val_loss: 157.1782\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 123.0192 - val_loss: 167.0664\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 120.2504 - val_loss: 164.5606\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 120.1623 - val_loss: 170.0449\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 121.0413 - val_loss: 169.1938\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 122.9677 - val_loss: 163.9662\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 123.0803 - val_loss: 166.9218\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 120.7376 - val_loss: 157.4313\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 117.9872 - val_loss: 167.4885\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 119.9154 - val_loss: 168.6841\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 121.8110 - val_loss: 160.8470\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 116.4110 - val_loss: 171.7668\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 120.5780 - val_loss: 158.8040\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 118.9992 - val_loss: 159.6837\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 114.1029 - val_loss: 168.0265\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 121.6935 - val_loss: 171.6005\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 122.0236 - val_loss: 171.4833\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 120.7193 - val_loss: 164.7909\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 126.6065 - val_loss: 170.5645\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 113.9410 - val_loss: 153.3414\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 119.4298 - val_loss: 160.4870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 118.6470 - val_loss: 178.0694\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 119.8898 - val_loss: 190.8286\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 121.0650 - val_loss: 151.2000\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 115.0813 - val_loss: 165.0473\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 118.8442 - val_loss: 177.9810\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 119.3203 - val_loss: 155.4857\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 124.5237 - val_loss: 172.9037\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 117.4276 - val_loss: 162.3477\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 114.7634 - val_loss: 161.9809\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 116.3950 - val_loss: 189.6283\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 114.5678 - val_loss: 158.7200\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 117.8800 - val_loss: 155.7610\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 112.3381 - val_loss: 163.3553\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 114.4574 - val_loss: 175.4180\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 118.9457 - val_loss: 185.7375\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 117.4463 - val_loss: 174.2145\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 116.1990 - val_loss: 180.0677\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 110.2335 - val_loss: 173.8266\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 114.5254 - val_loss: 163.6428\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 117.9692 - val_loss: 164.3413\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 112.1810 - val_loss: 166.6187\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 115.7662 - val_loss: 180.2848\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 115.6136 - val_loss: 157.2358\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 115.0023 - val_loss: 159.8210\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 117.1537 - val_loss: 158.6564\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 112.1056 - val_loss: 192.5059\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 115.3824 - val_loss: 187.2892\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 116.3280 - val_loss: 167.8802\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 123.0552 - val_loss: 187.2730\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 110.5401 - val_loss: 173.2700\n",
      "Epoch 473/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 111.8899 - val_loss: 161.5274\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 115.2329 - val_loss: 156.4842\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 110.3113 - val_loss: 178.4700\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 110.8898 - val_loss: 161.1233\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 111.4870 - val_loss: 151.3731\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 114.7390 - val_loss: 166.9710\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 109.5137 - val_loss: 172.4929\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 118.1913 - val_loss: 169.4373\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 112.4162 - val_loss: 167.7194\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 108.9699 - val_loss: 152.4715\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 114.1792 - val_loss: 165.5151\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 111.3495 - val_loss: 166.7387\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 111.6121 - val_loss: 176.7843\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 113.4103 - val_loss: 160.2233\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 110.1366 - val_loss: 170.9735\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 108.5805 - val_loss: 191.0042\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 114.1350 - val_loss: 166.0244\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 114.0639 - val_loss: 175.9990\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 111.9141 - val_loss: 183.5137\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 110.8511 - val_loss: 221.3174\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 113.2846 - val_loss: 177.5088\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 109.1641 - val_loss: 183.1910\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 111.7894 - val_loss: 162.1032\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 108.5409 - val_loss: 165.5950\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 113.3463 - val_loss: 160.7551\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 108.0006 - val_loss: 160.4160\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 108.2488 - val_loss: 174.2813\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 108.6362 - val_loss: 166.6149\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 109.6478 - val_loss: 167.6068\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 105.5709 - val_loss: 160.1757\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 112.9379 - val_loss: 167.1326\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 108.0981 - val_loss: 162.4607\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 108.6081 - val_loss: 170.1567\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 108.1002 - val_loss: 176.4952\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 112.7884 - val_loss: 163.5771\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 110.4459 - val_loss: 166.8943\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 123.4063 - val_loss: 180.3474\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 109.0043 - val_loss: 170.3012\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 108.2106 - val_loss: 157.5426\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 110.5570 - val_loss: 174.3831\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 112.8732 - val_loss: 166.7194\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 112.2796 - val_loss: 165.9681\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 110.5939 - val_loss: 177.9841\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 113.5514 - val_loss: 171.2539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 110.6014 - val_loss: 166.6772\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 110.7700 - val_loss: 165.2239\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 104.3743 - val_loss: 173.7635\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 105.3858 - val_loss: 158.9341\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 107.7947 - val_loss: 166.1578\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 112.8008 - val_loss: 169.7281\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 105.9455 - val_loss: 171.8729\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 108.5892 - val_loss: 165.6109\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 107.8882 - val_loss: 170.4398\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 109.6736 - val_loss: 161.3363\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 110.9584 - val_loss: 169.8455\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 103.4805 - val_loss: 161.3867\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 107.3052 - val_loss: 160.6200\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 102.7572 - val_loss: 171.0166\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 109.5702 - val_loss: 157.3845\n",
      "Epoch 532/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 106.4595 - val_loss: 167.1953\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 109.6656 - val_loss: 165.7062\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 109.0374 - val_loss: 166.8302\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 109.5248 - val_loss: 167.1744\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 111.4915 - val_loss: 164.8612\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 111.5620 - val_loss: 162.8320\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 106.4721 - val_loss: 179.1334\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 108.5939 - val_loss: 168.2058\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 107.9165 - val_loss: 182.6208\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 105.7908 - val_loss: 180.9465\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 104.5491 - val_loss: 164.9886\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 104.5645 - val_loss: 173.1065\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 103.6464 - val_loss: 158.9190\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 104.4515 - val_loss: 161.1290\n",
      "Epoch 00545: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1172a57f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "model.add(Dropout(0.01)) # Dropout Layer\n",
    "model.add(Dense(25, activation='relu')) # Hidden 2\n",
    "model.add(Dense(10, \n",
    "                kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01),activation='relu')) # Hidden 3 w/regularization\n",
    "model.add(Dense(1)) # Output\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=100, verbose=1, mode='auto')\n",
    "checkpoint = ModelCheckpoint(filepath=filename_checkpoint, verbose=0, save_best_only=True)\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpoint],verbose=1,epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 11.57839298248291\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(filename_checkpoint)\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "#chart_regression(pred.flatten(),y_test)\n",
    "#chart_regression(pred.flatten(),y_test,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[497.6311,\n",
       " 12.163704,\n",
       " 77.026581,\n",
       " 55.484741,\n",
       " 183.44357,\n",
       " 55.955681,\n",
       " 116.62433,\n",
       " 7.6676898,\n",
       " 203.21841,\n",
       " 15.014146]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "pos_pred = [n if n > 0 else n * -1 for n in pred[:,0]]\n",
    "\n",
    "pos_pred[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "extract_and_encode_features(df_test)\n",
    "\n",
    "ids_test = df_test['id']\n",
    "df_test.drop('id',1,inplace=True)\n",
    "\n",
    "names_test = df_test['name']\n",
    "df_test.drop('name',1,inplace=True)\n",
    "\n",
    "x_submit = df_test.as_matrix().astype(np.float32)\n",
    "pred_submit = model.predict(x_submit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = [n if n > 0 else n * -1 for n in pred_submit[:,0]]\n",
    "df_submit = pd.DataFrame({'id': ids_test,'cost': cost})\n",
    "df_submit = df_submit[['id', 'cost']]\n",
    "df_submit.to_csv(filename_submit, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
