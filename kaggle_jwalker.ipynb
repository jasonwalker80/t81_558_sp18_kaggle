{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n",
    "\n",
    "**Kaggle Assignment: **\n",
    "\n",
    "**Student Name: Jason Walker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## Description\n",
    "This is one of the projects from the course T81-855: Applications of Deep Learning at Washington University in St. Louis. All students must create a Kaggle account and submit a solution. Once you have submitted your solution entry log into Blackboard (at WUSTL) and submit a single file telling me your Kaggle name on the leaderboard (you do not need to register to Kaggle with your real name). This competition will be visible to the public, so there may be non-student submissions as well as student.\n",
    "\n",
    "The data set for this competition consists of a number of input columns that should be used to predict a stores sales. This is a regression problem. The inputs are a mixture of discrete and category values. The data set is from a simulation.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The evaluation pages describes how submissions will be scored and how students should format their submissions. The scores are in RMSE.\n",
    "Submission Format\n",
    "\n",
    "For every store in the dataset, submission files should contain a sales volume.\n",
    "\n",
    "The file should contain a header and have the following format:\n",
    "\n",
    "```\n",
    "100000,1.23\n",
    "100001,1.123\n",
    "100002,3.332\n",
    "100003,1.53\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The data contains data and costs for various office supplies. The data came from a simulation and do not directly correspond to any real-world items. See how well you can predict the cost of an item using the provided data. Feature engineering will likely help you. The *name* column may seem useless at first glance; however, it contains information that you can parse to help your predictions.\n",
    "File descriptions\n",
    "```\n",
    "    id - The identifier/primary key.\n",
    "    name - The name of this item.\n",
    "    manufacturer - The manufacturer.\n",
    "    pack - The number of items in this pack.\n",
    "    weight - The weight of a pack of these items.\n",
    "    height - The height of a pack of these items.\n",
    "    width - The width of a pack of these items.\n",
    "    length - The length of a pack of these items.\n",
    "    cost - The cost for this item pack. This is what you are to predict (the target). \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions\n",
    "\n",
    "You will see these at the top of every module and assignment.  These are simply a set of reusable functions that we will make use of.  Each of them will be explained as the semester progresses.  They are explained in greater detail as the course progresses.  Class 4 contains a complete overview of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n",
    "        \n",
    "# This function submits an assignment.  You can submit an assignment as much as you like, only the final\n",
    "# submission counts.  The paramaters are as follows:\n",
    "# data - Pandas dataframe output.\n",
    "# key - Your student key that was emailed to you.\n",
    "# no - The assignment class number, should be 1 through 1.\n",
    "# source_file - The full path to your Python or IPYNB file.  This must have \"_class1\" as part of its name.  \n",
    "# .             The number must match your assignment number.  For example \"_class2\" for class assignment #2.\n",
    "def submit(data,key,no,source_file=None):\n",
    "    if source_file is None and '__file__' not in globals(): raise Exception('Must specify a filename when a Jupyter notebook.')\n",
    "    if source_file is None: source_file = __file__\n",
    "    suffix = '_class{}'.format(no)\n",
    "    if suffix not in source_file: raise Exception('{} must be part of the filename.'.format(suffix))\n",
    "    with open(source_file, \"rb\") as image_file:\n",
    "        encoded_python = base64.b64encode(image_file.read()).decode('ascii')\n",
    "    ext = os.path.splitext(source_file)[-1].lower()\n",
    "    if ext not in ['.ipynb','.py']: raise Exception(\"Source file is {} must be .py or .ipynb\".format(ext))\n",
    "    r = requests.post(\"https://api.heatonresearch.com/assignment-submit\",\n",
    "        headers={'x-api-key':key}, json={'csv':base64.b64encode(data.to_csv(index=False).encode('ascii')).decode(\"ascii\"),\n",
    "        'assignment': no, 'ext':ext, 'py':encoded_python})\n",
    "    if r.status_code == 200:\n",
    "        print(\"Success: {}\".format(r.text))\n",
    "    else: print(\"Failure: {}\".format(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kaggle Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers\n",
    "\n",
    "path = './data'\n",
    "\n",
    "filename_test = os.path.join(path,\"test.csv\")\n",
    "filename_train = os.path.join(path,\"train.csv\")\n",
    "filename_sample = os.path.join(path,\"sample.csv\")\n",
    "filename_submit = os.path.join(path,\"submit.csv\")\n",
    "filename_checkpoint = os.path.join(path,\"checkpoint.hdf5\")\n",
    "\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "\n",
    "np.random.seed(42) # Uncomment this line to get the same shuffle each time\n",
    "df_train = df_train.reindex(np.random.permutation(df_train.index))\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Encode Features\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def extract_and_encode_features(df):\n",
    "    color_regex='(?P<color>red|blue|green|yellow|orange|pink|black|brown|white)'\n",
    "    df['color'] = df.name.str.extract(color_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    quality_regex='(?P<quality>generic|medium\\shigh\\squality|high\\squality)'\n",
    "    df['quality'] = df.name.str.extract(quality_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    size_regex='(?P<size>tiny|small|medium|large)'\n",
    "    df['size'] = df.name.str.extract(size_regex, flags=re.IGNORECASE, expand=False)\n",
    "\n",
    "    item_regex='(?P<item>paperclips|paperweights|ink\\spens|pencils|stapler|tablets|thumbtacks|post\\sit\\snotes)'\n",
    "    df['item'] = df.name.str.extract(item_regex, flags=re.IGNORECASE, expand=False)\n",
    "    \n",
    "    for column in ['pack','weight','height','width','length']:\n",
    "        missing_median(df,column)\n",
    "    \n",
    "    #df.insert(1,'surface_area',(df['height']*df['width']*df['length']).astype(int))\n",
    "    \n",
    "    ## encode numeric features\n",
    "    #for column in ['pack','weight','height','width','length','surface_area']:\n",
    "    #    encode_numeric_zscore(df,column)\n",
    "    for column in ['weight']:\n",
    "        encode_numeric_zscore(df,column)\n",
    "    \n",
    "    #for column in ['height','width','length']:\n",
    "    #    df.drop(column,1,inplace=True)\n",
    "        \n",
    "    # encode text/categorical features\n",
    "    for column in ['manufacturer','color','quality','size','item']:\n",
    "        encode_text_dummy(df,column)\n",
    "  \n",
    "extract_and_encode_features(df_train)\n",
    "\n",
    "ids_train = df_train['id']\n",
    "df_train.drop('id',1,inplace=True)\n",
    "\n",
    "names_train = df_train['name']\n",
    "df_train.drop('name',1,inplace=True)\n",
    "\n",
    "x,y = to_xy(df_train,'cost')\n",
    "\n",
    "# Cross-Validate\n",
    "#kf = KFold(5)\n",
    "\n",
    "# Used before KFold\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 32.52793884277344\n",
      "['pack', 'weight', 'height', 'width', 'length', 'manufacturer-6% Solution', 'manufacturer-Deep Office Supplies', 'manufacturer-Duck Lake', 'manufacturer-Offices-R-Us', 'manufacturer-WizBang', 'color-Black', 'color-Blue', 'color-Brown', 'color-Green', 'color-Pink', 'color-Red', 'color-White', 'quality-Generic', 'quality-High Quality', 'quality-Medium High Quality', 'size-Large', 'size-Medium', 'size-Small', 'size-Tiny', 'item-Ink Pens', 'item-Paperclips', 'item-Paperweights', 'item-Pencils', 'item-Post It Notes', 'item-Stapler', 'item-Tablets', 'item-Thumbtacks']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>item-Post It Notes</th>\n",
       "      <td>-59.719971</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Thumbtacks</th>\n",
       "      <td>-56.904186</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Pencils</th>\n",
       "      <td>-50.110344</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperclips</th>\n",
       "      <td>-44.058762</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Red</th>\n",
       "      <td>-38.178513</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Green</th>\n",
       "      <td>-25.862614</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Blue</th>\n",
       "      <td>-11.869884</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>-10.232891</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>-8.184170</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>width</th>\n",
       "      <td>-6.139114</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Generic</th>\n",
       "      <td>-4.688889</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Ink Pens</th>\n",
       "      <td>-4.276169</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Tiny</th>\n",
       "      <td>-2.336882</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Offices-R-Us</th>\n",
       "      <td>-0.913902</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Deep Office Supplies</th>\n",
       "      <td>-0.697884</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Small</th>\n",
       "      <td>-0.494411</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Brown</th>\n",
       "      <td>-0.430750</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack</th>\n",
       "      <td>0.019360</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-6% Solution</th>\n",
       "      <td>0.096964</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-Duck Lake</th>\n",
       "      <td>0.295799</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer-WizBang</th>\n",
       "      <td>1.219092</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Medium</th>\n",
       "      <td>3.020007</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-Medium High Quality</th>\n",
       "      <td>5.582911</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size-Large</th>\n",
       "      <td>6.065509</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Paperweights</th>\n",
       "      <td>7.183593</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality-High Quality</th>\n",
       "      <td>7.550890</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Black</th>\n",
       "      <td>13.954896</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-White</th>\n",
       "      <td>27.178518</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color-Pink</th>\n",
       "      <td>38.822678</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Stapler</th>\n",
       "      <td>48.685684</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>97.461044</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item-Tablets</th>\n",
       "      <td>159.199524</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         coef  positive\n",
       "item-Post It Notes                 -59.719971     False\n",
       "item-Thumbtacks                    -56.904186     False\n",
       "item-Pencils                       -50.110344     False\n",
       "item-Paperclips                    -44.058762     False\n",
       "color-Red                          -38.178513     False\n",
       "color-Green                        -25.862614     False\n",
       "color-Blue                         -11.869884     False\n",
       "height                             -10.232891     False\n",
       "length                              -8.184170     False\n",
       "width                               -6.139114     False\n",
       "quality-Generic                     -4.688889     False\n",
       "item-Ink Pens                       -4.276169     False\n",
       "size-Tiny                           -2.336882     False\n",
       "manufacturer-Offices-R-Us           -0.913902     False\n",
       "manufacturer-Deep Office Supplies   -0.697884     False\n",
       "size-Small                          -0.494411     False\n",
       "color-Brown                         -0.430750     False\n",
       "pack                                 0.019360      True\n",
       "manufacturer-6% Solution             0.096964      True\n",
       "manufacturer-Duck Lake               0.295799      True\n",
       "manufacturer-WizBang                 1.219092      True\n",
       "size-Medium                          3.020007      True\n",
       "quality-Medium High Quality          5.582911      True\n",
       "size-Large                           6.065509      True\n",
       "item-Paperweights                    7.183593      True\n",
       "quality-High Quality                 7.550890      True\n",
       "color-Black                         13.954896      True\n",
       "color-White                         27.178518      True\n",
       "color-Pink                          38.822678      True\n",
       "item-Stapler                        48.685684      True\n",
       "weight                              97.461044      True\n",
       "item-Tablets                       159.199524      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [ 171.07705688]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAD8CAYAAADExYYgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1ba43470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple function to evaluate the coefficients of a regression\n",
    "%matplotlib inline    \n",
    "from IPython.display import display, HTML    \n",
    "\n",
    "def report_coef(names,coef,intercept):\n",
    "    r = pd.DataFrame( { 'coef': coef, 'positive': coef>=0  }, index = names )\n",
    "    r = r.sort_values(by=['coef'])\n",
    "    display(r)\n",
    "    print(\"Intercept: {}\".format(intercept))\n",
    "    r['coef'].plot(kind='barh', color=r['positive'].map({True: 'b', False: 'r'}))\n",
    "    \n",
    "# Create linear regression\n",
    "regressor = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# Fit/train linear regression\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df_train.columns.values)\n",
    "names.remove(\"cost\")\n",
    "print(names)\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_[0,:],\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/10000\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 10583.2156 - val_loss: 11132.5401\n",
      "Epoch 2/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 7422.5652 - val_loss: 4223.0854\n",
      "Epoch 3/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 3269.0210 - val_loss: 1678.9112\n",
      "Epoch 4/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1783.8170 - val_loss: 1115.9956\n",
      "Epoch 5/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1072.6385 - val_loss: 676.2012\n",
      "Epoch 6/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 861.1920 - val_loss: 545.4521\n",
      "Epoch 7/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 660.5917 - val_loss: 428.0436\n",
      "Epoch 8/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 579.2083 - val_loss: 385.5396\n",
      "Epoch 9/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 579.1410 - val_loss: 402.5335\n",
      "Epoch 10/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 552.4788 - val_loss: 371.7780\n",
      "Epoch 11/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 491.6031 - val_loss: 448.6067\n",
      "Epoch 12/10000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 482.6001 - val_loss: 315.8760\n",
      "Epoch 13/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 436.6071 - val_loss: 631.8641\n",
      "Epoch 14/10000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 429.3681 - val_loss: 303.3870\n",
      "Epoch 15/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 420.9622 - val_loss: 312.2028\n",
      "Epoch 16/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 403.1641 - val_loss: 317.9726\n",
      "Epoch 17/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 371.4034 - val_loss: 287.5251\n",
      "Epoch 18/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 359.9024 - val_loss: 250.8142\n",
      "Epoch 19/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 383.9020 - val_loss: 252.7843\n",
      "Epoch 20/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 352.7162 - val_loss: 252.3834\n",
      "Epoch 21/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 402.9926 - val_loss: 265.3655\n",
      "Epoch 22/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 328.0229 - val_loss: 260.4040\n",
      "Epoch 23/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 312.5863 - val_loss: 232.1220\n",
      "Epoch 24/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 309.6511 - val_loss: 224.6328\n",
      "Epoch 25/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 313.9027 - val_loss: 309.7045\n",
      "Epoch 26/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 320.7781 - val_loss: 257.5849\n",
      "Epoch 27/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 346.7580 - val_loss: 357.3856\n",
      "Epoch 28/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 295.5480 - val_loss: 219.4931\n",
      "Epoch 29/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 283.0805 - val_loss: 380.8122\n",
      "Epoch 30/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 313.6324 - val_loss: 209.5758\n",
      "Epoch 31/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 334.0885 - val_loss: 206.8352\n",
      "Epoch 32/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 289.0490 - val_loss: 465.5768\n",
      "Epoch 33/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 307.2570 - val_loss: 266.5245\n",
      "Epoch 34/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 302.5081 - val_loss: 201.7194\n",
      "Epoch 35/10000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 273.2282 - val_loss: 204.6842\n",
      "Epoch 36/10000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 262.5066 - val_loss: 213.9587\n",
      "Epoch 37/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 282.3950 - val_loss: 219.8193\n",
      "Epoch 38/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 272.6025 - val_loss: 227.0762\n",
      "Epoch 39/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 257.5699 - val_loss: 197.8517\n",
      "Epoch 40/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 296.4008 - val_loss: 221.0947\n",
      "Epoch 41/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 248.7259 - val_loss: 214.7584\n",
      "Epoch 42/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 262.5519 - val_loss: 253.9479\n",
      "Epoch 43/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 235.8262 - val_loss: 207.3689\n",
      "Epoch 44/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 243.7188 - val_loss: 192.2692\n",
      "Epoch 45/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 242.9051 - val_loss: 183.4058\n",
      "Epoch 46/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 290.8674 - val_loss: 267.3037\n",
      "Epoch 47/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 257.2545 - val_loss: 186.9725\n",
      "Epoch 48/10000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 231.4345 - val_loss: 215.2282\n",
      "Epoch 49/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 254.5221 - val_loss: 263.8608\n",
      "Epoch 50/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 241.7682 - val_loss: 188.9434\n",
      "Epoch 51/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 235.7247 - val_loss: 180.8433\n",
      "Epoch 52/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 232.7889 - val_loss: 251.2355\n",
      "Epoch 53/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 243.8698 - val_loss: 190.1626\n",
      "Epoch 54/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 240.5089 - val_loss: 309.6297\n",
      "Epoch 55/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 245.7740 - val_loss: 187.4270\n",
      "Epoch 56/10000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 230.0438 - val_loss: 180.8968\n",
      "Epoch 57/10000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 239.3286 - val_loss: 184.6754\n",
      "Epoch 58/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 241.4344 - val_loss: 213.9667\n",
      "Epoch 59/10000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 239.1096 - val_loss: 301.6434\n",
      "Epoch 60/10000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 241.8054 - val_loss: 315.3526\n",
      "Epoch 61/10000\n",
      "7500/7500 [==============================] - 1s 129us/step - loss: 244.6975 - val_loss: 206.2948\n",
      "Epoch 62/10000\n",
      "7500/7500 [==============================] - 1s 118us/step - loss: 240.7730 - val_loss: 228.5652\n",
      "Epoch 63/10000\n",
      "7500/7500 [==============================] - 1s 113us/step - loss: 232.9409 - val_loss: 182.1614\n",
      "Epoch 64/10000\n",
      "7500/7500 [==============================] - 1s 101us/step - loss: 257.5077 - val_loss: 169.1433\n",
      "Epoch 65/10000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 218.3952 - val_loss: 195.7085\n",
      "Epoch 66/10000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 228.0565 - val_loss: 193.2361\n",
      "Epoch 67/10000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 221.6237 - val_loss: 192.3113\n",
      "Epoch 68/10000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 230.4146 - val_loss: 180.6549\n",
      "Epoch 69/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 239.5801 - val_loss: 503.0476\n",
      "Epoch 70/10000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 220.9371 - val_loss: 167.8498\n",
      "Epoch 71/10000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 209.0406 - val_loss: 225.1524\n",
      "Epoch 72/10000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 224.0131 - val_loss: 227.7144\n",
      "Epoch 73/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 222.6366 - val_loss: 186.4947\n",
      "Epoch 74/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 57us/step - loss: 225.5542 - val_loss: 171.3469\n",
      "Epoch 75/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 249.8724 - val_loss: 182.4669\n",
      "Epoch 76/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 208.7419 - val_loss: 169.6123\n",
      "Epoch 77/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 208.3172 - val_loss: 219.7847\n",
      "Epoch 78/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 212.0790 - val_loss: 196.2425\n",
      "Epoch 79/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 225.3904 - val_loss: 182.8838\n",
      "Epoch 80/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 204.3659 - val_loss: 195.9246\n",
      "Epoch 81/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 219.1826 - val_loss: 165.1997\n",
      "Epoch 82/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 228.1635 - val_loss: 194.4759\n",
      "Epoch 83/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 208.0995 - val_loss: 173.1791\n",
      "Epoch 84/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 236.3297 - val_loss: 187.5300\n",
      "Epoch 85/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 195.7090 - val_loss: 200.3764\n",
      "Epoch 86/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 237.7873 - val_loss: 207.3666\n",
      "Epoch 87/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 210.0695 - val_loss: 174.9853\n",
      "Epoch 88/10000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 199.2053 - val_loss: 168.0277\n",
      "Epoch 89/10000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 195.9991 - val_loss: 416.5773\n",
      "Epoch 90/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 207.0827 - val_loss: 176.8760\n",
      "Epoch 91/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 214.9625 - val_loss: 195.9211\n",
      "Epoch 92/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 195.8933 - val_loss: 198.4719\n",
      "Epoch 93/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 226.0637 - val_loss: 175.6558\n",
      "Epoch 94/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 197.8370 - val_loss: 174.9457\n",
      "Epoch 95/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 227.5959 - val_loss: 180.5052\n",
      "Epoch 96/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 193.1536 - val_loss: 163.0605\n",
      "Epoch 97/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 195.4904 - val_loss: 193.8227\n",
      "Epoch 98/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 198.8658 - val_loss: 175.1264\n",
      "Epoch 99/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 194.0999 - val_loss: 190.3547\n",
      "Epoch 100/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 216.3042 - val_loss: 163.0769\n",
      "Epoch 101/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 204.3141 - val_loss: 158.1920\n",
      "Epoch 102/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 202.7868 - val_loss: 179.0821\n",
      "Epoch 103/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 203.7706 - val_loss: 193.9454\n",
      "Epoch 104/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 194.5660 - val_loss: 163.9800\n",
      "Epoch 105/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 199.3416 - val_loss: 184.6948\n",
      "Epoch 106/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 187.3506 - val_loss: 168.5532\n",
      "Epoch 107/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 195.0264 - val_loss: 284.9695\n",
      "Epoch 108/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 194.4541 - val_loss: 160.4868\n",
      "Epoch 109/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 197.4867 - val_loss: 168.2152\n",
      "Epoch 110/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 198.0134 - val_loss: 220.0131\n",
      "Epoch 111/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 203.8724 - val_loss: 163.4728\n",
      "Epoch 112/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 194.6719 - val_loss: 157.7089\n",
      "Epoch 113/10000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 199.2404 - val_loss: 157.0866\n",
      "Epoch 114/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 183.6221 - val_loss: 157.4560\n",
      "Epoch 115/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 181.2487 - val_loss: 246.5912\n",
      "Epoch 116/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 214.8515 - val_loss: 154.6142\n",
      "Epoch 117/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 180.5928 - val_loss: 173.3475\n",
      "Epoch 118/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 219.2714 - val_loss: 168.6704\n",
      "Epoch 119/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 192.7066 - val_loss: 173.4119\n",
      "Epoch 120/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 191.3763 - val_loss: 163.0266\n",
      "Epoch 121/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 179.1883 - val_loss: 166.1011\n",
      "Epoch 122/10000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 190.1316 - val_loss: 155.6865\n",
      "Epoch 123/10000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 194.5477 - val_loss: 157.0447\n",
      "Epoch 124/10000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 196.9193 - val_loss: 159.9467\n",
      "Epoch 125/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 195.0948 - val_loss: 156.0738\n",
      "Epoch 126/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 178.7067 - val_loss: 196.2461\n",
      "Epoch 127/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 214.9234 - val_loss: 154.4681\n",
      "Epoch 128/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 181.2581 - val_loss: 150.2915\n",
      "Epoch 129/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 185.7697 - val_loss: 164.1764\n",
      "Epoch 130/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 193.6125 - val_loss: 156.0531\n",
      "Epoch 131/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 191.4734 - val_loss: 163.7321\n",
      "Epoch 132/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 182.0203 - val_loss: 174.4845\n",
      "Epoch 133/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 175.8845 - val_loss: 153.7191\n",
      "Epoch 134/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 190.9011 - val_loss: 287.1114\n",
      "Epoch 135/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 180.7980 - val_loss: 161.6625\n",
      "Epoch 136/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 178.0325 - val_loss: 168.0196\n",
      "Epoch 137/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 189.3235 - val_loss: 154.6910\n",
      "Epoch 138/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 183.1996 - val_loss: 165.1525\n",
      "Epoch 139/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 189.1072 - val_loss: 151.0990\n",
      "Epoch 140/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 181.0084 - val_loss: 161.7862\n",
      "Epoch 141/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 181.6269 - val_loss: 237.1091\n",
      "Epoch 142/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 200.1047 - val_loss: 153.9387\n",
      "Epoch 143/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 180.7234 - val_loss: 210.8393\n",
      "Epoch 144/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 178.4569 - val_loss: 209.1702\n",
      "Epoch 145/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 172.0979 - val_loss: 150.6224\n",
      "Epoch 146/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 178.3849 - val_loss: 183.9806\n",
      "Epoch 147/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 68us/step - loss: 180.2474 - val_loss: 152.7476\n",
      "Epoch 148/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 192.4680 - val_loss: 163.3193\n",
      "Epoch 149/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 192.4448 - val_loss: 173.7066\n",
      "Epoch 150/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 180.2384 - val_loss: 194.5697\n",
      "Epoch 151/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 178.1657 - val_loss: 174.7189\n",
      "Epoch 152/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 183.8110 - val_loss: 173.2184\n",
      "Epoch 153/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 183.9432 - val_loss: 166.4929\n",
      "Epoch 154/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 187.5417 - val_loss: 174.8813\n",
      "Epoch 155/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 180.7510 - val_loss: 150.4248\n",
      "Epoch 156/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 178.0298 - val_loss: 156.5817\n",
      "Epoch 157/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 170.0353 - val_loss: 150.4515\n",
      "Epoch 158/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 179.7475 - val_loss: 196.2360\n",
      "Epoch 159/10000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 172.4938 - val_loss: 300.8591\n",
      "Epoch 160/10000\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 175.3350 - val_loss: 216.4807\n",
      "Epoch 161/10000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 190.7222 - val_loss: 156.3392\n",
      "Epoch 162/10000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 172.0084 - val_loss: 162.5148\n",
      "Epoch 163/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 181.2402 - val_loss: 201.8351\n",
      "Epoch 164/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 174.1070 - val_loss: 160.4694\n",
      "Epoch 165/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 172.8001 - val_loss: 164.9435\n",
      "Epoch 166/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 181.7505 - val_loss: 169.7248\n",
      "Epoch 167/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 179.9932 - val_loss: 171.8316\n",
      "Epoch 168/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 191.3715 - val_loss: 152.2196\n",
      "Epoch 169/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 184.6091 - val_loss: 148.7175\n",
      "Epoch 170/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 164.5911 - val_loss: 210.9591\n",
      "Epoch 171/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 178.5142 - val_loss: 194.6492\n",
      "Epoch 172/10000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 166.9990 - val_loss: 145.3416\n",
      "Epoch 173/10000\n",
      "7500/7500 [==============================] - 1s 107us/step - loss: 173.4922 - val_loss: 181.9724\n",
      "Epoch 174/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 177.0103 - val_loss: 179.8181\n",
      "Epoch 175/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 173.8456 - val_loss: 161.8900\n",
      "Epoch 176/10000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 172.2622 - val_loss: 148.4361\n",
      "Epoch 177/10000\n",
      "7500/7500 [==============================] - 1s 103us/step - loss: 163.9693 - val_loss: 143.1740\n",
      "Epoch 178/10000\n",
      "7500/7500 [==============================] - 1s 106us/step - loss: 171.1665 - val_loss: 194.8585\n",
      "Epoch 179/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 163.8326 - val_loss: 157.0740\n",
      "Epoch 180/10000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 176.1504 - val_loss: 171.3006\n",
      "Epoch 181/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 169.6282 - val_loss: 170.8231\n",
      "Epoch 182/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 189.0334 - val_loss: 167.3912\n",
      "Epoch 183/10000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 169.0013 - val_loss: 207.8314\n",
      "Epoch 184/10000\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 170.1015 - val_loss: 144.2452\n",
      "Epoch 185/10000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 185.8272 - val_loss: 148.1273\n",
      "Epoch 186/10000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 172.9432 - val_loss: 148.4434\n",
      "Epoch 187/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 168.2645 - val_loss: 147.9724\n",
      "Epoch 188/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 165.9514 - val_loss: 145.2756\n",
      "Epoch 189/10000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 177.5372 - val_loss: 153.3391\n",
      "Epoch 190/10000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 162.4875 - val_loss: 157.1524\n",
      "Epoch 191/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 162.5374 - val_loss: 153.3161\n",
      "Epoch 192/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 185.2677 - val_loss: 143.8724\n",
      "Epoch 193/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 164.6764 - val_loss: 150.8579\n",
      "Epoch 194/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 169.8398 - val_loss: 158.0996\n",
      "Epoch 195/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 175.3622 - val_loss: 167.8592\n",
      "Epoch 196/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 167.9301 - val_loss: 164.4061\n",
      "Epoch 197/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 170.6311 - val_loss: 149.9833\n",
      "Epoch 198/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 172.5060 - val_loss: 148.7010\n",
      "Epoch 199/10000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 167.1849 - val_loss: 145.9740\n",
      "Epoch 200/10000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 165.0055 - val_loss: 175.8644\n",
      "Epoch 201/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 167.6063 - val_loss: 190.5194\n",
      "Epoch 202/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 188.8196 - val_loss: 353.5998\n",
      "Epoch 203/10000\n",
      "7500/7500 [==============================] - 1s 128us/step - loss: 185.5019 - val_loss: 150.5416\n",
      "Epoch 204/10000\n",
      "7500/7500 [==============================] - 1s 123us/step - loss: 163.7365 - val_loss: 162.9911\n",
      "Epoch 205/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 164.8609 - val_loss: 199.5951\n",
      "Epoch 206/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 163.0467 - val_loss: 148.6533\n",
      "Epoch 207/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 180.0517 - val_loss: 276.8334\n",
      "Epoch 208/10000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 170.4535 - val_loss: 143.8351\n",
      "Epoch 209/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 156.6496 - val_loss: 143.6536\n",
      "Epoch 210/10000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 168.7325 - val_loss: 143.3036\n",
      "Epoch 211/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 170.8296 - val_loss: 171.0931\n",
      "Epoch 212/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 173.8220 - val_loss: 192.1810\n",
      "Epoch 213/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 170.1599 - val_loss: 154.6727\n",
      "Epoch 214/10000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 161.5195 - val_loss: 151.9785\n",
      "Epoch 215/10000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 170.8072 - val_loss: 149.0218\n",
      "Epoch 216/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 169.6083 - val_loss: 155.6327\n",
      "Epoch 217/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 167.1780 - val_loss: 154.1239\n",
      "Epoch 218/10000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 168.2858 - val_loss: 251.2462\n",
      "Epoch 219/10000\n",
      "7500/7500 [==============================] - 1s 105us/step - loss: 171.5425 - val_loss: 160.8140\n",
      "Epoch 220/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 144us/step - loss: 167.1338 - val_loss: 152.9416\n",
      "Epoch 221/10000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 202.8664 - val_loss: 155.3391\n",
      "Epoch 222/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 164.5747 - val_loss: 148.2870\n",
      "Epoch 223/10000\n",
      "7500/7500 [==============================] - 1s 123us/step - loss: 162.3708 - val_loss: 153.5966\n",
      "Epoch 224/10000\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 159.2824 - val_loss: 170.7454\n",
      "Epoch 225/10000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 174.2808 - val_loss: 169.4576\n",
      "Epoch 226/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 163.2572 - val_loss: 149.0770\n",
      "Epoch 227/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 167.3406 - val_loss: 158.5503\n",
      "Epoch 228/10000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 159.9893 - val_loss: 150.6091\n",
      "Epoch 229/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 159.7258 - val_loss: 217.0446\n",
      "Epoch 230/10000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 174.5120 - val_loss: 159.7655\n",
      "Epoch 231/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 161.8169 - val_loss: 141.4014\n",
      "Epoch 232/10000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 162.6703 - val_loss: 150.0832\n",
      "Epoch 233/10000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 158.9472 - val_loss: 180.4450\n",
      "Epoch 234/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 161.4355 - val_loss: 218.3977\n",
      "Epoch 235/10000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 165.0808 - val_loss: 150.7945\n",
      "Epoch 236/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 155.3184 - val_loss: 141.2999\n",
      "Epoch 237/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 159.8730 - val_loss: 142.7223\n",
      "Epoch 238/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 156.7843 - val_loss: 150.5186\n",
      "Epoch 239/10000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 159.5705 - val_loss: 170.9799\n",
      "Epoch 240/10000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 167.1991 - val_loss: 143.4186\n",
      "Epoch 241/10000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 165.3260 - val_loss: 178.7996\n",
      "Epoch 242/10000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 158.3477 - val_loss: 140.7466\n",
      "Epoch 243/10000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 153.6376 - val_loss: 166.0965\n",
      "Epoch 244/10000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 161.5942 - val_loss: 165.7537\n",
      "Epoch 245/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 166.1865 - val_loss: 178.6213\n",
      "Epoch 246/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 168.3036 - val_loss: 271.5133\n",
      "Epoch 247/10000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 171.8896 - val_loss: 205.2799\n",
      "Epoch 248/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 164.7175 - val_loss: 165.8053\n",
      "Epoch 249/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 157.8115 - val_loss: 144.9033\n",
      "Epoch 250/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 165.1748 - val_loss: 153.3549\n",
      "Epoch 251/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 161.6885 - val_loss: 151.7633\n",
      "Epoch 252/10000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 158.7676 - val_loss: 152.1643\n",
      "Epoch 253/10000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 171.9810 - val_loss: 243.0088\n",
      "Epoch 254/10000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 160.3351 - val_loss: 168.4585\n",
      "Epoch 255/10000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 157.4003 - val_loss: 147.8061\n",
      "Epoch 256/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 163.4449 - val_loss: 142.9054\n",
      "Epoch 257/10000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 163.7151 - val_loss: 166.9594\n",
      "Epoch 258/10000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 160.5439 - val_loss: 154.6771\n",
      "Epoch 259/10000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 161.5164 - val_loss: 157.4629\n",
      "Epoch 260/10000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 155.2358 - val_loss: 145.9300\n",
      "Epoch 261/10000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 158.0486 - val_loss: 145.7598\n",
      "Epoch 262/10000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 163.5819 - val_loss: 157.8393\n",
      "Epoch 263/10000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 157.3402 - val_loss: 157.2358\n",
      "Epoch 264/10000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 159.7825 - val_loss: 143.8201\n",
      "Epoch 265/10000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 160.7732 - val_loss: 169.3736\n",
      "Epoch 266/10000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 152.0369 - val_loss: 138.8386\n",
      "Epoch 267/10000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 154.6001 - val_loss: 155.5593\n",
      "Epoch 268/10000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 156.8693 - val_loss: 153.0378\n",
      "Epoch 269/10000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 160.6511 - val_loss: 140.9573\n",
      "Epoch 270/10000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 168.4995 - val_loss: 150.9603\n",
      "Epoch 271/10000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 165.3569 - val_loss: 150.6934\n",
      "Epoch 272/10000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 156.1371 - val_loss: 143.0661\n",
      "Epoch 273/10000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 160.6062 - val_loss: 162.8943\n",
      "Epoch 274/10000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 160.2582 - val_loss: 163.6710\n",
      "Epoch 275/10000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 173.1785 - val_loss: 162.8254\n",
      "Epoch 276/10000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 164.3349 - val_loss: 156.1807\n",
      "Epoch 277/10000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 150.2147 - val_loss: 144.3135\n",
      "Epoch 278/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 204.6469 - val_loss: 148.4564\n",
      "Epoch 279/10000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 157.9710 - val_loss: 152.5554\n",
      "Epoch 280/10000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 151.2424 - val_loss: 145.8579\n",
      "Epoch 281/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 150.0319 - val_loss: 160.1698\n",
      "Epoch 282/10000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 162.8208 - val_loss: 140.0890\n",
      "Epoch 283/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 152.1475 - val_loss: 149.0859\n",
      "Epoch 284/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 161.8407 - val_loss: 159.9287\n",
      "Epoch 285/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 155.3215 - val_loss: 138.6210\n",
      "Epoch 286/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 154.6891 - val_loss: 141.6549\n",
      "Epoch 287/10000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 157.769 - 0s 54us/step - loss: 156.5652 - val_loss: 160.5856\n",
      "Epoch 288/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 154.9518 - val_loss: 176.7762\n",
      "Epoch 289/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 147.4637 - val_loss: 139.0896\n",
      "Epoch 290/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 150.7947 - val_loss: 159.8426\n",
      "Epoch 291/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 157.7594 - val_loss: 150.0981\n",
      "Epoch 292/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 157.5826 - val_loss: 137.9261\n",
      "Epoch 293/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 51us/step - loss: 154.4083 - val_loss: 141.2820\n",
      "Epoch 294/10000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 155.1712 - val_loss: 138.0855\n",
      "Epoch 295/10000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 195.9686 - val_loss: 575.7140\n",
      "Epoch 296/10000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 189.1106 - val_loss: 144.1228\n",
      "Epoch 297/10000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 158.3457 - val_loss: 148.8284\n",
      "Epoch 298/10000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 150.1268 - val_loss: 149.9901\n",
      "Epoch 299/10000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 154.9535 - val_loss: 153.6228\n",
      "Epoch 300/10000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 155.5853 - val_loss: 143.9706\n",
      "Epoch 301/10000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 156.9603 - val_loss: 150.1129\n",
      "Epoch 302/10000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 149.9889 - val_loss: 175.3172\n",
      "Epoch 303/10000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 155.1385 - val_loss: 152.8504\n",
      "Epoch 304/10000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 158.4772 - val_loss: 142.3491\n",
      "Epoch 305/10000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 156.6974 - val_loss: 148.2760\n",
      "Epoch 306/10000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 156.6643 - val_loss: 141.1000\n",
      "Epoch 307/10000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 159.4692 - val_loss: 140.1009\n",
      "Epoch 308/10000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 157.4823 - val_loss: 186.4435\n",
      "Epoch 309/10000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 155.9460 - val_loss: 215.0628\n",
      "Epoch 310/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 172.8434 - val_loss: 152.1393\n",
      "Epoch 311/10000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 147.2882 - val_loss: 152.3824\n",
      "Epoch 312/10000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 161.3808 - val_loss: 202.9437\n",
      "Epoch 313/10000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 158.5629 - val_loss: 178.6662\n",
      "Epoch 314/10000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 155.8818 - val_loss: 168.5103\n",
      "Epoch 315/10000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 158.4400 - val_loss: 159.7582\n",
      "Epoch 316/10000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 151.7306 - val_loss: 146.9568\n",
      "Epoch 317/10000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 149.1244 - val_loss: 180.2815\n",
      "Epoch 318/10000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 158.0713 - val_loss: 139.0251\n",
      "Epoch 319/10000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 161.9348 - val_loss: 138.0823\n",
      "Epoch 320/10000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 156.7863 - val_loss: 191.9170\n",
      "Epoch 321/10000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 165.2941 - val_loss: 181.2878\n",
      "Epoch 322/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 163.0635 - val_loss: 138.0458\n",
      "Epoch 323/10000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 148.1477 - val_loss: 154.8030\n",
      "Epoch 324/10000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 149.4497 - val_loss: 137.5736\n",
      "Epoch 325/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 151.7191 - val_loss: 147.6480\n",
      "Epoch 326/10000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 168.8694 - val_loss: 162.2467\n",
      "Epoch 327/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 189.7757 - val_loss: 187.7274\n",
      "Epoch 328/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 154.8244 - val_loss: 148.8971\n",
      "Epoch 329/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 147.5277 - val_loss: 140.4810\n",
      "Epoch 330/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 155.5240 - val_loss: 167.9382\n",
      "Epoch 331/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 145.9212 - val_loss: 138.2998\n",
      "Epoch 332/10000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 150.1085 - val_loss: 147.2994\n",
      "Epoch 333/10000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 149.4618 - val_loss: 147.3144\n",
      "Epoch 334/10000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 156.5736 - val_loss: 139.7919\n",
      "Epoch 335/10000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 156.1150 - val_loss: 144.0244\n",
      "Epoch 336/10000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 161.6852 - val_loss: 157.0173\n",
      "Epoch 337/10000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 148.0692 - val_loss: 157.4270\n",
      "Epoch 338/10000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 151.1852 - val_loss: 136.6732\n",
      "Epoch 339/10000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 157.5273 - val_loss: 149.6477\n",
      "Epoch 340/10000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 147.1076 - val_loss: 161.2620\n",
      "Epoch 341/10000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 152.2728 - val_loss: 157.0217\n",
      "Epoch 342/10000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 152.5198 - val_loss: 138.1841\n",
      "Epoch 343/10000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 147.8859 - val_loss: 140.9617\n",
      "Epoch 344/10000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 151.7993 - val_loss: 147.6895\n",
      "Epoch 345/10000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 169.0195 - val_loss: 138.8058\n",
      "Epoch 346/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 146.6639 - val_loss: 147.0431\n",
      "Epoch 347/10000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 154.2739 - val_loss: 140.4368\n",
      "Epoch 348/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 146.9062 - val_loss: 156.8908\n",
      "Epoch 349/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 150.5126 - val_loss: 139.1252\n",
      "Epoch 350/10000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 148.4926 - val_loss: 167.0206\n",
      "Epoch 351/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 146.8394 - val_loss: 139.2954\n",
      "Epoch 352/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 150.7671 - val_loss: 143.7443\n",
      "Epoch 353/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 151.2548 - val_loss: 221.4256\n",
      "Epoch 354/10000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 185.2701 - val_loss: 148.8587\n",
      "Epoch 355/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 146.6471 - val_loss: 165.8546\n",
      "Epoch 356/10000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 149.4468 - val_loss: 143.8082\n",
      "Epoch 357/10000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 148.1917 - val_loss: 143.4899\n",
      "Epoch 358/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 149.9157 - val_loss: 143.6385\n",
      "Epoch 359/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 145.2238 - val_loss: 165.8637\n",
      "Epoch 360/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 149.5219 - val_loss: 140.1928\n",
      "Epoch 361/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 144.6538 - val_loss: 191.9168\n",
      "Epoch 362/10000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 161.8258 - val_loss: 143.8184\n",
      "Epoch 363/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 148.3286 - val_loss: 143.8944\n",
      "Epoch 364/10000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 149.0209 - val_loss: 144.0203\n",
      "Epoch 365/10000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 148.9143 - val_loss: 158.7926\n",
      "Epoch 366/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 70us/step - loss: 151.5275 - val_loss: 146.5471\n",
      "Epoch 367/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 150.9928 - val_loss: 162.1042\n",
      "Epoch 368/10000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 147.3499 - val_loss: 197.7840\n",
      "Epoch 369/10000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 150.3397 - val_loss: 211.7762\n",
      "Epoch 370/10000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 151.3761 - val_loss: 141.6614\n",
      "Epoch 371/10000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 153.1580 - val_loss: 246.4148\n",
      "Epoch 372/10000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 152.2919 - val_loss: 139.2394\n",
      "Epoch 373/10000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 149.4743 - val_loss: 136.1554\n",
      "Epoch 374/10000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 180.4700 - val_loss: 255.4834\n",
      "Epoch 375/10000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 165.1658 - val_loss: 142.0065\n",
      "Epoch 376/10000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 150.6513 - val_loss: 137.7162\n",
      "Epoch 377/10000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 153.0471 - val_loss: 143.4664\n",
      "Epoch 378/10000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 149.8589 - val_loss: 140.9633\n",
      "Epoch 379/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 148.1875 - val_loss: 136.3043\n",
      "Epoch 380/10000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 143.2987 - val_loss: 157.1286\n",
      "Epoch 381/10000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 167.9943 - val_loss: 142.6898\n",
      "Epoch 382/10000\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 145.5327 - val_loss: 158.0387\n",
      "Epoch 383/10000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 148.1221 - val_loss: 141.4365\n",
      "Epoch 384/10000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 155.1582 - val_loss: 144.5198\n",
      "Epoch 385/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 146.7620 - val_loss: 137.0931\n",
      "Epoch 386/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 154.7689 - val_loss: 141.5380\n",
      "Epoch 387/10000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 149.6511 - val_loss: 159.6015\n",
      "Epoch 388/10000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 151.4098 - val_loss: 136.2038\n",
      "Epoch 389/10000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 153.1807 - val_loss: 163.3119\n",
      "Epoch 390/10000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 146.3578 - val_loss: 140.8762\n",
      "Epoch 391/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 150.6222 - val_loss: 138.3906\n",
      "Epoch 392/10000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 149.1486 - val_loss: 152.1887\n",
      "Epoch 393/10000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 141.9590 - val_loss: 171.8052\n",
      "Epoch 394/10000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 162.5460 - val_loss: 176.8113\n",
      "Epoch 395/10000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 160.5212 - val_loss: 135.4121\n",
      "Epoch 396/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 153.5044 - val_loss: 137.5012\n",
      "Epoch 397/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 143.5991 - val_loss: 140.5605\n",
      "Epoch 398/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 148.6091 - val_loss: 161.7567\n",
      "Epoch 399/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 153.6394 - val_loss: 136.8323\n",
      "Epoch 400/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 142.7208 - val_loss: 148.1398\n",
      "Epoch 401/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 165.6765 - val_loss: 140.7617\n",
      "Epoch 402/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 144.2656 - val_loss: 156.0766\n",
      "Epoch 403/10000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 145.8989 - val_loss: 138.5007\n",
      "Epoch 404/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 151.9104 - val_loss: 150.8493\n",
      "Epoch 405/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 149.8491 - val_loss: 142.0236\n",
      "Epoch 406/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 140.9621 - val_loss: 143.3642\n",
      "Epoch 407/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 145.9231 - val_loss: 150.0828\n",
      "Epoch 408/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 144.4656 - val_loss: 181.7805\n",
      "Epoch 409/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 144.6895 - val_loss: 139.0920\n",
      "Epoch 410/10000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 146.5618 - val_loss: 143.4227\n",
      "Epoch 411/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 143.2043 - val_loss: 137.4856\n",
      "Epoch 412/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 145.9177 - val_loss: 181.7268\n",
      "Epoch 413/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 145.2140 - val_loss: 151.2726\n",
      "Epoch 414/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 146.5092 - val_loss: 283.6970\n",
      "Epoch 415/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 155.0517 - val_loss: 147.3339\n",
      "Epoch 416/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 155.7509 - val_loss: 153.3906\n",
      "Epoch 417/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 142.4397 - val_loss: 140.0802\n",
      "Epoch 418/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 147.1459 - val_loss: 140.7843\n",
      "Epoch 419/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 142.9048 - val_loss: 158.4903\n",
      "Epoch 420/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 147.1561 - val_loss: 140.3547\n",
      "Epoch 421/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 146.3541 - val_loss: 138.5986\n",
      "Epoch 422/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 213.9541 - val_loss: 157.2877\n",
      "Epoch 423/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 149.6839 - val_loss: 156.4680\n",
      "Epoch 424/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 142.1655 - val_loss: 135.6487\n",
      "Epoch 425/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 145.0967 - val_loss: 144.2635\n",
      "Epoch 426/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 147.6432 - val_loss: 180.3010\n",
      "Epoch 427/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 145.5977 - val_loss: 155.5333\n",
      "Epoch 428/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 145.4793 - val_loss: 198.5194\n",
      "Epoch 429/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 144.6446 - val_loss: 135.3021\n",
      "Epoch 430/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 146.3619 - val_loss: 147.3658\n",
      "Epoch 431/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 146.3439 - val_loss: 143.8876\n",
      "Epoch 432/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 149.8571 - val_loss: 137.5296\n",
      "Epoch 433/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 146.3439 - val_loss: 217.4397\n",
      "Epoch 434/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 142.2341 - val_loss: 147.4250\n",
      "Epoch 435/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 150.8262 - val_loss: 203.0083\n",
      "Epoch 436/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 148.0050 - val_loss: 137.4204\n",
      "Epoch 437/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 144.0507 - val_loss: 140.4527\n",
      "Epoch 438/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 162.2005 - val_loss: 142.6612\n",
      "Epoch 439/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 58us/step - loss: 144.2362 - val_loss: 138.2847\n",
      "Epoch 440/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 150.1783 - val_loss: 194.0664\n",
      "Epoch 441/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 147.7319 - val_loss: 137.0457\n",
      "Epoch 442/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 142.9853 - val_loss: 151.2804\n",
      "Epoch 443/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 156.6739 - val_loss: 139.6940\n",
      "Epoch 444/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 148.9825 - val_loss: 144.8353\n",
      "Epoch 445/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 144.6735 - val_loss: 140.1530\n",
      "Epoch 446/10000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 144.8555 - val_loss: 175.5771\n",
      "Epoch 447/10000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 144.1301 - val_loss: 135.6942\n",
      "Epoch 448/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 143.2257 - val_loss: 143.0772\n",
      "Epoch 449/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 146.9135 - val_loss: 183.4070\n",
      "Epoch 450/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 144.5963 - val_loss: 156.7179\n",
      "Epoch 451/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 139.5423 - val_loss: 139.4756\n",
      "Epoch 452/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 185.0381 - val_loss: 782.9612\n",
      "Epoch 453/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 180.6792 - val_loss: 146.3810\n",
      "Epoch 454/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 139.6410 - val_loss: 145.9188\n",
      "Epoch 455/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 136.1975 - val_loss: 165.1486\n",
      "Epoch 456/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 143.4530 - val_loss: 141.5965\n",
      "Epoch 457/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 143.7303 - val_loss: 140.7344\n",
      "Epoch 458/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 139.8131 - val_loss: 140.7800\n",
      "Epoch 459/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 164.0402 - val_loss: 138.8294\n",
      "Epoch 460/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 146.5577 - val_loss: 147.3241\n",
      "Epoch 461/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 142.1846 - val_loss: 172.0948\n",
      "Epoch 462/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 141.3304 - val_loss: 144.2353\n",
      "Epoch 463/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 140.7595 - val_loss: 152.4303\n",
      "Epoch 464/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 150.0259 - val_loss: 135.6781\n",
      "Epoch 465/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 146.4142 - val_loss: 172.3977\n",
      "Epoch 466/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 138.1539 - val_loss: 137.5125\n",
      "Epoch 467/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 138.7400 - val_loss: 136.9739\n",
      "Epoch 468/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 138.4120 - val_loss: 145.9038\n",
      "Epoch 469/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 139.5404 - val_loss: 141.4498\n",
      "Epoch 470/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 140.5559 - val_loss: 141.6919\n",
      "Epoch 471/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 138.8099 - val_loss: 138.7484\n",
      "Epoch 472/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 150.6026 - val_loss: 142.0574\n",
      "Epoch 473/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 141.6233 - val_loss: 133.8372\n",
      "Epoch 474/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 150.9567 - val_loss: 142.6781\n",
      "Epoch 475/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 139.6278 - val_loss: 139.1505\n",
      "Epoch 476/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 138.7822 - val_loss: 136.3981\n",
      "Epoch 477/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 137.7612 - val_loss: 157.6618\n",
      "Epoch 478/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 140.8157 - val_loss: 166.5588\n",
      "Epoch 479/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 137.6247 - val_loss: 161.1769\n",
      "Epoch 480/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 140.1084 - val_loss: 194.8822\n",
      "Epoch 481/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 147.4900 - val_loss: 150.9701\n",
      "Epoch 482/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 228.1025 - val_loss: 144.1303\n",
      "Epoch 483/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 153.9457 - val_loss: 140.7004\n",
      "Epoch 484/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 141.7137 - val_loss: 140.3964\n",
      "Epoch 485/10000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 135.8618 - val_loss: 141.3594\n",
      "Epoch 486/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 137.2654 - val_loss: 141.2874\n",
      "Epoch 487/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 140.2369 - val_loss: 136.4813\n",
      "Epoch 488/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 143.0878 - val_loss: 147.2929\n",
      "Epoch 489/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 141.4590 - val_loss: 137.3170\n",
      "Epoch 490/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 157.3789 - val_loss: 135.4363\n",
      "Epoch 491/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 141.4582 - val_loss: 138.6621\n",
      "Epoch 492/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 141.7276 - val_loss: 142.5223\n",
      "Epoch 493/10000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 144.4507 - val_loss: 193.2158\n",
      "Epoch 494/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 161.7196 - val_loss: 142.6646\n",
      "Epoch 495/10000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 137.4132 - val_loss: 142.3157\n",
      "Epoch 496/10000\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 157.5571 - val_loss: 144.2605\n",
      "Epoch 497/10000\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 134.8155 - val_loss: 179.3649\n",
      "Epoch 498/10000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 141.0210 - val_loss: 145.2872\n",
      "Epoch 499/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 141.4809 - val_loss: 136.4436\n",
      "Epoch 500/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 145.1628 - val_loss: 165.4555\n",
      "Epoch 501/10000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 144.6772 - val_loss: 135.9777\n",
      "Epoch 502/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 136.8519 - val_loss: 142.5534\n",
      "Epoch 503/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 144.8454 - val_loss: 138.3447\n",
      "Epoch 504/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 141.5492 - val_loss: 158.3628\n",
      "Epoch 505/10000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 189.4703 - val_loss: 141.3453\n",
      "Epoch 506/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 137.0857 - val_loss: 141.1758\n",
      "Epoch 507/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 139.8172 - val_loss: 142.3480\n",
      "Epoch 508/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 135.5329 - val_loss: 137.9284\n",
      "Epoch 509/10000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 140.0838 - val_loss: 147.4724\n",
      "Epoch 510/10000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 135.6126 - val_loss: 138.7736\n",
      "Epoch 511/10000\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 138.4826 - val_loss: 138.8476\n",
      "Epoch 512/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 69us/step - loss: 140.9486 - val_loss: 149.6207\n",
      "Epoch 513/10000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 140.7035 - val_loss: 143.5770\n",
      "Epoch 514/10000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 147.8332 - val_loss: 145.6078\n",
      "Epoch 515/10000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 146.7280 - val_loss: 164.0243\n",
      "Epoch 516/10000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 138.9461 - val_loss: 135.7538\n",
      "Epoch 517/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 144.3438 - val_loss: 136.9056\n",
      "Epoch 518/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 207.2210 - val_loss: 255.2010\n",
      "Epoch 519/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 138.2749 - val_loss: 140.6229\n",
      "Epoch 520/10000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 134.6465 - val_loss: 144.7129\n",
      "Epoch 521/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 138.4199 - val_loss: 135.0152\n",
      "Epoch 522/10000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 136.7215 - val_loss: 152.2287\n",
      "Epoch 523/10000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 135.2720 - val_loss: 143.4475\n",
      "Epoch 524/10000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 138.6614 - val_loss: 140.1336\n",
      "Epoch 525/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 139.6525 - val_loss: 136.9816\n",
      "Epoch 526/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 142.4718 - val_loss: 148.6811\n",
      "Epoch 527/10000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 134.3800 - val_loss: 141.6152\n",
      "Epoch 528/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 138.1502 - val_loss: 143.0546\n",
      "Epoch 529/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 137.8763 - val_loss: 134.0153\n",
      "Epoch 530/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 133.2315 - val_loss: 147.2055\n",
      "Epoch 531/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 139.5842 - val_loss: 140.7354\n",
      "Epoch 532/10000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 144.2254 - val_loss: 141.5260\n",
      "Epoch 533/10000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 149.7849 - val_loss: 143.0282\n",
      "Epoch 534/10000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 151.2158 - val_loss: 157.5508\n",
      "Epoch 535/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 141.4224 - val_loss: 139.7544\n",
      "Epoch 536/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 141.7811 - val_loss: 140.0679\n",
      "Epoch 537/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 138.3670 - val_loss: 142.7928\n",
      "Epoch 538/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 137.9207 - val_loss: 151.9076\n",
      "Epoch 539/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 137.6222 - val_loss: 133.2273\n",
      "Epoch 540/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 147.2640 - val_loss: 152.0768\n",
      "Epoch 541/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 137.7597 - val_loss: 145.0455\n",
      "Epoch 542/10000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 139.4019 - val_loss: 164.5751\n",
      "Epoch 543/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 141.0414 - val_loss: 149.7005\n",
      "Epoch 544/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 140.6576 - val_loss: 136.2204\n",
      "Epoch 545/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 143.6174 - val_loss: 137.3310\n",
      "Epoch 546/10000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 141.1855 - val_loss: 158.3080\n",
      "Epoch 547/10000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 148.1620 - val_loss: 138.8364\n",
      "Epoch 548/10000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 148.3596 - val_loss: 146.2281\n",
      "Epoch 549/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 136.0225 - val_loss: 143.0142\n",
      "Epoch 550/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 134.6476 - val_loss: 145.7145\n",
      "Epoch 551/10000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 136.7923 - val_loss: 136.4342\n",
      "Epoch 552/10000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 139.9302 - val_loss: 150.7830\n",
      "Epoch 553/10000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 151.0638 - val_loss: 197.5886\n",
      "Epoch 554/10000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 142.4761 - val_loss: 142.0854\n",
      "Epoch 555/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 135.2832 - val_loss: 149.2051\n",
      "Epoch 556/10000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 135.5961 - val_loss: 171.9941\n",
      "Epoch 557/10000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 141.0105 - val_loss: 143.4285\n",
      "Epoch 558/10000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 134.9100 - val_loss: 137.5935\n",
      "Epoch 559/10000\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 136.1003 - val_loss: 137.6648\n",
      "Epoch 560/10000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 136.5769 - val_loss: 134.8842\n",
      "Epoch 561/10000\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 141.8399 - val_loss: 176.8341\n",
      "Epoch 562/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 150.6472 - val_loss: 169.5826\n",
      "Epoch 563/10000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 139.3513 - val_loss: 137.8340\n",
      "Epoch 564/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 141.8820 - val_loss: 135.6352\n",
      "Epoch 565/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 135.7024 - val_loss: 137.2016\n",
      "Epoch 566/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 137.7064 - val_loss: 166.1320\n",
      "Epoch 567/10000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 134.1064 - val_loss: 145.3500\n",
      "Epoch 568/10000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 136.2691 - val_loss: 155.0141\n",
      "Epoch 569/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 139.5764 - val_loss: 134.0642\n",
      "Epoch 570/10000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 133.8796 - val_loss: 136.9870\n",
      "Epoch 571/10000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 137.9068 - val_loss: 183.3209\n",
      "Epoch 572/10000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 137.3056 - val_loss: 137.8940\n",
      "Epoch 573/10000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 145.7340 - val_loss: 138.4951\n",
      "Epoch 574/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 136.1367 - val_loss: 201.7947\n",
      "Epoch 575/10000\n",
      "7500/7500 [==============================] - 1s 101us/step - loss: 140.2789 - val_loss: 146.8645\n",
      "Epoch 576/10000\n",
      "7500/7500 [==============================] - 1s 101us/step - loss: 132.0622 - val_loss: 136.9157\n",
      "Epoch 577/10000\n",
      "7500/7500 [==============================] - 1s 103us/step - loss: 173.7706 - val_loss: 242.4782\n",
      "Epoch 578/10000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 133.7628 - val_loss: 166.4546\n",
      "Epoch 579/10000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 139.5987 - val_loss: 137.2689\n",
      "Epoch 580/10000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 134.6973 - val_loss: 138.1452\n",
      "Epoch 581/10000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 133.7508 - val_loss: 136.1222\n",
      "Epoch 582/10000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 133.0000 - val_loss: 135.4837\n",
      "Epoch 583/10000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 136.8448 - val_loss: 143.1061\n",
      "Epoch 584/10000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 137.2016 - val_loss: 153.3756\n",
      "Epoch 585/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 73us/step - loss: 137.2977 - val_loss: 140.0101\n",
      "Epoch 586/10000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 130.6558 - val_loss: 148.3223\n",
      "Epoch 587/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 138.7822 - val_loss: 137.7238\n",
      "Epoch 588/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 139.2597 - val_loss: 138.7731\n",
      "Epoch 589/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 135.8062 - val_loss: 148.7862\n",
      "Epoch 590/10000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 133.8895 - val_loss: 141.8993\n",
      "Epoch 591/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 152.1341 - val_loss: 143.9916\n",
      "Epoch 592/10000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 147.2859 - val_loss: 138.6997\n",
      "Epoch 593/10000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 131.9015 - val_loss: 189.1203\n",
      "Epoch 594/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 136.8371 - val_loss: 150.6091\n",
      "Epoch 595/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 189.8892 - val_loss: 155.1167\n",
      "Epoch 596/10000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 136.4156 - val_loss: 145.1343\n",
      "Epoch 597/10000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 138.3869 - val_loss: 139.6267\n",
      "Epoch 598/10000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 138.5723 - val_loss: 140.4212\n",
      "Epoch 599/10000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 146.5992 - val_loss: 137.6443\n",
      "Epoch 600/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 132.7328 - val_loss: 143.0779\n",
      "Epoch 601/10000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 138.8292 - val_loss: 191.4355\n",
      "Epoch 602/10000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 133.8532 - val_loss: 137.0880\n",
      "Epoch 603/10000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 135.7099 - val_loss: 173.7091\n",
      "Epoch 604/10000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 134.9956 - val_loss: 150.5871\n",
      "Epoch 605/10000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 136.0761 - val_loss: 136.6576\n",
      "Epoch 606/10000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 144.2620 - val_loss: 142.1656\n",
      "Epoch 607/10000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 134.1473 - val_loss: 139.0663\n",
      "Epoch 608/10000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 133.6077 - val_loss: 160.1481\n",
      "Epoch 609/10000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 136.3999 - val_loss: 136.9585\n",
      "Epoch 610/10000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 127.1521 - val_loss: 165.5922\n",
      "Epoch 611/10000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 132.3315 - val_loss: 164.2455\n",
      "Epoch 612/10000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 134.3616 - val_loss: 239.5835\n",
      "Epoch 613/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 140.0280 - val_loss: 144.9922\n",
      "Epoch 614/10000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 133.6497 - val_loss: 140.0286\n",
      "Epoch 615/10000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 138.8612 - val_loss: 140.8921\n",
      "Epoch 616/10000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 130.1698 - val_loss: 154.5652\n",
      "Epoch 617/10000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 132.1851 - val_loss: 139.4233\n",
      "Epoch 618/10000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 131.9548 - val_loss: 141.9267\n",
      "Epoch 619/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 136.0601 - val_loss: 162.4387\n",
      "Epoch 620/10000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 134.9073 - val_loss: 141.8053\n",
      "Epoch 621/10000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 145.2207 - val_loss: 181.0879\n",
      "Epoch 622/10000\n",
      "7500/7500 [==============================] - 1s 108us/step - loss: 192.2194 - val_loss: 170.5208\n",
      "Epoch 623/10000\n",
      "7500/7500 [==============================] - 1s 103us/step - loss: 135.3784 - val_loss: 145.9700\n",
      "Epoch 624/10000\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 131.7080 - val_loss: 148.6185\n",
      "Epoch 625/10000\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 143.9900 - val_loss: 140.9265\n",
      "Epoch 626/10000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 136.2618 - val_loss: 141.0292\n",
      "Epoch 627/10000\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 135.5531 - val_loss: 157.6733\n",
      "Epoch 628/10000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 131.7545 - val_loss: 140.9978\n",
      "Epoch 629/10000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 138.2487 - val_loss: 138.3434\n",
      "Epoch 630/10000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 132.9137 - val_loss: 134.7799\n",
      "Epoch 631/10000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 132.7121 - val_loss: 152.9060\n",
      "Epoch 632/10000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 138.6577 - val_loss: 139.2118\n",
      "Epoch 633/10000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 172.9641 - val_loss: 142.7545\n",
      "Epoch 634/10000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 148.6696 - val_loss: 143.5692\n",
      "Epoch 635/10000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 128.7251 - val_loss: 155.6645\n",
      "Epoch 636/10000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 133.0341 - val_loss: 138.3918\n",
      "Epoch 637/10000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 127.5351 - val_loss: 163.9801\n",
      "Epoch 638/10000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 130.5837 - val_loss: 139.1232\n",
      "Epoch 639/10000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 127.8756 - val_loss: 152.4949\n",
      "Epoch 00639: early stopping\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=filename_checkpoint, verbose=0, save_best_only=True)\n",
    "\n",
    "# Turn off KFold\n",
    "if (0):\n",
    "    oos_y = []\n",
    "    oos_pred = []\n",
    "    fold = 0\n",
    "    \n",
    "    for train, test in kf.split(x):\n",
    "        fold+=1\n",
    "        print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "        x_train = x[train]\n",
    "        y_train = y[train]\n",
    "        x_test = x[test]\n",
    "        y_test = y[test]\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "model.add(Dropout(0.01)) # Dropout Layer\n",
    "model.add(Dense(25, activation='relu')) # Hidden 2\n",
    "model.add(Dense(10, \n",
    "                kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01),activation='relu')) # Hidden 3 w/regularization\n",
    "model.add(Dense(1)) # Output\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=100, verbose=1, mode='auto')\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpoint],verbose=1,epochs=10000)\n",
    "    \n",
    "pred = model.predict(x_test)\n",
    "    \n",
    "#    oos_y.append(y_test)\n",
    "#    oos_pred.append(pred)        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold score (RMSE): 12.331077575683594\n"
     ]
    }
   ],
   "source": [
    "# Measure this fold's RMSE\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Fold score (RMSE): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final, out of sample score (RMSE): 12.102914810180664\n"
     ]
    }
   ],
   "source": [
    "# Build the oos prediction list and calculate the error.\n",
    "#oos_y = np.concatenate(oos_y)\n",
    "#oos_pred = np.concatenate(oos_pred)\n",
    "#score = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))\n",
    "#print(\"Final, out of sample score (RMSE): {}\".format(score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#pred = model.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "#score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "#print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "#chart_regression(pred.flatten(),y_test)\n",
    "#chart_regression(pred.flatten(),y_test,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(filename_checkpoint)\n",
    "\n",
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "extract_and_encode_features(df_test)\n",
    "\n",
    "ids_test = df_test['id']\n",
    "df_test.drop('id',1,inplace=True)\n",
    "\n",
    "names_test = df_test['name']\n",
    "df_test.drop('name',1,inplace=True)\n",
    "\n",
    "x_submit = df_test.as_matrix().astype(np.float32)\n",
    "pred_submit = model.predict(x_submit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = [n if n > 0 else 0 for n in pred_submit[:,0]]\n",
    "df_submit = pd.DataFrame({'id': ids_test,'cost': cost})\n",
    "df_submit = df_submit[['id', 'cost']]\n",
    "df_submit.to_csv(filename_submit, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
